{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import os \n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data type\n",
    "DTYPE='float32'\n",
    "tf.keras.backend.set_floatx(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PINN neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(number_hidden_layers = 2, num_neurons_per_layer = 50):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input(1))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    model.add(tf.keras.layers.Dense(num_neurons_per_layer, activation=tf.keras.activations.get('tanh'), \n",
    "                                    kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(3, activation=tf.keras.activations.get('softmax')))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# A basic PINN Tensorflow class for solving a continuous-time in-homogeneous Markov chains\n",
    "class PINN:\n",
    "    # Initialize the class\n",
    "    def __init__(self, X_u, Y_u, X_r, model):\n",
    "        \n",
    "        ######################################################################################\n",
    "        # Normalization constants\n",
    "        self.mu_x, self.sigma_x = X_r.mean(0), X_r.std(0)\n",
    "\n",
    "        # Normalize inputs\n",
    "        X_u = (X_u - self.mu_x)/self.sigma_x\n",
    "        X_r = (X_r - self.mu_x)/self.sigma_x\n",
    "\n",
    "        # Store data                \n",
    "        self.X_u = X_u\n",
    "        self.Y_u = Y_u\n",
    "        self.X_r = X_r\n",
    "\n",
    "        self.Xu_tf = tf.convert_to_tensor(X_u, dtype=tf.float32)\n",
    "        self.Yu_tf = tf.convert_to_tensor(Y_u, dtype=tf.float32)\n",
    "        self.Xr_tf = tf.convert_to_tensor(X_r, dtype=tf.float32)\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        self.iter = 0\n",
    "        self.loss_log = []\n",
    "        self.weights_log = []\n",
    "        self.gradients_log = []\n",
    "        \n",
    "    def get_r(self):\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(self.Xr_tf)\n",
    "            \n",
    "            u = self.model(self.Xr_tf)\n",
    "            \n",
    "            u1 = u[:,0:1]\n",
    "            u2 = u[:,1:2]\n",
    "            u3 = u[:,2:3]\n",
    "            \n",
    "        #derivatives of state probability\n",
    "        u_x_1 = g.gradient(u1, self.Xr_tf)/self.sigma_x\n",
    "        u_x_2 = g.gradient(u2, self.Xr_tf)/self.sigma_x\n",
    "        u_x_3 = g.gradient(u3, self.Xr_tf)/self.sigma_x\n",
    "        \n",
    "        #calculate residuals\n",
    "        residual_1 = u_x_1-(-1.286e-4*u1)\n",
    "        residual_2 = u_x_2-(5.6e-5*u1-1.006e-4*u2)\n",
    "        residual_3 = u_x_3-(7.26e-5*u1+1.006e-4*u2)\n",
    "        \n",
    "        #total residual\n",
    "        residual = tf.reduce_mean(tf.square(residual_1))+tf.reduce_mean(tf.square(residual_2))+\\\n",
    "        tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        loss_1 = tf.reduce_mean(tf.square(residual_1))\n",
    "        loss_2 = tf.reduce_mean(tf.square(residual_2))\n",
    "        loss_3 = tf.reduce_mean(tf.square(residual_3))\n",
    "        \n",
    "        del g\n",
    "        \n",
    "        return loss_1, loss_2, loss_3\n",
    "    \n",
    "\n",
    "    def loss_fn(self, weight = 1):\n",
    "        u_pred = self.model(self.Xu_tf)\n",
    "\n",
    "        loss_u = tf.reduce_mean(tf.square(self.Yu_tf - u_pred))\n",
    "        loss_1, loss_2, loss_3 = self.get_r()\n",
    "        \n",
    "        return [loss_u, loss_1+loss_2+loss_3]\n",
    "    \n",
    "    \n",
    "    def get_grad(self):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(self.model.trainable_variables)\n",
    "            loss = self.loss_fn()\n",
    "        \n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        \n",
    "        loss_u, loss_r = loss[0], loss[1:]\n",
    "        g_u = tape.gradient(loss_u, self.model.trainable_variables)\n",
    "        g_r = tape.gradient(loss_r, self.model.trainable_variables)\n",
    "        \n",
    "        return loss, g, g_u, g_r\n",
    "    \n",
    "    def get_grad_by_PCG(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn()\n",
    "            \n",
    "            assert type(loss) is list\n",
    "            loss = tf.stack(loss)\n",
    "            tf.random.shuffle(loss)\n",
    "\n",
    "            grads_task = tf.vectorized_map(lambda x: tf.concat([tf.reshape(grad, [-1,]) \n",
    "                                for grad in tape.gradient(x, self.model.trainable_variables)\n",
    "                                if grad is not None], axis=0), loss)\n",
    "        \n",
    "        num_tasks = len(loss)\n",
    "\n",
    "        # Compute gradient projections.\n",
    "    def get_grad_by_PCG(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn()\n",
    "            \n",
    "            assert type(loss) is list\n",
    "            loss = tf.stack(loss)\n",
    "            tf.random.shuffle(loss)\n",
    "\n",
    "            grads_task = tf.vectorized_map(lambda x: tf.concat([tf.reshape(grad, [-1,]) \n",
    "                                for grad in tape.gradient(x, self.model.trainable_variables)\n",
    "                                if grad is not None], axis=0), loss)\n",
    "        \n",
    "        num_tasks = len(loss)\n",
    "\n",
    "        # Compute gradient projections.\n",
    "        def proj_grad(grad_task):\n",
    "            for k in range(num_tasks):\n",
    "                inner_product = tf.reduce_sum(grad_task*grads_task[k])\n",
    "                proj_direction = inner_product / tf.reduce_sum(grads_task[k]*grads_task[k])\n",
    "                grad_task = grad_task - tf.minimum(proj_direction, 0.) * grads_task[k]\n",
    "            return grad_task\n",
    "\n",
    "        proj_grads_flatten = tf.vectorized_map(proj_grad, grads_task)\n",
    "\n",
    "        # Unpack flattened projected gradients back to their original shapes.\n",
    "        proj_grads = []\n",
    "        for j in range(num_tasks):\n",
    "            start_idx = 0\n",
    "            for idx, var in enumerate(self.model.trainable_variables):\n",
    "                grad_shape = var.get_shape()\n",
    "                flatten_dim = np.prod([grad_shape.dims[i].value for i in range(len(grad_shape.dims))])\n",
    "                proj_grad = proj_grads_flatten[j][start_idx:start_idx+flatten_dim]\n",
    "                proj_grad = tf.reshape(proj_grad, grad_shape)\n",
    "                if len(proj_grads) < len(self.model.trainable_variables):\n",
    "                    proj_grads.append(proj_grad)\n",
    "                else:\n",
    "                    proj_grads[idx] += proj_grad               \n",
    "                start_idx += flatten_dim\n",
    "\n",
    "        grads_and_vars = list(zip(proj_grads, self.model.trainable_variables))\n",
    "        \n",
    "        del tape\n",
    "        return loss, proj_grads, None, None\n",
    "    \n",
    "    def callback(self):\n",
    "        if self.iter % 5 == 0:\n",
    "            print('Iteration {:05d}: loss = {}'.format(self.iter, ','.join(map(str, self.current_loss))))\n",
    "        self.iter += 1\n",
    "    \n",
    "    def train(self, N, optimizer, method):\n",
    "        \"\"\"This method performs a gradient descent type optimization.\"\"\"\n",
    "        \n",
    "        @tf.function\n",
    "        def train_step():\n",
    "            if method == 'original':\n",
    "                loss, grad_theta, g_u, g_r = self.get_grad()\n",
    "                \n",
    "            if method == 'PCG_gradient':\n",
    "                loss, grad_theta, g_u, g_r = self.get_grad_by_PCG()\n",
    "            \n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            \n",
    "            return loss, grad_theta, g_u, g_r\n",
    "        \n",
    "        for i in range(N):\n",
    "            \n",
    "            loss, grad_theta, g_u, g_r = train_step()\n",
    "            \n",
    "            self.current_loss = tf.convert_to_tensor(loss).numpy()\n",
    "            self.loss_log.append(self.current_loss)\n",
    "            self.callback()\n",
    "            \n",
    "            if i%10000 == 0:\n",
    "                self.weights_log.append(PINN_solver.model.get_weights())\n",
    "                self.gradients_log.append([grad_theta, g_u, g_r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# Number of training data\n",
    "N_u = 1                        # Boundary condition data on u(x)  \n",
    "N_r = 5000                     # Number of collocation points for minimizing the PDE residual\n",
    "lb  = np.array([0.0])         # Left boundary of the domain\n",
    "ub  = np.array([60000.0])          # Right boundary of the domain\n",
    "\n",
    "# Generate training data\n",
    "x_u = np.array([[0]])  ##TZ\n",
    "y_u = np.array([[1,0,0]])   ##TZ                    # Solution at boundary points (dimension N_u x 1)\n",
    "x_r = np.linspace(lb, ub, N_r)     # Location of collocation points (dimension N_r x 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 80000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PINN without PCGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_solver = PINN(x_u, y_u, x_r, init_model())\n",
    "initial_weights = PINN_solver.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: loss = 0.3108814,5.3529483e-09\n",
      "Iteration 00005: loss = 0.1549211,4.6981725e-09\n",
      "Iteration 00010: loss = 0.054250892,5.035244e-09\n",
      "Iteration 00015: loss = 0.016276916,5.9040635e-09\n",
      "Iteration 00020: loss = 0.005566288,6.743946e-09\n",
      "Iteration 00025: loss = 0.0024359932,7.359234e-09\n",
      "Iteration 00030: loss = 0.0013419633,7.7722255e-09\n",
      "Iteration 00035: loss = 0.0008823997,8.043897e-09\n",
      "Iteration 00040: loss = 0.0006581627,8.223413e-09\n",
      "Iteration 00045: loss = 0.0005352322,8.344002e-09\n",
      "Iteration 00050: loss = 0.0004612557,8.427121e-09\n",
      "Iteration 00055: loss = 0.00041304846,8.4864595e-09\n",
      "Iteration 00060: loss = 0.0003792852,8.530727e-09\n",
      "Iteration 00065: loss = 0.0003540154,8.565456e-09\n",
      "Iteration 00070: loss = 0.00033394035,8.594148e-09\n",
      "Iteration 00075: loss = 0.00031716845,8.619022e-09\n",
      "Iteration 00080: loss = 0.00030257608,8.641474e-09\n",
      "Iteration 00085: loss = 0.00028949775,8.662376e-09\n",
      "Iteration 00090: loss = 0.00027752787,8.682267e-09\n",
      "Iteration 00095: loss = 0.0002664063,8.701488e-09\n",
      "Iteration 00100: loss = 0.00025597602,8.72024e-09\n",
      "Iteration 00105: loss = 0.00024613843,8.738643e-09\n",
      "Iteration 00110: loss = 0.00023682082,8.756775e-09\n",
      "Iteration 00115: loss = 0.00022797368,8.774672e-09\n",
      "Iteration 00120: loss = 0.00021956186,8.792359e-09\n",
      "Iteration 00125: loss = 0.00021155558,8.809845e-09\n",
      "Iteration 00130: loss = 0.00020393135,8.82714e-09\n",
      "Iteration 00135: loss = 0.00019666762,8.84424e-09\n",
      "Iteration 00140: loss = 0.00018974184,8.861147e-09\n",
      "Iteration 00145: loss = 0.00018314167,8.877863e-09\n",
      "Iteration 00150: loss = 0.00017684513,8.89438e-09\n",
      "Iteration 00155: loss = 0.00017083803,8.910703e-09\n",
      "Iteration 00160: loss = 0.00016510634,8.926831e-09\n",
      "Iteration 00165: loss = 0.00015963505,8.942759e-09\n",
      "Iteration 00170: loss = 0.00015440979,8.958493e-09\n",
      "Iteration 00175: loss = 0.00014941649,8.974033e-09\n",
      "Iteration 00180: loss = 0.00014464601,8.989376e-09\n",
      "Iteration 00185: loss = 0.00014008537,9.004529e-09\n",
      "Iteration 00190: loss = 0.00013572465,9.019487e-09\n",
      "Iteration 00195: loss = 0.00013154988,9.034258e-09\n",
      "Iteration 00200: loss = 0.00012755474,9.048842e-09\n",
      "Iteration 00205: loss = 0.0001237277,9.063242e-09\n",
      "Iteration 00210: loss = 0.000120062345,9.077458e-09\n",
      "Iteration 00215: loss = 0.00011654937,9.091496e-09\n",
      "Iteration 00220: loss = 0.000113180744,9.105355e-09\n",
      "Iteration 00225: loss = 0.000109949,9.1190415e-09\n",
      "Iteration 00230: loss = 0.000106846994,9.132558e-09\n",
      "Iteration 00235: loss = 0.00010386835,9.145903e-09\n",
      "Iteration 00240: loss = 0.00010100877,9.159084e-09\n",
      "Iteration 00245: loss = 9.825983e-05,9.172103e-09\n",
      "Iteration 00250: loss = 9.561709e-05,9.184962e-09\n",
      "Iteration 00255: loss = 9.307528e-05,9.197666e-09\n",
      "Iteration 00260: loss = 9.0630645e-05,9.210215e-09\n",
      "Iteration 00265: loss = 8.8277186e-05,9.222613e-09\n",
      "Iteration 00270: loss = 8.601091e-05,9.2348635e-09\n",
      "Iteration 00275: loss = 8.382745e-05,9.246967e-09\n",
      "Iteration 00280: loss = 8.1724116e-05,9.258928e-09\n",
      "Iteration 00285: loss = 7.969528e-05,9.270751e-09\n",
      "Iteration 00290: loss = 7.7739205e-05,9.282437e-09\n",
      "Iteration 00295: loss = 7.585207e-05,9.293986e-09\n",
      "Iteration 00300: loss = 7.403035e-05,9.305405e-09\n",
      "Iteration 00305: loss = 7.2271454e-05,9.316695e-09\n",
      "Iteration 00310: loss = 7.0572634e-05,9.327856e-09\n",
      "Iteration 00315: loss = 6.893158e-05,9.338893e-09\n",
      "Iteration 00320: loss = 6.7344656e-05,9.349809e-09\n",
      "Iteration 00325: loss = 6.58104e-05,9.360604e-09\n",
      "Iteration 00330: loss = 6.432765e-05,9.3712815e-09\n",
      "Iteration 00335: loss = 6.2892635e-05,9.381844e-09\n",
      "Iteration 00340: loss = 6.150324e-05,9.392293e-09\n",
      "Iteration 00345: loss = 6.015848e-05,9.402632e-09\n",
      "Iteration 00350: loss = 5.8854992e-05,9.412861e-09\n",
      "Iteration 00355: loss = 5.7593785e-05,9.422983e-09\n",
      "Iteration 00360: loss = 5.6371147e-05,9.433e-09\n",
      "Iteration 00365: loss = 5.518555e-05,9.442914e-09\n",
      "Iteration 00370: loss = 5.4035598e-05,9.452729e-09\n",
      "Iteration 00375: loss = 5.2921223e-05,9.4624415e-09\n",
      "Iteration 00380: loss = 5.1840612e-05,9.472059e-09\n",
      "Iteration 00385: loss = 5.079121e-05,9.481581e-09\n",
      "Iteration 00390: loss = 4.9772327e-05,9.491007e-09\n",
      "Iteration 00395: loss = 4.8782706e-05,9.500344e-09\n",
      "Iteration 00400: loss = 4.7821643e-05,9.509589e-09\n",
      "Iteration 00405: loss = 4.6889516e-05,9.518743e-09\n",
      "Iteration 00410: loss = 4.5981393e-05,9.527812e-09\n",
      "Iteration 00415: loss = 4.5099758e-05,9.5367945e-09\n",
      "Iteration 00420: loss = 4.4243166e-05,9.545692e-09\n",
      "Iteration 00425: loss = 4.34095e-05,9.554507e-09\n",
      "Iteration 00430: loss = 4.2598596e-05,9.563242e-09\n",
      "Iteration 00435: loss = 4.181026e-05,9.5718935e-09\n",
      "Iteration 00440: loss = 4.1042174e-05,9.58047e-09\n",
      "Iteration 00445: loss = 4.029489e-05,9.588966e-09\n",
      "Iteration 00450: loss = 3.956654e-05,9.597389e-09\n",
      "Iteration 00455: loss = 3.8858736e-05,9.605737e-09\n",
      "Iteration 00460: loss = 3.8168964e-05,9.614009e-09\n",
      "Iteration 00465: loss = 3.7495753e-05,9.62221e-09\n",
      "Iteration 00470: loss = 3.6840782e-05,9.630339e-09\n",
      "Iteration 00475: loss = 3.62026e-05,9.638399e-09\n",
      "Iteration 00480: loss = 3.557986e-05,9.646392e-09\n",
      "Iteration 00485: loss = 3.4972214e-05,9.654314e-09\n",
      "Iteration 00490: loss = 3.4380293e-05,9.6621715e-09\n",
      "Iteration 00495: loss = 3.3803102e-05,9.669963e-09\n",
      "Iteration 00500: loss = 3.3239372e-05,9.677688e-09\n",
      "Iteration 00505: loss = 3.2689102e-05,9.685351e-09\n",
      "Iteration 00510: loss = 3.2152337e-05,9.692952e-09\n",
      "Iteration 00515: loss = 3.1629115e-05,9.700492e-09\n",
      "Iteration 00520: loss = 3.1117568e-05,9.7079695e-09\n",
      "Iteration 00525: loss = 3.0617794e-05,9.7153885e-09\n",
      "Iteration 00530: loss = 3.012979e-05,9.722749e-09\n",
      "Iteration 00535: loss = 2.9653369e-05,9.7300505e-09\n",
      "Iteration 00540: loss = 2.9187366e-05,9.7372945e-09\n",
      "Iteration 00545: loss = 2.8732473e-05,9.744484e-09\n",
      "Iteration 00550: loss = 2.8286699e-05,9.751617e-09\n",
      "Iteration 00555: loss = 2.78519e-05,9.758697e-09\n",
      "Iteration 00560: loss = 2.7426948e-05,9.765722e-09\n",
      "Iteration 00565: loss = 2.701083e-05,9.772696e-09\n",
      "Iteration 00570: loss = 2.6604524e-05,9.779615e-09\n",
      "Iteration 00575: loss = 2.6206386e-05,9.7864845e-09\n",
      "Iteration 00580: loss = 2.5817104e-05,9.793304e-09\n",
      "Iteration 00585: loss = 2.5436524e-05,9.800071e-09\n",
      "Iteration 00590: loss = 2.506336e-05,9.806791e-09\n",
      "Iteration 00595: loss = 2.4698864e-05,9.813462e-09\n",
      "Iteration 00600: loss = 2.4341449e-05,9.820084e-09\n",
      "Iteration 00605: loss = 2.3992126e-05,9.8266595e-09\n",
      "Iteration 00610: loss = 2.3649656e-05,9.833189e-09\n",
      "Iteration 00615: loss = 2.3314149e-05,9.839671e-09\n",
      "Iteration 00620: loss = 2.2985761e-05,9.846109e-09\n",
      "Iteration 00625: loss = 2.2664382e-05,9.852503e-09\n",
      "Iteration 00630: loss = 2.2348808e-05,9.858853e-09\n",
      "Iteration 00635: loss = 2.2039703e-05,9.865159e-09\n",
      "Iteration 00640: loss = 2.1737244e-05,9.871423e-09\n",
      "Iteration 00645: loss = 2.1440537e-05,9.877644e-09\n",
      "Iteration 00650: loss = 2.1149186e-05,9.883822e-09\n",
      "Iteration 00655: loss = 2.0864421e-05,9.889959e-09\n",
      "Iteration 00660: loss = 2.0584574e-05,9.896057e-09\n",
      "Iteration 00665: loss = 2.0310572e-05,9.902114e-09\n",
      "Iteration 00670: loss = 2.0041814e-05,9.90813e-09\n",
      "Iteration 00675: loss = 1.9777948e-05,9.9141095e-09\n",
      "Iteration 00680: loss = 1.951918e-05,9.920049e-09\n",
      "Iteration 00685: loss = 1.9265392e-05,9.925949e-09\n",
      "Iteration 00690: loss = 1.9016e-05,9.9318145e-09\n",
      "Iteration 00695: loss = 1.8771929e-05,9.93764e-09\n",
      "Iteration 00700: loss = 1.8531648e-05,9.943433e-09\n",
      "Iteration 00705: loss = 1.8296241e-05,9.949183e-09\n",
      "Iteration 00710: loss = 1.8065193e-05,9.954903e-09\n",
      "Iteration 00715: loss = 1.7838218e-05,9.960586e-09\n",
      "Iteration 00720: loss = 1.7615197e-05,9.966234e-09\n",
      "Iteration 00725: loss = 1.7396573e-05,9.971847e-09\n",
      "Iteration 00730: loss = 1.7181555e-05,9.977426e-09\n",
      "Iteration 00735: loss = 1.6970553e-05,9.982972e-09\n",
      "Iteration 00740: loss = 1.6763062e-05,9.988484e-09\n",
      "Iteration 00745: loss = 1.655945e-05,9.993964e-09\n",
      "Iteration 00750: loss = 1.635922e-05,9.999411e-09\n",
      "Iteration 00755: loss = 1.6162796e-05,1.0004825e-08\n",
      "Iteration 00760: loss = 1.5969397e-05,1.0010208e-08\n",
      "Iteration 00765: loss = 1.577945e-05,1.001556e-08\n",
      "Iteration 00770: loss = 1.5592906e-05,1.0020881e-08\n",
      "Iteration 00775: loss = 1.5409465e-05,1.002617e-08\n",
      "Iteration 00780: loss = 1.5228696e-05,1.0031432e-08\n",
      "Iteration 00785: loss = 1.5051805e-05,1.0036661e-08\n",
      "Iteration 00790: loss = 1.48770305e-05,1.0041861e-08\n",
      "Iteration 00795: loss = 1.4705632e-05,1.0047033e-08\n",
      "Iteration 00800: loss = 1.453756e-05,1.0052173e-08\n",
      "Iteration 00805: loss = 1.4371696e-05,1.0057288e-08\n",
      "Iteration 00810: loss = 1.4208424e-05,1.0062373e-08\n",
      "Iteration 00815: loss = 1.4047728e-05,1.00674304e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00820: loss = 1.3889749e-05,1.0072461e-08\n",
      "Iteration 00825: loss = 1.3734698e-05,1.0077462e-08\n",
      "Iteration 00830: loss = 1.3582084e-05,1.0082438e-08\n",
      "Iteration 00835: loss = 1.3431665e-05,1.0087387e-08\n",
      "Iteration 00840: loss = 1.3283837e-05,1.009231e-08\n",
      "Iteration 00845: loss = 1.31377465e-05,1.0097203e-08\n",
      "Iteration 00850: loss = 1.2994979e-05,1.0102073e-08\n",
      "Iteration 00855: loss = 1.28536785e-05,1.0106918e-08\n",
      "Iteration 00860: loss = 1.2714638e-05,1.0111737e-08\n",
      "Iteration 00865: loss = 1.2578011e-05,1.011653e-08\n",
      "Iteration 00870: loss = 1.2443175e-05,1.0121299e-08\n",
      "Iteration 00875: loss = 1.2310891e-05,1.0126044e-08\n",
      "Iteration 00880: loss = 1.2179941e-05,1.0130764e-08\n",
      "Iteration 00885: loss = 1.2051697e-05,1.0135459e-08\n",
      "Iteration 00890: loss = 1.1925126e-05,1.0140132e-08\n",
      "Iteration 00895: loss = 1.1800602e-05,1.0144781e-08\n",
      "Iteration 00900: loss = 1.167753e-05,1.0149405e-08\n",
      "Iteration 00905: loss = 1.1556641e-05,1.0154007e-08\n",
      "Iteration 00910: loss = 1.1437524e-05,1.0158587e-08\n",
      "Iteration 00915: loss = 1.1319782e-05,1.0163144e-08\n",
      "Iteration 00920: loss = 1.120416e-05,1.01676765e-08\n",
      "Iteration 00925: loss = 1.10898645e-05,1.017219e-08\n",
      "Iteration 00930: loss = 1.0977829e-05,1.0176679e-08\n",
      "Iteration 00935: loss = 1.0866896e-05,1.01811475e-08\n",
      "Iteration 00940: loss = 1.0757608e-05,1.0185595e-08\n",
      "Iteration 00945: loss = 1.06506695e-05,1.01900195e-08\n",
      "Iteration 00950: loss = 1.0544055e-05,1.0194425e-08\n",
      "Iteration 00955: loss = 1.043959e-05,1.0198809e-08\n",
      "Iteration 00960: loss = 1.033685e-05,1.0203172e-08\n",
      "Iteration 00965: loss = 1.0235114e-05,1.0207514e-08\n",
      "Iteration 00970: loss = 1.0135096e-05,1.0211835e-08\n",
      "Iteration 00975: loss = 1.0036213e-05,1.0216137e-08\n",
      "Iteration 00980: loss = 9.93883e-06,1.0220418e-08\n",
      "Iteration 00985: loss = 9.842738e-06,1.02246815e-08\n",
      "Iteration 00990: loss = 9.748275e-06,1.0228924e-08\n",
      "Iteration 00995: loss = 9.654551e-06,1.0233147e-08\n",
      "Iteration 01000: loss = 9.562605e-06,1.0237352e-08\n",
      "Iteration 01005: loss = 9.471367e-06,1.0241537e-08\n",
      "Iteration 01010: loss = 9.381518e-06,1.0245703e-08\n",
      "Iteration 01015: loss = 9.293385e-06,1.02498525e-08\n",
      "Iteration 01020: loss = 9.205924e-06,1.0253981e-08\n",
      "Iteration 01025: loss = 9.120148e-06,1.0258092e-08\n",
      "Iteration 01030: loss = 9.035027e-06,1.0262185e-08\n",
      "Iteration 01035: loss = 8.951553e-06,1.0266259e-08\n",
      "Iteration 01040: loss = 8.868538e-06,1.0270317e-08\n",
      "Iteration 01045: loss = 8.786652e-06,1.02743565e-08\n",
      "Iteration 01050: loss = 8.706361e-06,1.0278376e-08\n",
      "Iteration 01055: loss = 8.626996e-06,1.0282383e-08\n",
      "Iteration 01060: loss = 8.548385e-06,1.0286371e-08\n",
      "Iteration 01065: loss = 8.470849e-06,1.0290338e-08\n",
      "Iteration 01070: loss = 8.394683e-06,1.0294295e-08\n",
      "Iteration 01075: loss = 8.319239e-06,1.0298232e-08\n",
      "Iteration 01080: loss = 8.244516e-06,1.030215e-08\n",
      "Iteration 01085: loss = 8.170799e-06,1.03060565e-08\n",
      "Iteration 01090: loss = 8.098109e-06,1.0309943e-08\n",
      "Iteration 01095: loss = 8.026415e-06,1.0313814e-08\n",
      "Iteration 01100: loss = 7.955705e-06,1.0317669e-08\n",
      "Iteration 01105: loss = 7.885669e-06,1.032151e-08\n",
      "Iteration 01110: loss = 7.816586e-06,1.0325334e-08\n",
      "Iteration 01115: loss = 7.748626e-06,1.0329143e-08\n",
      "Iteration 01120: loss = 7.681142e-06,1.03329345e-08\n",
      "Iteration 01125: loss = 7.6145957e-06,1.0336712e-08\n",
      "Iteration 01130: loss = 7.548982e-06,1.0340474e-08\n",
      "Iteration 01135: loss = 7.4841378e-06,1.03442215e-08\n",
      "Iteration 01140: loss = 7.4197346e-06,1.0347952e-08\n",
      "Iteration 01145: loss = 7.356393e-06,1.03516715e-08\n",
      "Iteration 01150: loss = 7.293786e-06,1.0355372e-08\n",
      "Iteration 01155: loss = 7.2322123e-06,1.0359061e-08\n",
      "Iteration 01160: loss = 7.171226e-06,1.0362734e-08\n",
      "Iteration 01165: loss = 7.1110826e-06,1.0366393e-08\n",
      "Iteration 01170: loss = 7.0513574e-06,1.0370041e-08\n",
      "Iteration 01175: loss = 6.99204e-06,1.037367e-08\n",
      "Iteration 01180: loss = 6.934011e-06,1.0377287e-08\n",
      "Iteration 01185: loss = 6.876508e-06,1.0380891e-08\n",
      "Iteration 01190: loss = 6.8195477e-06,1.0384481e-08\n",
      "Iteration 01195: loss = 6.763247e-06,1.0388055e-08\n",
      "Iteration 01200: loss = 6.707623e-06,1.0391617e-08\n",
      "Iteration 01205: loss = 6.652943e-06,1.0395167e-08\n",
      "Iteration 01210: loss = 6.5986274e-06,1.0398701e-08\n",
      "Iteration 01215: loss = 6.544953e-06,1.0402223e-08\n",
      "Iteration 01220: loss = 6.491773e-06,1.0405734e-08\n",
      "Iteration 01225: loss = 6.4393676e-06,1.0409228e-08\n",
      "Iteration 01230: loss = 6.3878706e-06,1.04127125e-08\n",
      "Iteration 01235: loss = 6.3364264e-06,1.04161835e-08\n",
      "Iteration 01240: loss = 6.2857357e-06,1.041964e-08\n",
      "Iteration 01245: loss = 6.2355098e-06,1.0423086e-08\n",
      "Iteration 01250: loss = 6.1861633e-06,1.0426519e-08\n",
      "Iteration 01255: loss = 6.137276e-06,1.0429938e-08\n",
      "Iteration 01260: loss = 6.08884e-06,1.0433346e-08\n",
      "Iteration 01265: loss = 6.0409834e-06,1.0436741e-08\n",
      "Iteration 01270: loss = 5.99357e-06,1.0440124e-08\n",
      "Iteration 01275: loss = 5.9468625e-06,1.0443496e-08\n",
      "Iteration 01280: loss = 5.900322e-06,1.0446857e-08\n",
      "Iteration 01285: loss = 5.854743e-06,1.0450204e-08\n",
      "Iteration 01290: loss = 5.8093137e-06,1.04535385e-08\n",
      "Iteration 01295: loss = 5.764306e-06,1.0456864e-08\n",
      "Iteration 01300: loss = 5.719976e-06,1.0460176e-08\n",
      "Iteration 01305: loss = 5.676326e-06,1.0463477e-08\n",
      "Iteration 01310: loss = 5.632808e-06,1.0466767e-08\n",
      "Iteration 01315: loss = 5.5896944e-06,1.0470045e-08\n",
      "Iteration 01320: loss = 5.5475048e-06,1.0473313e-08\n",
      "Iteration 01325: loss = 5.5054356e-06,1.0476569e-08\n",
      "Iteration 01330: loss = 5.4637553e-06,1.0479814e-08\n",
      "Iteration 01335: loss = 5.4225934e-06,1.0483049e-08\n",
      "Iteration 01340: loss = 5.382071e-06,1.04862705e-08\n",
      "Iteration 01345: loss = 5.3419185e-06,1.0489483e-08\n",
      "Iteration 01350: loss = 5.3020085e-06,1.0492685e-08\n",
      "Iteration 01355: loss = 5.2624678e-06,1.0495874e-08\n",
      "Iteration 01360: loss = 5.223543e-06,1.0499057e-08\n",
      "Iteration 01365: loss = 5.1848583e-06,1.0502226e-08\n",
      "Iteration 01370: loss = 5.1465227e-06,1.0505384e-08\n",
      "Iteration 01375: loss = 5.1088005e-06,1.0508533e-08\n",
      "Iteration 01380: loss = 5.071301e-06,1.0511672e-08\n",
      "Iteration 01385: loss = 5.0345166e-06,1.05148015e-08\n",
      "Iteration 01390: loss = 4.9978285e-06,1.0517917e-08\n",
      "Iteration 01395: loss = 4.961477e-06,1.0521028e-08\n",
      "Iteration 01400: loss = 4.92534e-06,1.0524124e-08\n",
      "Iteration 01405: loss = 4.889906e-06,1.0527213e-08\n",
      "Iteration 01410: loss = 4.854435e-06,1.0530291e-08\n",
      "Iteration 01415: loss = 4.819782e-06,1.053336e-08\n",
      "Iteration 01420: loss = 4.785447e-06,1.0536418e-08\n",
      "Iteration 01425: loss = 4.7510707e-06,1.0539466e-08\n",
      "Iteration 01430: loss = 4.717374e-06,1.05425055e-08\n",
      "Iteration 01435: loss = 4.6838722e-06,1.0545534e-08\n",
      "Iteration 01440: loss = 4.6507953e-06,1.0548557e-08\n",
      "Iteration 01445: loss = 4.617678e-06,1.0551567e-08\n",
      "Iteration 01450: loss = 4.585219e-06,1.0554569e-08\n",
      "Iteration 01455: loss = 4.552944e-06,1.0557563e-08\n",
      "Iteration 01460: loss = 4.521449e-06,1.0560544e-08\n",
      "Iteration 01465: loss = 4.4897693e-06,1.0563519e-08\n",
      "Iteration 01470: loss = 4.4585067e-06,1.0566484e-08\n",
      "Iteration 01475: loss = 4.427414e-06,1.056944e-08\n",
      "Iteration 01480: loss = 4.396974e-06,1.0572387e-08\n",
      "Iteration 01485: loss = 4.3666955e-06,1.05753255e-08\n",
      "Iteration 01490: loss = 4.336352e-06,1.0578254e-08\n",
      "Iteration 01495: loss = 4.3068726e-06,1.0581175e-08\n",
      "Iteration 01500: loss = 4.277324e-06,1.0584088e-08\n",
      "Iteration 01505: loss = 4.2481656e-06,1.0586991e-08\n",
      "Iteration 01510: loss = 4.2189417e-06,1.05898845e-08\n",
      "Iteration 01515: loss = 4.1903336e-06,1.0592771e-08\n",
      "Iteration 01520: loss = 4.1621056e-06,1.0595647e-08\n",
      "Iteration 01525: loss = 4.133808e-06,1.0598517e-08\n",
      "Iteration 01530: loss = 4.106001e-06,1.0601376e-08\n",
      "Iteration 01535: loss = 4.0782315e-06,1.0604229e-08\n",
      "Iteration 01540: loss = 4.0508367e-06,1.06070726e-08\n",
      "Iteration 01545: loss = 4.0240325e-06,1.06099085e-08\n",
      "Iteration 01550: loss = 3.9969323e-06,1.0612736e-08\n",
      "Iteration 01555: loss = 3.9704187e-06,1.0615557e-08\n",
      "Iteration 01560: loss = 3.9440465e-06,1.0618367e-08\n",
      "Iteration 01565: loss = 3.9180363e-06,1.0621172e-08\n",
      "Iteration 01570: loss = 3.892161e-06,1.0623967e-08\n",
      "Iteration 01575: loss = 3.8664252e-06,1.0626753e-08\n",
      "Iteration 01580: loss = 3.840934e-06,1.0629533e-08\n",
      "Iteration 01585: loss = 3.815901e-06,1.0632305e-08\n",
      "Iteration 01590: loss = 3.7910022e-06,1.063507e-08\n",
      "Iteration 01595: loss = 3.7662342e-06,1.0637826e-08\n",
      "Iteration 01600: loss = 3.7418079e-06,1.06405755e-08\n",
      "Iteration 01605: loss = 3.7175103e-06,1.0643316e-08\n",
      "Iteration 01610: loss = 3.6934468e-06,1.0646048e-08\n",
      "Iteration 01615: loss = 3.6696158e-06,1.0648774e-08\n",
      "Iteration 01620: loss = 3.6459007e-06,1.0651493e-08\n",
      "Iteration 01625: loss = 3.6225267e-06,1.0654205e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01630: loss = 3.5993783e-06,1.0656908e-08\n",
      "Iteration 01635: loss = 3.576238e-06,1.0659605e-08\n",
      "Iteration 01640: loss = 3.5534304e-06,1.0662294e-08\n",
      "Iteration 01645: loss = 3.5307373e-06,1.0664975e-08\n",
      "Iteration 01650: loss = 3.5085802e-06,1.0667652e-08\n",
      "Iteration 01655: loss = 3.4862196e-06,1.0670318e-08\n",
      "Iteration 01660: loss = 3.4642835e-06,1.0672981e-08\n",
      "Iteration 01665: loss = 3.4424627e-06,1.0675632e-08\n",
      "Iteration 01670: loss = 3.4206416e-06,1.0678278e-08\n",
      "Iteration 01675: loss = 3.3992399e-06,1.0680916e-08\n",
      "Iteration 01680: loss = 3.3781519e-06,1.0683549e-08\n",
      "Iteration 01685: loss = 3.3569634e-06,1.0686177e-08\n",
      "Iteration 01690: loss = 3.335984e-06,1.0688793e-08\n",
      "Iteration 01695: loss = 3.3154138e-06,1.0691407e-08\n",
      "Iteration 01700: loss = 3.294742e-06,1.069401e-08\n",
      "Iteration 01705: loss = 3.2742712e-06,1.0696608e-08\n",
      "Iteration 01710: loss = 3.2542055e-06,1.06992e-08\n",
      "Iteration 01715: loss = 3.234235e-06,1.0701786e-08\n",
      "Iteration 01720: loss = 3.214264e-06,1.0704362e-08\n",
      "Iteration 01725: loss = 3.1946922e-06,1.0706936e-08\n",
      "Iteration 01730: loss = 3.1750149e-06,1.0709501e-08\n",
      "Iteration 01735: loss = 3.155534e-06,1.0712058e-08\n",
      "Iteration 01740: loss = 3.1364425e-06,1.07146105e-08\n",
      "Iteration 01745: loss = 3.1174443e-06,1.0717157e-08\n",
      "Iteration 01750: loss = 3.0984374e-06,1.0719697e-08\n",
      "Iteration 01755: loss = 3.0798208e-06,1.072223e-08\n",
      "Iteration 01760: loss = 3.0610947e-06,1.0724757e-08\n",
      "Iteration 01765: loss = 3.042752e-06,1.0727276e-08\n",
      "Iteration 01770: loss = 3.024593e-06,1.0729791e-08\n",
      "Iteration 01775: loss = 3.006422e-06,1.0732298e-08\n",
      "Iteration 01780: loss = 2.9882424e-06,1.0734801e-08\n",
      "Iteration 01785: loss = 2.9705327e-06,1.0737295e-08\n",
      "Iteration 01790: loss = 2.9528146e-06,1.0739787e-08\n",
      "Iteration 01795: loss = 2.935274e-06,1.0742268e-08\n",
      "Iteration 01800: loss = 2.91791e-06,1.0744746e-08\n",
      "Iteration 01805: loss = 2.9007215e-06,1.0747218e-08\n",
      "Iteration 01810: loss = 2.883524e-06,1.0749686e-08\n",
      "Iteration 01815: loss = 2.8664963e-06,1.0752144e-08\n",
      "Iteration 01820: loss = 2.8497423e-06,1.0754595e-08\n",
      "Iteration 01825: loss = 2.832968e-06,1.0757045e-08\n",
      "Iteration 01830: loss = 2.8163652e-06,1.0759486e-08\n",
      "Iteration 01835: loss = 2.799934e-06,1.0761923e-08\n",
      "Iteration 01840: loss = 2.7836707e-06,1.0764353e-08\n",
      "Iteration 01845: loss = 2.7675767e-06,1.07667795e-08\n",
      "Iteration 01850: loss = 2.7514652e-06,1.0769198e-08\n",
      "Iteration 01855: loss = 2.7355193e-06,1.0771612e-08\n",
      "Iteration 01860: loss = 2.7197366e-06,1.0774019e-08\n",
      "Iteration 01865: loss = 2.7039362e-06,1.0776421e-08\n",
      "Iteration 01870: loss = 2.688481e-06,1.0778817e-08\n",
      "Iteration 01875: loss = 2.672913e-06,1.0781209e-08\n",
      "Iteration 01880: loss = 2.6576918e-06,1.0783595e-08\n",
      "Iteration 01885: loss = 2.6424466e-06,1.0785975e-08\n",
      "Iteration 01890: loss = 2.627362e-06,1.078835e-08\n",
      "Iteration 01895: loss = 2.6126174e-06,1.0790718e-08\n",
      "Iteration 01900: loss = 2.5974866e-06,1.0793081e-08\n",
      "Iteration 01905: loss = 2.5828738e-06,1.0795438e-08\n",
      "Iteration 01910: loss = 2.568234e-06,1.0797793e-08\n",
      "Iteration 01915: loss = 2.5537527e-06,1.0800141e-08\n",
      "Iteration 01920: loss = 2.5392453e-06,1.08024825e-08\n",
      "Iteration 01925: loss = 2.5249801e-06,1.080482e-08\n",
      "Iteration 01930: loss = 2.5108677e-06,1.0807151e-08\n",
      "Iteration 01935: loss = 2.4967296e-06,1.0809478e-08\n",
      "Iteration 01940: loss = 2.4829162e-06,1.08117995e-08\n",
      "Iteration 01945: loss = 2.4689027e-06,1.0814117e-08\n",
      "Iteration 01950: loss = 2.4552112e-06,1.0816428e-08\n",
      "Iteration 01955: loss = 2.4414942e-06,1.0818735e-08\n",
      "Iteration 01960: loss = 2.4279232e-06,1.08210365e-08\n",
      "Iteration 01965: loss = 2.4144992e-06,1.0823332e-08\n",
      "Iteration 01970: loss = 2.4012186e-06,1.0825621e-08\n",
      "Iteration 01975: loss = 2.3877387e-06,1.0827909e-08\n",
      "Iteration 01980: loss = 2.3747475e-06,1.083019e-08\n",
      "Iteration 01985: loss = 2.3617272e-06,1.0832466e-08\n",
      "Iteration 01990: loss = 2.3487617e-06,1.08347376e-08\n",
      "Iteration 01995: loss = 2.3359394e-06,1.0837004e-08\n",
      "Iteration 02000: loss = 2.323088e-06,1.0839265e-08\n",
      "Iteration 02005: loss = 2.3103746e-06,1.0841523e-08\n",
      "Iteration 02010: loss = 2.2978004e-06,1.0843773e-08\n",
      "Iteration 02015: loss = 2.2853649e-06,1.0846024e-08\n",
      "Iteration 02020: loss = 2.2729016e-06,1.0848263e-08\n",
      "Iteration 02025: loss = 2.2606534e-06,1.0850504e-08\n",
      "Iteration 02030: loss = 2.2485447e-06,1.0852734e-08\n",
      "Iteration 02035: loss = 2.2362346e-06,1.0854965e-08\n",
      "Iteration 02040: loss = 2.2242289e-06,1.0857186e-08\n",
      "Iteration 02045: loss = 2.21219e-06,1.0859406e-08\n",
      "Iteration 02050: loss = 2.2003671e-06,1.086162e-08\n",
      "Iteration 02055: loss = 2.1885146e-06,1.0863829e-08\n",
      "Iteration 02060: loss = 2.1769574e-06,1.0866036e-08\n",
      "Iteration 02065: loss = 2.1653648e-06,1.0868236e-08\n",
      "Iteration 02070: loss = 2.1539042e-06,1.0870435e-08\n",
      "Iteration 02075: loss = 2.1423268e-06,1.08726255e-08\n",
      "Iteration 02080: loss = 2.131043e-06,1.0874812e-08\n",
      "Iteration 02085: loss = 2.1197254e-06,1.0876994e-08\n",
      "Iteration 02090: loss = 2.1082908e-06,1.0879173e-08\n",
      "Iteration 02095: loss = 2.097311e-06,1.0881347e-08\n",
      "Iteration 02100: loss = 2.0861328e-06,1.0883518e-08\n",
      "Iteration 02105: loss = 2.075243e-06,1.0885682e-08\n",
      "Iteration 02110: loss = 2.0644782e-06,1.0887844e-08\n",
      "Iteration 02115: loss = 2.053596e-06,1.089e-08\n",
      "Iteration 02120: loss = 2.0428395e-06,1.0892155e-08\n",
      "Iteration 02125: loss = 2.0322075e-06,1.0894301e-08\n",
      "Iteration 02130: loss = 2.0214577e-06,1.0896445e-08\n",
      "Iteration 02135: loss = 2.0109908e-06,1.0898584e-08\n",
      "Iteration 02140: loss = 2.0006464e-06,1.0900721e-08\n",
      "Iteration 02145: loss = 1.9900283e-06,1.0902852e-08\n",
      "Iteration 02150: loss = 1.9798465e-06,1.09049765e-08\n",
      "Iteration 02155: loss = 1.9696283e-06,1.09071e-08\n",
      "Iteration 02160: loss = 1.9594486e-06,1.090922e-08\n",
      "Iteration 02165: loss = 1.9495465e-06,1.0911334e-08\n",
      "Iteration 02170: loss = 1.93945e-06,1.0913445e-08\n",
      "Iteration 02175: loss = 1.929395e-06,1.0915551e-08\n",
      "Iteration 02180: loss = 1.919455e-06,1.0917654e-08\n",
      "Iteration 02185: loss = 1.909789e-06,1.0919752e-08\n",
      "Iteration 02190: loss = 1.8998526e-06,1.09218465e-08\n",
      "Iteration 02195: loss = 1.8903417e-06,1.09239355e-08\n",
      "Iteration 02200: loss = 1.8806367e-06,1.0926023e-08\n",
      "Iteration 02205: loss = 1.8711243e-06,1.0928107e-08\n",
      "Iteration 02210: loss = 1.8615727e-06,1.0930182e-08\n",
      "Iteration 02215: loss = 1.8521354e-06,1.0932259e-08\n",
      "Iteration 02220: loss = 1.842734e-06,1.0934331e-08\n",
      "Iteration 02225: loss = 1.8334473e-06,1.0936397e-08\n",
      "Iteration 02230: loss = 1.8242703e-06,1.0938459e-08\n",
      "Iteration 02235: loss = 1.8151319e-06,1.0940517e-08\n",
      "Iteration 02240: loss = 1.8059517e-06,1.0942572e-08\n",
      "Iteration 02245: loss = 1.7968831e-06,1.0944628e-08\n",
      "Iteration 02250: loss = 1.787851e-06,1.0946673e-08\n",
      "Iteration 02255: loss = 1.7788549e-06,1.0948717e-08\n",
      "Iteration 02260: loss = 1.7701163e-06,1.09507585e-08\n",
      "Iteration 02265: loss = 1.7613369e-06,1.0952793e-08\n",
      "Iteration 02270: loss = 1.7524425e-06,1.09548255e-08\n",
      "Iteration 02275: loss = 1.7438055e-06,1.0956858e-08\n",
      "Iteration 02280: loss = 1.7351282e-06,1.095888e-08\n",
      "Iteration 02285: loss = 1.7264839e-06,1.0960903e-08\n",
      "Iteration 02290: loss = 1.7180924e-06,1.0962921e-08\n",
      "Iteration 02295: loss = 1.7095136e-06,1.0964936e-08\n",
      "Iteration 02300: loss = 1.7011142e-06,1.09669465e-08\n",
      "Iteration 02305: loss = 1.6927482e-06,1.09689555e-08\n",
      "Iteration 02310: loss = 1.6844857e-06,1.0970959e-08\n",
      "Iteration 02315: loss = 1.6761823e-06,1.0972958e-08\n",
      "Iteration 02320: loss = 1.6680532e-06,1.0974955e-08\n",
      "Iteration 02325: loss = 1.6598142e-06,1.0976947e-08\n",
      "Iteration 02330: loss = 1.6518185e-06,1.0978937e-08\n",
      "Iteration 02335: loss = 1.6436406e-06,1.0980923e-08\n",
      "Iteration 02340: loss = 1.6356361e-06,1.0982907e-08\n",
      "Iteration 02345: loss = 1.6277328e-06,1.0984886e-08\n",
      "Iteration 02350: loss = 1.6197902e-06,1.0986861e-08\n",
      "Iteration 02355: loss = 1.612016e-06,1.0988835e-08\n",
      "Iteration 02360: loss = 1.604133e-06,1.0990805e-08\n",
      "Iteration 02365: loss = 1.596488e-06,1.0992769e-08\n",
      "Iteration 02370: loss = 1.5888018e-06,1.0994731e-08\n",
      "Iteration 02375: loss = 1.5810034e-06,1.0996688e-08\n",
      "Iteration 02380: loss = 1.5733755e-06,1.0998643e-08\n",
      "Iteration 02385: loss = 1.5658483e-06,1.1000596e-08\n",
      "Iteration 02390: loss = 1.5584166e-06,1.1002545e-08\n",
      "Iteration 02395: loss = 1.5508731e-06,1.100449e-08\n",
      "Iteration 02400: loss = 1.5434971e-06,1.1006434e-08\n",
      "Iteration 02405: loss = 1.5362178e-06,1.1008369e-08\n",
      "Iteration 02410: loss = 1.5288956e-06,1.1010306e-08\n",
      "Iteration 02415: loss = 1.5214627e-06,1.1012237e-08\n",
      "Iteration 02420: loss = 1.514332e-06,1.1014167e-08\n",
      "Iteration 02425: loss = 1.5071596e-06,1.1016093e-08\n",
      "Iteration 02430: loss = 1.5000815e-06,1.1018016e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 02435: loss = 1.4928936e-06,1.1019934e-08\n",
      "Iteration 02440: loss = 1.4857302e-06,1.1021849e-08\n",
      "Iteration 02445: loss = 1.4787967e-06,1.1023764e-08\n",
      "Iteration 02450: loss = 1.4718235e-06,1.1025672e-08\n",
      "Iteration 02455: loss = 1.4650082e-06,1.1027578e-08\n",
      "Iteration 02460: loss = 1.4580854e-06,1.1029481e-08\n",
      "Iteration 02465: loss = 1.4511862e-06,1.1031383e-08\n",
      "Iteration 02470: loss = 1.4443776e-06,1.1033281e-08\n",
      "Iteration 02475: loss = 1.4377969e-06,1.1035174e-08\n",
      "Iteration 02480: loss = 1.430972e-06,1.1037065e-08\n",
      "Iteration 02485: loss = 1.4241714e-06,1.1038955e-08\n",
      "Iteration 02490: loss = 1.4177289e-06,1.1040838e-08\n",
      "Iteration 02495: loss = 1.4109743e-06,1.10427205e-08\n",
      "Iteration 02500: loss = 1.4045135e-06,1.1044599e-08\n",
      "Iteration 02505: loss = 1.3980734e-06,1.1046474e-08\n",
      "Iteration 02510: loss = 1.3913929e-06,1.1048348e-08\n",
      "Iteration 02515: loss = 1.3850653e-06,1.1050217e-08\n",
      "Iteration 02520: loss = 1.378694e-06,1.1052084e-08\n",
      "Iteration 02525: loss = 1.3723462e-06,1.105395e-08\n",
      "Iteration 02530: loss = 1.3660207e-06,1.1055808e-08\n",
      "Iteration 02535: loss = 1.3597836e-06,1.1057668e-08\n",
      "Iteration 02540: loss = 1.3535029e-06,1.1059523e-08\n",
      "Iteration 02545: loss = 1.347244e-06,1.1061374e-08\n",
      "Iteration 02550: loss = 1.3411376e-06,1.1063223e-08\n",
      "Iteration 02555: loss = 1.3349231e-06,1.1065069e-08\n",
      "Iteration 02560: loss = 1.3289228e-06,1.10669145e-08\n",
      "Iteration 02565: loss = 1.3228813e-06,1.1068754e-08\n",
      "Iteration 02570: loss = 1.3167299e-06,1.1070594e-08\n",
      "Iteration 02575: loss = 1.3108587e-06,1.1072427e-08\n",
      "Iteration 02580: loss = 1.3048798e-06,1.107426e-08\n",
      "Iteration 02585: loss = 1.2989843e-06,1.107609e-08\n",
      "Iteration 02590: loss = 1.2930474e-06,1.1077916e-08\n",
      "Iteration 02595: loss = 1.2872565e-06,1.1079741e-08\n",
      "Iteration 02600: loss = 1.2813601e-06,1.1081561e-08\n",
      "Iteration 02605: loss = 1.2756099e-06,1.1083379e-08\n",
      "Iteration 02610: loss = 1.2699433e-06,1.1085194e-08\n",
      "Iteration 02615: loss = 1.264107e-06,1.1087005e-08\n",
      "Iteration 02620: loss = 1.2584163e-06,1.1088817e-08\n",
      "Iteration 02625: loss = 1.2527466e-06,1.1090624e-08\n",
      "Iteration 02630: loss = 1.2472201e-06,1.10924265e-08\n",
      "Iteration 02635: loss = 1.2415874e-06,1.10942295e-08\n",
      "Iteration 02640: loss = 1.2360371e-06,1.109603e-08\n",
      "Iteration 02645: loss = 1.230569e-06,1.1097827e-08\n",
      "Iteration 02650: loss = 1.2249939e-06,1.109962e-08\n",
      "Iteration 02655: loss = 1.2195619e-06,1.110141e-08\n",
      "Iteration 02660: loss = 1.2141484e-06,1.1103201e-08\n",
      "Iteration 02665: loss = 1.2088159e-06,1.1104986e-08\n",
      "Iteration 02670: loss = 1.2034386e-06,1.1106769e-08\n",
      "Iteration 02675: loss = 1.1979577e-06,1.110855e-08\n",
      "Iteration 02680: loss = 1.1927405e-06,1.111033e-08\n",
      "Iteration 02685: loss = 1.1874191e-06,1.1112104e-08\n",
      "Iteration 02690: loss = 1.1822372e-06,1.1113876e-08\n",
      "Iteration 02695: loss = 1.1770117e-06,1.1115647e-08\n",
      "Iteration 02700: loss = 1.1718653e-06,1.1117416e-08\n",
      "Iteration 02705: loss = 1.1666142e-06,1.1119179e-08\n",
      "Iteration 02710: loss = 1.1615018e-06,1.1120942e-08\n",
      "Iteration 02715: loss = 1.1565261e-06,1.1122703e-08\n",
      "Iteration 02720: loss = 1.1513281e-06,1.112446e-08\n",
      "Iteration 02725: loss = 1.1463268e-06,1.1126217e-08\n",
      "Iteration 02730: loss = 1.1414033e-06,1.1127968e-08\n",
      "Iteration 02735: loss = 1.1363744e-06,1.1129721e-08\n",
      "Iteration 02740: loss = 1.1314833e-06,1.11314655e-08\n",
      "Iteration 02745: loss = 1.1264879e-06,1.1133212e-08\n",
      "Iteration 02750: loss = 1.1216295e-06,1.1134954e-08\n",
      "Iteration 02755: loss = 1.1167282e-06,1.1136694e-08\n",
      "Iteration 02760: loss = 1.1119008e-06,1.11384315e-08\n",
      "Iteration 02765: loss = 1.10709e-06,1.1140168e-08\n",
      "Iteration 02770: loss = 1.102294e-06,1.11419e-08\n",
      "Iteration 02775: loss = 1.0975153e-06,1.1143632e-08\n",
      "Iteration 02780: loss = 1.0926343e-06,1.114536e-08\n",
      "Iteration 02785: loss = 1.0880624e-06,1.1147085e-08\n",
      "Iteration 02790: loss = 1.0833305e-06,1.1148808e-08\n",
      "Iteration 02795: loss = 1.0786133e-06,1.1150529e-08\n",
      "Iteration 02800: loss = 1.074145e-06,1.1152249e-08\n",
      "Iteration 02805: loss = 1.0694586e-06,1.1153965e-08\n",
      "Iteration 02810: loss = 1.0649034e-06,1.1155679e-08\n",
      "Iteration 02815: loss = 1.0603052e-06,1.1157392e-08\n",
      "Iteration 02820: loss = 1.0557806e-06,1.11591e-08\n",
      "Iteration 02825: loss = 1.0512691e-06,1.1160807e-08\n",
      "Iteration 02830: loss = 1.046773e-06,1.1162511e-08\n",
      "Iteration 02835: loss = 1.0422925e-06,1.1164213e-08\n",
      "Iteration 02840: loss = 1.0378254e-06,1.11659135e-08\n",
      "Iteration 02845: loss = 1.0334863e-06,1.116761e-08\n",
      "Iteration 02850: loss = 1.0291062e-06,1.1169308e-08\n",
      "Iteration 02855: loss = 1.0246821e-06,1.1171001e-08\n",
      "Iteration 02860: loss = 1.0203855e-06,1.1172691e-08\n",
      "Iteration 02865: loss = 1.01599e-06,1.1174378e-08\n",
      "Iteration 02870: loss = 1.0117209e-06,1.1176065e-08\n",
      "Iteration 02875: loss = 1.0074667e-06,1.117775e-08\n",
      "Iteration 02880: loss = 1.0032253e-06,1.1179431e-08\n",
      "Iteration 02885: loss = 9.990534e-07,1.1181111e-08\n",
      "Iteration 02890: loss = 9.948395e-07,1.118279e-08\n",
      "Iteration 02895: loss = 9.906377e-07,1.1184465e-08\n",
      "Iteration 02900: loss = 9.864503e-07,1.11861365e-08\n",
      "Iteration 02905: loss = 9.822759e-07,1.118781e-08\n",
      "Iteration 02910: loss = 9.781153e-07,1.1189478e-08\n",
      "Iteration 02915: loss = 9.740786e-07,1.1191144e-08\n",
      "Iteration 02920: loss = 9.699992e-07,1.119281e-08\n",
      "Iteration 02925: loss = 9.659888e-07,1.1194471e-08\n",
      "Iteration 02930: loss = 9.619893e-07,1.119613e-08\n",
      "Iteration 02935: loss = 9.578938e-07,1.1197791e-08\n",
      "Iteration 02940: loss = 9.539194e-07,1.11994485e-08\n",
      "Iteration 02945: loss = 9.499585e-07,1.12011e-08\n",
      "Iteration 02950: loss = 9.4601097e-07,1.12027525e-08\n",
      "Iteration 02955: loss = 9.420756e-07,1.1204403e-08\n",
      "Iteration 02960: loss = 9.3820546e-07,1.120605e-08\n",
      "Iteration 02965: loss = 9.3429435e-07,1.1207695e-08\n",
      "Iteration 02970: loss = 9.303956e-07,1.12093375e-08\n",
      "Iteration 02975: loss = 9.266162e-07,1.1210979e-08\n",
      "Iteration 02980: loss = 9.2285e-07,1.1212618e-08\n",
      "Iteration 02985: loss = 9.1887995e-07,1.1214256e-08\n",
      "Iteration 02990: loss = 9.151363e-07,1.121589e-08\n",
      "Iteration 02995: loss = 9.114577e-07,1.1217525e-08\n",
      "Iteration 03000: loss = 9.077374e-07,1.1219155e-08\n",
      "Iteration 03005: loss = 9.0402864e-07,1.1220785e-08\n",
      "Iteration 03010: loss = 9.0022536e-07,1.12224114e-08\n",
      "Iteration 03015: loss = 8.965398e-07,1.1224039e-08\n",
      "Iteration 03020: loss = 8.9286567e-07,1.12256595e-08\n",
      "Iteration 03025: loss = 8.892027e-07,1.122728e-08\n",
      "Iteration 03030: loss = 8.855515e-07,1.12289e-08\n",
      "Iteration 03035: loss = 8.8196316e-07,1.1230518e-08\n",
      "Iteration 03040: loss = 8.784384e-07,1.1232135e-08\n",
      "Iteration 03045: loss = 8.7482084e-07,1.1233748e-08\n",
      "Iteration 03050: loss = 8.712143e-07,1.1235359e-08\n",
      "Iteration 03055: loss = 8.678275e-07,1.1236965e-08\n",
      "Iteration 03060: loss = 8.642426e-07,1.12385745e-08\n",
      "Iteration 03065: loss = 8.6066825e-07,1.124018e-08\n",
      "Iteration 03070: loss = 8.572092e-07,1.12417835e-08\n",
      "Iteration 03075: loss = 8.5365645e-07,1.1243386e-08\n",
      "Iteration 03080: loss = 8.503732e-07,1.12449845e-08\n",
      "Iteration 03085: loss = 8.4684234e-07,1.1246582e-08\n",
      "Iteration 03090: loss = 8.434251e-07,1.1248179e-08\n",
      "Iteration 03095: loss = 8.401207e-07,1.1249771e-08\n",
      "Iteration 03100: loss = 8.3662206e-07,1.12513625e-08\n",
      "Iteration 03105: loss = 8.333379e-07,1.1252954e-08\n",
      "Iteration 03110: loss = 8.2996183e-07,1.1254542e-08\n",
      "Iteration 03115: loss = 8.2659585e-07,1.12561285e-08\n",
      "Iteration 03120: loss = 8.233415e-07,1.1257713e-08\n",
      "Iteration 03125: loss = 8.200465e-07,1.1259296e-08\n",
      "Iteration 03130: loss = 8.1671055e-07,1.1260878e-08\n",
      "Iteration 03135: loss = 8.1348577e-07,1.1262458e-08\n",
      "Iteration 03140: loss = 8.101702e-07,1.1264034e-08\n",
      "Iteration 03145: loss = 8.069644e-07,1.1265607e-08\n",
      "Iteration 03150: loss = 8.037694e-07,1.1267184e-08\n",
      "Iteration 03155: loss = 8.004822e-07,1.1268755e-08\n",
      "Iteration 03160: loss = 7.9740647e-07,1.1270323e-08\n",
      "Iteration 03165: loss = 7.9423944e-07,1.127189e-08\n",
      "Iteration 03170: loss = 7.911313e-07,1.1273459e-08\n",
      "Iteration 03175: loss = 7.879834e-07,1.1275023e-08\n",
      "Iteration 03180: loss = 7.847463e-07,1.1276583e-08\n",
      "Iteration 03185: loss = 7.8171587e-07,1.1278143e-08\n",
      "Iteration 03190: loss = 7.7859505e-07,1.1279704e-08\n",
      "Iteration 03195: loss = 7.7548503e-07,1.1281262e-08\n",
      "Iteration 03200: loss = 7.724818e-07,1.1282817e-08\n",
      "Iteration 03205: loss = 7.6938863e-07,1.1284369e-08\n",
      "Iteration 03210: loss = 7.664036e-07,1.12859215e-08\n",
      "Iteration 03215: loss = 7.633292e-07,1.1287474e-08\n",
      "Iteration 03220: loss = 7.6041056e-07,1.1289021e-08\n",
      "Iteration 03225: loss = 7.573549e-07,1.12905685e-08\n",
      "Iteration 03230: loss = 7.544047e-07,1.1292114e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 03235: loss = 7.515598e-07,1.1293658e-08\n",
      "Iteration 03240: loss = 7.4853e-07,1.1295198e-08\n",
      "Iteration 03245: loss = 7.456065e-07,1.1296739e-08\n",
      "Iteration 03250: loss = 7.4269036e-07,1.1298275e-08\n",
      "Iteration 03255: loss = 7.396875e-07,1.12998135e-08\n",
      "Iteration 03260: loss = 7.368842e-07,1.1301347e-08\n",
      "Iteration 03265: loss = 7.3399514e-07,1.130288e-08\n",
      "Iteration 03270: loss = 7.3116115e-07,1.1304413e-08\n",
      "Iteration 03275: loss = 7.2838384e-07,1.1305941e-08\n",
      "Iteration 03280: loss = 7.255185e-07,1.130747e-08\n",
      "Iteration 03285: loss = 7.227568e-07,1.1308996e-08\n",
      "Iteration 03290: loss = 7.1990803e-07,1.13105205e-08\n",
      "Iteration 03295: loss = 7.170684e-07,1.1312045e-08\n",
      "Iteration 03300: loss = 7.1433027e-07,1.1313567e-08\n",
      "Iteration 03305: loss = 7.115068e-07,1.1315087e-08\n",
      "Iteration 03310: loss = 7.0878576e-07,1.1316604e-08\n",
      "Iteration 03315: loss = 7.0597775e-07,1.13181216e-08\n",
      "Iteration 03320: loss = 7.0327275e-07,1.1319638e-08\n",
      "Iteration 03325: loss = 7.007161e-07,1.1321149e-08\n",
      "Iteration 03330: loss = 6.9793197e-07,1.1322663e-08\n",
      "Iteration 03335: loss = 6.9524964e-07,1.1324171e-08\n",
      "Iteration 03340: loss = 6.9257493e-07,1.13256835e-08\n",
      "Iteration 03345: loss = 6.899079e-07,1.1327189e-08\n",
      "Iteration 03350: loss = 6.872483e-07,1.1328696e-08\n",
      "Iteration 03355: loss = 6.845967e-07,1.13302e-08\n",
      "Iteration 03360: loss = 6.81953e-07,1.1331702e-08\n",
      "Iteration 03365: loss = 6.7931666e-07,1.1333201e-08\n",
      "Iteration 03370: loss = 6.7677996e-07,1.1334702e-08\n",
      "Iteration 03375: loss = 6.7415846e-07,1.13362e-08\n",
      "Iteration 03380: loss = 6.7168276e-07,1.13376935e-08\n",
      "Iteration 03385: loss = 6.690755e-07,1.1339189e-08\n",
      "Iteration 03390: loss = 6.6647607e-07,1.1340681e-08\n",
      "Iteration 03395: loss = 6.6397587e-07,1.13421725e-08\n",
      "Iteration 03400: loss = 6.6139097e-07,1.1343663e-08\n",
      "Iteration 03405: loss = 6.589051e-07,1.134515e-08\n",
      "Iteration 03410: loss = 6.564253e-07,1.1346636e-08\n",
      "Iteration 03415: loss = 6.5386297e-07,1.1348122e-08\n",
      "Iteration 03420: loss = 6.5139693e-07,1.1349608e-08\n",
      "Iteration 03425: loss = 6.4893896e-07,1.1351091e-08\n",
      "Iteration 03430: loss = 6.4648793e-07,1.13525696e-08\n",
      "Iteration 03435: loss = 6.441335e-07,1.1354048e-08\n",
      "Iteration 03440: loss = 6.417402e-07,1.1355528e-08\n",
      "Iteration 03445: loss = 6.3921976e-07,1.13570024e-08\n",
      "Iteration 03450: loss = 6.3679636e-07,1.1358477e-08\n",
      "Iteration 03455: loss = 6.343792e-07,1.1359951e-08\n",
      "Iteration 03460: loss = 6.319694e-07,1.1361424e-08\n",
      "Iteration 03465: loss = 6.296546e-07,1.1362895e-08\n",
      "Iteration 03470: loss = 6.272577e-07,1.1364362e-08\n",
      "Iteration 03475: loss = 6.248676e-07,1.1365829e-08\n",
      "Iteration 03480: loss = 6.225729e-07,1.13672955e-08\n",
      "Iteration 03485: loss = 6.2028425e-07,1.1368758e-08\n",
      "Iteration 03490: loss = 6.1791405e-07,1.137022e-08\n",
      "Iteration 03495: loss = 6.155502e-07,1.1371682e-08\n",
      "Iteration 03500: loss = 6.132802e-07,1.1373144e-08\n",
      "Iteration 03505: loss = 6.1097325e-07,1.13746e-08\n",
      "Iteration 03510: loss = 6.088042e-07,1.1376057e-08\n",
      "Iteration 03515: loss = 6.0646624e-07,1.1377512e-08\n",
      "Iteration 03520: loss = 6.041346e-07,1.1378968e-08\n",
      "Iteration 03525: loss = 6.0198363e-07,1.138042e-08\n",
      "Iteration 03530: loss = 5.996649e-07,1.13818714e-08\n",
      "Iteration 03535: loss = 5.974392e-07,1.1383322e-08\n",
      "Iteration 03540: loss = 5.9521943e-07,1.1384771e-08\n",
      "Iteration 03545: loss = 5.9300646e-07,1.1386222e-08\n",
      "Iteration 03550: loss = 5.9088507e-07,1.1387667e-08\n",
      "Iteration 03555: loss = 5.885974e-07,1.1389112e-08\n",
      "Iteration 03560: loss = 5.8648834e-07,1.1390554e-08\n",
      "Iteration 03565: loss = 5.8421375e-07,1.1391997e-08\n",
      "Iteration 03570: loss = 5.821587e-07,1.1393437e-08\n",
      "Iteration 03575: loss = 5.799816e-07,1.1394874e-08\n",
      "Iteration 03580: loss = 5.778102e-07,1.1396312e-08\n",
      "Iteration 03585: loss = 5.757303e-07,1.13977485e-08\n",
      "Iteration 03590: loss = 5.735704e-07,1.1399185e-08\n",
      "Iteration 03595: loss = 5.7150174e-07,1.1400617e-08\n",
      "Iteration 03600: loss = 5.693536e-07,1.1402049e-08\n",
      "Iteration 03605: loss = 5.672958e-07,1.1403479e-08\n",
      "Iteration 03610: loss = 5.651597e-07,1.1404911e-08\n",
      "Iteration 03615: loss = 5.6311364e-07,1.1406337e-08\n",
      "Iteration 03620: loss = 5.610734e-07,1.14077645e-08\n",
      "Iteration 03625: loss = 5.5903826e-07,1.140919e-08\n",
      "Iteration 03630: loss = 5.5692476e-07,1.1410614e-08\n",
      "Iteration 03635: loss = 5.5481775e-07,1.1412036e-08\n",
      "Iteration 03640: loss = 5.5284045e-07,1.14134595e-08\n",
      "Iteration 03645: loss = 5.508274e-07,1.14148815e-08\n",
      "Iteration 03650: loss = 5.4882025e-07,1.1416299e-08\n",
      "Iteration 03655: loss = 5.468178e-07,1.1417716e-08\n",
      "Iteration 03660: loss = 5.448206e-07,1.1419132e-08\n",
      "Iteration 03665: loss = 5.4282947e-07,1.1420548e-08\n",
      "Iteration 03670: loss = 5.4084313e-07,1.1421963e-08\n",
      "Iteration 03675: loss = 5.3886237e-07,1.1423374e-08\n",
      "Iteration 03680: loss = 5.368869e-07,1.1424784e-08\n",
      "Iteration 03685: loss = 5.349986e-07,1.1426197e-08\n",
      "Iteration 03690: loss = 5.3303415e-07,1.14276055e-08\n",
      "Iteration 03695: loss = 5.3115633e-07,1.14290115e-08\n",
      "Iteration 03700: loss = 5.292014e-07,1.1430419e-08\n",
      "Iteration 03705: loss = 5.272523e-07,1.1431822e-08\n",
      "Iteration 03710: loss = 5.253898e-07,1.1433226e-08\n",
      "Iteration 03715: loss = 5.2345075e-07,1.1434628e-08\n",
      "Iteration 03720: loss = 5.215577e-07,1.143603e-08\n",
      "Iteration 03725: loss = 5.197099e-07,1.1437431e-08\n",
      "Iteration 03730: loss = 5.1786685e-07,1.1438829e-08\n",
      "Iteration 03735: loss = 5.159481e-07,1.1440224e-08\n",
      "Iteration 03740: loss = 5.1403487e-07,1.1441621e-08\n",
      "Iteration 03745: loss = 5.122867e-07,1.1443016e-08\n",
      "Iteration 03750: loss = 5.103839e-07,1.1444411e-08\n",
      "Iteration 03755: loss = 5.085648e-07,1.14458025e-08\n",
      "Iteration 03760: loss = 5.0675175e-07,1.1447191e-08\n",
      "Iteration 03765: loss = 5.0494293e-07,1.1448581e-08\n",
      "Iteration 03770: loss = 5.030597e-07,1.1449972e-08\n",
      "Iteration 03775: loss = 5.0133946e-07,1.1451359e-08\n",
      "Iteration 03780: loss = 4.994657e-07,1.14527445e-08\n",
      "Iteration 03785: loss = 4.9775474e-07,1.1454131e-08\n",
      "Iteration 03790: loss = 4.9589147e-07,1.1455513e-08\n",
      "Iteration 03795: loss = 4.941109e-07,1.1456895e-08\n",
      "Iteration 03800: loss = 4.9237474e-07,1.1458277e-08\n",
      "Iteration 03805: loss = 4.906036e-07,1.1459656e-08\n",
      "Iteration 03810: loss = 4.8891656e-07,1.14610375e-08\n",
      "Iteration 03815: loss = 4.8707636e-07,1.1462413e-08\n",
      "Iteration 03820: loss = 4.853975e-07,1.146379e-08\n",
      "Iteration 03825: loss = 4.8372317e-07,1.1465167e-08\n",
      "Iteration 03830: loss = 4.819752e-07,1.146654e-08\n",
      "Iteration 03835: loss = 4.802317e-07,1.1467912e-08\n",
      "Iteration 03840: loss = 4.784931e-07,1.1469285e-08\n",
      "Iteration 03845: loss = 4.7683582e-07,1.1470656e-08\n",
      "Iteration 03850: loss = 4.7518336e-07,1.1472026e-08\n",
      "Iteration 03855: loss = 4.734578e-07,1.1473395e-08\n",
      "Iteration 03860: loss = 4.7181416e-07,1.14747625e-08\n",
      "Iteration 03865: loss = 4.7009758e-07,1.1476127e-08\n",
      "Iteration 03870: loss = 4.6838542e-07,1.14774945e-08\n",
      "Iteration 03875: loss = 4.667545e-07,1.1478857e-08\n",
      "Iteration 03880: loss = 4.6512773e-07,1.1480219e-08\n",
      "Iteration 03885: loss = 4.6346747e-07,1.1481583e-08\n",
      "Iteration 03890: loss = 4.6184883e-07,1.1482943e-08\n",
      "Iteration 03895: loss = 4.602347e-07,1.1484303e-08\n",
      "Iteration 03900: loss = 4.586246e-07,1.148566e-08\n",
      "Iteration 03905: loss = 4.5701873e-07,1.14870184e-08\n",
      "Iteration 03910: loss = 4.5534145e-07,1.1488374e-08\n",
      "Iteration 03915: loss = 4.5374395e-07,1.1489729e-08\n",
      "Iteration 03920: loss = 4.5215089e-07,1.1491083e-08\n",
      "Iteration 03925: loss = 4.5063697e-07,1.14924354e-08\n",
      "Iteration 03930: loss = 4.4905156e-07,1.1493788e-08\n",
      "Iteration 03935: loss = 4.4747017e-07,1.14951355e-08\n",
      "Iteration 03940: loss = 4.4589356e-07,1.14964855e-08\n",
      "Iteration 03945: loss = 4.4432034e-07,1.1497835e-08\n",
      "Iteration 03950: loss = 4.4275112e-07,1.149918e-08\n",
      "Iteration 03955: loss = 4.4118647e-07,1.1500528e-08\n",
      "Iteration 03960: loss = 4.3962552e-07,1.1501874e-08\n",
      "Iteration 03965: loss = 4.3806813e-07,1.1503216e-08\n",
      "Iteration 03970: loss = 4.3658943e-07,1.150456e-08\n",
      "Iteration 03975: loss = 4.3504022e-07,1.15059e-08\n",
      "Iteration 03980: loss = 4.3353202e-07,1.1507241e-08\n",
      "Iteration 03985: loss = 4.3206435e-07,1.1508581e-08\n",
      "Iteration 03990: loss = 4.3052697e-07,1.150992e-08\n",
      "Iteration 03995: loss = 4.290672e-07,1.1511258e-08\n",
      "Iteration 04000: loss = 4.2761062e-07,1.15125935e-08\n",
      "Iteration 04005: loss = 4.2608485e-07,1.15139285e-08\n",
      "Iteration 04010: loss = 4.246361e-07,1.1515262e-08\n",
      "Iteration 04015: loss = 4.2311777e-07,1.1516595e-08\n",
      "Iteration 04020: loss = 4.2167702e-07,1.1517928e-08\n",
      "Iteration 04025: loss = 4.2016623e-07,1.15192575e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04030: loss = 4.1873264e-07,1.1520587e-08\n",
      "Iteration 04035: loss = 4.1730232e-07,1.1521916e-08\n",
      "Iteration 04040: loss = 4.158758e-07,1.1523243e-08\n",
      "Iteration 04045: loss = 4.1438045e-07,1.1524568e-08\n",
      "Iteration 04050: loss = 4.1296107e-07,1.1525895e-08\n",
      "Iteration 04055: loss = 4.1147322e-07,1.1527218e-08\n",
      "Iteration 04060: loss = 4.1013251e-07,1.1528543e-08\n",
      "Iteration 04065: loss = 4.0872374e-07,1.1529865e-08\n",
      "Iteration 04070: loss = 4.0724674e-07,1.1531188e-08\n",
      "Iteration 04075: loss = 4.05917e-07,1.1532506e-08\n",
      "Iteration 04080: loss = 4.0451883e-07,1.1533828e-08\n",
      "Iteration 04085: loss = 4.0308836e-07,1.1535144e-08\n",
      "Iteration 04090: loss = 4.016973e-07,1.1536464e-08\n",
      "Iteration 04095: loss = 4.0038046e-07,1.15377805e-08\n",
      "Iteration 04100: loss = 3.9899612e-07,1.1539094e-08\n",
      "Iteration 04105: loss = 3.9761503e-07,1.154041e-08\n",
      "Iteration 04110: loss = 3.9623774e-07,1.1541725e-08\n",
      "Iteration 04115: loss = 3.9486386e-07,1.1543037e-08\n",
      "Iteration 04120: loss = 3.9356405e-07,1.1544349e-08\n",
      "Iteration 04125: loss = 3.9219685e-07,1.1545659e-08\n",
      "Iteration 04130: loss = 3.9083318e-07,1.1546967e-08\n",
      "Iteration 04135: loss = 3.894726e-07,1.1548277e-08\n",
      "Iteration 04140: loss = 3.8811595e-07,1.1549583e-08\n",
      "Iteration 04145: loss = 3.8683152e-07,1.1550892e-08\n",
      "Iteration 04150: loss = 3.8548157e-07,1.1552197e-08\n",
      "Iteration 04155: loss = 3.8420401e-07,1.1553502e-08\n",
      "Iteration 04160: loss = 3.8286058e-07,1.1554805e-08\n",
      "Iteration 04165: loss = 3.8158956e-07,1.1556107e-08\n",
      "Iteration 04170: loss = 3.8025271e-07,1.1557411e-08\n",
      "Iteration 04175: loss = 3.7898818e-07,1.15587095e-08\n",
      "Iteration 04180: loss = 3.777269e-07,1.1560011e-08\n",
      "Iteration 04185: loss = 3.7639958e-07,1.1561308e-08\n",
      "Iteration 04190: loss = 3.751439e-07,1.1562606e-08\n",
      "Iteration 04195: loss = 3.738579e-07,1.1563905e-08\n",
      "Iteration 04200: loss = 3.7260864e-07,1.1565202e-08\n",
      "Iteration 04205: loss = 3.7136283e-07,1.1566497e-08\n",
      "Iteration 04210: loss = 3.700517e-07,1.156779e-08\n",
      "Iteration 04215: loss = 3.6881207e-07,1.1569082e-08\n",
      "Iteration 04220: loss = 3.67643e-07,1.1570375e-08\n",
      "Iteration 04225: loss = 3.663417e-07,1.1571665e-08\n",
      "Iteration 04230: loss = 3.6511088e-07,1.1572956e-08\n",
      "Iteration 04235: loss = 3.638834e-07,1.1574246e-08\n",
      "Iteration 04240: loss = 3.62591e-07,1.1575533e-08\n",
      "Iteration 04245: loss = 3.614372e-07,1.1576821e-08\n",
      "Iteration 04250: loss = 3.6021856e-07,1.15781065e-08\n",
      "Iteration 04255: loss = 3.5893595e-07,1.15793934e-08\n",
      "Iteration 04260: loss = 3.5772345e-07,1.15806795e-08\n",
      "Iteration 04265: loss = 3.5658073e-07,1.1581963e-08\n",
      "Iteration 04270: loss = 3.5537388e-07,1.15832455e-08\n",
      "Iteration 04275: loss = 3.5417023e-07,1.1584525e-08\n",
      "Iteration 04280: loss = 3.5296955e-07,1.1585807e-08\n",
      "Iteration 04285: loss = 3.5177163e-07,1.1587089e-08\n",
      "Iteration 04290: loss = 3.5057687e-07,1.1588366e-08\n",
      "Iteration 04295: loss = 3.4945117e-07,1.1589643e-08\n",
      "Iteration 04300: loss = 3.4826175e-07,1.1590924e-08\n",
      "Iteration 04305: loss = 3.471417e-07,1.1592199e-08\n",
      "Iteration 04310: loss = 3.4589212e-07,1.1593474e-08\n",
      "Iteration 04315: loss = 3.4471157e-07,1.1594748e-08\n",
      "Iteration 04320: loss = 3.4363265e-07,1.1596023e-08\n",
      "Iteration 04325: loss = 3.4245787e-07,1.1597296e-08\n",
      "Iteration 04330: loss = 3.4135135e-07,1.1598569e-08\n",
      "Iteration 04335: loss = 3.4018183e-07,1.159984e-08\n",
      "Iteration 04340: loss = 3.3908054e-07,1.1601112e-08\n",
      "Iteration 04345: loss = 3.3791687e-07,1.16023795e-08\n",
      "Iteration 04350: loss = 3.368208e-07,1.1603649e-08\n",
      "Iteration 04355: loss = 3.3566297e-07,1.1604915e-08\n",
      "Iteration 04360: loss = 3.3457232e-07,1.1606183e-08\n",
      "Iteration 04365: loss = 3.334844e-07,1.16074474e-08\n",
      "Iteration 04370: loss = 3.3233474e-07,1.1608716e-08\n",
      "Iteration 04375: loss = 3.3125204e-07,1.1609978e-08\n",
      "Iteration 04380: loss = 3.301076e-07,1.1611242e-08\n",
      "Iteration 04385: loss = 3.2903014e-07,1.1612503e-08\n",
      "Iteration 04390: loss = 3.2795546e-07,1.1613766e-08\n",
      "Iteration 04395: loss = 3.2681905e-07,1.1615026e-08\n",
      "Iteration 04400: loss = 3.2574948e-07,1.1616286e-08\n",
      "Iteration 04405: loss = 3.2468267e-07,1.1617547e-08\n",
      "Iteration 04410: loss = 3.2355467e-07,1.1618804e-08\n",
      "Iteration 04415: loss = 3.2249292e-07,1.1620062e-08\n",
      "Iteration 04420: loss = 3.214334e-07,1.1621317e-08\n",
      "Iteration 04425: loss = 3.2044025e-07,1.1622575e-08\n",
      "Iteration 04430: loss = 3.1932254e-07,1.162383e-08\n",
      "Iteration 04435: loss = 3.1827082e-07,1.1625083e-08\n",
      "Iteration 04440: loss = 3.1722143e-07,1.1626337e-08\n",
      "Iteration 04445: loss = 3.1617492e-07,1.1627588e-08\n",
      "Iteration 04450: loss = 3.1513082e-07,1.1628837e-08\n",
      "Iteration 04455: loss = 3.1405781e-07,1.1630091e-08\n",
      "Iteration 04460: loss = 3.1308159e-07,1.163134e-08\n",
      "Iteration 04465: loss = 3.1204502e-07,1.1632587e-08\n",
      "Iteration 04470: loss = 3.1101058e-07,1.1633837e-08\n",
      "Iteration 04475: loss = 3.0997856e-07,1.1635083e-08\n",
      "Iteration 04480: loss = 3.0901188e-07,1.1636331e-08\n",
      "Iteration 04485: loss = 3.0798455e-07,1.16375745e-08\n",
      "Iteration 04490: loss = 3.069603e-07,1.16388215e-08\n",
      "Iteration 04495: loss = 3.0587626e-07,1.1640064e-08\n",
      "Iteration 04500: loss = 3.0491893e-07,1.1641307e-08\n",
      "Iteration 04505: loss = 3.039014e-07,1.164255e-08\n",
      "Iteration 04510: loss = 3.0288663e-07,1.1643792e-08\n",
      "Iteration 04515: loss = 3.019358e-07,1.164503e-08\n",
      "Iteration 04520: loss = 3.00926e-07,1.1646271e-08\n",
      "Iteration 04525: loss = 2.9991836e-07,1.1647507e-08\n",
      "Iteration 04530: loss = 2.9897458e-07,1.1648744e-08\n",
      "Iteration 04535: loss = 2.9797147e-07,1.1649982e-08\n",
      "Iteration 04540: loss = 2.9703213e-07,1.1651217e-08\n",
      "Iteration 04545: loss = 2.9609512e-07,1.1652453e-08\n",
      "Iteration 04550: loss = 2.9509889e-07,1.1653691e-08\n",
      "Iteration 04555: loss = 2.9410538e-07,1.1654923e-08\n",
      "Iteration 04560: loss = 2.9317476e-07,1.1656157e-08\n",
      "Iteration 04565: loss = 2.9218586e-07,1.16573915e-08\n",
      "Iteration 04570: loss = 2.9125957e-07,1.16586225e-08\n",
      "Iteration 04575: loss = 2.9027524e-07,1.1659854e-08\n",
      "Iteration 04580: loss = 2.8935347e-07,1.1661083e-08\n",
      "Iteration 04585: loss = 2.8843374e-07,1.1662314e-08\n",
      "Iteration 04590: loss = 2.8745612e-07,1.1663541e-08\n",
      "Iteration 04595: loss = 2.8654105e-07,1.166477e-08\n",
      "Iteration 04600: loss = 2.855676e-07,1.1665998e-08\n",
      "Iteration 04605: loss = 2.846567e-07,1.1667223e-08\n",
      "Iteration 04610: loss = 2.8377764e-07,1.166845e-08\n",
      "Iteration 04615: loss = 2.8287096e-07,1.1669674e-08\n",
      "Iteration 04620: loss = 2.8190672e-07,1.1670899e-08\n",
      "Iteration 04625: loss = 2.8100433e-07,1.1672121e-08\n",
      "Iteration 04630: loss = 2.8010376e-07,1.1673344e-08\n",
      "Iteration 04635: loss = 2.7920555e-07,1.1674566e-08\n",
      "Iteration 04640: loss = 2.7830927e-07,1.1675789e-08\n",
      "Iteration 04645: loss = 2.7741513e-07,1.1677007e-08\n",
      "Iteration 04650: loss = 2.7652288e-07,1.1678229e-08\n",
      "Iteration 04655: loss = 2.7557397e-07,1.1679447e-08\n",
      "Iteration 04660: loss = 2.747448e-07,1.1680664e-08\n",
      "Iteration 04665: loss = 2.7380045e-07,1.1681883e-08\n",
      "Iteration 04670: loss = 2.7291665e-07,1.1683099e-08\n",
      "Iteration 04675: loss = 2.7209333e-07,1.1684314e-08\n",
      "Iteration 04680: loss = 2.7115522e-07,1.1685531e-08\n",
      "Iteration 04685: loss = 2.7033593e-07,1.1686746e-08\n",
      "Iteration 04690: loss = 2.694604e-07,1.1687959e-08\n",
      "Iteration 04695: loss = 2.6852862e-07,1.16891705e-08\n",
      "Iteration 04700: loss = 2.6771517e-07,1.1690383e-08\n",
      "Iteration 04705: loss = 2.6684543e-07,1.1691596e-08\n",
      "Iteration 04710: loss = 2.659779e-07,1.1692806e-08\n",
      "Iteration 04715: loss = 2.6511233e-07,1.1694016e-08\n",
      "Iteration 04720: loss = 2.643064e-07,1.1695224e-08\n",
      "Iteration 04725: loss = 2.6338748e-07,1.1696435e-08\n",
      "Iteration 04730: loss = 2.6258533e-07,1.169764e-08\n",
      "Iteration 04735: loss = 2.617276e-07,1.1698848e-08\n",
      "Iteration 04740: loss = 2.6092908e-07,1.1700054e-08\n",
      "Iteration 04745: loss = 2.6001803e-07,1.17012595e-08\n",
      "Iteration 04750: loss = 2.592237e-07,1.1702464e-08\n",
      "Iteration 04755: loss = 2.5837377e-07,1.17036665e-08\n",
      "Iteration 04760: loss = 2.5758277e-07,1.1704872e-08\n",
      "Iteration 04765: loss = 2.5673683e-07,1.1706075e-08\n",
      "Iteration 04770: loss = 2.558927e-07,1.1707276e-08\n",
      "Iteration 04775: loss = 2.5510735e-07,1.1708473e-08\n",
      "Iteration 04780: loss = 2.5429554e-07,1.1709678e-08\n",
      "Iteration 04785: loss = 2.535135e-07,1.17108785e-08\n",
      "Iteration 04790: loss = 2.5267704e-07,1.1712077e-08\n",
      "Iteration 04795: loss = 2.518427e-07,1.1713274e-08\n",
      "Iteration 04800: loss = 2.5106638e-07,1.1714475e-08\n",
      "Iteration 04805: loss = 2.5029172e-07,1.1715669e-08\n",
      "Iteration 04810: loss = 2.4946272e-07,1.17168675e-08\n",
      "Iteration 04815: loss = 2.4869158e-07,1.1718062e-08\n",
      "Iteration 04820: loss = 2.4786672e-07,1.1719256e-08\n",
      "Iteration 04825: loss = 2.47099e-07,1.17204495e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04830: loss = 2.4627758e-07,1.1721645e-08\n",
      "Iteration 04835: loss = 2.4551358e-07,1.1722838e-08\n",
      "Iteration 04840: loss = 2.447514e-07,1.172403e-08\n",
      "Iteration 04845: loss = 2.4399083e-07,1.172522e-08\n",
      "Iteration 04850: loss = 2.431765e-07,1.1726414e-08\n",
      "Iteration 04855: loss = 2.4241965e-07,1.1727602e-08\n",
      "Iteration 04860: loss = 2.4166425e-07,1.1728792e-08\n",
      "Iteration 04865: loss = 2.4091057e-07,1.1729979e-08\n",
      "Iteration 04870: loss = 2.4015858e-07,1.1731168e-08\n",
      "Iteration 04875: loss = 2.3935362e-07,1.1732354e-08\n",
      "Iteration 04880: loss = 2.386051e-07,1.1733539e-08\n",
      "Iteration 04885: loss = 2.3785856e-07,1.1734723e-08\n",
      "Iteration 04890: loss = 2.371133e-07,1.173591e-08\n",
      "Iteration 04895: loss = 2.3637006e-07,1.1737095e-08\n",
      "Iteration 04900: loss = 2.3562815e-07,1.173828e-08\n",
      "Iteration 04905: loss = 2.3488805e-07,1.1739462e-08\n",
      "Iteration 04910: loss = 2.3414971e-07,1.1740646e-08\n",
      "Iteration 04915: loss = 2.3335878e-07,1.1741826e-08\n",
      "Iteration 04920: loss = 2.3262366e-07,1.17430075e-08\n",
      "Iteration 04925: loss = 2.3189055e-07,1.1744187e-08\n",
      "Iteration 04930: loss = 2.3115872e-07,1.1745369e-08\n",
      "Iteration 04935: loss = 2.3042855e-07,1.1746547e-08\n",
      "Iteration 04940: loss = 2.2970023e-07,1.17477255e-08\n",
      "Iteration 04945: loss = 2.2897329e-07,1.1748902e-08\n",
      "Iteration 04950: loss = 2.2824831e-07,1.1750083e-08\n",
      "Iteration 04955: loss = 2.2752472e-07,1.175126e-08\n",
      "Iteration 04960: loss = 2.2680297e-07,1.1752434e-08\n",
      "Iteration 04965: loss = 2.2608259e-07,1.175361e-08\n",
      "Iteration 04970: loss = 2.2536399e-07,1.1754785e-08\n",
      "Iteration 04975: loss = 2.2464707e-07,1.1755958e-08\n",
      "Iteration 04980: loss = 2.24011e-07,1.1757132e-08\n",
      "Iteration 04985: loss = 2.232973e-07,1.17583046e-08\n",
      "Iteration 04990: loss = 2.2258513e-07,1.1759477e-08\n",
      "Iteration 04995: loss = 2.2187453e-07,1.17606485e-08\n",
      "Iteration 05000: loss = 2.2121795e-07,1.1761819e-08\n",
      "Iteration 05005: loss = 2.205105e-07,1.1762991e-08\n",
      "Iteration 05010: loss = 2.1985709e-07,1.1764158e-08\n",
      "Iteration 05015: loss = 2.1915257e-07,1.1765328e-08\n",
      "Iteration 05020: loss = 2.1844943e-07,1.1766495e-08\n",
      "Iteration 05025: loss = 2.1774815e-07,1.1767664e-08\n",
      "Iteration 05030: loss = 2.1704868e-07,1.176883e-08\n",
      "Iteration 05035: loss = 2.1640271e-07,1.1769997e-08\n",
      "Iteration 05040: loss = 2.157581e-07,1.1771162e-08\n",
      "Iteration 05045: loss = 2.1506271e-07,1.17723244e-08\n",
      "Iteration 05050: loss = 2.143692e-07,1.1773492e-08\n",
      "Iteration 05055: loss = 2.1367715e-07,1.1774656e-08\n",
      "Iteration 05060: loss = 2.1303849e-07,1.1775818e-08\n",
      "Iteration 05065: loss = 2.1240113e-07,1.1776981e-08\n",
      "Iteration 05070: loss = 2.1171336e-07,1.1778144e-08\n",
      "Iteration 05075: loss = 2.1102738e-07,1.1779305e-08\n",
      "Iteration 05080: loss = 2.1039448e-07,1.1780465e-08\n",
      "Iteration 05085: loss = 2.0971133e-07,1.1781624e-08\n",
      "Iteration 05090: loss = 2.0908136e-07,1.1782784e-08\n",
      "Iteration 05095: loss = 2.0845242e-07,1.1783941e-08\n",
      "Iteration 05100: loss = 2.077739e-07,1.1785101e-08\n",
      "Iteration 05105: loss = 2.0714806e-07,1.1786257e-08\n",
      "Iteration 05110: loss = 2.0652347e-07,1.1787415e-08\n",
      "Iteration 05115: loss = 2.0584929e-07,1.1788572e-08\n",
      "Iteration 05120: loss = 2.0517666e-07,1.1789725e-08\n",
      "Iteration 05125: loss = 2.046072e-07,1.1790883e-08\n",
      "Iteration 05130: loss = 2.039374e-07,1.1792038e-08\n",
      "Iteration 05135: loss = 2.0326911e-07,1.17931895e-08\n",
      "Iteration 05140: loss = 2.0270352e-07,1.1794341e-08\n",
      "Iteration 05145: loss = 2.0203817e-07,1.1795494e-08\n",
      "Iteration 05150: loss = 2.0142461e-07,1.1796647e-08\n",
      "Iteration 05155: loss = 2.0081234e-07,1.1797798e-08\n",
      "Iteration 05160: loss = 2.001514e-07,1.17989485e-08\n",
      "Iteration 05165: loss = 1.9959185e-07,1.1800098e-08\n",
      "Iteration 05170: loss = 1.9893393e-07,1.1801248e-08\n",
      "Iteration 05175: loss = 1.9832692e-07,1.1802396e-08\n",
      "Iteration 05180: loss = 1.9772148e-07,1.1803542e-08\n",
      "Iteration 05185: loss = 1.9706744e-07,1.1804691e-08\n",
      "Iteration 05190: loss = 1.9651465e-07,1.180584e-08\n",
      "Iteration 05195: loss = 1.9586345e-07,1.18069865e-08\n",
      "Iteration 05200: loss = 1.9531285e-07,1.180813e-08\n",
      "Iteration 05205: loss = 1.9466471e-07,1.1809279e-08\n",
      "Iteration 05210: loss = 1.9409181e-07,1.1810425e-08\n",
      "Iteration 05215: loss = 1.9349558e-07,1.1811567e-08\n",
      "Iteration 05220: loss = 1.9290083e-07,1.181271e-08\n",
      "Iteration 05225: loss = 1.9230696e-07,1.1813853e-08\n",
      "Iteration 05230: loss = 1.9171456e-07,1.1814998e-08\n",
      "Iteration 05235: loss = 1.9112368e-07,1.1816139e-08\n",
      "Iteration 05240: loss = 1.905339e-07,1.1817278e-08\n",
      "Iteration 05245: loss = 1.899456e-07,1.1818422e-08\n",
      "Iteration 05250: loss = 1.8935818e-07,1.181956e-08\n",
      "Iteration 05255: loss = 1.887724e-07,1.1820702e-08\n",
      "Iteration 05260: loss = 1.8818791e-07,1.18218395e-08\n",
      "Iteration 05265: loss = 1.8760441e-07,1.182298e-08\n",
      "Iteration 05270: loss = 1.8702252e-07,1.18241195e-08\n",
      "Iteration 05275: loss = 1.864902e-07,1.18252546e-08\n",
      "Iteration 05280: loss = 1.8591068e-07,1.1826392e-08\n",
      "Iteration 05285: loss = 1.8533218e-07,1.1827528e-08\n",
      "Iteration 05290: loss = 1.8475508e-07,1.1828664e-08\n",
      "Iteration 05295: loss = 1.841792e-07,1.18297985e-08\n",
      "Iteration 05300: loss = 1.8365292e-07,1.18309345e-08\n",
      "Iteration 05305: loss = 1.8303147e-07,1.1832069e-08\n",
      "Iteration 05310: loss = 1.8250728e-07,1.1833202e-08\n",
      "Iteration 05315: loss = 1.8198428e-07,1.1834336e-08\n",
      "Iteration 05320: loss = 1.813669e-07,1.1835467e-08\n",
      "Iteration 05325: loss = 1.8084602e-07,1.18365975e-08\n",
      "Iteration 05330: loss = 1.80279e-07,1.1837731e-08\n",
      "Iteration 05335: loss = 1.7971296e-07,1.18388614e-08\n",
      "Iteration 05340: loss = 1.7919594e-07,1.183999e-08\n",
      "Iteration 05345: loss = 1.7863238e-07,1.1841123e-08\n",
      "Iteration 05350: loss = 1.7807008e-07,1.184225e-08\n",
      "Iteration 05355: loss = 1.7755605e-07,1.18433805e-08\n",
      "Iteration 05360: loss = 1.7699615e-07,1.1844508e-08\n",
      "Iteration 05365: loss = 1.7648465e-07,1.1845637e-08\n",
      "Iteration 05370: loss = 1.7592693e-07,1.1846764e-08\n",
      "Iteration 05375: loss = 1.7537063e-07,1.1847891e-08\n",
      "Iteration 05380: loss = 1.7486246e-07,1.1849018e-08\n",
      "Iteration 05385: loss = 1.7435514e-07,1.1850143e-08\n",
      "Iteration 05390: loss = 1.7375555e-07,1.1851266e-08\n",
      "Iteration 05395: loss = 1.7325065e-07,1.1852389e-08\n",
      "Iteration 05400: loss = 1.72747e-07,1.1853513e-08\n",
      "Iteration 05405: loss = 1.7219782e-07,1.1854638e-08\n",
      "Iteration 05410: loss = 1.7169616e-07,1.1855758e-08\n",
      "Iteration 05415: loss = 1.7114913e-07,1.1856881e-08\n",
      "Iteration 05420: loss = 1.7064987e-07,1.1858003e-08\n",
      "Iteration 05425: loss = 1.7010511e-07,1.1859127e-08\n",
      "Iteration 05430: loss = 1.6960796e-07,1.1860243e-08\n",
      "Iteration 05435: loss = 1.6911169e-07,1.1861363e-08\n",
      "Iteration 05440: loss = 1.6861657e-07,1.1862482e-08\n",
      "Iteration 05445: loss = 1.6803064e-07,1.1863599e-08\n",
      "Iteration 05450: loss = 1.6753769e-07,1.186472e-08\n",
      "Iteration 05455: loss = 1.6704577e-07,1.18658345e-08\n",
      "Iteration 05460: loss = 1.6655495e-07,1.1866953e-08\n",
      "Iteration 05465: loss = 1.660195e-07,1.186807e-08\n",
      "Iteration 05470: loss = 1.6553075e-07,1.1869186e-08\n",
      "Iteration 05475: loss = 1.6504322e-07,1.1870302e-08\n",
      "Iteration 05480: loss = 1.6455643e-07,1.1871414e-08\n",
      "Iteration 05485: loss = 1.6409359e-07,1.1872528e-08\n",
      "Iteration 05490: loss = 1.6351834e-07,1.18736425e-08\n",
      "Iteration 05495: loss = 1.6303484e-07,1.1874757e-08\n",
      "Iteration 05500: loss = 1.6255255e-07,1.187587e-08\n",
      "Iteration 05505: loss = 1.6207083e-07,1.1876979e-08\n",
      "Iteration 05510: loss = 1.6159053e-07,1.1878093e-08\n",
      "Iteration 05515: loss = 1.6111113e-07,1.1879204e-08\n",
      "Iteration 05520: loss = 1.6058772e-07,1.18803145e-08\n",
      "Iteration 05525: loss = 1.6011052e-07,1.1881426e-08\n",
      "Iteration 05530: loss = 1.5963415e-07,1.1882535e-08\n",
      "Iteration 05535: loss = 1.5915889e-07,1.18836425e-08\n",
      "Iteration 05540: loss = 1.5868456e-07,1.1884751e-08\n",
      "Iteration 05545: loss = 1.5821132e-07,1.1885859e-08\n",
      "Iteration 05550: loss = 1.5773884e-07,1.1886968e-08\n",
      "Iteration 05555: loss = 1.5726772e-07,1.1888076e-08\n",
      "Iteration 05560: loss = 1.5675303e-07,1.1889182e-08\n",
      "Iteration 05565: loss = 1.5628387e-07,1.189029e-08\n",
      "Iteration 05570: loss = 1.5581558e-07,1.1891396e-08\n",
      "Iteration 05575: loss = 1.5534825e-07,1.18925e-08\n",
      "Iteration 05580: loss = 1.5488196e-07,1.1893604e-08\n",
      "Iteration 05585: loss = 1.5446078e-07,1.1894709e-08\n",
      "Iteration 05590: loss = 1.5399638e-07,1.1895811e-08\n",
      "Iteration 05595: loss = 1.5353301e-07,1.18969155e-08\n",
      "Iteration 05600: loss = 1.5307062e-07,1.1898017e-08\n",
      "Iteration 05605: loss = 1.5260919e-07,1.189912e-08\n",
      "Iteration 05610: loss = 1.5214879e-07,1.190022e-08\n",
      "Iteration 05615: loss = 1.5168916e-07,1.1901323e-08\n",
      "Iteration 05620: loss = 1.512309e-07,1.1902423e-08\n",
      "Iteration 05625: loss = 1.5072969e-07,1.19035235e-08\n",
      "Iteration 05630: loss = 1.5031681e-07,1.1904625e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 05635: loss = 1.4986126e-07,1.1905723e-08\n",
      "Iteration 05640: loss = 1.4940663e-07,1.1906821e-08\n",
      "Iteration 05645: loss = 1.4895298e-07,1.190792e-08\n",
      "Iteration 05650: loss = 1.4850018e-07,1.19090195e-08\n",
      "Iteration 05655: loss = 1.4804849e-07,1.1910116e-08\n",
      "Iteration 05660: loss = 1.4759794e-07,1.1911214e-08\n",
      "Iteration 05665: loss = 1.4719114e-07,1.1912309e-08\n",
      "Iteration 05670: loss = 1.4674204e-07,1.19134045e-08\n",
      "Iteration 05675: loss = 1.4629411e-07,1.1914501e-08\n",
      "Iteration 05680: loss = 1.4584727e-07,1.19155965e-08\n",
      "Iteration 05685: loss = 1.4540119e-07,1.1916692e-08\n",
      "Iteration 05690: loss = 1.450414e-07,1.1917784e-08\n",
      "Iteration 05695: loss = 1.4459711e-07,1.1918879e-08\n",
      "Iteration 05700: loss = 1.4415387e-07,1.191997e-08\n",
      "Iteration 05705: loss = 1.4371143e-07,1.19210615e-08\n",
      "Iteration 05710: loss = 1.4331228e-07,1.1922155e-08\n",
      "Iteration 05715: loss = 1.4287176e-07,1.19232455e-08\n",
      "Iteration 05720: loss = 1.4243194e-07,1.1924337e-08\n",
      "Iteration 05725: loss = 1.4199331e-07,1.1925428e-08\n",
      "Iteration 05730: loss = 1.4159764e-07,1.1926518e-08\n",
      "Iteration 05735: loss = 1.411608e-07,1.1927607e-08\n",
      "Iteration 05740: loss = 1.4072465e-07,1.1928694e-08\n",
      "Iteration 05745: loss = 1.4033178e-07,1.1929785e-08\n",
      "Iteration 05750: loss = 1.3989752e-07,1.19308705e-08\n",
      "Iteration 05755: loss = 1.3946429e-07,1.19319585e-08\n",
      "Iteration 05760: loss = 1.3907375e-07,1.1933045e-08\n",
      "Iteration 05765: loss = 1.3868399e-07,1.1934131e-08\n",
      "Iteration 05770: loss = 1.382534e-07,1.1935217e-08\n",
      "Iteration 05775: loss = 1.3782362e-07,1.1936302e-08\n",
      "Iteration 05780: loss = 1.3743634e-07,1.1937388e-08\n",
      "Iteration 05785: loss = 1.370083e-07,1.193847e-08\n",
      "Iteration 05790: loss = 1.3658139e-07,1.1939555e-08\n",
      "Iteration 05795: loss = 1.3619665e-07,1.1940639e-08\n",
      "Iteration 05800: loss = 1.3581273e-07,1.1941722e-08\n",
      "Iteration 05805: loss = 1.3542942e-07,1.1942803e-08\n",
      "Iteration 05810: loss = 1.350059e-07,1.1943886e-08\n",
      "Iteration 05815: loss = 1.3460365e-07,1.1944967e-08\n",
      "Iteration 05820: loss = 1.3422289e-07,1.19460495e-08\n",
      "Iteration 05825: loss = 1.3380188e-07,1.19471295e-08\n",
      "Iteration 05830: loss = 1.3346379e-07,1.19482095e-08\n",
      "Iteration 05835: loss = 1.330444e-07,1.1949291e-08\n",
      "Iteration 05840: loss = 1.3262593e-07,1.195037e-08\n",
      "Iteration 05845: loss = 1.3224917e-07,1.195145e-08\n",
      "Iteration 05850: loss = 1.3183251e-07,1.1952528e-08\n",
      "Iteration 05855: loss = 1.3149779e-07,1.1953606e-08\n",
      "Iteration 05860: loss = 1.3108287e-07,1.1954681e-08\n",
      "Iteration 05865: loss = 1.3070907e-07,1.1955758e-08\n",
      "Iteration 05870: loss = 1.30296e-07,1.1956836e-08\n",
      "Iteration 05875: loss = 1.2988346e-07,1.1957912e-08\n",
      "Iteration 05880: loss = 1.2955243e-07,1.1958988e-08\n",
      "Iteration 05885: loss = 1.2914148e-07,1.1960065e-08\n",
      "Iteration 05890: loss = 1.2877163e-07,1.1961138e-08\n",
      "Iteration 05895: loss = 1.2836264e-07,1.1962213e-08\n",
      "Iteration 05900: loss = 1.2803432e-07,1.1963286e-08\n",
      "Iteration 05905: loss = 1.2762699e-07,1.1964362e-08\n",
      "Iteration 05910: loss = 1.2726025e-07,1.1965433e-08\n",
      "Iteration 05915: loss = 1.2685452e-07,1.1966508e-08\n",
      "Iteration 05920: loss = 1.2652909e-07,1.1967578e-08\n",
      "Iteration 05925: loss = 1.2612485e-07,1.1968651e-08\n",
      "Iteration 05930: loss = 1.2576137e-07,1.1969722e-08\n",
      "Iteration 05935: loss = 1.2535864e-07,1.1970794e-08\n",
      "Iteration 05940: loss = 1.2503617e-07,1.1971864e-08\n",
      "Iteration 05945: loss = 1.2467483e-07,1.1972936e-08\n",
      "Iteration 05950: loss = 1.2427445e-07,1.1974006e-08\n",
      "Iteration 05955: loss = 1.2395405e-07,1.1975076e-08\n",
      "Iteration 05960: loss = 1.2355548e-07,1.1976143e-08\n",
      "Iteration 05965: loss = 1.2319705e-07,1.1977212e-08\n",
      "Iteration 05970: loss = 1.2283931e-07,1.197828e-08\n",
      "Iteration 05975: loss = 1.224824e-07,1.1979347e-08\n",
      "Iteration 05980: loss = 1.221262e-07,1.1980415e-08\n",
      "Iteration 05985: loss = 1.217708e-07,1.1981479e-08\n",
      "Iteration 05990: loss = 1.2141604e-07,1.1982546e-08\n",
      "Iteration 05995: loss = 1.2102298e-07,1.1983611e-08\n",
      "Iteration 06000: loss = 1.2070863e-07,1.1984677e-08\n",
      "Iteration 06005: loss = 1.203562e-07,1.1985743e-08\n",
      "Iteration 06010: loss = 1.1996539e-07,1.1986806e-08\n",
      "Iteration 06015: loss = 1.1965314e-07,1.198787e-08\n",
      "Iteration 06020: loss = 1.1930273e-07,1.1988933e-08\n",
      "Iteration 06025: loss = 1.1891428e-07,1.19899966e-08\n",
      "Iteration 06030: loss = 1.1860405e-07,1.1991059e-08\n",
      "Iteration 06035: loss = 1.1821718e-07,1.1992121e-08\n",
      "Iteration 06040: loss = 1.17869725e-07,1.1993182e-08\n",
      "Iteration 06045: loss = 1.17561456e-07,1.1994241e-08\n",
      "Iteration 06050: loss = 1.17176945e-07,1.1995306e-08\n",
      "Iteration 06055: loss = 1.16869785e-07,1.1996366e-08\n",
      "Iteration 06060: loss = 1.1652512e-07,1.1997425e-08\n",
      "Iteration 06065: loss = 1.1618108e-07,1.1998485e-08\n",
      "Iteration 06070: loss = 1.1583782e-07,1.1999545e-08\n",
      "Iteration 06075: loss = 1.15495254e-07,1.2000602e-08\n",
      "Iteration 06080: loss = 1.1519135e-07,1.2001658e-08\n",
      "Iteration 06085: loss = 1.148122e-07,1.2002719e-08\n",
      "Iteration 06090: loss = 1.1447165e-07,1.2003777e-08\n",
      "Iteration 06095: loss = 1.1416976e-07,1.2004834e-08\n",
      "Iteration 06100: loss = 1.1383073e-07,1.2005889e-08\n",
      "Iteration 06105: loss = 1.1349213e-07,1.2006945e-08\n",
      "Iteration 06110: loss = 1.13154464e-07,1.2008002e-08\n",
      "Iteration 06115: loss = 1.1285519e-07,1.2009057e-08\n",
      "Iteration 06120: loss = 1.1251877e-07,1.201011e-08\n",
      "Iteration 06125: loss = 1.1218295e-07,1.2011167e-08\n",
      "Iteration 06130: loss = 1.11847974e-07,1.2012221e-08\n",
      "Iteration 06135: loss = 1.1151359e-07,1.2013276e-08\n",
      "Iteration 06140: loss = 1.1121734e-07,1.2014326e-08\n",
      "Iteration 06145: loss = 1.10847004e-07,1.2015381e-08\n",
      "Iteration 06150: loss = 1.1055204e-07,1.2016433e-08\n",
      "Iteration 06155: loss = 1.1022039e-07,1.2017486e-08\n",
      "Iteration 06160: loss = 1.0992667e-07,1.20185355e-08\n",
      "Iteration 06165: loss = 1.09596186e-07,1.2019587e-08\n",
      "Iteration 06170: loss = 1.09266516e-07,1.2020638e-08\n",
      "Iteration 06175: loss = 1.0893742e-07,1.2021686e-08\n",
      "Iteration 06180: loss = 1.08646105e-07,1.2022738e-08\n",
      "Iteration 06185: loss = 1.0831847e-07,1.2023787e-08\n",
      "Iteration 06190: loss = 1.08028246e-07,1.2024836e-08\n",
      "Iteration 06195: loss = 1.07664995e-07,1.2025885e-08\n",
      "Iteration 06200: loss = 1.07376195e-07,1.2026933e-08\n",
      "Iteration 06205: loss = 1.0705108e-07,1.2027982e-08\n",
      "Iteration 06210: loss = 1.0676325e-07,1.2029027e-08\n",
      "Iteration 06215: loss = 1.0643958e-07,1.2030074e-08\n",
      "Iteration 06220: loss = 1.0615301e-07,1.2031122e-08\n",
      "Iteration 06225: loss = 1.05830715e-07,1.2032166e-08\n",
      "Iteration 06230: loss = 1.0550887e-07,1.2033212e-08\n",
      "Iteration 06235: loss = 1.05224096e-07,1.2034257e-08\n",
      "Iteration 06240: loss = 1.0490357e-07,1.20353e-08\n",
      "Iteration 06245: loss = 1.0463817e-07,1.2036345e-08\n",
      "Iteration 06250: loss = 1.0431879e-07,1.2037388e-08\n",
      "Iteration 06255: loss = 1.040363e-07,1.2038433e-08\n",
      "Iteration 06260: loss = 1.0371824e-07,1.2039477e-08\n",
      "Iteration 06265: loss = 1.03437e-07,1.2040521e-08\n",
      "Iteration 06270: loss = 1.0312028e-07,1.204156e-08\n",
      "Iteration 06275: loss = 1.0284006e-07,1.2042602e-08\n",
      "Iteration 06280: loss = 1.0256034e-07,1.2043646e-08\n",
      "Iteration 06285: loss = 1.0220964e-07,1.2044685e-08\n",
      "Iteration 06290: loss = 1.01931256e-07,1.2045726e-08\n",
      "Iteration 06295: loss = 1.0161762e-07,1.2046767e-08\n",
      "Iteration 06300: loss = 1.0134039e-07,1.2047806e-08\n",
      "Iteration 06305: loss = 1.01063584e-07,1.2048847e-08\n",
      "Iteration 06310: loss = 1.0075184e-07,1.2049885e-08\n",
      "Iteration 06315: loss = 1.0047631e-07,1.2050924e-08\n",
      "Iteration 06320: loss = 1.0016583e-07,1.2051963e-08\n",
      "Iteration 06325: loss = 9.989119e-08,1.2053002e-08\n",
      "Iteration 06330: loss = 9.9617274e-08,1.2054038e-08\n",
      "Iteration 06335: loss = 9.930858e-08,1.2055076e-08\n",
      "Iteration 06340: loss = 9.9035816e-08,1.2056112e-08\n",
      "Iteration 06345: loss = 9.8763564e-08,1.2057147e-08\n",
      "Iteration 06350: loss = 9.845667e-08,1.2058184e-08\n",
      "Iteration 06355: loss = 9.818561e-08,1.205922e-08\n",
      "Iteration 06360: loss = 9.7914885e-08,1.2060253e-08\n",
      "Iteration 06365: loss = 9.7609814e-08,1.206129e-08\n",
      "Iteration 06370: loss = 9.734041e-08,1.2062322e-08\n",
      "Iteration 06375: loss = 9.7036555e-08,1.20633565e-08\n",
      "Iteration 06380: loss = 9.676807e-08,1.2064391e-08\n",
      "Iteration 06385: loss = 9.650021e-08,1.2065424e-08\n",
      "Iteration 06390: loss = 9.619804e-08,1.2066458e-08\n",
      "Iteration 06395: loss = 9.593138e-08,1.206749e-08\n",
      "Iteration 06400: loss = 9.566511e-08,1.2068522e-08\n",
      "Iteration 06405: loss = 9.539941e-08,1.2069554e-08\n",
      "Iteration 06410: loss = 9.5099615e-08,1.20705845e-08\n",
      "Iteration 06415: loss = 9.486964e-08,1.2071618e-08\n",
      "Iteration 06420: loss = 9.460545e-08,1.2072649e-08\n",
      "Iteration 06425: loss = 9.430746e-08,1.2073675e-08\n",
      "Iteration 06430: loss = 9.404431e-08,1.20747075e-08\n",
      "Iteration 06435: loss = 9.378187e-08,1.2075738e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06440: loss = 9.348556e-08,1.2076766e-08\n",
      "Iteration 06445: loss = 9.3224095e-08,1.2077793e-08\n",
      "Iteration 06450: loss = 9.296317e-08,1.2078824e-08\n",
      "Iteration 06455: loss = 9.270273e-08,1.2079852e-08\n",
      "Iteration 06460: loss = 9.240876e-08,1.2080877e-08\n",
      "Iteration 06465: loss = 9.214935e-08,1.2081907e-08\n",
      "Iteration 06470: loss = 9.192456e-08,1.2082934e-08\n",
      "Iteration 06475: loss = 9.163216e-08,1.2083961e-08\n",
      "Iteration 06480: loss = 9.137444e-08,1.2084986e-08\n",
      "Iteration 06485: loss = 9.111708e-08,1.208601e-08\n",
      "Iteration 06490: loss = 9.0860254e-08,1.2087036e-08\n",
      "Iteration 06495: loss = 9.057012e-08,1.208806e-08\n",
      "Iteration 06500: loss = 9.031453e-08,1.2089086e-08\n",
      "Iteration 06505: loss = 9.009295e-08,1.2090109e-08\n",
      "Iteration 06510: loss = 8.98382e-08,1.20911325e-08\n",
      "Iteration 06515: loss = 8.9550355e-08,1.2092157e-08\n",
      "Iteration 06520: loss = 8.929671e-08,1.2093178e-08\n",
      "Iteration 06525: loss = 8.904342e-08,1.2094203e-08\n",
      "Iteration 06530: loss = 8.8790785e-08,1.20952235e-08\n",
      "Iteration 06535: loss = 8.8571824e-08,1.2096243e-08\n",
      "Iteration 06540: loss = 8.82867e-08,1.2097265e-08\n",
      "Iteration 06545: loss = 8.8035584e-08,1.2098289e-08\n",
      "Iteration 06550: loss = 8.778488e-08,1.2099308e-08\n",
      "Iteration 06555: loss = 8.753466e-08,1.2100328e-08\n",
      "Iteration 06560: loss = 8.7284924e-08,1.2101346e-08\n",
      "Iteration 06565: loss = 8.7035794e-08,1.2102367e-08\n",
      "Iteration 06570: loss = 8.678709e-08,1.2103387e-08\n",
      "Iteration 06575: loss = 8.6538854e-08,1.21044055e-08\n",
      "Iteration 06580: loss = 8.629109e-08,1.2105423e-08\n",
      "Iteration 06585: loss = 8.60768e-08,1.2106444e-08\n",
      "Iteration 06590: loss = 8.579719e-08,1.210746e-08\n",
      "Iteration 06595: loss = 8.555091e-08,1.2108477e-08\n",
      "Iteration 06600: loss = 8.530515e-08,1.2109494e-08\n",
      "Iteration 06605: loss = 8.509261e-08,1.2110512e-08\n",
      "Iteration 06610: loss = 8.484775e-08,1.2111528e-08\n",
      "Iteration 06615: loss = 8.457088e-08,1.2112545e-08\n",
      "Iteration 06620: loss = 8.432705e-08,1.2113558e-08\n",
      "Iteration 06625: loss = 8.411613e-08,1.2114573e-08\n",
      "Iteration 06630: loss = 8.3873324e-08,1.21155885e-08\n",
      "Iteration 06635: loss = 8.3630916e-08,1.2116603e-08\n",
      "Iteration 06640: loss = 8.338896e-08,1.2117619e-08\n",
      "Iteration 06645: loss = 8.3179884e-08,1.21186305e-08\n",
      "Iteration 06650: loss = 8.2906546e-08,1.2119643e-08\n",
      "Iteration 06655: loss = 8.266601e-08,1.212066e-08\n",
      "Iteration 06660: loss = 8.245812e-08,1.2121672e-08\n",
      "Iteration 06665: loss = 8.22186e-08,1.2122685e-08\n",
      "Iteration 06670: loss = 8.1979444e-08,1.2123696e-08\n",
      "Iteration 06675: loss = 8.1740865e-08,1.2124707e-08\n",
      "Iteration 06680: loss = 8.153475e-08,1.2125719e-08\n",
      "Iteration 06685: loss = 8.129695e-08,1.2126732e-08\n",
      "Iteration 06690: loss = 8.10278e-08,1.2127742e-08\n",
      "Iteration 06695: loss = 8.08229e-08,1.21287504e-08\n",
      "Iteration 06700: loss = 8.0586574e-08,1.212976e-08\n",
      "Iteration 06705: loss = 8.035072e-08,1.213077e-08\n",
      "Iteration 06710: loss = 8.0147124e-08,1.2131778e-08\n",
      "Iteration 06715: loss = 7.99121e-08,1.2132786e-08\n",
      "Iteration 06720: loss = 7.9677655e-08,1.2133798e-08\n",
      "Iteration 06725: loss = 7.94752e-08,1.2134803e-08\n",
      "Iteration 06730: loss = 7.9241666e-08,1.2135812e-08\n",
      "Iteration 06735: loss = 7.904002e-08,1.21368195e-08\n",
      "Iteration 06740: loss = 7.877581e-08,1.2137826e-08\n",
      "Iteration 06745: loss = 7.854365e-08,1.2138833e-08\n",
      "Iteration 06750: loss = 7.834328e-08,1.2139838e-08\n",
      "Iteration 06755: loss = 7.811193e-08,1.2140843e-08\n",
      "Iteration 06760: loss = 7.788115e-08,1.214185e-08\n",
      "Iteration 06765: loss = 7.7681925e-08,1.2142854e-08\n",
      "Iteration 06770: loss = 7.7452036e-08,1.214386e-08\n",
      "Iteration 06775: loss = 7.7253674e-08,1.2144863e-08\n",
      "Iteration 06780: loss = 7.70245e-08,1.2145868e-08\n",
      "Iteration 06785: loss = 7.679599e-08,1.2146872e-08\n",
      "Iteration 06790: loss = 7.659873e-08,1.2147876e-08\n",
      "Iteration 06795: loss = 7.6371016e-08,1.21488775e-08\n",
      "Iteration 06800: loss = 7.617471e-08,1.2149881e-08\n",
      "Iteration 06805: loss = 7.5947725e-08,1.2150881e-08\n",
      "Iteration 06810: loss = 7.572129e-08,1.2151883e-08\n",
      "Iteration 06815: loss = 7.5526096e-08,1.2152885e-08\n",
      "Iteration 06820: loss = 7.53005e-08,1.2153887e-08\n",
      "Iteration 06825: loss = 7.5106094e-08,1.2154887e-08\n",
      "Iteration 06830: loss = 7.4881335e-08,1.2155887e-08\n",
      "Iteration 06835: loss = 7.470308e-08,1.2156887e-08\n",
      "Iteration 06840: loss = 7.4479125e-08,1.2157888e-08\n",
      "Iteration 06845: loss = 7.428624e-08,1.2158888e-08\n",
      "Iteration 06850: loss = 7.406323e-08,1.2159885e-08\n",
      "Iteration 06855: loss = 7.384052e-08,1.2160885e-08\n",
      "Iteration 06860: loss = 7.36489e-08,1.2161884e-08\n",
      "Iteration 06865: loss = 7.342713e-08,1.2162882e-08\n",
      "Iteration 06870: loss = 7.32361e-08,1.2163879e-08\n",
      "Iteration 06875: loss = 7.3015215e-08,1.2164877e-08\n",
      "Iteration 06880: loss = 7.2824974e-08,1.2165875e-08\n",
      "Iteration 06885: loss = 7.260491e-08,1.2166871e-08\n",
      "Iteration 06890: loss = 7.2415574e-08,1.2167868e-08\n",
      "Iteration 06895: loss = 7.219625e-08,1.2168864e-08\n",
      "Iteration 06900: loss = 7.200757e-08,1.216986e-08\n",
      "Iteration 06905: loss = 7.1789145e-08,1.21708545e-08\n",
      "Iteration 06910: loss = 7.16012e-08,1.2171852e-08\n",
      "Iteration 06915: loss = 7.138368e-08,1.2172842e-08\n",
      "Iteration 06920: loss = 7.1196375e-08,1.2173839e-08\n",
      "Iteration 06925: loss = 7.0979716e-08,1.2174834e-08\n",
      "Iteration 06930: loss = 7.079335e-08,1.2175827e-08\n",
      "Iteration 06935: loss = 7.057739e-08,1.2176818e-08\n",
      "Iteration 06940: loss = 7.039167e-08,1.21778125e-08\n",
      "Iteration 06945: loss = 7.020628e-08,1.21788055e-08\n",
      "Iteration 06950: loss = 6.9991664e-08,1.2179798e-08\n",
      "Iteration 06955: loss = 6.980704e-08,1.2180788e-08\n",
      "Iteration 06960: loss = 6.95932e-08,1.2181779e-08\n",
      "Iteration 06965: loss = 6.940931e-08,1.2182772e-08\n",
      "Iteration 06970: loss = 6.9196275e-08,1.2183761e-08\n",
      "Iteration 06975: loss = 6.9013105e-08,1.2184753e-08\n",
      "Iteration 06980: loss = 6.883041e-08,1.2185743e-08\n",
      "Iteration 06985: loss = 6.8647914e-08,1.2186732e-08\n",
      "Iteration 06990: loss = 6.8465766e-08,1.2187718e-08\n",
      "Iteration 06995: loss = 6.825471e-08,1.2188708e-08\n",
      "Iteration 07000: loss = 6.807335e-08,1.21896955e-08\n",
      "Iteration 07005: loss = 6.786307e-08,1.2190685e-08\n",
      "Iteration 07010: loss = 6.768246e-08,1.2191673e-08\n",
      "Iteration 07015: loss = 6.747297e-08,1.2192663e-08\n",
      "Iteration 07020: loss = 6.7293094e-08,1.219365e-08\n",
      "Iteration 07025: loss = 6.711352e-08,1.2194636e-08\n",
      "Iteration 07030: loss = 6.690521e-08,1.2195621e-08\n",
      "Iteration 07035: loss = 6.675532e-08,1.2196606e-08\n",
      "Iteration 07040: loss = 6.654791e-08,1.2197594e-08\n",
      "Iteration 07045: loss = 6.63697e-08,1.2198582e-08\n",
      "Iteration 07050: loss = 6.619186e-08,1.2199565e-08\n",
      "Iteration 07055: loss = 6.5985546e-08,1.2200549e-08\n",
      "Iteration 07060: loss = 6.580844e-08,1.2201534e-08\n",
      "Iteration 07065: loss = 6.5631646e-08,1.2202519e-08\n",
      "Iteration 07070: loss = 6.5426455e-08,1.2203501e-08\n",
      "Iteration 07075: loss = 6.525043e-08,1.2204486e-08\n",
      "Iteration 07080: loss = 6.507473e-08,1.2205469e-08\n",
      "Iteration 07085: loss = 6.489925e-08,1.2206453e-08\n",
      "Iteration 07090: loss = 6.472414e-08,1.2207436e-08\n",
      "Iteration 07095: loss = 6.4520954e-08,1.2208417e-08\n",
      "Iteration 07100: loss = 6.43466e-08,1.22094e-08\n",
      "Iteration 07105: loss = 6.417256e-08,1.2210382e-08\n",
      "Iteration 07110: loss = 6.397041e-08,1.2211364e-08\n",
      "Iteration 07115: loss = 6.382543e-08,1.2212345e-08\n",
      "Iteration 07120: loss = 6.365235e-08,1.2213328e-08\n",
      "Iteration 07125: loss = 6.3451445e-08,1.2214308e-08\n",
      "Iteration 07130: loss = 6.327905e-08,1.2215287e-08\n",
      "Iteration 07135: loss = 6.310703e-08,1.2216267e-08\n",
      "Iteration 07140: loss = 6.2907205e-08,1.22172485e-08\n",
      "Iteration 07145: loss = 6.2764016e-08,1.2218226e-08\n",
      "Iteration 07150: loss = 6.259297e-08,1.2219206e-08\n",
      "Iteration 07155: loss = 6.242221e-08,1.2220186e-08\n",
      "Iteration 07160: loss = 6.222381e-08,1.2221164e-08\n",
      "Iteration 07165: loss = 6.205381e-08,1.22221415e-08\n",
      "Iteration 07170: loss = 6.191198e-08,1.22231185e-08\n",
      "Iteration 07175: loss = 6.1714694e-08,1.2224096e-08\n",
      "Iteration 07180: loss = 6.154561e-08,1.2225073e-08\n",
      "Iteration 07185: loss = 6.137685e-08,1.22260495e-08\n",
      "Iteration 07190: loss = 6.120846e-08,1.22270265e-08\n",
      "Iteration 07195: loss = 6.104033e-08,1.22280035e-08\n",
      "Iteration 07200: loss = 6.087256e-08,1.2228977e-08\n",
      "Iteration 07205: loss = 6.07052e-08,1.2229955e-08\n",
      "Iteration 07210: loss = 6.0510416e-08,1.2230929e-08\n",
      "Iteration 07215: loss = 6.0343645e-08,1.22319035e-08\n",
      "Iteration 07220: loss = 6.020464e-08,1.223288e-08\n",
      "Iteration 07225: loss = 6.003849e-08,1.2233853e-08\n",
      "Iteration 07230: loss = 5.984517e-08,1.2234829e-08\n",
      "Iteration 07235: loss = 5.967963e-08,1.2235802e-08\n",
      "Iteration 07240: loss = 5.95145e-08,1.2236773e-08\n",
      "Iteration 07245: loss = 5.937692e-08,1.2237746e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 07250: loss = 5.91851e-08,1.2238719e-08\n",
      "Iteration 07255: loss = 5.9020824e-08,1.2239691e-08\n",
      "Iteration 07260: loss = 5.885693e-08,1.2240665e-08\n",
      "Iteration 07265: loss = 5.8720403e-08,1.2241635e-08\n",
      "Iteration 07270: loss = 5.855711e-08,1.2242606e-08\n",
      "Iteration 07275: loss = 5.836698e-08,1.2243577e-08\n",
      "Iteration 07280: loss = 5.8204293e-08,1.2244546e-08\n",
      "Iteration 07285: loss = 5.806895e-08,1.2245517e-08\n",
      "Iteration 07290: loss = 5.790686e-08,1.22464865e-08\n",
      "Iteration 07295: loss = 5.7745055e-08,1.2247456e-08\n",
      "Iteration 07300: loss = 5.7556694e-08,1.2248424e-08\n",
      "Iteration 07305: loss = 5.7422444e-08,1.2249393e-08\n",
      "Iteration 07310: loss = 5.7261584e-08,1.2250363e-08\n",
      "Iteration 07315: loss = 5.7101005e-08,1.2251331e-08\n",
      "Iteration 07320: loss = 5.6940845e-08,1.2252297e-08\n",
      "Iteration 07325: loss = 5.6780873e-08,1.2253263e-08\n",
      "Iteration 07330: loss = 5.6621285e-08,1.2254231e-08\n",
      "Iteration 07335: loss = 5.646197e-08,1.2255199e-08\n",
      "Iteration 07340: loss = 5.630294e-08,1.2256164e-08\n",
      "Iteration 07345: loss = 5.6170805e-08,1.2257131e-08\n",
      "Iteration 07350: loss = 5.6012393e-08,1.2258095e-08\n",
      "Iteration 07355: loss = 5.5827797e-08,1.2259062e-08\n",
      "Iteration 07360: loss = 5.569647e-08,1.2260026e-08\n",
      "Iteration 07365: loss = 5.553892e-08,1.2260992e-08\n",
      "Iteration 07370: loss = 5.5381722e-08,1.2261957e-08\n",
      "Iteration 07375: loss = 5.5251167e-08,1.22629205e-08\n",
      "Iteration 07380: loss = 5.5094485e-08,1.2263884e-08\n",
      "Iteration 07385: loss = 5.4911908e-08,1.22648505e-08\n",
      "Iteration 07390: loss = 5.4755883e-08,1.2265813e-08\n",
      "Iteration 07395: loss = 5.46264e-08,1.2266776e-08\n",
      "Iteration 07400: loss = 5.4470956e-08,1.226774e-08\n",
      "Iteration 07405: loss = 5.4315766e-08,1.2268701e-08\n",
      "Iteration 07410: loss = 5.4187055e-08,1.22696635e-08\n",
      "Iteration 07415: loss = 5.4032444e-08,1.22706245e-08\n",
      "Iteration 07420: loss = 5.3878093e-08,1.2271587e-08\n",
      "Iteration 07425: loss = 5.3724154e-08,1.2272549e-08\n",
      "Iteration 07430: loss = 5.3570414e-08,1.2273511e-08\n",
      "Iteration 07435: loss = 5.341703e-08,1.227447e-08\n",
      "Iteration 07440: loss = 5.328975e-08,1.2275429e-08\n",
      "Iteration 07445: loss = 5.3136976e-08,1.22763915e-08\n",
      "Iteration 07450: loss = 5.298442e-08,1.2277351e-08\n",
      "Iteration 07455: loss = 5.2857953e-08,1.227831e-08\n",
      "Iteration 07460: loss = 5.270593e-08,1.2279271e-08\n",
      "Iteration 07465: loss = 5.2554242e-08,1.228023e-08\n",
      "Iteration 07470: loss = 5.2428508e-08,1.2281188e-08\n",
      "Iteration 07475: loss = 5.225174e-08,1.2282145e-08\n",
      "Iteration 07480: loss = 5.210086e-08,1.2283103e-08\n",
      "Iteration 07485: loss = 5.1975956e-08,1.2284061e-08\n",
      "Iteration 07490: loss = 5.1825683e-08,1.2285018e-08\n",
      "Iteration 07495: loss = 5.16757e-08,1.2285977e-08\n",
      "Iteration 07500: loss = 5.155144e-08,1.2286934e-08\n",
      "Iteration 07505: loss = 5.140196e-08,1.228789e-08\n",
      "Iteration 07510: loss = 5.1278302e-08,1.2288848e-08\n",
      "Iteration 07515: loss = 5.1129415e-08,1.2289805e-08\n",
      "Iteration 07520: loss = 5.098079e-08,1.2290757e-08\n",
      "Iteration 07525: loss = 5.0857754e-08,1.2291713e-08\n",
      "Iteration 07530: loss = 5.070974e-08,1.2292668e-08\n",
      "Iteration 07535: loss = 5.056195e-08,1.2293622e-08\n",
      "Iteration 07540: loss = 5.0439624e-08,1.2294579e-08\n",
      "Iteration 07545: loss = 5.029243e-08,1.2295532e-08\n",
      "Iteration 07550: loss = 5.0170595e-08,1.2296486e-08\n",
      "Iteration 07555: loss = 5.0023946e-08,1.2297439e-08\n",
      "Iteration 07560: loss = 4.9877524e-08,1.229839e-08\n",
      "Iteration 07565: loss = 4.975642e-08,1.22993455e-08\n",
      "Iteration 07570: loss = 4.9610573e-08,1.2300298e-08\n",
      "Iteration 07575: loss = 4.9440057e-08,1.2301249e-08\n",
      "Iteration 07580: loss = 4.9319677e-08,1.2302202e-08\n",
      "Iteration 07585: loss = 4.9174673e-08,1.23031505e-08\n",
      "Iteration 07590: loss = 4.90548e-08,1.2304102e-08\n",
      "Iteration 07595: loss = 4.8910277e-08,1.2305052e-08\n",
      "Iteration 07600: loss = 4.8790884e-08,1.2306002e-08\n",
      "Iteration 07605: loss = 4.8646896e-08,1.2306954e-08\n",
      "Iteration 07610: loss = 4.8503228e-08,1.2307902e-08\n",
      "Iteration 07615: loss = 4.8384532e-08,1.2308851e-08\n",
      "Iteration 07620: loss = 4.8241404e-08,1.2309799e-08\n",
      "Iteration 07625: loss = 4.812315e-08,1.2310747e-08\n",
      "Iteration 07630: loss = 4.800502e-08,1.23116966e-08\n",
      "Iteration 07635: loss = 4.7862642e-08,1.2312645e-08\n",
      "Iteration 07640: loss = 4.7745086e-08,1.2313592e-08\n",
      "Iteration 07645: loss = 4.7603177e-08,1.23145405e-08\n",
      "Iteration 07650: loss = 4.7486072e-08,1.2315486e-08\n",
      "Iteration 07655: loss = 4.734473e-08,1.2316434e-08\n",
      "Iteration 07660: loss = 4.7228003e-08,1.2317383e-08\n",
      "Iteration 07665: loss = 4.7087187e-08,1.2318326e-08\n",
      "Iteration 07670: loss = 4.697092e-08,1.2319273e-08\n",
      "Iteration 07675: loss = 4.6830618e-08,1.2320217e-08\n",
      "Iteration 07680: loss = 4.669054e-08,1.2321163e-08\n",
      "Iteration 07685: loss = 4.6575057e-08,1.2322109e-08\n",
      "Iteration 07690: loss = 4.6435463e-08,1.2323053e-08\n",
      "Iteration 07695: loss = 4.632037e-08,1.2323997e-08\n",
      "Iteration 07700: loss = 4.6181345e-08,1.2324943e-08\n",
      "Iteration 07705: loss = 4.6066706e-08,1.2325886e-08\n",
      "Iteration 07710: loss = 4.5928157e-08,1.232683e-08\n",
      "Iteration 07715: loss = 4.5813948e-08,1.2327773e-08\n",
      "Iteration 07720: loss = 4.567599e-08,1.2328715e-08\n",
      "Iteration 07725: loss = 4.5562214e-08,1.2329658e-08\n",
      "Iteration 07730: loss = 4.542467e-08,1.23306e-08\n",
      "Iteration 07735: loss = 4.531134e-08,1.2331542e-08\n",
      "Iteration 07740: loss = 4.519821e-08,1.2332484e-08\n",
      "Iteration 07745: loss = 4.506147e-08,1.2333428e-08\n",
      "Iteration 07750: loss = 4.4948735e-08,1.2334367e-08\n",
      "Iteration 07755: loss = 4.4824358e-08,1.2335308e-08\n",
      "Iteration 07760: loss = 4.4712113e-08,1.2336248e-08\n",
      "Iteration 07765: loss = 4.4576325e-08,1.2337187e-08\n",
      "Iteration 07770: loss = 4.4464525e-08,1.2338127e-08\n",
      "Iteration 07775: loss = 4.432925e-08,1.2339067e-08\n",
      "Iteration 07780: loss = 4.4217824e-08,1.2340005e-08\n",
      "Iteration 07785: loss = 4.410659e-08,1.2340944e-08\n",
      "Iteration 07790: loss = 4.3995538e-08,1.2341884e-08\n",
      "Iteration 07795: loss = 4.3861263e-08,1.2342823e-08\n",
      "Iteration 07800: loss = 4.3750685e-08,1.2343762e-08\n",
      "Iteration 07805: loss = 4.3616875e-08,1.2344699e-08\n",
      "Iteration 07810: loss = 4.350677e-08,1.2345633e-08\n",
      "Iteration 07815: loss = 4.337345e-08,1.2346573e-08\n",
      "Iteration 07820: loss = 4.3263707e-08,1.2347508e-08\n",
      "Iteration 07825: loss = 4.3130907e-08,1.2348446e-08\n",
      "Iteration 07830: loss = 4.3044896e-08,1.2349382e-08\n",
      "Iteration 07835: loss = 4.2912532e-08,1.2350319e-08\n",
      "Iteration 07840: loss = 4.2803595e-08,1.23512525e-08\n",
      "Iteration 07845: loss = 4.267174e-08,1.2352188e-08\n",
      "Iteration 07850: loss = 4.2563286e-08,1.2353125e-08\n",
      "Iteration 07855: loss = 4.245503e-08,1.2354058e-08\n",
      "Iteration 07860: loss = 4.2323823e-08,1.2354995e-08\n",
      "Iteration 07865: loss = 4.2239055e-08,1.2355927e-08\n",
      "Iteration 07870: loss = 4.210837e-08,1.2356862e-08\n",
      "Iteration 07875: loss = 4.200085e-08,1.2357797e-08\n",
      "Iteration 07880: loss = 4.187068e-08,1.235873e-08\n",
      "Iteration 07885: loss = 4.1763602e-08,1.2359662e-08\n",
      "Iteration 07890: loss = 4.163388e-08,1.2360596e-08\n",
      "Iteration 07895: loss = 4.155008e-08,1.2361527e-08\n",
      "Iteration 07900: loss = 4.1420794e-08,1.2362459e-08\n",
      "Iteration 07905: loss = 4.131459e-08,1.2363392e-08\n",
      "Iteration 07910: loss = 4.1185757e-08,1.2364323e-08\n",
      "Iteration 07915: loss = 4.107996e-08,1.2365254e-08\n",
      "Iteration 07920: loss = 4.0997065e-08,1.2366185e-08\n",
      "Iteration 07925: loss = 4.0868915e-08,1.23671136e-08\n",
      "Iteration 07930: loss = 4.0763677e-08,1.2368044e-08\n",
      "Iteration 07935: loss = 4.0635957e-08,1.2368975e-08\n",
      "Iteration 07940: loss = 4.053113e-08,1.2369906e-08\n",
      "Iteration 07945: loss = 4.0403975e-08,1.2370834e-08\n",
      "Iteration 07950: loss = 4.0322096e-08,1.2371762e-08\n",
      "Iteration 07955: loss = 4.0217852e-08,1.2372691e-08\n",
      "Iteration 07960: loss = 4.0091283e-08,1.23736195e-08\n",
      "Iteration 07965: loss = 3.9987427e-08,1.2374548e-08\n",
      "Iteration 07970: loss = 3.9883773e-08,1.2375478e-08\n",
      "Iteration 07975: loss = 3.9780293e-08,1.2376406e-08\n",
      "Iteration 07980: loss = 3.965471e-08,1.2377333e-08\n",
      "Iteration 07985: loss = 3.9551615e-08,1.2378262e-08\n",
      "Iteration 07990: loss = 3.944869e-08,1.23791875e-08\n",
      "Iteration 07995: loss = 3.9346023e-08,1.2380115e-08\n",
      "Iteration 08000: loss = 3.924354e-08,1.2381039e-08\n",
      "Iteration 08005: loss = 3.9119044e-08,1.2381966e-08\n",
      "Iteration 08010: loss = 3.9016953e-08,1.2382893e-08\n",
      "Iteration 08015: loss = 3.893714e-08,1.2383818e-08\n",
      "Iteration 08020: loss = 3.881335e-08,1.2384743e-08\n",
      "Iteration 08025: loss = 3.8711804e-08,1.2385668e-08\n",
      "Iteration 08030: loss = 3.8588365e-08,1.2386594e-08\n",
      "Iteration 08035: loss = 3.8509253e-08,1.2387516e-08\n",
      "Iteration 08040: loss = 3.840826e-08,1.23884405e-08\n",
      "Iteration 08045: loss = 3.8285524e-08,1.2389361e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08050: loss = 3.818497e-08,1.2390286e-08\n",
      "Iteration 08055: loss = 3.8084508e-08,1.239121e-08\n",
      "Iteration 08060: loss = 3.798431e-08,1.239213e-08\n",
      "Iteration 08065: loss = 3.7884316e-08,1.2393055e-08\n",
      "Iteration 08070: loss = 3.776265e-08,1.2393977e-08\n",
      "Iteration 08075: loss = 3.7684742e-08,1.23948976e-08\n",
      "Iteration 08080: loss = 3.756357e-08,1.239582e-08\n",
      "Iteration 08085: loss = 3.7464293e-08,1.2396742e-08\n",
      "Iteration 08090: loss = 3.736518e-08,1.2397662e-08\n",
      "Iteration 08095: loss = 3.7266272e-08,1.2398582e-08\n",
      "Iteration 08100: loss = 3.7167567e-08,1.2399503e-08\n",
      "Iteration 08105: loss = 3.7069018e-08,1.2400424e-08\n",
      "Iteration 08110: loss = 3.6970636e-08,1.2401344e-08\n",
      "Iteration 08115: loss = 3.687249e-08,1.2402261e-08\n",
      "Iteration 08120: loss = 3.67745e-08,1.24031825e-08\n",
      "Iteration 08125: loss = 3.6655162e-08,1.2404101e-08\n",
      "Iteration 08130: loss = 3.657901e-08,1.24050175e-08\n",
      "Iteration 08135: loss = 3.646014e-08,1.2405937e-08\n",
      "Iteration 08140: loss = 3.636284e-08,1.2406854e-08\n",
      "Iteration 08145: loss = 3.6287137e-08,1.2407774e-08\n",
      "Iteration 08150: loss = 3.6168895e-08,1.240869e-08\n",
      "Iteration 08155: loss = 3.6072144e-08,1.2409606e-08\n",
      "Iteration 08160: loss = 3.5996887e-08,1.2410524e-08\n",
      "Iteration 08165: loss = 3.5879253e-08,1.2411441e-08\n",
      "Iteration 08170: loss = 3.578307e-08,1.2412357e-08\n",
      "Iteration 08175: loss = 3.5708243e-08,1.2413275e-08\n",
      "Iteration 08180: loss = 3.5591228e-08,1.2414191e-08\n",
      "Iteration 08185: loss = 3.5495574e-08,1.2415104e-08\n",
      "Iteration 08190: loss = 3.5400063e-08,1.2416019e-08\n",
      "Iteration 08195: loss = 3.5304765e-08,1.2416932e-08\n",
      "Iteration 08200: loss = 3.5209638e-08,1.241785e-08\n",
      "Iteration 08205: loss = 3.5114724e-08,1.2418763e-08\n",
      "Iteration 08210: loss = 3.501995e-08,1.2419676e-08\n",
      "Iteration 08215: loss = 3.4925332e-08,1.2420589e-08\n",
      "Iteration 08220: loss = 3.4851883e-08,1.2421504e-08\n",
      "Iteration 08225: loss = 3.4736672e-08,1.2422414e-08\n",
      "Iteration 08230: loss = 3.4642603e-08,1.2423327e-08\n",
      "Iteration 08235: loss = 3.4569542e-08,1.2424238e-08\n",
      "Iteration 08240: loss = 3.4454995e-08,1.2425152e-08\n",
      "Iteration 08245: loss = 3.436142e-08,1.2426063e-08\n",
      "Iteration 08250: loss = 3.4288817e-08,1.2426975e-08\n",
      "Iteration 08255: loss = 3.4174818e-08,1.2427884e-08\n",
      "Iteration 08260: loss = 3.408184e-08,1.2428794e-08\n",
      "Iteration 08265: loss = 3.400963e-08,1.2429706e-08\n",
      "Iteration 08270: loss = 3.391691e-08,1.2430618e-08\n",
      "Iteration 08275: loss = 3.3803754e-08,1.2431527e-08\n",
      "Iteration 08280: loss = 3.3732004e-08,1.24324355e-08\n",
      "Iteration 08285: loss = 3.3639832e-08,1.2433343e-08\n",
      "Iteration 08290: loss = 3.3547796e-08,1.2434253e-08\n",
      "Iteration 08295: loss = 3.3455937e-08,1.24351605e-08\n",
      "Iteration 08300: loss = 3.3364252e-08,1.243607e-08\n",
      "Iteration 08305: loss = 3.3272723e-08,1.2436978e-08\n",
      "Iteration 08310: loss = 3.3181404e-08,1.2437885e-08\n",
      "Iteration 08315: loss = 3.309023e-08,1.2438793e-08\n",
      "Iteration 08320: loss = 3.30196e-08,1.2439699e-08\n",
      "Iteration 08325: loss = 3.290839e-08,1.2440607e-08\n",
      "Iteration 08330: loss = 3.2838027e-08,1.2441514e-08\n",
      "Iteration 08335: loss = 3.2747522e-08,1.244242e-08\n",
      "Iteration 08340: loss = 3.2657173e-08,1.24433255e-08\n",
      "Iteration 08345: loss = 3.2566984e-08,1.2444231e-08\n",
      "Iteration 08350: loss = 3.2476954e-08,1.2445134e-08\n",
      "Iteration 08355: loss = 3.2407304e-08,1.2446041e-08\n",
      "Iteration 08360: loss = 3.2297425e-08,1.2446946e-08\n",
      "Iteration 08365: loss = 3.2228034e-08,1.244785e-08\n",
      "Iteration 08370: loss = 3.213866e-08,1.2448751e-08\n",
      "Iteration 08375: loss = 3.2049464e-08,1.2449657e-08\n",
      "Iteration 08380: loss = 3.1960443e-08,1.2450561e-08\n",
      "Iteration 08385: loss = 3.1871533e-08,1.2451464e-08\n",
      "Iteration 08390: loss = 3.1802855e-08,1.2452366e-08\n",
      "Iteration 08395: loss = 3.1714247e-08,1.2453269e-08\n",
      "Iteration 08400: loss = 3.1605886e-08,1.2454171e-08\n",
      "Iteration 08405: loss = 3.15376e-08,1.24550725e-08\n",
      "Iteration 08410: loss = 3.144954e-08,1.2455974e-08\n",
      "Iteration 08415: loss = 3.138147e-08,1.2456877e-08\n",
      "Iteration 08420: loss = 3.1273853e-08,1.2457777e-08\n",
      "Iteration 08425: loss = 3.1206067e-08,1.24586785e-08\n",
      "Iteration 08430: loss = 3.1118606e-08,1.2459577e-08\n",
      "Iteration 08435: loss = 3.105111e-08,1.2460479e-08\n",
      "Iteration 08440: loss = 3.0944236e-08,1.2461378e-08\n",
      "Iteration 08445: loss = 3.0857265e-08,1.24622765e-08\n",
      "Iteration 08450: loss = 3.0790172e-08,1.2463175e-08\n",
      "Iteration 08455: loss = 3.0703497e-08,1.2464078e-08\n",
      "Iteration 08460: loss = 3.0617016e-08,1.2464975e-08\n",
      "Iteration 08465: loss = 3.0530696e-08,1.24658746e-08\n",
      "Iteration 08470: loss = 3.046411e-08,1.2466772e-08\n",
      "Iteration 08475: loss = 3.0378043e-08,1.2467668e-08\n",
      "Iteration 08480: loss = 3.029215e-08,1.2468567e-08\n",
      "Iteration 08485: loss = 3.020648e-08,1.2469463e-08\n",
      "Iteration 08490: loss = 3.014037e-08,1.2470361e-08\n",
      "Iteration 08495: loss = 3.005501e-08,1.2471256e-08\n",
      "Iteration 08500: loss = 2.998915e-08,1.2472151e-08\n",
      "Iteration 08505: loss = 2.9884642e-08,1.2473048e-08\n",
      "Iteration 08510: loss = 2.981903e-08,1.24739445e-08\n",
      "Iteration 08515: loss = 2.9734254e-08,1.2474839e-08\n",
      "Iteration 08520: loss = 2.9668948e-08,1.24757324e-08\n",
      "Iteration 08525: loss = 2.9584422e-08,1.24766295e-08\n",
      "Iteration 08530: loss = 2.9480804e-08,1.2477524e-08\n",
      "Iteration 08535: loss = 2.941586e-08,1.2478418e-08\n",
      "Iteration 08540: loss = 2.9331849e-08,1.2479312e-08\n",
      "Iteration 08545: loss = 2.9267165e-08,1.2480204e-08\n",
      "Iteration 08550: loss = 2.9183404e-08,1.2481099e-08\n",
      "Iteration 08555: loss = 2.9099825e-08,1.2481991e-08\n",
      "Iteration 08560: loss = 2.9016396e-08,1.2482884e-08\n",
      "Iteration 08565: loss = 2.8952252e-08,1.2483776e-08\n",
      "Iteration 08570: loss = 2.8869094e-08,1.2484668e-08\n",
      "Iteration 08575: loss = 2.8805127e-08,1.2485561e-08\n",
      "Iteration 08580: loss = 2.8722297e-08,1.2486452e-08\n",
      "Iteration 08585: loss = 2.8639628e-08,1.24873445e-08\n",
      "Iteration 08590: loss = 2.857602e-08,1.24882344e-08\n",
      "Iteration 08595: loss = 2.849363e-08,1.2489124e-08\n",
      "Iteration 08600: loss = 2.8430255e-08,1.2490017e-08\n",
      "Iteration 08605: loss = 2.8329262e-08,1.2490905e-08\n",
      "Iteration 08610: loss = 2.8266186e-08,1.2491798e-08\n",
      "Iteration 08615: loss = 2.8184404e-08,1.2492686e-08\n",
      "Iteration 08620: loss = 2.812152e-08,1.2493574e-08\n",
      "Iteration 08625: loss = 2.8040006e-08,1.2494462e-08\n",
      "Iteration 08630: loss = 2.7977377e-08,1.2495352e-08\n",
      "Iteration 08635: loss = 2.7877384e-08,1.24962405e-08\n",
      "Iteration 08640: loss = 2.7815029e-08,1.2497129e-08\n",
      "Iteration 08645: loss = 2.773408e-08,1.2498015e-08\n",
      "Iteration 08650: loss = 2.7671952e-08,1.2498905e-08\n",
      "Iteration 08655: loss = 2.7591346e-08,1.24997905e-08\n",
      "Iteration 08660: loss = 2.7529437e-08,1.2500677e-08\n",
      "Iteration 08665: loss = 2.7449063e-08,1.25015625e-08\n",
      "Iteration 08670: loss = 2.7368836e-08,1.2502448e-08\n",
      "Iteration 08675: loss = 2.7307282e-08,1.2503334e-08\n",
      "Iteration 08680: loss = 2.722736e-08,1.2504218e-08\n",
      "Iteration 08685: loss = 2.7166086e-08,1.2505103e-08\n",
      "Iteration 08690: loss = 2.7086422e-08,1.25059865e-08\n",
      "Iteration 08695: loss = 2.7006882e-08,1.2506871e-08\n",
      "Iteration 08700: loss = 2.6927564e-08,1.2507755e-08\n",
      "Iteration 08705: loss = 2.686671e-08,1.2508641e-08\n",
      "Iteration 08710: loss = 2.6806001e-08,1.2509523e-08\n",
      "Iteration 08715: loss = 2.6727037e-08,1.2510406e-08\n",
      "Iteration 08720: loss = 2.666655e-08,1.2511289e-08\n",
      "Iteration 08725: loss = 2.658786e-08,1.2512174e-08\n",
      "Iteration 08730: loss = 2.6509353e-08,1.2513052e-08\n",
      "Iteration 08735: loss = 2.6430955e-08,1.2513935e-08\n",
      "Iteration 08740: loss = 2.6370946e-08,1.2514816e-08\n",
      "Iteration 08745: loss = 2.6311048e-08,1.2515698e-08\n",
      "Iteration 08750: loss = 2.6233039e-08,1.251658e-08\n",
      "Iteration 08755: loss = 2.617333e-08,1.2517461e-08\n",
      "Iteration 08760: loss = 2.6095641e-08,1.251834e-08\n",
      "Iteration 08765: loss = 2.6018062e-08,1.2519222e-08\n",
      "Iteration 08770: loss = 2.5958727e-08,1.25201005e-08\n",
      "Iteration 08775: loss = 2.5881448e-08,1.2520981e-08\n",
      "Iteration 08780: loss = 2.5822311e-08,1.2521859e-08\n",
      "Iteration 08785: loss = 2.5745303e-08,1.25227375e-08\n",
      "Iteration 08790: loss = 2.5686381e-08,1.2523618e-08\n",
      "Iteration 08795: loss = 2.5627605e-08,1.2524496e-08\n",
      "Iteration 08800: loss = 2.5550968e-08,1.2525374e-08\n",
      "Iteration 08805: loss = 2.5474478e-08,1.2526254e-08\n",
      "Iteration 08810: loss = 2.5398123e-08,1.2527126e-08\n",
      "Iteration 08815: loss = 2.5339816e-08,1.2528008e-08\n",
      "Iteration 08820: loss = 2.5281585e-08,1.2528883e-08\n",
      "Iteration 08825: loss = 2.5205635e-08,1.2529759e-08\n",
      "Iteration 08830: loss = 2.514763e-08,1.2530634e-08\n",
      "Iteration 08835: loss = 2.5071968e-08,1.253151e-08\n",
      "Iteration 08840: loss = 2.5014124e-08,1.2532385e-08\n",
      "Iteration 08845: loss = 2.4956464e-08,1.25332615e-08\n",
      "Iteration 08850: loss = 2.4863466e-08,1.2534136e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08855: loss = 2.4806026e-08,1.253501e-08\n",
      "Iteration 08860: loss = 2.4748658e-08,1.2535885e-08\n",
      "Iteration 08865: loss = 2.467378e-08,1.2536758e-08\n",
      "Iteration 08870: loss = 2.461664e-08,1.2537633e-08\n",
      "Iteration 08875: loss = 2.4559602e-08,1.2538506e-08\n",
      "Iteration 08880: loss = 2.4485098e-08,1.253938e-08\n",
      "Iteration 08885: loss = 2.442826e-08,1.2540253e-08\n",
      "Iteration 08890: loss = 2.4354017e-08,1.25411255e-08\n",
      "Iteration 08895: loss = 2.4297384e-08,1.25419986e-08\n",
      "Iteration 08900: loss = 2.4240885e-08,1.2542869e-08\n",
      "Iteration 08905: loss = 2.414958e-08,1.2543742e-08\n",
      "Iteration 08910: loss = 2.4093316e-08,1.25446125e-08\n",
      "Iteration 08915: loss = 2.4037108e-08,1.2545484e-08\n",
      "Iteration 08920: loss = 2.3963649e-08,1.2546355e-08\n",
      "Iteration 08925: loss = 2.3907688e-08,1.2547227e-08\n",
      "Iteration 08930: loss = 2.3851806e-08,1.2548097e-08\n",
      "Iteration 08935: loss = 2.3778723e-08,1.2548966e-08\n",
      "Iteration 08940: loss = 2.3723082e-08,1.2549834e-08\n",
      "Iteration 08945: loss = 2.366747e-08,1.25507045e-08\n",
      "Iteration 08950: loss = 2.3594774e-08,1.2551573e-08\n",
      "Iteration 08955: loss = 2.3539444e-08,1.2552444e-08\n",
      "Iteration 08960: loss = 2.3484153e-08,1.2553311e-08\n",
      "Iteration 08965: loss = 2.3411829e-08,1.2554178e-08\n",
      "Iteration 08970: loss = 2.333966e-08,1.2555046e-08\n",
      "Iteration 08975: loss = 2.3284713e-08,1.2555911e-08\n",
      "Iteration 08980: loss = 2.3212799e-08,1.25567805e-08\n",
      "Iteration 08985: loss = 2.315805e-08,1.2557646e-08\n",
      "Iteration 08990: loss = 2.3103416e-08,1.25585125e-08\n",
      "Iteration 08995: loss = 2.3031838e-08,1.2559378e-08\n",
      "Iteration 09000: loss = 2.2977439e-08,1.2560246e-08\n",
      "Iteration 09005: loss = 2.2923132e-08,1.256111e-08\n",
      "Iteration 09010: loss = 2.2851907e-08,1.2561975e-08\n",
      "Iteration 09015: loss = 2.2797787e-08,1.256284e-08\n",
      "Iteration 09020: loss = 2.2743764e-08,1.2563702e-08\n",
      "Iteration 09025: loss = 2.2689813e-08,1.2564567e-08\n",
      "Iteration 09030: loss = 2.26191e-08,1.256543e-08\n",
      "Iteration 09035: loss = 2.2565386e-08,1.25662964e-08\n",
      "Iteration 09040: loss = 2.2511744e-08,1.2567159e-08\n",
      "Iteration 09045: loss = 2.2441403e-08,1.256802e-08\n",
      "Iteration 09050: loss = 2.238796e-08,1.2568881e-08\n",
      "Iteration 09055: loss = 2.233464e-08,1.2569745e-08\n",
      "Iteration 09060: loss = 2.2264679e-08,1.2570608e-08\n",
      "Iteration 09065: loss = 2.2211514e-08,1.2571468e-08\n",
      "Iteration 09070: loss = 2.21418e-08,1.2572331e-08\n",
      "Iteration 09075: loss = 2.2088846e-08,1.25731905e-08\n",
      "Iteration 09080: loss = 2.2019366e-08,1.2574049e-08\n",
      "Iteration 09085: loss = 2.1966656e-08,1.2574911e-08\n",
      "Iteration 09090: loss = 2.1914012e-08,1.257577e-08\n",
      "Iteration 09095: loss = 2.184489e-08,1.2576631e-08\n",
      "Iteration 09100: loss = 2.1792465e-08,1.2577489e-08\n",
      "Iteration 09105: loss = 2.1740114e-08,1.2578346e-08\n",
      "Iteration 09110: loss = 2.168786e-08,1.2579206e-08\n",
      "Iteration 09115: loss = 2.1619227e-08,1.2580066e-08\n",
      "Iteration 09120: loss = 2.1567141e-08,1.2580921e-08\n",
      "Iteration 09125: loss = 2.151519e-08,1.2581782e-08\n",
      "Iteration 09130: loss = 2.1463322e-08,1.2582637e-08\n",
      "Iteration 09135: loss = 2.1395119e-08,1.2583496e-08\n",
      "Iteration 09140: loss = 2.1343451e-08,1.2584353e-08\n",
      "Iteration 09145: loss = 2.1291873e-08,1.2585208e-08\n",
      "Iteration 09150: loss = 2.1240362e-08,1.25860655e-08\n",
      "Iteration 09155: loss = 2.117263e-08,1.2586922e-08\n",
      "Iteration 09160: loss = 2.112135e-08,1.2587778e-08\n",
      "Iteration 09165: loss = 2.107013e-08,1.2588632e-08\n",
      "Iteration 09170: loss = 2.101902e-08,1.2589488e-08\n",
      "Iteration 09175: loss = 2.0951752e-08,1.2590343e-08\n",
      "Iteration 09180: loss = 2.0900826e-08,1.2591197e-08\n",
      "Iteration 09185: loss = 2.0850004e-08,1.259205e-08\n",
      "Iteration 09190: loss = 2.0799265e-08,1.2592904e-08\n",
      "Iteration 09195: loss = 2.0732427e-08,1.2593761e-08\n",
      "Iteration 09200: loss = 2.0681876e-08,1.2594612e-08\n",
      "Iteration 09205: loss = 2.0631425e-08,1.2595464e-08\n",
      "Iteration 09210: loss = 2.0581068e-08,1.2596315e-08\n",
      "Iteration 09215: loss = 2.0514705e-08,1.2597167e-08\n",
      "Iteration 09220: loss = 2.0464512e-08,1.2598018e-08\n",
      "Iteration 09225: loss = 2.0414447e-08,1.25988695e-08\n",
      "Iteration 09230: loss = 2.0364439e-08,1.2599722e-08\n",
      "Iteration 09235: loss = 2.029852e-08,1.2600573e-08\n",
      "Iteration 09240: loss = 2.0248713e-08,1.2601422e-08\n",
      "Iteration 09245: loss = 2.0198982e-08,1.2602273e-08\n",
      "Iteration 09250: loss = 2.014933e-08,1.2603122e-08\n",
      "Iteration 09255: loss = 2.0099792e-08,1.2603971e-08\n",
      "Iteration 09260: loss = 2.0034449e-08,1.2604822e-08\n",
      "Iteration 09265: loss = 1.9985073e-08,1.2605673e-08\n",
      "Iteration 09270: loss = 1.9935806e-08,1.260652e-08\n",
      "Iteration 09275: loss = 1.9886608e-08,1.2607369e-08\n",
      "Iteration 09280: loss = 1.982171e-08,1.2608217e-08\n",
      "Iteration 09285: loss = 1.9772719e-08,1.2609064e-08\n",
      "Iteration 09290: loss = 1.9723815e-08,1.2609912e-08\n",
      "Iteration 09295: loss = 1.967499e-08,1.26107595e-08\n",
      "Iteration 09300: loss = 1.9626244e-08,1.2611607e-08\n",
      "Iteration 09305: loss = 1.956189e-08,1.2612453e-08\n",
      "Iteration 09310: loss = 1.9513339e-08,1.2613299e-08\n",
      "Iteration 09315: loss = 1.946485e-08,1.26141435e-08\n",
      "Iteration 09320: loss = 1.941648e-08,1.2614991e-08\n",
      "Iteration 09325: loss = 1.9383778e-08,1.26158355e-08\n",
      "Iteration 09330: loss = 1.9319979e-08,1.2616681e-08\n",
      "Iteration 09335: loss = 1.9271825e-08,1.2617526e-08\n",
      "Iteration 09340: loss = 1.9223789e-08,1.2618369e-08\n",
      "Iteration 09345: loss = 1.917583e-08,1.2619211e-08\n",
      "Iteration 09350: loss = 1.912795e-08,1.2620057e-08\n",
      "Iteration 09355: loss = 1.9080156e-08,1.2620901e-08\n",
      "Iteration 09360: loss = 1.901697e-08,1.2621742e-08\n",
      "Iteration 09365: loss = 1.8969384e-08,1.2622586e-08\n",
      "Iteration 09370: loss = 1.892186e-08,1.2623428e-08\n",
      "Iteration 09375: loss = 1.8874411e-08,1.262427e-08\n",
      "Iteration 09380: loss = 1.8827043e-08,1.2625112e-08\n",
      "Iteration 09385: loss = 1.8764409e-08,1.2625954e-08\n",
      "Iteration 09390: loss = 1.8717227e-08,1.2626793e-08\n",
      "Iteration 09395: loss = 1.8670159e-08,1.2627634e-08\n",
      "Iteration 09400: loss = 1.8623151e-08,1.2628472e-08\n",
      "Iteration 09405: loss = 1.857623e-08,1.2629313e-08\n",
      "Iteration 09410: loss = 1.8529391e-08,1.2630151e-08\n",
      "Iteration 09415: loss = 1.8467395e-08,1.2630993e-08\n",
      "Iteration 09420: loss = 1.842076e-08,1.2631829e-08\n",
      "Iteration 09425: loss = 1.8389386e-08,1.263267e-08\n",
      "Iteration 09430: loss = 1.834288e-08,1.2633507e-08\n",
      "Iteration 09435: loss = 1.8296452e-08,1.2634344e-08\n",
      "Iteration 09440: loss = 1.8250125e-08,1.2635182e-08\n",
      "Iteration 09445: loss = 1.8188722e-08,1.2636017e-08\n",
      "Iteration 09450: loss = 1.8142577e-08,1.2636855e-08\n",
      "Iteration 09455: loss = 1.8096475e-08,1.263769e-08\n",
      "Iteration 09460: loss = 1.8050498e-08,1.26385284e-08\n",
      "Iteration 09465: loss = 1.8004567e-08,1.2639363e-08\n",
      "Iteration 09470: loss = 1.7958731e-08,1.2640197e-08\n",
      "Iteration 09475: loss = 1.7912978e-08,1.2641032e-08\n",
      "Iteration 09480: loss = 1.785232e-08,1.264187e-08\n",
      "Iteration 09485: loss = 1.7806743e-08,1.2642702e-08\n",
      "Iteration 09490: loss = 1.7761256e-08,1.2643538e-08\n",
      "Iteration 09495: loss = 1.7730764e-08,1.264437e-08\n",
      "Iteration 09500: loss = 1.7685402e-08,1.2645205e-08\n",
      "Iteration 09505: loss = 1.7640144e-08,1.2646034e-08\n",
      "Iteration 09510: loss = 1.7594955e-08,1.2646869e-08\n",
      "Iteration 09515: loss = 1.754983e-08,1.2647701e-08\n",
      "Iteration 09520: loss = 1.7489969e-08,1.2648535e-08\n",
      "Iteration 09525: loss = 1.7445034e-08,1.2649365e-08\n",
      "Iteration 09530: loss = 1.7400188e-08,1.26501964e-08\n",
      "Iteration 09535: loss = 1.735541e-08,1.2651028e-08\n",
      "Iteration 09540: loss = 1.731072e-08,1.2651858e-08\n",
      "Iteration 09545: loss = 1.7266116e-08,1.265269e-08\n",
      "Iteration 09550: loss = 1.722157e-08,1.2653519e-08\n",
      "Iteration 09555: loss = 1.7191809e-08,1.2654349e-08\n",
      "Iteration 09560: loss = 1.7132717e-08,1.2655179e-08\n",
      "Iteration 09565: loss = 1.7088407e-08,1.2656009e-08\n",
      "Iteration 09570: loss = 1.7044192e-08,1.2656836e-08\n",
      "Iteration 09575: loss = 1.7000062e-08,1.2657666e-08\n",
      "Iteration 09580: loss = 1.6955966e-08,1.2658494e-08\n",
      "Iteration 09585: loss = 1.6911981e-08,1.2659321e-08\n",
      "Iteration 09590: loss = 1.6868063e-08,1.26601485e-08\n",
      "Iteration 09595: loss = 1.6824254e-08,1.2660978e-08\n",
      "Iteration 09600: loss = 1.6795017e-08,1.2661804e-08\n",
      "Iteration 09605: loss = 1.6736804e-08,1.2662632e-08\n",
      "Iteration 09610: loss = 1.6693217e-08,1.2663458e-08\n",
      "Iteration 09615: loss = 1.6649667e-08,1.2664284e-08\n",
      "Iteration 09620: loss = 1.660625e-08,1.2665109e-08\n",
      "Iteration 09625: loss = 1.6562854e-08,1.2665933e-08\n",
      "Iteration 09630: loss = 1.6519584e-08,1.2666759e-08\n",
      "Iteration 09635: loss = 1.6476385e-08,1.2667584e-08\n",
      "Iteration 09640: loss = 1.6433232e-08,1.2668409e-08\n",
      "Iteration 09645: loss = 1.6404542e-08,1.2669233e-08\n",
      "Iteration 09650: loss = 1.6361522e-08,1.2670056e-08\n",
      "Iteration 09655: loss = 1.6318609e-08,1.26708795e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 09660: loss = 1.6261465e-08,1.2671703e-08\n",
      "Iteration 09665: loss = 1.6218694e-08,1.2672526e-08\n",
      "Iteration 09670: loss = 1.6176028e-08,1.2673348e-08\n",
      "Iteration 09675: loss = 1.6133434e-08,1.2674169e-08\n",
      "Iteration 09680: loss = 1.6090898e-08,1.267499e-08\n",
      "Iteration 09685: loss = 1.606265e-08,1.267581e-08\n",
      "Iteration 09690: loss = 1.602026e-08,1.2676633e-08\n",
      "Iteration 09695: loss = 1.597794e-08,1.2677455e-08\n",
      "Iteration 09700: loss = 1.5935703e-08,1.2678273e-08\n",
      "Iteration 09705: loss = 1.5893542e-08,1.2679092e-08\n",
      "Iteration 09710: loss = 1.5851429e-08,1.2679913e-08\n",
      "Iteration 09715: loss = 1.5809436e-08,1.2680733e-08\n",
      "Iteration 09720: loss = 1.5767503e-08,1.2681552e-08\n",
      "Iteration 09725: loss = 1.572564e-08,1.26823725e-08\n",
      "Iteration 09730: loss = 1.568384e-08,1.26831905e-08\n",
      "Iteration 09735: loss = 1.5642136e-08,1.2684008e-08\n",
      "Iteration 09740: loss = 1.5600493e-08,1.2684825e-08\n",
      "Iteration 09745: loss = 1.5558923e-08,1.2685643e-08\n",
      "Iteration 09750: loss = 1.5517434e-08,1.268646e-08\n",
      "Iteration 09755: loss = 1.5476024e-08,1.2687275e-08\n",
      "Iteration 09760: loss = 1.5448622e-08,1.2688091e-08\n",
      "Iteration 09765: loss = 1.5407329e-08,1.26889095e-08\n",
      "Iteration 09770: loss = 1.5366128e-08,1.2689725e-08\n",
      "Iteration 09775: loss = 1.5324984e-08,1.2690538e-08\n",
      "Iteration 09780: loss = 1.5283934e-08,1.2691352e-08\n",
      "Iteration 09785: loss = 1.524294e-08,1.2692167e-08\n",
      "Iteration 09790: loss = 1.5202028e-08,1.2692981e-08\n",
      "Iteration 09795: loss = 1.5175e-08,1.2693795e-08\n",
      "Iteration 09800: loss = 1.5134207e-08,1.269461e-08\n",
      "Iteration 09805: loss = 1.5093493e-08,1.2695422e-08\n",
      "Iteration 09810: loss = 1.5052857e-08,1.2696232e-08\n",
      "Iteration 09815: loss = 1.5012295e-08,1.2697049e-08\n",
      "Iteration 09820: loss = 1.4971794e-08,1.2697859e-08\n",
      "Iteration 09825: loss = 1.4938218e-08,1.26986714e-08\n",
      "Iteration 09830: loss = 1.4897871e-08,1.2699482e-08\n",
      "Iteration 09835: loss = 1.4857581e-08,1.2700293e-08\n",
      "Iteration 09840: loss = 1.481738e-08,1.2701104e-08\n",
      "Iteration 09845: loss = 1.4777219e-08,1.2701916e-08\n",
      "Iteration 09850: loss = 1.4737181e-08,1.2702726e-08\n",
      "Iteration 09855: loss = 1.4710757e-08,1.2703536e-08\n",
      "Iteration 09860: loss = 1.4670822e-08,1.2704345e-08\n",
      "Iteration 09865: loss = 1.4630936e-08,1.2705154e-08\n",
      "Iteration 09870: loss = 1.4591142e-08,1.27059625e-08\n",
      "Iteration 09875: loss = 1.4551418e-08,1.2706773e-08\n",
      "Iteration 09880: loss = 1.4511763e-08,1.2707581e-08\n",
      "Iteration 09885: loss = 1.4485689e-08,1.2708389e-08\n",
      "Iteration 09890: loss = 1.4446168e-08,1.2709197e-08\n",
      "Iteration 09895: loss = 1.4406692e-08,1.2710004e-08\n",
      "Iteration 09900: loss = 1.4367316e-08,1.2710811e-08\n",
      "Iteration 09905: loss = 1.4327995e-08,1.27116175e-08\n",
      "Iteration 09910: loss = 1.4288766e-08,1.2712424e-08\n",
      "Iteration 09915: loss = 1.4262972e-08,1.271323e-08\n",
      "Iteration 09920: loss = 1.4223857e-08,1.27140325e-08\n",
      "Iteration 09925: loss = 1.4184824e-08,1.2714839e-08\n",
      "Iteration 09930: loss = 1.4145841e-08,1.27156445e-08\n",
      "Iteration 09935: loss = 1.41069405e-08,1.2716449e-08\n",
      "Iteration 09940: loss = 1.4081418e-08,1.2717252e-08\n",
      "Iteration 09945: loss = 1.4042641e-08,1.2718056e-08\n",
      "Iteration 09950: loss = 1.4003931e-08,1.2718861e-08\n",
      "Iteration 09955: loss = 1.3965288e-08,1.2719662e-08\n",
      "Iteration 09960: loss = 1.3926717e-08,1.27204665e-08\n",
      "Iteration 09965: loss = 1.3901432e-08,1.2721268e-08\n",
      "Iteration 09970: loss = 1.3863004e-08,1.2722069e-08\n",
      "Iteration 09975: loss = 1.3824617e-08,1.2722872e-08\n",
      "Iteration 09980: loss = 1.3786301e-08,1.2723673e-08\n",
      "Iteration 09985: loss = 1.3748083e-08,1.2724473e-08\n",
      "Iteration 09990: loss = 1.3723066e-08,1.27252715e-08\n",
      "Iteration 09995: loss = 1.3684943e-08,1.27260735e-08\n",
      "Iteration 10000: loss = 1.3646898e-08,1.2726872e-08\n",
      "Iteration 10005: loss = 1.3608933e-08,1.2727673e-08\n",
      "Iteration 10010: loss = 1.3571012e-08,1.2728472e-08\n",
      "Iteration 10015: loss = 1.35462175e-08,1.2729268e-08\n",
      "Iteration 10020: loss = 1.3508445e-08,1.2730068e-08\n",
      "Iteration 10025: loss = 1.3470731e-08,1.2730863e-08\n",
      "Iteration 10030: loss = 1.3433072e-08,1.2731661e-08\n",
      "Iteration 10035: loss = 1.3395496e-08,1.2732458e-08\n",
      "Iteration 10040: loss = 1.337096e-08,1.2733254e-08\n",
      "Iteration 10045: loss = 1.33335085e-08,1.273405e-08\n",
      "Iteration 10050: loss = 1.3296113e-08,1.2734845e-08\n",
      "Iteration 10055: loss = 1.32587985e-08,1.2735641e-08\n",
      "Iteration 10060: loss = 1.3234444e-08,1.2736436e-08\n",
      "Iteration 10065: loss = 1.3210131e-08,1.2737231e-08\n",
      "Iteration 10070: loss = 1.3172991e-08,1.2738024e-08\n",
      "Iteration 10075: loss = 1.3135902e-08,1.2738818e-08\n",
      "Iteration 10080: loss = 1.3098883e-08,1.2739612e-08\n",
      "Iteration 10085: loss = 1.3061954e-08,1.2740406e-08\n",
      "Iteration 10090: loss = 1.3037866e-08,1.27411965e-08\n",
      "Iteration 10095: loss = 1.3001064e-08,1.2741989e-08\n",
      "Iteration 10100: loss = 1.296429e-08,1.2742784e-08\n",
      "Iteration 10105: loss = 1.29275834e-08,1.2743575e-08\n",
      "Iteration 10110: loss = 1.2903701e-08,1.2744366e-08\n",
      "Iteration 10115: loss = 1.2867126e-08,1.2745156e-08\n",
      "Iteration 10120: loss = 1.2830629e-08,1.27459465e-08\n",
      "Iteration 10125: loss = 1.2794185e-08,1.2746739e-08\n",
      "Iteration 10130: loss = 1.2757813e-08,1.27475275e-08\n",
      "Iteration 10135: loss = 1.2734152e-08,1.2748318e-08\n",
      "Iteration 10140: loss = 1.269789e-08,1.2749105e-08\n",
      "Iteration 10145: loss = 1.2661711e-08,1.2749898e-08\n",
      "Iteration 10150: loss = 1.2625573e-08,1.2750685e-08\n",
      "Iteration 10155: loss = 1.2602126e-08,1.27514745e-08\n",
      "Iteration 10160: loss = 1.2566104e-08,1.2752261e-08\n",
      "Iteration 10165: loss = 1.2542735e-08,1.2753048e-08\n",
      "Iteration 10170: loss = 1.2506836e-08,1.2753837e-08\n",
      "Iteration 10175: loss = 1.2483551e-08,1.2754624e-08\n",
      "Iteration 10180: loss = 1.244777e-08,1.2755409e-08\n",
      "Iteration 10185: loss = 1.24120545e-08,1.2756194e-08\n",
      "Iteration 10190: loss = 1.2376411e-08,1.275698e-08\n",
      "Iteration 10195: loss = 1.2353312e-08,1.2757766e-08\n",
      "Iteration 10200: loss = 1.2317777e-08,1.275855e-08\n",
      "Iteration 10205: loss = 1.2282311e-08,1.2759334e-08\n",
      "Iteration 10210: loss = 1.224691e-08,1.2760117e-08\n",
      "Iteration 10215: loss = 1.2223997e-08,1.2760902e-08\n",
      "Iteration 10220: loss = 1.21887e-08,1.2761684e-08\n",
      "Iteration 10225: loss = 1.2153497e-08,1.2762468e-08\n",
      "Iteration 10230: loss = 1.21183374e-08,1.2763252e-08\n",
      "Iteration 10235: loss = 1.21079085e-08,1.2764032e-08\n",
      "Iteration 10240: loss = 1.2072857e-08,1.2764815e-08\n",
      "Iteration 10245: loss = 1.2037869e-08,1.2765596e-08\n",
      "Iteration 10250: loss = 1.200294e-08,1.2766376e-08\n",
      "Iteration 10255: loss = 1.1980357e-08,1.2767158e-08\n",
      "Iteration 10260: loss = 1.1945541e-08,1.2767936e-08\n",
      "Iteration 10265: loss = 1.1910802e-08,1.2768716e-08\n",
      "Iteration 10270: loss = 1.1876117e-08,1.2769497e-08\n",
      "Iteration 10275: loss = 1.1853708e-08,1.2770273e-08\n",
      "Iteration 10280: loss = 1.1819147e-08,1.2771051e-08\n",
      "Iteration 10285: loss = 1.178464e-08,1.2771829e-08\n",
      "Iteration 10290: loss = 1.1762346e-08,1.2772608e-08\n",
      "Iteration 10295: loss = 1.17401e-08,1.2773382e-08\n",
      "Iteration 10300: loss = 1.1705759e-08,1.277416e-08\n",
      "Iteration 10305: loss = 1.1671468e-08,1.2774937e-08\n",
      "Iteration 10310: loss = 1.1637258e-08,1.2775713e-08\n",
      "Iteration 10315: loss = 1.1615189e-08,1.2776489e-08\n",
      "Iteration 10320: loss = 1.1581073e-08,1.2777263e-08\n",
      "Iteration 10325: loss = 1.1547026e-08,1.2778038e-08\n",
      "Iteration 10330: loss = 1.151307e-08,1.2778809e-08\n",
      "Iteration 10335: loss = 1.149116e-08,1.2779584e-08\n",
      "Iteration 10340: loss = 1.14692975e-08,1.278036e-08\n",
      "Iteration 10345: loss = 1.1435463e-08,1.2781131e-08\n",
      "Iteration 10350: loss = 1.14136895e-08,1.2781904e-08\n",
      "Iteration 10355: loss = 1.13799805e-08,1.2782676e-08\n",
      "Iteration 10360: loss = 1.1346326e-08,1.2783447e-08\n",
      "Iteration 10365: loss = 1.1312739e-08,1.2784221e-08\n",
      "Iteration 10370: loss = 1.1291138e-08,1.2784992e-08\n",
      "Iteration 10375: loss = 1.1257644e-08,1.2785762e-08\n",
      "Iteration 10380: loss = 1.12242375e-08,1.2786534e-08\n",
      "Iteration 10385: loss = 1.12027365e-08,1.2787302e-08\n",
      "Iteration 10390: loss = 1.1181284e-08,1.2788072e-08\n",
      "Iteration 10395: loss = 1.114802e-08,1.278884e-08\n",
      "Iteration 10400: loss = 1.1114831e-08,1.2789608e-08\n",
      "Iteration 10405: loss = 1.1093509e-08,1.2790377e-08\n",
      "Iteration 10410: loss = 1.10604175e-08,1.2791145e-08\n",
      "Iteration 10415: loss = 1.10273985e-08,1.27919115e-08\n",
      "Iteration 10420: loss = 1.09944365e-08,1.2792679e-08\n",
      "Iteration 10425: loss = 1.0985034e-08,1.2793445e-08\n",
      "Iteration 10430: loss = 1.095215e-08,1.2794211e-08\n",
      "Iteration 10435: loss = 1.091933e-08,1.27949775e-08\n",
      "Iteration 10440: loss = 1.0898295e-08,1.2795743e-08\n",
      "Iteration 10445: loss = 1.0865588e-08,1.2796505e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10450: loss = 1.0832945e-08,1.2797271e-08\n",
      "Iteration 10455: loss = 1.0812028e-08,1.2798034e-08\n",
      "Iteration 10460: loss = 1.0791138e-08,1.27988e-08\n",
      "Iteration 10465: loss = 1.0758634e-08,1.2799561e-08\n",
      "Iteration 10470: loss = 1.0726204e-08,1.2800326e-08\n",
      "Iteration 10475: loss = 1.0705437e-08,1.2801087e-08\n",
      "Iteration 10480: loss = 1.0673105e-08,1.280185e-08\n",
      "Iteration 10485: loss = 1.0640842e-08,1.2802611e-08\n",
      "Iteration 10490: loss = 1.0620216e-08,1.2803374e-08\n",
      "Iteration 10495: loss = 1.0599588e-08,1.2804134e-08\n",
      "Iteration 10500: loss = 1.0567465e-08,1.2804896e-08\n",
      "Iteration 10505: loss = 1.0535417e-08,1.2805653e-08\n",
      "Iteration 10510: loss = 1.0514921e-08,1.2806412e-08\n",
      "Iteration 10515: loss = 1.0482979e-08,1.2807174e-08\n",
      "Iteration 10520: loss = 1.0451086e-08,1.28079325e-08\n",
      "Iteration 10525: loss = 1.0430714e-08,1.2808693e-08\n",
      "Iteration 10530: loss = 1.0410382e-08,1.28094495e-08\n",
      "Iteration 10535: loss = 1.0378629e-08,1.2810208e-08\n",
      "Iteration 10540: loss = 1.0358371e-08,1.2810965e-08\n",
      "Iteration 10545: loss = 1.0326745e-08,1.2811723e-08\n",
      "Iteration 10550: loss = 1.029517e-08,1.2812479e-08\n",
      "Iteration 10555: loss = 1.0275023e-08,1.2813235e-08\n",
      "Iteration 10560: loss = 1.0254894e-08,1.2813992e-08\n",
      "Iteration 10565: loss = 1.022347e-08,1.2814747e-08\n",
      "Iteration 10570: loss = 1.0203419e-08,1.28155015e-08\n",
      "Iteration 10575: loss = 1.01721165e-08,1.2816257e-08\n",
      "Iteration 10580: loss = 1.0140852e-08,1.28170115e-08\n",
      "Iteration 10585: loss = 1.0109645e-08,1.2817764e-08\n",
      "Iteration 10590: loss = 1.0101018e-08,1.2818519e-08\n",
      "Iteration 10595: loss = 1.0069912e-08,1.2819271e-08\n",
      "Iteration 10600: loss = 1.00388595e-08,1.2820024e-08\n",
      "Iteration 10605: loss = 1.0019076e-08,1.28207756e-08\n",
      "Iteration 10610: loss = 9.9881206e-09,1.2821529e-08\n",
      "Iteration 10615: loss = 9.957238e-09,1.2822279e-08\n",
      "Iteration 10620: loss = 9.948764e-09,1.2823029e-08\n",
      "Iteration 10625: loss = 9.917947e-09,1.2823779e-08\n",
      "Iteration 10630: loss = 9.887198e-09,1.2824529e-08\n",
      "Iteration 10635: loss = 9.867645e-09,1.28252795e-08\n",
      "Iteration 10640: loss = 9.836994e-09,1.2826026e-08\n",
      "Iteration 10645: loss = 9.817517e-09,1.2826776e-08\n",
      "Iteration 10650: loss = 9.798078e-09,1.2827524e-08\n",
      "Iteration 10655: loss = 9.767571e-09,1.2828271e-08\n",
      "Iteration 10660: loss = 9.737117e-09,1.282902e-08\n",
      "Iteration 10665: loss = 9.717784e-09,1.2829766e-08\n",
      "Iteration 10670: loss = 9.687445e-09,1.2830511e-08\n",
      "Iteration 10675: loss = 9.668191e-09,1.2831258e-08\n",
      "Iteration 10680: loss = 9.648961e-09,1.2832003e-08\n",
      "Iteration 10685: loss = 9.618757e-09,1.2832748e-08\n",
      "Iteration 10690: loss = 9.5886135e-09,1.28334925e-08\n",
      "Iteration 10695: loss = 9.5695e-09,1.2834234e-08\n",
      "Iteration 10700: loss = 9.550405e-09,1.2834979e-08\n",
      "Iteration 10705: loss = 9.520416e-09,1.2835724e-08\n",
      "Iteration 10710: loss = 9.501391e-09,1.2836466e-08\n",
      "Iteration 10715: loss = 9.4714965e-09,1.2837208e-08\n",
      "Iteration 10720: loss = 9.4416555e-09,1.2837951e-08\n",
      "Iteration 10725: loss = 9.433639e-09,1.2838692e-08\n",
      "Iteration 10730: loss = 9.403874e-09,1.2839434e-08\n",
      "Iteration 10735: loss = 9.374175e-09,1.2840173e-08\n",
      "Iteration 10740: loss = 9.355376e-09,1.2840913e-08\n",
      "Iteration 10745: loss = 9.325765e-09,1.2841654e-08\n",
      "Iteration 10750: loss = 9.307043e-09,1.2842395e-08\n",
      "Iteration 10755: loss = 9.288335e-09,1.2843132e-08\n",
      "Iteration 10760: loss = 9.258872e-09,1.284387e-08\n",
      "Iteration 10765: loss = 9.240242e-09,1.2844609e-08\n",
      "Iteration 10770: loss = 9.210867e-09,1.2845345e-08\n",
      "Iteration 10775: loss = 9.192317e-09,1.2846083e-08\n",
      "Iteration 10780: loss = 9.173788e-09,1.2846819e-08\n",
      "Iteration 10785: loss = 9.144558e-09,1.2847555e-08\n",
      "Iteration 10790: loss = 9.115382e-09,1.2848291e-08\n",
      "Iteration 10795: loss = 9.1076595e-09,1.28490285e-08\n",
      "Iteration 10800: loss = 9.078572e-09,1.2849761e-08\n",
      "Iteration 10805: loss = 9.049538e-09,1.28504976e-08\n",
      "Iteration 10810: loss = 9.031219e-09,1.285123e-08\n",
      "Iteration 10815: loss = 9.002281e-09,1.2851964e-08\n",
      "Iteration 10820: loss = 8.9840215e-09,1.2852698e-08\n",
      "Iteration 10825: loss = 8.965803e-09,1.285343e-08\n",
      "Iteration 10830: loss = 8.9370005e-09,1.2854162e-08\n",
      "Iteration 10835: loss = 8.918843e-09,1.2854892e-08\n",
      "Iteration 10840: loss = 8.900728e-09,1.2855624e-08\n",
      "Iteration 10845: loss = 8.872051e-09,1.2856355e-08\n",
      "Iteration 10850: loss = 8.8540055e-09,1.2857087e-08\n",
      "Iteration 10855: loss = 8.825435e-09,1.2857813e-08\n",
      "Iteration 10860: loss = 8.7969445e-09,1.2858546e-08\n",
      "Iteration 10865: loss = 8.789494e-09,1.2859272e-08\n",
      "Iteration 10870: loss = 8.761069e-09,1.2860001e-08\n",
      "Iteration 10875: loss = 8.73269e-09,1.2860729e-08\n",
      "Iteration 10880: loss = 8.7148395e-09,1.2861456e-08\n",
      "Iteration 10885: loss = 8.697017e-09,1.2862184e-08\n",
      "Iteration 10890: loss = 8.679213e-09,1.2862912e-08\n",
      "Iteration 10895: loss = 8.651013e-09,1.2863638e-08\n",
      "Iteration 10900: loss = 8.622869e-09,1.2864364e-08\n",
      "Iteration 10905: loss = 8.61559e-09,1.2865087e-08\n",
      "Iteration 10910: loss = 8.587516e-09,1.2865815e-08\n",
      "Iteration 10915: loss = 8.559514e-09,1.2866537e-08\n",
      "Iteration 10920: loss = 8.541924e-09,1.2867261e-08\n",
      "Iteration 10925: loss = 8.524361e-09,1.2867985e-08\n",
      "Iteration 10930: loss = 8.506817e-09,1.2868707e-08\n",
      "Iteration 10935: loss = 8.478974e-09,1.2869432e-08\n",
      "Iteration 10940: loss = 8.451203e-09,1.2870151e-08\n",
      "Iteration 10945: loss = 8.444061e-09,1.2870874e-08\n",
      "Iteration 10950: loss = 8.416361e-09,1.28715945e-08\n",
      "Iteration 10955: loss = 8.388713e-09,1.28723165e-08\n",
      "Iteration 10960: loss = 8.371376e-09,1.2873036e-08\n",
      "Iteration 10965: loss = 8.354061e-09,1.2873756e-08\n",
      "Iteration 10970: loss = 8.336781e-09,1.2874475e-08\n",
      "Iteration 10975: loss = 8.309297e-09,1.2875195e-08\n",
      "Iteration 10980: loss = 8.2818685e-09,1.28759154e-08\n",
      "Iteration 10985: loss = 8.274892e-09,1.2876631e-08\n",
      "Iteration 10990: loss = 8.247543e-09,1.2877349e-08\n",
      "Iteration 10995: loss = 8.230425e-09,1.2878067e-08\n",
      "Iteration 11000: loss = 8.203164e-09,1.28787825e-08\n",
      "Iteration 11005: loss = 8.186102e-09,1.2879498e-08\n",
      "Iteration 11010: loss = 8.169081e-09,1.2880214e-08\n",
      "Iteration 11015: loss = 8.14195e-09,1.2880931e-08\n",
      "Iteration 11020: loss = 8.1249825e-09,1.2881644e-08\n",
      "Iteration 11025: loss = 8.10805e-09,1.2882358e-08\n",
      "Iteration 11030: loss = 8.081061e-09,1.2883072e-08\n",
      "Iteration 11035: loss = 8.06418e-09,1.2883786e-08\n",
      "Iteration 11040: loss = 8.047327e-09,1.2884499e-08\n",
      "Iteration 11045: loss = 8.030521e-09,1.2885211e-08\n",
      "Iteration 11050: loss = 8.003694e-09,1.2885922e-08\n",
      "Iteration 11055: loss = 7.976918e-09,1.2886632e-08\n",
      "Iteration 11060: loss = 7.970207e-09,1.2887344e-08\n",
      "Iteration 11065: loss = 7.943515e-09,1.2888053e-08\n",
      "Iteration 11070: loss = 7.926842e-09,1.2888763e-08\n",
      "Iteration 11075: loss = 7.900241e-09,1.2889474e-08\n",
      "Iteration 11080: loss = 7.883647e-09,1.28901805e-08\n",
      "Iteration 11085: loss = 7.86707e-09,1.2890889e-08\n",
      "Iteration 11090: loss = 7.840595e-09,1.2891597e-08\n",
      "Iteration 11095: loss = 7.834011e-09,1.2892308e-08\n",
      "Iteration 11100: loss = 7.807601e-09,1.2893013e-08\n",
      "Iteration 11105: loss = 7.781247e-09,1.2893717e-08\n",
      "Iteration 11110: loss = 7.764832e-09,1.2894424e-08\n",
      "Iteration 11115: loss = 7.748432e-09,1.28951285e-08\n",
      "Iteration 11120: loss = 7.732068e-09,1.2895834e-08\n",
      "Iteration 11125: loss = 7.705881e-09,1.2896537e-08\n",
      "Iteration 11130: loss = 7.68958e-09,1.2897242e-08\n",
      "Iteration 11135: loss = 7.6732976e-09,1.2897946e-08\n",
      "Iteration 11140: loss = 7.6472375e-09,1.28986475e-08\n",
      "Iteration 11145: loss = 7.6310185e-09,1.2899348e-08\n",
      "Iteration 11150: loss = 7.614842e-09,1.2900052e-08\n",
      "Iteration 11155: loss = 7.588903e-09,1.29007525e-08\n",
      "Iteration 11160: loss = 7.572781e-09,1.2901452e-08\n",
      "Iteration 11165: loss = 7.556693e-09,1.2902154e-08\n",
      "Iteration 11170: loss = 7.540611e-09,1.2902855e-08\n",
      "Iteration 11175: loss = 7.514846e-09,1.2903551e-08\n",
      "Iteration 11180: loss = 7.508535e-09,1.2904251e-08\n",
      "Iteration 11185: loss = 7.482843e-09,1.29049464e-08\n",
      "Iteration 11190: loss = 7.457192e-09,1.2905645e-08\n",
      "Iteration 11195: loss = 7.441276e-09,1.29063435e-08\n",
      "Iteration 11200: loss = 7.425385e-09,1.290704e-08\n",
      "Iteration 11205: loss = 7.4095077e-09,1.2907734e-08\n",
      "Iteration 11210: loss = 7.3840227e-09,1.2908429e-08\n",
      "Iteration 11215: loss = 7.368211e-09,1.2909126e-08\n",
      "Iteration 11220: loss = 7.3524276e-09,1.2909819e-08\n",
      "Iteration 11225: loss = 7.3270816e-09,1.29105135e-08\n",
      "Iteration 11230: loss = 7.3209385e-09,1.2911209e-08\n",
      "Iteration 11235: loss = 7.295652e-09,1.2911901e-08\n",
      "Iteration 11240: loss = 7.279974e-09,1.2912592e-08\n",
      "Iteration 11245: loss = 7.25478e-09,1.29132856e-08\n",
      "Iteration 11250: loss = 7.239168e-09,1.2913977e-08\n",
      "Iteration 11255: loss = 7.2235795e-09,1.2914667e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11260: loss = 7.198503e-09,1.2915359e-08\n",
      "Iteration 11265: loss = 7.19248e-09,1.2916049e-08\n",
      "Iteration 11270: loss = 7.1674715e-09,1.2916737e-08\n",
      "Iteration 11275: loss = 7.1519928e-09,1.2917427e-08\n",
      "Iteration 11280: loss = 7.1365522e-09,1.2918116e-08\n",
      "Iteration 11285: loss = 7.121122e-09,1.2918805e-08\n",
      "Iteration 11290: loss = 7.0962662e-09,1.2919491e-08\n",
      "Iteration 11295: loss = 7.08091e-09,1.2920177e-08\n",
      "Iteration 11300: loss = 7.065566e-09,1.2920864e-08\n",
      "Iteration 11305: loss = 7.040835e-09,1.2921549e-08\n",
      "Iteration 11310: loss = 7.034965e-09,1.2922234e-08\n",
      "Iteration 11315: loss = 7.010312e-09,1.292292e-08\n",
      "Iteration 11320: loss = 6.995073e-09,1.29236035e-08\n",
      "Iteration 11325: loss = 6.979859e-09,1.2924288e-08\n",
      "Iteration 11330: loss = 6.9553336e-09,1.29249695e-08\n",
      "Iteration 11335: loss = 6.940185e-09,1.29256525e-08\n",
      "Iteration 11340: loss = 6.9250583e-09,1.2926337e-08\n",
      "Iteration 11345: loss = 6.9099655e-09,1.2927018e-08\n",
      "Iteration 11350: loss = 6.8855925e-09,1.2927699e-08\n",
      "Iteration 11355: loss = 6.8798403e-09,1.292838e-08\n",
      "Iteration 11360: loss = 6.855524e-09,1.2929059e-08\n",
      "Iteration 11365: loss = 6.8405375e-09,1.2929738e-08\n",
      "Iteration 11370: loss = 6.8255726e-09,1.2930417e-08\n",
      "Iteration 11375: loss = 6.8013812e-09,1.2931094e-08\n",
      "Iteration 11380: loss = 6.7864825e-09,1.2931774e-08\n",
      "Iteration 11385: loss = 6.771596e-09,1.29324516e-08\n",
      "Iteration 11390: loss = 6.7567387e-09,1.2933132e-08\n",
      "Iteration 11395: loss = 6.732702e-09,1.2933805e-08\n",
      "Iteration 11400: loss = 6.727095e-09,1.293448e-08\n",
      "Iteration 11405: loss = 6.7031216e-09,1.2935155e-08\n",
      "Iteration 11410: loss = 6.6883774e-09,1.2935829e-08\n",
      "Iteration 11415: loss = 6.673647e-09,1.2936503e-08\n",
      "Iteration 11420: loss = 6.658947e-09,1.293718e-08\n",
      "Iteration 11425: loss = 6.635124e-09,1.2937852e-08\n",
      "Iteration 11430: loss = 6.620476e-09,1.2938526e-08\n",
      "Iteration 11435: loss = 6.605857e-09,1.2939198e-08\n",
      "Iteration 11440: loss = 6.582162e-09,1.293987e-08\n",
      "Iteration 11445: loss = 6.5766854e-09,1.2940541e-08\n",
      "Iteration 11450: loss = 6.5530545e-09,1.2941212e-08\n",
      "Iteration 11455: loss = 6.538537e-09,1.2941881e-08\n",
      "Iteration 11460: loss = 6.5240453e-09,1.2942552e-08\n",
      "Iteration 11465: loss = 6.509584e-09,1.294322e-08\n",
      "Iteration 11470: loss = 6.4951333e-09,1.2943888e-08\n",
      "Iteration 11475: loss = 6.480715e-09,1.2944556e-08\n",
      "Iteration 11480: loss = 6.4573005e-09,1.2945224e-08\n",
      "Iteration 11485: loss = 6.451939e-09,1.2945891e-08\n",
      "Iteration 11490: loss = 6.4285857e-09,1.2946556e-08\n",
      "Iteration 11495: loss = 6.405299e-09,1.2947222e-08\n",
      "Iteration 11500: loss = 6.3999845e-09,1.2947887e-08\n",
      "Iteration 11505: loss = 6.3767502e-09,1.2948552e-08\n",
      "Iteration 11510: loss = 6.3625034e-09,1.2949218e-08\n",
      "Iteration 11515: loss = 6.348285e-09,1.2949879e-08\n",
      "Iteration 11520: loss = 6.334091e-09,1.2950543e-08\n",
      "Iteration 11525: loss = 6.319929e-09,1.2951206e-08\n",
      "Iteration 11530: loss = 6.305781e-09,1.2951867e-08\n",
      "Iteration 11535: loss = 6.2827623e-09,1.2952529e-08\n",
      "Iteration 11540: loss = 6.2775616e-09,1.2953189e-08\n",
      "Iteration 11545: loss = 6.2546093e-09,1.29538495e-08\n",
      "Iteration 11550: loss = 6.2405676e-09,1.295451e-08\n",
      "Iteration 11555: loss = 6.2265477e-09,1.295517e-08\n",
      "Iteration 11560: loss = 6.21255e-09,1.29558275e-08\n",
      "Iteration 11565: loss = 6.189755e-09,1.2956484e-08\n",
      "Iteration 11570: loss = 6.1758114e-09,1.2957144e-08\n",
      "Iteration 11575: loss = 6.1618946e-09,1.29578e-08\n",
      "Iteration 11580: loss = 6.1479923e-09,1.2958456e-08\n",
      "Iteration 11585: loss = 6.134122e-09,1.2959113e-08\n",
      "Iteration 11590: loss = 6.1115064e-09,1.2959767e-08\n",
      "Iteration 11595: loss = 6.106445e-09,1.2960422e-08\n",
      "Iteration 11600: loss = 6.083886e-09,1.2961077e-08\n",
      "Iteration 11605: loss = 6.0701155e-09,1.29617295e-08\n",
      "Iteration 11610: loss = 6.0563696e-09,1.2962382e-08\n",
      "Iteration 11615: loss = 6.042637e-09,1.2963035e-08\n",
      "Iteration 11620: loss = 6.028936e-09,1.2963686e-08\n",
      "Iteration 11625: loss = 6.0152536e-09,1.2964335e-08\n",
      "Iteration 11630: loss = 5.9929106e-09,1.2964987e-08\n",
      "Iteration 11635: loss = 5.987958e-09,1.2965638e-08\n",
      "Iteration 11640: loss = 5.965681e-09,1.2966288e-08\n",
      "Iteration 11645: loss = 5.960747e-09,1.2966936e-08\n",
      "Iteration 11650: loss = 5.938538e-09,1.2967584e-08\n",
      "Iteration 11655: loss = 5.9249934e-09,1.2968234e-08\n",
      "Iteration 11660: loss = 5.9114798e-09,1.2968879e-08\n",
      "Iteration 11665: loss = 5.8979848e-09,1.2969528e-08\n",
      "Iteration 11670: loss = 5.8759118e-09,1.29701725e-08\n",
      "Iteration 11675: loss = 5.8710636e-09,1.29708155e-08\n",
      "Iteration 11680: loss = 5.8490617e-09,1.297146e-08\n",
      "Iteration 11685: loss = 5.844234e-09,1.2972106e-08\n",
      "Iteration 11690: loss = 5.82229e-09,1.297275e-08\n",
      "Iteration 11695: loss = 5.80893e-09,1.2973392e-08\n",
      "Iteration 11700: loss = 5.795607e-09,1.2974034e-08\n",
      "Iteration 11705: loss = 5.782301e-09,1.2974676e-08\n",
      "Iteration 11710: loss = 5.7690115e-09,1.2975316e-08\n",
      "Iteration 11715: loss = 5.7557528e-09,1.2975958e-08\n",
      "Iteration 11720: loss = 5.7340177e-09,1.2976599e-08\n",
      "Iteration 11725: loss = 5.7292957e-09,1.2977237e-08\n",
      "Iteration 11730: loss = 5.707621e-09,1.2977874e-08\n",
      "Iteration 11735: loss = 5.7029186e-09,1.2978513e-08\n",
      "Iteration 11740: loss = 5.6813056e-09,1.2979153e-08\n",
      "Iteration 11745: loss = 5.668184e-09,1.2979786e-08\n",
      "Iteration 11750: loss = 5.655091e-09,1.2980426e-08\n",
      "Iteration 11755: loss = 5.642009e-09,1.29810624e-08\n",
      "Iteration 11760: loss = 5.6289537e-09,1.2981696e-08\n",
      "Iteration 11765: loss = 5.615917e-09,1.2982331e-08\n",
      "Iteration 11770: loss = 5.5945155e-09,1.2982964e-08\n",
      "Iteration 11775: loss = 5.5899094e-09,1.2983597e-08\n",
      "Iteration 11780: loss = 5.5685696e-09,1.29842315e-08\n",
      "Iteration 11785: loss = 5.5639937e-09,1.2984866e-08\n",
      "Iteration 11790: loss = 5.5427116e-09,1.2985496e-08\n",
      "Iteration 11795: loss = 5.5381597e-09,1.2986127e-08\n",
      "Iteration 11800: loss = 5.516945e-09,1.2986756e-08\n",
      "Iteration 11805: loss = 5.5040856e-09,1.2987387e-08\n",
      "Iteration 11810: loss = 5.491252e-09,1.2988016e-08\n",
      "Iteration 11815: loss = 5.4784404e-09,1.2988645e-08\n",
      "Iteration 11820: loss = 5.465656e-09,1.29892745e-08\n",
      "Iteration 11825: loss = 5.4528915e-09,1.2989899e-08\n",
      "Iteration 11830: loss = 5.431874e-09,1.2990528e-08\n",
      "Iteration 11835: loss = 5.427412e-09,1.2991153e-08\n",
      "Iteration 11840: loss = 5.4064553e-09,1.2991778e-08\n",
      "Iteration 11845: loss = 5.402022e-09,1.2992403e-08\n",
      "Iteration 11850: loss = 5.3811284e-09,1.2993029e-08\n",
      "Iteration 11855: loss = 5.376707e-09,1.2993652e-08\n",
      "Iteration 11860: loss = 5.355877e-09,1.2994276e-08\n",
      "Iteration 11865: loss = 5.3432854e-09,1.29948985e-08\n",
      "Iteration 11870: loss = 5.330713e-09,1.2995519e-08\n",
      "Iteration 11875: loss = 5.318166e-09,1.299614e-08\n",
      "Iteration 11880: loss = 5.3056417e-09,1.2996758e-08\n",
      "Iteration 11885: loss = 5.2931344e-09,1.2997379e-08\n",
      "Iteration 11890: loss = 5.2806475e-09,1.2997998e-08\n",
      "Iteration 11895: loss = 5.2681806e-09,1.2998619e-08\n",
      "Iteration 11900: loss = 5.247619e-09,1.2999234e-08\n",
      "Iteration 11905: loss = 5.2433173e-09,1.2999852e-08\n",
      "Iteration 11910: loss = 5.222807e-09,1.30004665e-08\n",
      "Iteration 11915: loss = 5.218531e-09,1.3001085e-08\n",
      "Iteration 11920: loss = 5.1980797e-09,1.3001699e-08\n",
      "Iteration 11925: loss = 5.193824e-09,1.3002314e-08\n",
      "Iteration 11930: loss = 5.181512e-09,1.30029285e-08\n",
      "Iteration 11935: loss = 5.161148e-09,1.3003541e-08\n",
      "Iteration 11940: loss = 5.1569313e-09,1.3004153e-08\n",
      "Iteration 11945: loss = 5.1366276e-09,1.3004766e-08\n",
      "Iteration 11950: loss = 5.132432e-09,1.3005378e-08\n",
      "Iteration 11955: loss = 5.11219e-09,1.30059865e-08\n",
      "Iteration 11960: loss = 5.1080167e-09,1.30065985e-08\n",
      "Iteration 11965: loss = 5.08784e-09,1.3007207e-08\n",
      "Iteration 11970: loss = 5.083686e-09,1.3007816e-08\n",
      "Iteration 11975: loss = 5.0635673e-09,1.3008425e-08\n",
      "Iteration 11980: loss = 5.0514557e-09,1.300903e-08\n",
      "Iteration 11985: loss = 5.0393703e-09,1.3009638e-08\n",
      "Iteration 11990: loss = 5.027303e-09,1.30102435e-08\n",
      "Iteration 11995: loss = 5.015258e-09,1.3010851e-08\n",
      "Iteration 12000: loss = 5.0032316e-09,1.3011455e-08\n",
      "Iteration 12005: loss = 4.9912257e-09,1.3012061e-08\n",
      "Iteration 12010: loss = 4.9792352e-09,1.3012663e-08\n",
      "Iteration 12015: loss = 4.9751794e-09,1.3013267e-08\n",
      "Iteration 12020: loss = 4.9553317e-09,1.3013871e-08\n",
      "Iteration 12025: loss = 4.943401e-09,1.3014472e-08\n",
      "Iteration 12030: loss = 4.931494e-09,1.3015073e-08\n",
      "Iteration 12035: loss = 4.919609e-09,1.3015675e-08\n",
      "Iteration 12040: loss = 4.907742e-09,1.3016274e-08\n",
      "Iteration 12045: loss = 4.895897e-09,1.3016873e-08\n",
      "Iteration 12050: loss = 4.884071e-09,1.3017472e-08\n",
      "Iteration 12055: loss = 4.8722657e-09,1.3018069e-08\n",
      "Iteration 12060: loss = 4.860476e-09,1.3018666e-08\n",
      "Iteration 12065: loss = 4.848712e-09,1.3019266e-08\n",
      "Iteration 12070: loss = 4.8369677e-09,1.3019858e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12075: loss = 4.8252375e-09,1.3020454e-08\n",
      "Iteration 12080: loss = 4.8135296e-09,1.3021048e-08\n",
      "Iteration 12085: loss = 4.8018376e-09,1.3021642e-08\n",
      "Iteration 12090: loss = 4.7901705e-09,1.30222375e-08\n",
      "Iteration 12095: loss = 4.778523e-09,1.3022827e-08\n",
      "Iteration 12100: loss = 4.7668935e-09,1.302342e-08\n",
      "Iteration 12105: loss = 4.7552846e-09,1.302401e-08\n",
      "Iteration 12110: loss = 4.7436877e-09,1.3024602e-08\n",
      "Iteration 12115: loss = 4.732125e-09,1.3025191e-08\n",
      "Iteration 12120: loss = 4.720572e-09,1.3025782e-08\n",
      "Iteration 12125: loss = 4.709038e-09,1.302637e-08\n",
      "Iteration 12130: loss = 4.6975224e-09,1.3026957e-08\n",
      "Iteration 12135: loss = 4.6937108e-09,1.3027546e-08\n",
      "Iteration 12140: loss = 4.6745527e-09,1.30281315e-08\n",
      "Iteration 12145: loss = 4.663108e-09,1.3028718e-08\n",
      "Iteration 12150: loss = 4.651669e-09,1.3029303e-08\n",
      "Iteration 12155: loss = 4.6402584e-09,1.30298865e-08\n",
      "Iteration 12160: loss = 4.6288604e-09,1.3030471e-08\n",
      "Iteration 12165: loss = 4.617487e-09,1.3031054e-08\n",
      "Iteration 12170: loss = 4.6061257e-09,1.3031635e-08\n",
      "Iteration 12175: loss = 4.594788e-09,1.3032217e-08\n",
      "Iteration 12180: loss = 4.5834736e-09,1.3032799e-08\n",
      "Iteration 12185: loss = 4.572171e-09,1.3033379e-08\n",
      "Iteration 12190: loss = 4.5684607e-09,1.303396e-08\n",
      "Iteration 12195: loss = 4.549635e-09,1.3034537e-08\n",
      "Iteration 12200: loss = 4.545944e-09,1.3035116e-08\n",
      "Iteration 12205: loss = 4.5271653e-09,1.30356925e-08\n",
      "Iteration 12210: loss = 4.523498e-09,1.3036271e-08\n",
      "Iteration 12215: loss = 4.504778e-09,1.3036846e-08\n",
      "Iteration 12220: loss = 4.5011284e-09,1.3037422e-08\n",
      "Iteration 12225: loss = 4.4899706e-09,1.3037994e-08\n",
      "Iteration 12230: loss = 4.478838e-09,1.3038568e-08\n",
      "Iteration 12235: loss = 4.467718e-09,1.3039143e-08\n",
      "Iteration 12240: loss = 4.456615e-09,1.3039712e-08\n",
      "Iteration 12245: loss = 4.445534e-09,1.3040286e-08\n",
      "Iteration 12250: loss = 4.4344763e-09,1.3040856e-08\n",
      "Iteration 12255: loss = 4.423427e-09,1.30414275e-08\n",
      "Iteration 12260: loss = 4.40496e-09,1.3041997e-08\n",
      "Iteration 12265: loss = 4.4014006e-09,1.3042564e-08\n",
      "Iteration 12270: loss = 4.390414e-09,1.3043135e-08\n",
      "Iteration 12275: loss = 4.379445e-09,1.3043703e-08\n",
      "Iteration 12280: loss = 4.368495e-09,1.3044268e-08\n",
      "Iteration 12285: loss = 4.3575623e-09,1.30448345e-08\n",
      "Iteration 12290: loss = 4.3466524e-09,1.30454e-08\n",
      "Iteration 12295: loss = 4.3357544e-09,1.3045964e-08\n",
      "Iteration 12300: loss = 4.3248876e-09,1.3046527e-08\n",
      "Iteration 12305: loss = 4.3213926e-09,1.3047092e-08\n",
      "Iteration 12310: loss = 4.303191e-09,1.3047654e-08\n",
      "Iteration 12315: loss = 4.2997166e-09,1.3048215e-08\n",
      "Iteration 12320: loss = 4.281567e-09,1.3048777e-08\n",
      "Iteration 12325: loss = 4.278115e-09,1.3049338e-08\n",
      "Iteration 12330: loss = 4.2600186e-09,1.3049897e-08\n",
      "Iteration 12335: loss = 4.2565844e-09,1.3050457e-08\n",
      "Iteration 12340: loss = 4.245847e-09,1.3051016e-08\n",
      "Iteration 12345: loss = 4.2351265e-09,1.3051573e-08\n",
      "Iteration 12350: loss = 4.22443e-09,1.3052129e-08\n",
      "Iteration 12355: loss = 4.213748e-09,1.3052686e-08\n",
      "Iteration 12360: loss = 4.203082e-09,1.3053244e-08\n",
      "Iteration 12365: loss = 4.1924415e-09,1.3053798e-08\n",
      "Iteration 12370: loss = 4.181812e-09,1.3054351e-08\n",
      "Iteration 12375: loss = 4.1784447e-09,1.3054907e-08\n",
      "Iteration 12380: loss = 4.1606074e-09,1.3055457e-08\n",
      "Iteration 12385: loss = 4.1572625e-09,1.305601e-08\n",
      "Iteration 12390: loss = 4.1394785e-09,1.305656e-08\n",
      "Iteration 12395: loss = 4.136148e-09,1.3057111e-08\n",
      "Iteration 12400: loss = 4.118427e-09,1.305766e-08\n",
      "Iteration 12405: loss = 4.1151127e-09,1.3058209e-08\n",
      "Iteration 12410: loss = 4.1046193e-09,1.3058757e-08\n",
      "Iteration 12415: loss = 4.094145e-09,1.3059303e-08\n",
      "Iteration 12420: loss = 4.0836867e-09,1.305985e-08\n",
      "Iteration 12425: loss = 4.0732497e-09,1.3060395e-08\n",
      "Iteration 12430: loss = 4.0628305e-09,1.3060942e-08\n",
      "Iteration 12435: loss = 4.052428e-09,1.3061485e-08\n",
      "Iteration 12440: loss = 4.0491726e-09,1.3062029e-08\n",
      "Iteration 12445: loss = 4.0387955e-09,1.3062572e-08\n",
      "Iteration 12450: loss = 4.0284385e-09,1.30631115e-08\n",
      "Iteration 12455: loss = 4.018099e-09,1.3063657e-08\n",
      "Iteration 12460: loss = 4.0077768e-09,1.3064196e-08\n",
      "Iteration 12465: loss = 3.9974704e-09,1.3064735e-08\n",
      "Iteration 12470: loss = 3.994265e-09,1.3065273e-08\n",
      "Iteration 12475: loss = 3.9769144e-09,1.3065815e-08\n",
      "Iteration 12480: loss = 3.9737302e-09,1.3066348e-08\n",
      "Iteration 12485: loss = 3.9564303e-09,1.3066888e-08\n",
      "Iteration 12490: loss = 3.9532613e-09,1.3067424e-08\n",
      "Iteration 12495: loss = 3.9430508e-09,1.3067958e-08\n",
      "Iteration 12500: loss = 3.9328634e-09,1.3068494e-08\n",
      "Iteration 12505: loss = 3.9226924e-09,1.3069027e-08\n",
      "Iteration 12510: loss = 3.9125374e-09,1.3069559e-08\n",
      "Iteration 12515: loss = 3.902405e-09,1.3070091e-08\n",
      "Iteration 12520: loss = 3.8922825e-09,1.3070622e-08\n",
      "Iteration 12525: loss = 3.882183e-09,1.3071154e-08\n",
      "Iteration 12530: loss = 3.879073e-09,1.3071683e-08\n",
      "Iteration 12535: loss = 3.862031e-09,1.3072212e-08\n",
      "Iteration 12540: loss = 3.858941e-09,1.3072742e-08\n",
      "Iteration 12545: loss = 3.841943e-09,1.3073268e-08\n",
      "Iteration 12550: loss = 3.838875e-09,1.3073796e-08\n",
      "Iteration 12555: loss = 3.835803e-09,1.307432e-08\n",
      "Iteration 12560: loss = 3.818879e-09,1.3074847e-08\n",
      "Iteration 12565: loss = 3.815821e-09,1.307537e-08\n",
      "Iteration 12570: loss = 3.798949e-09,1.3075896e-08\n",
      "Iteration 12575: loss = 3.795913e-09,1.3076416e-08\n",
      "Iteration 12580: loss = 3.785984e-09,1.307694e-08\n",
      "Iteration 12585: loss = 3.776075e-09,1.3077462e-08\n",
      "Iteration 12590: loss = 3.766178e-09,1.3077981e-08\n",
      "Iteration 12595: loss = 3.756305e-09,1.3078502e-08\n",
      "Iteration 12600: loss = 3.7464427e-09,1.30790205e-08\n",
      "Iteration 12605: loss = 3.7434535e-09,1.3079538e-08\n",
      "Iteration 12610: loss = 3.7267767e-09,1.3080057e-08\n",
      "Iteration 12615: loss = 3.7238028e-09,1.3080574e-08\n",
      "Iteration 12620: loss = 3.707178e-09,1.3081089e-08\n",
      "Iteration 12625: loss = 3.7042236e-09,1.3081604e-08\n",
      "Iteration 12630: loss = 3.7012697e-09,1.30821185e-08\n",
      "Iteration 12635: loss = 3.684704e-09,1.30826345e-08\n",
      "Iteration 12640: loss = 3.681772e-09,1.3083147e-08\n",
      "Iteration 12645: loss = 3.6652563e-09,1.3083658e-08\n",
      "Iteration 12650: loss = 3.6623382e-09,1.3084169e-08\n",
      "Iteration 12655: loss = 3.6526464e-09,1.3084679e-08\n",
      "Iteration 12660: loss = 3.6429701e-09,1.3085189e-08\n",
      "Iteration 12665: loss = 3.6333148e-09,1.3085698e-08\n",
      "Iteration 12670: loss = 3.6236747e-09,1.3086207e-08\n",
      "Iteration 12675: loss = 3.6140555e-09,1.3086712e-08\n",
      "Iteration 12680: loss = 3.6111791e-09,1.308722e-08\n",
      "Iteration 12685: loss = 3.6015801e-09,1.3087723e-08\n",
      "Iteration 12690: loss = 3.5919934e-09,1.3088229e-08\n",
      "Iteration 12695: loss = 3.5891459e-09,1.3088734e-08\n",
      "Iteration 12700: loss = 3.5728858e-09,1.3089238e-08\n",
      "Iteration 12705: loss = 3.5700491e-09,1.3089739e-08\n",
      "Iteration 12710: loss = 3.5538419e-09,1.3090241e-08\n",
      "Iteration 12715: loss = 3.551017e-09,1.3090741e-08\n",
      "Iteration 12720: loss = 3.5415237e-09,1.309124e-08\n",
      "Iteration 12725: loss = 3.5320555e-09,1.3091738e-08\n",
      "Iteration 12730: loss = 3.5225962e-09,1.3092239e-08\n",
      "Iteration 12735: loss = 3.513157e-09,1.3092734e-08\n",
      "Iteration 12740: loss = 3.5103722e-09,1.3093231e-08\n",
      "Iteration 12745: loss = 3.50095e-09,1.3093727e-08\n",
      "Iteration 12750: loss = 3.4915513e-09,1.3094219e-08\n",
      "Iteration 12755: loss = 3.4821686e-09,1.3094715e-08\n",
      "Iteration 12760: loss = 3.4728034e-09,1.3095205e-08\n",
      "Iteration 12765: loss = 3.4634569e-09,1.3095699e-08\n",
      "Iteration 12770: loss = 3.4607137e-09,1.3096187e-08\n",
      "Iteration 12775: loss = 3.4448078e-09,1.3096679e-08\n",
      "Iteration 12780: loss = 3.4420793e-09,1.3097168e-08\n",
      "Iteration 12785: loss = 3.4393544e-09,1.3097656e-08\n",
      "Iteration 12790: loss = 3.4235086e-09,1.3098147e-08\n",
      "Iteration 12795: loss = 3.420798e-09,1.3098633e-08\n",
      "Iteration 12800: loss = 3.405005e-09,1.3099121e-08\n",
      "Iteration 12805: loss = 3.4023084e-09,1.3099605e-08\n",
      "Iteration 12810: loss = 3.3930938e-09,1.3100088e-08\n",
      "Iteration 12815: loss = 3.3838898e-09,1.31005695e-08\n",
      "Iteration 12820: loss = 3.3812144e-09,1.3101053e-08\n",
      "Iteration 12825: loss = 3.372036e-09,1.3101536e-08\n",
      "Iteration 12830: loss = 3.3628735e-09,1.3102016e-08\n",
      "Iteration 12835: loss = 3.353722e-09,1.3102497e-08\n",
      "Iteration 12840: loss = 3.344595e-09,1.3102976e-08\n",
      "Iteration 12845: loss = 3.3354792e-09,1.3103454e-08\n",
      "Iteration 12850: loss = 3.3328424e-09,1.3103934e-08\n",
      "Iteration 12855: loss = 3.3172964e-09,1.3104409e-08\n",
      "Iteration 12860: loss = 3.3146768e-09,1.3104885e-08\n",
      "Iteration 12865: loss = 3.312066e-09,1.3105358e-08\n",
      "Iteration 12870: loss = 3.2965783e-09,1.3105833e-08\n",
      "Iteration 12875: loss = 3.293979e-09,1.3106307e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12880: loss = 3.2785374e-09,1.310678e-08\n",
      "Iteration 12885: loss = 3.2759522e-09,1.310725e-08\n",
      "Iteration 12890: loss = 3.2669671e-09,1.3107721e-08\n",
      "Iteration 12895: loss = 3.257995e-09,1.3108192e-08\n",
      "Iteration 12900: loss = 3.2554288e-09,1.3108661e-08\n",
      "Iteration 12905: loss = 3.2464798e-09,1.310913e-08\n",
      "Iteration 12910: loss = 3.2375438e-09,1.3109597e-08\n",
      "Iteration 12915: loss = 3.2286325e-09,1.3110063e-08\n",
      "Iteration 12920: loss = 3.2197283e-09,1.3110528e-08\n",
      "Iteration 12925: loss = 3.2171972e-09,1.3110993e-08\n",
      "Iteration 12930: loss = 3.2083183e-09,1.3111455e-08\n",
      "Iteration 12935: loss = 3.1994558e-09,1.311192e-08\n",
      "Iteration 12940: loss = 3.1906084e-09,1.3112381e-08\n",
      "Iteration 12945: loss = 3.1881033e-09,1.3112842e-08\n",
      "Iteration 12950: loss = 3.1729652e-09,1.3113302e-08\n",
      "Iteration 12955: loss = 3.170471e-09,1.3113761e-08\n",
      "Iteration 12960: loss = 3.1679876e-09,1.3114219e-08\n",
      "Iteration 12965: loss = 3.1529055e-09,1.3114679e-08\n",
      "Iteration 12970: loss = 3.1504275e-09,1.3115136e-08\n",
      "Iteration 12975: loss = 3.1353968e-09,1.3115591e-08\n",
      "Iteration 12980: loss = 3.1329404e-09,1.3116045e-08\n",
      "Iteration 12985: loss = 3.1304808e-09,1.3116498e-08\n",
      "Iteration 12990: loss = 3.1155116e-09,1.3116953e-08\n",
      "Iteration 12995: loss = 3.1130722e-09,1.3117404e-08\n",
      "Iteration 13000: loss = 3.1043814e-09,1.3117857e-08\n",
      "Iteration 13005: loss = 3.0957163e-09,1.31183056e-08\n",
      "Iteration 13010: loss = 3.0870593e-09,1.3118756e-08\n",
      "Iteration 13015: loss = 3.0846445e-09,1.3119204e-08\n",
      "Iteration 13020: loss = 3.0760132e-09,1.3119651e-08\n",
      "Iteration 13025: loss = 3.0673952e-09,1.3120098e-08\n",
      "Iteration 13030: loss = 3.064998e-09,1.3120544e-08\n",
      "Iteration 13035: loss = 3.0502136e-09,1.3120987e-08\n",
      "Iteration 13040: loss = 3.0478298e-09,1.3121432e-08\n",
      "Iteration 13045: loss = 3.0392657e-09,1.3121875e-08\n",
      "Iteration 13050: loss = 3.0307168e-09,1.3122315e-08\n",
      "Iteration 13055: loss = 3.0283465e-09,1.3122754e-08\n",
      "Iteration 13060: loss = 3.013672e-09,1.3123197e-08\n",
      "Iteration 13065: loss = 3.0113145e-09,1.3123637e-08\n",
      "Iteration 13070: loss = 3.0089653e-09,1.3124074e-08\n",
      "Iteration 13075: loss = 2.9943419e-09,1.3124513e-08\n",
      "Iteration 13080: loss = 2.9920024e-09,1.3124948e-08\n",
      "Iteration 13085: loss = 2.9835483e-09,1.3125384e-08\n",
      "Iteration 13090: loss = 2.9751084e-09,1.3125816e-08\n",
      "Iteration 13095: loss = 2.9727867e-09,1.312625e-08\n",
      "Iteration 13100: loss = 2.9643663e-09,1.3126685e-08\n",
      "Iteration 13105: loss = 2.9559617e-09,1.31271145e-08\n",
      "Iteration 13110: loss = 2.947574e-09,1.3127546e-08\n",
      "Iteration 13115: loss = 2.9391976e-09,1.3127978e-08\n",
      "Iteration 13120: loss = 2.9369067e-09,1.3128406e-08\n",
      "Iteration 13125: loss = 2.9285554e-09,1.3128832e-08\n",
      "Iteration 13130: loss = 2.9202154e-09,1.31292595e-08\n",
      "Iteration 13135: loss = 2.9118947e-09,1.3129686e-08\n",
      "Iteration 13140: loss = 2.9096274e-09,1.3130114e-08\n",
      "Iteration 13145: loss = 2.9013256e-09,1.3130537e-08\n",
      "Iteration 13150: loss = 2.8930434e-09,1.3130959e-08\n",
      "Iteration 13155: loss = 2.8907925e-09,1.3131383e-08\n",
      "Iteration 13160: loss = 2.8765197e-09,1.3131803e-08\n",
      "Iteration 13165: loss = 2.8742821e-09,1.3132224e-08\n",
      "Iteration 13170: loss = 2.87205e-09,1.3132642e-08\n",
      "Iteration 13175: loss = 2.8578295e-09,1.3133061e-08\n",
      "Iteration 13180: loss = 2.8556084e-09,1.3133476e-08\n",
      "Iteration 13185: loss = 2.8474094e-09,1.3133896e-08\n",
      "Iteration 13190: loss = 2.839229e-09,1.3134309e-08\n",
      "Iteration 13195: loss = 2.8370233e-09,1.3134722e-08\n",
      "Iteration 13200: loss = 2.8288643e-09,1.3135138e-08\n",
      "Iteration 13205: loss = 2.820718e-09,1.31355495e-08\n",
      "Iteration 13210: loss = 2.8125893e-09,1.313596e-08\n",
      "Iteration 13215: loss = 2.810408e-09,1.3136371e-08\n",
      "Iteration 13220: loss = 2.8022955e-09,1.3136784e-08\n",
      "Iteration 13225: loss = 2.7941998e-09,1.313719e-08\n",
      "Iteration 13230: loss = 2.7920368e-09,1.3137597e-08\n",
      "Iteration 13235: loss = 2.7839613e-09,1.3138004e-08\n",
      "Iteration 13240: loss = 2.7758995e-09,1.31384095e-08\n",
      "Iteration 13245: loss = 2.7737537e-09,1.31388145e-08\n",
      "Iteration 13250: loss = 2.7598241e-09,1.313922e-08\n",
      "Iteration 13255: loss = 2.7576876e-09,1.313962e-08\n",
      "Iteration 13260: loss = 2.755557e-09,1.3140022e-08\n",
      "Iteration 13265: loss = 2.7416862e-09,1.3140426e-08\n",
      "Iteration 13270: loss = 2.739566e-09,1.31408235e-08\n",
      "Iteration 13275: loss = 2.7315918e-09,1.3141223e-08\n",
      "Iteration 13280: loss = 2.7236313e-09,1.314162e-08\n",
      "Iteration 13285: loss = 2.721529e-09,1.3142017e-08\n",
      "Iteration 13290: loss = 2.7135878e-09,1.3142413e-08\n",
      "Iteration 13295: loss = 2.7056632e-09,1.31428095e-08\n",
      "Iteration 13300: loss = 2.703576e-09,1.3143203e-08\n",
      "Iteration 13305: loss = 2.6956697e-09,1.3143596e-08\n",
      "Iteration 13310: loss = 2.6877844e-09,1.3143991e-08\n",
      "Iteration 13315: loss = 2.6799043e-09,1.314438e-08\n",
      "Iteration 13320: loss = 2.677842e-09,1.3144771e-08\n",
      "Iteration 13325: loss = 2.6699845e-09,1.314516e-08\n",
      "Iteration 13330: loss = 2.6621467e-09,1.3145549e-08\n",
      "Iteration 13335: loss = 2.6600915e-09,1.3145937e-08\n",
      "Iteration 13340: loss = 2.652271e-09,1.31463205e-08\n",
      "Iteration 13345: loss = 2.6444684e-09,1.3146706e-08\n",
      "Iteration 13350: loss = 2.6424347e-09,1.3147091e-08\n",
      "Iteration 13355: loss = 2.6289007e-09,1.3147474e-08\n",
      "Iteration 13360: loss = 2.626873e-09,1.3147855e-08\n",
      "Iteration 13365: loss = 2.6248586e-09,1.3148236e-08\n",
      "Iteration 13370: loss = 2.6113824e-09,1.3148616e-08\n",
      "Iteration 13375: loss = 2.609374e-09,1.3148997e-08\n",
      "Iteration 13380: loss = 2.6073745e-09,1.3149373e-08\n",
      "Iteration 13385: loss = 2.5939493e-09,1.314975e-08\n",
      "Iteration 13390: loss = 2.5919533e-09,1.3150124e-08\n",
      "Iteration 13395: loss = 2.5899656e-09,1.31505e-08\n",
      "Iteration 13400: loss = 2.5765943e-09,1.3150876e-08\n",
      "Iteration 13405: loss = 2.574618e-09,1.3151247e-08\n",
      "Iteration 13410: loss = 2.5669618e-09,1.315162e-08\n",
      "Iteration 13415: loss = 2.5593268e-09,1.315199e-08\n",
      "Iteration 13420: loss = 2.557363e-09,1.3152359e-08\n",
      "Iteration 13425: loss = 2.5497446e-09,1.315273e-08\n",
      "Iteration 13430: loss = 2.5421407e-09,1.3153098e-08\n",
      "Iteration 13435: loss = 2.5401912e-09,1.31534605e-08\n",
      "Iteration 13440: loss = 2.5326095e-09,1.3153828e-08\n",
      "Iteration 13445: loss = 2.5250417e-09,1.31541915e-08\n",
      "Iteration 13450: loss = 2.5231046e-09,1.31545566e-08\n",
      "Iteration 13455: loss = 2.5155558e-09,1.3154917e-08\n",
      "Iteration 13460: loss = 2.5080187e-09,1.315528e-08\n",
      "Iteration 13465: loss = 2.5004976e-09,1.315564e-08\n",
      "Iteration 13470: loss = 2.4985825e-09,1.3155999e-08\n",
      "Iteration 13475: loss = 2.491081e-09,1.3156359e-08\n",
      "Iteration 13480: loss = 2.48359e-09,1.3156715e-08\n",
      "Iteration 13485: loss = 2.4816944e-09,1.315707e-08\n",
      "Iteration 13490: loss = 2.4742224e-09,1.3157427e-08\n",
      "Iteration 13495: loss = 2.466772e-09,1.3157779e-08\n",
      "Iteration 13500: loss = 2.4648872e-09,1.31581315e-08\n",
      "Iteration 13505: loss = 2.4630056e-09,1.3158483e-08\n",
      "Iteration 13510: loss = 2.4500293e-09,1.3158836e-08\n",
      "Iteration 13515: loss = 2.4481626e-09,1.3159184e-08\n",
      "Iteration 13520: loss = 2.4462963e-09,1.315953e-08\n",
      "Iteration 13525: loss = 2.4333697e-09,1.31598785e-08\n",
      "Iteration 13530: loss = 2.4315174e-09,1.3160227e-08\n",
      "Iteration 13535: loss = 2.429666e-09,1.3160572e-08\n",
      "Iteration 13540: loss = 2.4167932e-09,1.3160918e-08\n",
      "Iteration 13545: loss = 2.4149527e-09,1.3161259e-08\n",
      "Iteration 13550: loss = 2.4076148e-09,1.3161602e-08\n",
      "Iteration 13555: loss = 2.4002922e-09,1.3161943e-08\n",
      "Iteration 13560: loss = 2.3984648e-09,1.3162283e-08\n",
      "Iteration 13565: loss = 2.3911626e-09,1.3162623e-08\n",
      "Iteration 13570: loss = 2.3838733e-09,1.316296e-08\n",
      "Iteration 13575: loss = 2.3820592e-09,1.3163295e-08\n",
      "Iteration 13580: loss = 2.3747888e-09,1.3163632e-08\n",
      "Iteration 13585: loss = 2.3729858e-09,1.3163964e-08\n",
      "Iteration 13590: loss = 2.3657354e-09,1.3164299e-08\n",
      "Iteration 13595: loss = 2.3584985e-09,1.3164634e-08\n",
      "Iteration 13600: loss = 2.3567073e-09,1.3164962e-08\n",
      "Iteration 13605: loss = 2.3494897e-09,1.3165292e-08\n",
      "Iteration 13610: loss = 2.3422857e-09,1.31656215e-08\n",
      "Iteration 13615: loss = 2.3405085e-09,1.3165948e-08\n",
      "Iteration 13620: loss = 2.3333198e-09,1.3166274e-08\n",
      "Iteration 13625: loss = 2.3261502e-09,1.3166602e-08\n",
      "Iteration 13630: loss = 2.324387e-09,1.3166925e-08\n",
      "Iteration 13635: loss = 2.3172306e-09,1.3167249e-08\n",
      "Iteration 13640: loss = 2.3100941e-09,1.3167572e-08\n",
      "Iteration 13645: loss = 2.308343e-09,1.31678926e-08\n",
      "Iteration 13650: loss = 2.3066002e-09,1.3168211e-08\n",
      "Iteration 13655: loss = 2.2941131e-09,1.3168531e-08\n",
      "Iteration 13660: loss = 2.2923783e-09,1.3168849e-08\n",
      "Iteration 13665: loss = 2.290646e-09,1.3169164e-08\n",
      "Iteration 13670: loss = 2.278212e-09,1.3169483e-08\n",
      "Iteration 13675: loss = 2.276489e-09,1.3169796e-08\n",
      "Iteration 13680: loss = 2.2747695e-09,1.317011e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13685: loss = 2.262391e-09,1.3170422e-08\n",
      "Iteration 13690: loss = 2.2606792e-09,1.3170734e-08\n",
      "Iteration 13695: loss = 2.2589763e-09,1.3171046e-08\n",
      "Iteration 13700: loss = 2.2519544e-09,1.3171352e-08\n",
      "Iteration 13705: loss = 2.2449498e-09,1.3171661e-08\n",
      "Iteration 13710: loss = 2.2432571e-09,1.3171967e-08\n",
      "Iteration 13715: loss = 2.2362692e-09,1.3172273e-08\n",
      "Iteration 13720: loss = 2.229292e-09,1.3172575e-08\n",
      "Iteration 13725: loss = 2.2276134e-09,1.317288e-08\n",
      "Iteration 13730: loss = 2.2206572e-09,1.3173178e-08\n",
      "Iteration 13735: loss = 2.2137152e-09,1.3173481e-08\n",
      "Iteration 13740: loss = 2.2120503e-09,1.3173781e-08\n",
      "Iteration 13745: loss = 2.2051234e-09,1.3174078e-08\n",
      "Iteration 13750: loss = 2.2034687e-09,1.3174375e-08\n",
      "Iteration 13755: loss = 2.19656e-09,1.3174671e-08\n",
      "Iteration 13760: loss = 2.1896647e-09,1.3174967e-08\n",
      "Iteration 13765: loss = 2.1880213e-09,1.31752635e-08\n",
      "Iteration 13770: loss = 2.1811442e-09,1.3175557e-08\n",
      "Iteration 13775: loss = 2.1742823e-09,1.3175848e-08\n",
      "Iteration 13780: loss = 2.1726507e-09,1.3176136e-08\n",
      "Iteration 13785: loss = 2.165806e-09,1.3176427e-08\n",
      "Iteration 13790: loss = 2.1589746e-09,1.3176715e-08\n",
      "Iteration 13795: loss = 2.1573558e-09,1.3177002e-08\n",
      "Iteration 13800: loss = 2.1557394e-09,1.3177288e-08\n",
      "Iteration 13805: loss = 2.1437436e-09,1.3177575e-08\n",
      "Iteration 13810: loss = 2.1421367e-09,1.3177858e-08\n",
      "Iteration 13815: loss = 2.1405349e-09,1.3178138e-08\n",
      "Iteration 13820: loss = 2.1285889e-09,1.317842e-08\n",
      "Iteration 13825: loss = 2.126992e-09,1.3178699e-08\n",
      "Iteration 13830: loss = 2.1254e-09,1.3178978e-08\n",
      "Iteration 13835: loss = 2.1186566e-09,1.3179259e-08\n",
      "Iteration 13840: loss = 2.1119217e-09,1.3179532e-08\n",
      "Iteration 13845: loss = 2.1103428e-09,1.31798075e-08\n",
      "Iteration 13850: loss = 2.1036255e-09,1.3180082e-08\n",
      "Iteration 13855: loss = 2.0969257e-09,1.31803555e-08\n",
      "Iteration 13860: loss = 2.0953572e-09,1.3180628e-08\n",
      "Iteration 13865: loss = 2.0886766e-09,1.3180896e-08\n",
      "Iteration 13870: loss = 2.0820061e-09,1.3181165e-08\n",
      "Iteration 13875: loss = 2.0804507e-09,1.3181433e-08\n",
      "Iteration 13880: loss = 2.0789004e-09,1.3181699e-08\n",
      "Iteration 13885: loss = 2.07225e-09,1.3181965e-08\n",
      "Iteration 13890: loss = 2.0656128e-09,1.31822295e-08\n",
      "Iteration 13895: loss = 2.0640738e-09,1.3182492e-08\n",
      "Iteration 13900: loss = 2.0574549e-09,1.3182754e-08\n",
      "Iteration 13905: loss = 2.050853e-09,1.3183014e-08\n",
      "Iteration 13910: loss = 2.0493236e-09,1.3183274e-08\n",
      "Iteration 13915: loss = 2.0427373e-09,1.3183533e-08\n",
      "Iteration 13920: loss = 2.041218e-09,1.318379e-08\n",
      "Iteration 13925: loss = 2.034646e-09,1.31840485e-08\n",
      "Iteration 13930: loss = 2.0280895e-09,1.3184302e-08\n",
      "Iteration 13935: loss = 2.0265827e-09,1.3184554e-08\n",
      "Iteration 13940: loss = 2.0200428e-09,1.3184808e-08\n",
      "Iteration 13945: loss = 2.013516e-09,1.31850575e-08\n",
      "Iteration 13950: loss = 2.012021e-09,1.3185308e-08\n",
      "Iteration 13955: loss = 2.0105275e-09,1.3185556e-08\n",
      "Iteration 13960: loss = 2.0040207e-09,1.31858044e-08\n",
      "Iteration 13965: loss = 1.9975304e-09,1.3186049e-08\n",
      "Iteration 13970: loss = 1.9960489e-09,1.3186294e-08\n",
      "Iteration 13975: loss = 1.9945712e-09,1.3186538e-08\n",
      "Iteration 13980: loss = 1.98311e-09,1.31867814e-08\n",
      "Iteration 13985: loss = 1.9816448e-09,1.3187022e-08\n",
      "Iteration 13990: loss = 1.9801754e-09,1.3187262e-08\n",
      "Iteration 13995: loss = 1.9687663e-09,1.3187503e-08\n",
      "Iteration 14000: loss = 1.9673072e-09,1.3187739e-08\n",
      "Iteration 14005: loss = 1.9658526e-09,1.3187975e-08\n",
      "Iteration 14010: loss = 1.9594402e-09,1.31882105e-08\n",
      "Iteration 14015: loss = 1.953042e-09,1.3188442e-08\n",
      "Iteration 14020: loss = 1.9515987e-09,1.3188674e-08\n",
      "Iteration 14025: loss = 1.9501571e-09,1.3188906e-08\n",
      "Iteration 14030: loss = 1.9388475e-09,1.3189135e-08\n",
      "Iteration 14035: loss = 1.9374145e-09,1.3189364e-08\n",
      "Iteration 14040: loss = 1.9359871e-09,1.31895925e-08\n",
      "Iteration 14045: loss = 1.9296367e-09,1.3189818e-08\n",
      "Iteration 14050: loss = 1.923304e-09,1.3190041e-08\n",
      "Iteration 14055: loss = 1.9218864e-09,1.3190266e-08\n",
      "Iteration 14060: loss = 1.9155693e-09,1.31904905e-08\n",
      "Iteration 14065: loss = 1.914156e-09,1.319071e-08\n",
      "Iteration 14070: loss = 1.907857e-09,1.3190928e-08\n",
      "Iteration 14075: loss = 1.9064523e-09,1.3191148e-08\n",
      "Iteration 14080: loss = 1.9001691e-09,1.31913644e-08\n",
      "Iteration 14085: loss = 1.8938955e-09,1.3191583e-08\n",
      "Iteration 14090: loss = 1.8925033e-09,1.3191796e-08\n",
      "Iteration 14095: loss = 1.8862487e-09,1.3192008e-08\n",
      "Iteration 14100: loss = 1.884864e-09,1.3192222e-08\n",
      "Iteration 14105: loss = 1.8786261e-09,1.3192432e-08\n",
      "Iteration 14110: loss = 1.877247e-09,1.3192642e-08\n",
      "Iteration 14115: loss = 1.871024e-09,1.319285e-08\n",
      "Iteration 14120: loss = 1.8696522e-09,1.3193055e-08\n",
      "Iteration 14125: loss = 1.8634483e-09,1.3193263e-08\n",
      "Iteration 14130: loss = 1.8572535e-09,1.3193466e-08\n",
      "Iteration 14135: loss = 1.8558942e-09,1.3193671e-08\n",
      "Iteration 14140: loss = 1.8497182e-09,1.3193871e-08\n",
      "Iteration 14145: loss = 1.8435534e-09,1.3194073e-08\n",
      "Iteration 14150: loss = 1.8422028e-09,1.3194271e-08\n",
      "Iteration 14155: loss = 1.8408547e-09,1.31944695e-08\n",
      "Iteration 14160: loss = 1.834715e-09,1.3194668e-08\n",
      "Iteration 14165: loss = 1.8285832e-09,1.3194863e-08\n",
      "Iteration 14170: loss = 1.8272477e-09,1.3195054e-08\n",
      "Iteration 14175: loss = 1.8259149e-09,1.31952484e-08\n",
      "Iteration 14180: loss = 1.8150305e-09,1.3195439e-08\n",
      "Iteration 14185: loss = 1.8137051e-09,1.31956295e-08\n",
      "Iteration 14190: loss = 1.8123822e-09,1.3195816e-08\n",
      "Iteration 14195: loss = 1.8110621e-09,1.3196007e-08\n",
      "Iteration 14200: loss = 1.800232e-09,1.3196191e-08\n",
      "Iteration 14205: loss = 1.7989187e-09,1.3196375e-08\n",
      "Iteration 14210: loss = 1.7976095e-09,1.319656e-08\n",
      "Iteration 14215: loss = 1.7868262e-09,1.31967415e-08\n",
      "Iteration 14220: loss = 1.785523e-09,1.3196922e-08\n",
      "Iteration 14225: loss = 1.7842234e-09,1.3197101e-08\n",
      "Iteration 14230: loss = 1.7829279e-09,1.319728e-08\n",
      "Iteration 14235: loss = 1.7721965e-09,1.31974565e-08\n",
      "Iteration 14240: loss = 1.7709075e-09,1.3197629e-08\n",
      "Iteration 14245: loss = 1.7696214e-09,1.3197804e-08\n",
      "Iteration 14250: loss = 1.763634e-09,1.3197974e-08\n",
      "Iteration 14255: loss = 1.7576611e-09,1.3198145e-08\n",
      "Iteration 14260: loss = 1.756383e-09,1.3198317e-08\n",
      "Iteration 14265: loss = 1.7551084e-09,1.3198485e-08\n",
      "Iteration 14270: loss = 1.7491527e-09,1.3198651e-08\n",
      "Iteration 14275: loss = 1.7432087e-09,1.3198816e-08\n",
      "Iteration 14280: loss = 1.7419427e-09,1.31989815e-08\n",
      "Iteration 14285: loss = 1.7406818e-09,1.3199145e-08\n",
      "Iteration 14290: loss = 1.7347596e-09,1.31993065e-08\n",
      "Iteration 14295: loss = 1.7288474e-09,1.3199466e-08\n",
      "Iteration 14300: loss = 1.7275967e-09,1.3199626e-08\n",
      "Iteration 14305: loss = 1.7217007e-09,1.3199784e-08\n",
      "Iteration 14310: loss = 1.7158203e-09,1.3199939e-08\n",
      "Iteration 14315: loss = 1.7145766e-09,1.3200095e-08\n",
      "Iteration 14320: loss = 1.7133379e-09,1.3200248e-08\n",
      "Iteration 14325: loss = 1.7074727e-09,1.32004025e-08\n",
      "Iteration 14330: loss = 1.7016212e-09,1.3200549e-08\n",
      "Iteration 14335: loss = 1.700393e-09,1.3200704e-08\n",
      "Iteration 14340: loss = 1.6945599e-09,1.3200849e-08\n",
      "Iteration 14345: loss = 1.6933349e-09,1.3200997e-08\n",
      "Iteration 14350: loss = 1.6875165e-09,1.32011415e-08\n",
      "Iteration 14355: loss = 1.6862982e-09,1.3201287e-08\n",
      "Iteration 14360: loss = 1.6804954e-09,1.320143e-08\n",
      "Iteration 14365: loss = 1.6792843e-09,1.320157e-08\n",
      "Iteration 14370: loss = 1.6734957e-09,1.3201709e-08\n",
      "Iteration 14375: loss = 1.6722895e-09,1.32018485e-08\n",
      "Iteration 14380: loss = 1.6665167e-09,1.3201985e-08\n",
      "Iteration 14385: loss = 1.6653203e-09,1.3202121e-08\n",
      "Iteration 14390: loss = 1.659561e-09,1.3202255e-08\n",
      "Iteration 14395: loss = 1.6583677e-09,1.3202388e-08\n",
      "Iteration 14400: loss = 1.6526279e-09,1.3202521e-08\n",
      "Iteration 14405: loss = 1.6514411e-09,1.320265e-08\n",
      "Iteration 14410: loss = 1.645713e-09,1.3202779e-08\n",
      "Iteration 14415: loss = 1.640001e-09,1.3202907e-08\n",
      "Iteration 14420: loss = 1.6388243e-09,1.32030324e-08\n",
      "Iteration 14425: loss = 1.6376505e-09,1.3203158e-08\n",
      "Iteration 14430: loss = 1.6319531e-09,1.3203279e-08\n",
      "Iteration 14435: loss = 1.6262715e-09,1.32034e-08\n",
      "Iteration 14440: loss = 1.6251055e-09,1.3203522e-08\n",
      "Iteration 14445: loss = 1.6239428e-09,1.3203638e-08\n",
      "Iteration 14450: loss = 1.6182781e-09,1.3203756e-08\n",
      "Iteration 14455: loss = 1.6126283e-09,1.3203872e-08\n",
      "Iteration 14460: loss = 1.6114736e-09,1.3203986e-08\n",
      "Iteration 14465: loss = 1.610324e-09,1.3204099e-08\n",
      "Iteration 14470: loss = 1.6046878e-09,1.320421e-08\n",
      "Iteration 14475: loss = 1.5990685e-09,1.3204321e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14480: loss = 1.5979255e-09,1.320443e-08\n",
      "Iteration 14485: loss = 1.5967867e-09,1.3204535e-08\n",
      "Iteration 14490: loss = 1.5867263e-09,1.3204642e-08\n",
      "Iteration 14495: loss = 1.5855921e-09,1.3204747e-08\n",
      "Iteration 14500: loss = 1.5844632e-09,1.3204849e-08\n",
      "Iteration 14505: loss = 1.5833347e-09,1.320495e-08\n",
      "Iteration 14510: loss = 1.5733246e-09,1.32050495e-08\n",
      "Iteration 14515: loss = 1.5722028e-09,1.3205149e-08\n",
      "Iteration 14520: loss = 1.5710845e-09,1.3205247e-08\n",
      "Iteration 14525: loss = 1.5699687e-09,1.320534e-08\n",
      "Iteration 14530: loss = 1.5600053e-09,1.3205435e-08\n",
      "Iteration 14535: loss = 1.5588957e-09,1.3205529e-08\n",
      "Iteration 14540: loss = 1.5577887e-09,1.3205621e-08\n",
      "Iteration 14545: loss = 1.5566846e-09,1.32057085e-08\n",
      "Iteration 14550: loss = 1.5467699e-09,1.3205797e-08\n",
      "Iteration 14555: loss = 1.5456725e-09,1.3205885e-08\n",
      "Iteration 14560: loss = 1.5445759e-09,1.32059705e-08\n",
      "Iteration 14565: loss = 1.5434808e-09,1.3206053e-08\n",
      "Iteration 14570: loss = 1.5379981e-09,1.32061375e-08\n",
      "Iteration 14575: loss = 1.5325293e-09,1.3206219e-08\n",
      "Iteration 14580: loss = 1.5314443e-09,1.3206297e-08\n",
      "Iteration 14585: loss = 1.5303625e-09,1.3206375e-08\n",
      "Iteration 14590: loss = 1.5249088e-09,1.3206456e-08\n",
      "Iteration 14595: loss = 1.5194677e-09,1.3206528e-08\n",
      "Iteration 14600: loss = 1.5183956e-09,1.3206602e-08\n",
      "Iteration 14605: loss = 1.5173232e-09,1.3206675e-08\n",
      "Iteration 14610: loss = 1.5119003e-09,1.3206746e-08\n",
      "Iteration 14615: loss = 1.5064918e-09,1.3206813e-08\n",
      "Iteration 14620: loss = 1.5054272e-09,1.320688e-08\n",
      "Iteration 14625: loss = 1.504365e-09,1.3206947e-08\n",
      "Iteration 14630: loss = 1.4989735e-09,1.3207012e-08\n",
      "Iteration 14635: loss = 1.4935929e-09,1.3207078e-08\n",
      "Iteration 14640: loss = 1.4925395e-09,1.3207137e-08\n",
      "Iteration 14645: loss = 1.4914906e-09,1.3207198e-08\n",
      "Iteration 14650: loss = 1.4861264e-09,1.32072575e-08\n",
      "Iteration 14655: loss = 1.4807765e-09,1.3207317e-08\n",
      "Iteration 14660: loss = 1.479734e-09,1.3207372e-08\n",
      "Iteration 14665: loss = 1.4786942e-09,1.3207426e-08\n",
      "Iteration 14670: loss = 1.4733607e-09,1.32074796e-08\n",
      "Iteration 14675: loss = 1.4723281e-09,1.3207531e-08\n",
      "Iteration 14680: loss = 1.4670095e-09,1.3207581e-08\n",
      "Iteration 14685: loss = 1.4659826e-09,1.320763e-08\n",
      "Iteration 14690: loss = 1.4606792e-09,1.3207677e-08\n",
      "Iteration 14695: loss = 1.4596563e-09,1.3207721e-08\n",
      "Iteration 14700: loss = 1.4543651e-09,1.32077655e-08\n",
      "Iteration 14705: loss = 1.4533477e-09,1.3207806e-08\n",
      "Iteration 14710: loss = 1.4480724e-09,1.32078455e-08\n",
      "Iteration 14715: loss = 1.4470626e-09,1.3207885e-08\n",
      "Iteration 14720: loss = 1.4418035e-09,1.3207922e-08\n",
      "Iteration 14725: loss = 1.4407956e-09,1.3207957e-08\n",
      "Iteration 14730: loss = 1.4355507e-09,1.3207992e-08\n",
      "Iteration 14735: loss = 1.4345466e-09,1.3208025e-08\n",
      "Iteration 14740: loss = 1.4293179e-09,1.3208055e-08\n",
      "Iteration 14745: loss = 1.4283187e-09,1.3208084e-08\n",
      "Iteration 14750: loss = 1.427323e-09,1.3208114e-08\n",
      "Iteration 14755: loss = 1.42211e-09,1.3208137e-08\n",
      "Iteration 14760: loss = 1.4211196e-09,1.3208165e-08\n",
      "Iteration 14765: loss = 1.4159207e-09,1.32081865e-08\n",
      "Iteration 14770: loss = 1.4149371e-09,1.320821e-08\n",
      "Iteration 14775: loss = 1.4097523e-09,1.3208229e-08\n",
      "Iteration 14780: loss = 1.4087708e-09,1.320825e-08\n",
      "Iteration 14785: loss = 1.403604e-09,1.32082665e-08\n",
      "Iteration 14790: loss = 1.4026259e-09,1.3208282e-08\n",
      "Iteration 14795: loss = 1.3974716e-09,1.3208294e-08\n",
      "Iteration 14800: loss = 1.3965028e-09,1.3208309e-08\n",
      "Iteration 14805: loss = 1.3955344e-09,1.3208318e-08\n",
      "Iteration 14810: loss = 1.3903966e-09,1.32083295e-08\n",
      "Iteration 14815: loss = 1.3852689e-09,1.3208336e-08\n",
      "Iteration 14820: loss = 1.3843118e-09,1.3208343e-08\n",
      "Iteration 14825: loss = 1.3833518e-09,1.3208346e-08\n",
      "Iteration 14830: loss = 1.3782425e-09,1.320835e-08\n",
      "Iteration 14835: loss = 1.3772898e-09,1.3208351e-08\n",
      "Iteration 14840: loss = 1.3721938e-09,1.3208351e-08\n",
      "Iteration 14845: loss = 1.3712457e-09,1.320835e-08\n",
      "Iteration 14850: loss = 1.3661651e-09,1.3208346e-08\n",
      "Iteration 14855: loss = 1.3652222e-09,1.3208341e-08\n",
      "Iteration 14860: loss = 1.360156e-09,1.3208335e-08\n",
      "Iteration 14865: loss = 1.3592186e-09,1.3208327e-08\n",
      "Iteration 14870: loss = 1.3582819e-09,1.3208314e-08\n",
      "Iteration 14875: loss = 1.3532319e-09,1.3208306e-08\n",
      "Iteration 14880: loss = 1.3481934e-09,1.3208293e-08\n",
      "Iteration 14885: loss = 1.3472646e-09,1.3208277e-08\n",
      "Iteration 14890: loss = 1.3463392e-09,1.3208263e-08\n",
      "Iteration 14895: loss = 1.3413164e-09,1.3208245e-08\n",
      "Iteration 14900: loss = 1.3403937e-09,1.3208226e-08\n",
      "Iteration 14905: loss = 1.3353877e-09,1.3208202e-08\n",
      "Iteration 14910: loss = 1.3344704e-09,1.320818e-08\n",
      "Iteration 14915: loss = 1.3335558e-09,1.3208158e-08\n",
      "Iteration 14920: loss = 1.3285651e-09,1.320813e-08\n",
      "Iteration 14925: loss = 1.3235845e-09,1.3208104e-08\n",
      "Iteration 14930: loss = 1.3226772e-09,1.3208073e-08\n",
      "Iteration 14935: loss = 1.3217724e-09,1.3208042e-08\n",
      "Iteration 14940: loss = 1.3168088e-09,1.3208011e-08\n",
      "Iteration 14945: loss = 1.3118582e-09,1.3207975e-08\n",
      "Iteration 14950: loss = 1.3109589e-09,1.3207938e-08\n",
      "Iteration 14955: loss = 1.3100646e-09,1.3207901e-08\n",
      "Iteration 14960: loss = 1.3091709e-09,1.3207863e-08\n",
      "Iteration 14965: loss = 1.3042394e-09,1.3207822e-08\n",
      "Iteration 14970: loss = 1.2993184e-09,1.320778e-08\n",
      "Iteration 14975: loss = 1.2984316e-09,1.32077345e-08\n",
      "Iteration 14980: loss = 1.2975461e-09,1.3207693e-08\n",
      "Iteration 14985: loss = 1.2966641e-09,1.3207645e-08\n",
      "Iteration 14990: loss = 1.2877489e-09,1.3207593e-08\n",
      "Iteration 14995: loss = 1.2868718e-09,1.32075435e-08\n",
      "Iteration 15000: loss = 1.2859983e-09,1.3207489e-08\n",
      "Iteration 15005: loss = 1.285124e-09,1.3207437e-08\n",
      "Iteration 15010: loss = 1.2802494e-09,1.3207381e-08\n",
      "Iteration 15015: loss = 1.2753875e-09,1.3207324e-08\n",
      "Iteration 15020: loss = 1.2745184e-09,1.32072655e-08\n",
      "Iteration 15025: loss = 1.2736553e-09,1.3207206e-08\n",
      "Iteration 15030: loss = 1.272793e-09,1.3207144e-08\n",
      "Iteration 15035: loss = 1.2679471e-09,1.320708e-08\n",
      "Iteration 15040: loss = 1.2631145e-09,1.3207016e-08\n",
      "Iteration 15045: loss = 1.2622605e-09,1.3206947e-08\n",
      "Iteration 15050: loss = 1.2614055e-09,1.3206878e-08\n",
      "Iteration 15055: loss = 1.25659e-09,1.3206808e-08\n",
      "Iteration 15060: loss = 1.251784e-09,1.3206735e-08\n",
      "Iteration 15065: loss = 1.2509375e-09,1.3206662e-08\n",
      "Iteration 15070: loss = 1.2500915e-09,1.3206584e-08\n",
      "Iteration 15075: loss = 1.2492495e-09,1.3206506e-08\n",
      "Iteration 15080: loss = 1.2444628e-09,1.320643e-08\n",
      "Iteration 15085: loss = 1.2396862e-09,1.3206348e-08\n",
      "Iteration 15090: loss = 1.2388498e-09,1.3206266e-08\n",
      "Iteration 15095: loss = 1.2380156e-09,1.3206182e-08\n",
      "Iteration 15100: loss = 1.237183e-09,1.3206097e-08\n",
      "Iteration 15105: loss = 1.2324239e-09,1.3206009e-08\n",
      "Iteration 15110: loss = 1.2276776e-09,1.3205922e-08\n",
      "Iteration 15115: loss = 1.2268537e-09,1.32058275e-08\n",
      "Iteration 15120: loss = 1.2260302e-09,1.3205736e-08\n",
      "Iteration 15125: loss = 1.225207e-09,1.3205644e-08\n",
      "Iteration 15130: loss = 1.2165818e-09,1.3205547e-08\n",
      "Iteration 15135: loss = 1.2157642e-09,1.32054465e-08\n",
      "Iteration 15140: loss = 1.2149494e-09,1.3205344e-08\n",
      "Iteration 15145: loss = 1.2141357e-09,1.3205246e-08\n",
      "Iteration 15150: loss = 1.2094349e-09,1.3205145e-08\n",
      "Iteration 15155: loss = 1.2047454e-09,1.3205038e-08\n",
      "Iteration 15160: loss = 1.2039387e-09,1.3204932e-08\n",
      "Iteration 15165: loss = 1.2031335e-09,1.3204824e-08\n",
      "Iteration 15170: loss = 1.2023297e-09,1.32047155e-08\n",
      "Iteration 15175: loss = 1.1976589e-09,1.3204604e-08\n",
      "Iteration 15180: loss = 1.1929983e-09,1.3204489e-08\n",
      "Iteration 15185: loss = 1.1922009e-09,1.32043745e-08\n",
      "Iteration 15190: loss = 1.1914066e-09,1.32042555e-08\n",
      "Iteration 15195: loss = 1.1906133e-09,1.32041365e-08\n",
      "Iteration 15200: loss = 1.1859713e-09,1.32040165e-08\n",
      "Iteration 15205: loss = 1.1813396e-09,1.3203895e-08\n",
      "Iteration 15210: loss = 1.1805532e-09,1.3203769e-08\n",
      "Iteration 15215: loss = 1.1797693e-09,1.3203645e-08\n",
      "Iteration 15220: loss = 1.1789861e-09,1.32035165e-08\n",
      "Iteration 15225: loss = 1.1743734e-09,1.3203389e-08\n",
      "Iteration 15230: loss = 1.1697707e-09,1.3203257e-08\n",
      "Iteration 15235: loss = 1.1689938e-09,1.3203126e-08\n",
      "Iteration 15240: loss = 1.1682199e-09,1.3202988e-08\n",
      "Iteration 15245: loss = 1.1674461e-09,1.3202851e-08\n",
      "Iteration 15250: loss = 1.1628623e-09,1.3202713e-08\n",
      "Iteration 15255: loss = 1.1582891e-09,1.3202576e-08\n",
      "Iteration 15260: loss = 1.1575231e-09,1.3202432e-08\n",
      "Iteration 15265: loss = 1.156757e-09,1.3202287e-08\n",
      "Iteration 15270: loss = 1.1559936e-09,1.3202144e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15275: loss = 1.1514377e-09,1.3201999e-08\n",
      "Iteration 15280: loss = 1.1468937e-09,1.32018485e-08\n",
      "Iteration 15285: loss = 1.1461371e-09,1.32017e-08\n",
      "Iteration 15290: loss = 1.1453818e-09,1.32015465e-08\n",
      "Iteration 15295: loss = 1.1446272e-09,1.3201394e-08\n",
      "Iteration 15300: loss = 1.1400987e-09,1.3201237e-08\n",
      "Iteration 15305: loss = 1.1355837e-09,1.32010785e-08\n",
      "Iteration 15310: loss = 1.134836e-09,1.32009195e-08\n",
      "Iteration 15315: loss = 1.134091e-09,1.3200757e-08\n",
      "Iteration 15320: loss = 1.1333473e-09,1.3200595e-08\n",
      "Iteration 15325: loss = 1.1288486e-09,1.3200433e-08\n",
      "Iteration 15330: loss = 1.1243616e-09,1.3200261e-08\n",
      "Iteration 15335: loss = 1.1236233e-09,1.3200093e-08\n",
      "Iteration 15340: loss = 1.1228883e-09,1.3199919e-08\n",
      "Iteration 15345: loss = 1.1221528e-09,1.319975e-08\n",
      "Iteration 15350: loss = 1.1176834e-09,1.31995765e-08\n",
      "Iteration 15355: loss = 1.1132256e-09,1.3199398e-08\n",
      "Iteration 15360: loss = 1.1124982e-09,1.319922e-08\n",
      "Iteration 15365: loss = 1.11177e-09,1.3199039e-08\n",
      "Iteration 15370: loss = 1.1110456e-09,1.3198857e-08\n",
      "Iteration 15375: loss = 1.1066044e-09,1.3198674e-08\n",
      "Iteration 15380: loss = 1.1021746e-09,1.319849e-08\n",
      "Iteration 15385: loss = 1.1014549e-09,1.3198303e-08\n",
      "Iteration 15390: loss = 1.1007378e-09,1.3198113e-08\n",
      "Iteration 15395: loss = 1.1000233e-09,1.3197922e-08\n",
      "Iteration 15400: loss = 1.0956077e-09,1.3197732e-08\n",
      "Iteration 15405: loss = 1.0912058e-09,1.31975355e-08\n",
      "Iteration 15410: loss = 1.0904956e-09,1.319734e-08\n",
      "Iteration 15415: loss = 1.0897888e-09,1.3197142e-08\n",
      "Iteration 15420: loss = 1.0890824e-09,1.3196943e-08\n",
      "Iteration 15425: loss = 1.0846958e-09,1.3196741e-08\n",
      "Iteration 15430: loss = 1.080322e-09,1.3196536e-08\n",
      "Iteration 15435: loss = 1.0796212e-09,1.319633e-08\n",
      "Iteration 15440: loss = 1.078922e-09,1.3196123e-08\n",
      "Iteration 15445: loss = 1.0782232e-09,1.31959155e-08\n",
      "Iteration 15450: loss = 1.073866e-09,1.3195706e-08\n",
      "Iteration 15455: loss = 1.0731719e-09,1.3195494e-08\n",
      "Iteration 15460: loss = 1.0688289e-09,1.3195275e-08\n",
      "Iteration 15465: loss = 1.068137e-09,1.3195061e-08\n",
      "Iteration 15470: loss = 1.067449e-09,1.319484e-08\n",
      "Iteration 15475: loss = 1.063118e-09,1.31946205e-08\n",
      "Iteration 15480: loss = 1.0624337e-09,1.31944e-08\n",
      "Iteration 15485: loss = 1.058116e-09,1.3194178e-08\n",
      "Iteration 15490: loss = 1.0574333e-09,1.3193952e-08\n",
      "Iteration 15495: loss = 1.0567538e-09,1.3193723e-08\n",
      "Iteration 15500: loss = 1.0560749e-09,1.31934925e-08\n",
      "Iteration 15505: loss = 1.051775e-09,1.3193262e-08\n",
      "Iteration 15510: loss = 1.0511003e-09,1.3193029e-08\n",
      "Iteration 15515: loss = 1.0468123e-09,1.31927935e-08\n",
      "Iteration 15520: loss = 1.0461421e-09,1.31925555e-08\n",
      "Iteration 15525: loss = 1.0454725e-09,1.3192316e-08\n",
      "Iteration 15530: loss = 1.0411997e-09,1.3192074e-08\n",
      "Iteration 15535: loss = 1.0405329e-09,1.3191832e-08\n",
      "Iteration 15540: loss = 1.0362738e-09,1.3191587e-08\n",
      "Iteration 15545: loss = 1.0356106e-09,1.3191338e-08\n",
      "Iteration 15550: loss = 1.0349491e-09,1.3191088e-08\n",
      "Iteration 15555: loss = 1.030705e-09,1.3190839e-08\n",
      "Iteration 15560: loss = 1.0300459e-09,1.3190587e-08\n",
      "Iteration 15565: loss = 1.0293905e-09,1.3190331e-08\n",
      "Iteration 15570: loss = 1.0251594e-09,1.3190076e-08\n",
      "Iteration 15575: loss = 1.0245081e-09,1.3189816e-08\n",
      "Iteration 15580: loss = 1.0238551e-09,1.3189553e-08\n",
      "Iteration 15585: loss = 1.0196398e-09,1.3189293e-08\n",
      "Iteration 15590: loss = 1.018991e-09,1.3189029e-08\n",
      "Iteration 15595: loss = 1.014789e-09,1.3188762e-08\n",
      "Iteration 15600: loss = 1.0141438e-09,1.3188492e-08\n",
      "Iteration 15605: loss = 1.0135016e-09,1.3188221e-08\n",
      "Iteration 15610: loss = 1.009312e-09,1.3187951e-08\n",
      "Iteration 15615: loss = 1.0086733e-09,1.3187676e-08\n",
      "Iteration 15620: loss = 1.0080354e-09,1.31874e-08\n",
      "Iteration 15625: loss = 1.0038608e-09,1.3187122e-08\n",
      "Iteration 15630: loss = 1.0032265e-09,1.3186844e-08\n",
      "Iteration 15635: loss = 9.990643e-10,1.3186561e-08\n",
      "Iteration 15640: loss = 9.984334e-10,1.3186279e-08\n",
      "Iteration 15645: loss = 9.978043e-10,1.3185992e-08\n",
      "Iteration 15650: loss = 9.936562e-10,1.3185705e-08\n",
      "Iteration 15655: loss = 9.930285e-10,1.31854145e-08\n",
      "Iteration 15660: loss = 9.924045e-10,1.3185123e-08\n",
      "Iteration 15665: loss = 9.882716e-10,1.3184829e-08\n",
      "Iteration 15670: loss = 9.876494e-10,1.31845335e-08\n",
      "Iteration 15675: loss = 9.870301e-10,1.3184239e-08\n",
      "Iteration 15680: loss = 9.829101e-10,1.3183938e-08\n",
      "Iteration 15685: loss = 9.822935e-10,1.31836355e-08\n",
      "Iteration 15690: loss = 9.781873e-10,1.31833335e-08\n",
      "Iteration 15695: loss = 9.775744e-10,1.3183027e-08\n",
      "Iteration 15700: loss = 9.769624e-10,1.318272e-08\n",
      "Iteration 15705: loss = 9.76352e-10,1.3182411e-08\n",
      "Iteration 15710: loss = 9.722619e-10,1.3182098e-08\n",
      "Iteration 15715: loss = 9.716543e-10,1.31817846e-08\n",
      "Iteration 15720: loss = 9.675757e-10,1.3181471e-08\n",
      "Iteration 15725: loss = 9.669732e-10,1.3181153e-08\n",
      "Iteration 15730: loss = 9.663683e-10,1.3180833e-08\n",
      "Iteration 15735: loss = 9.657679e-10,1.3180513e-08\n",
      "Iteration 15740: loss = 9.617049e-10,1.3180191e-08\n",
      "Iteration 15745: loss = 9.576521e-10,1.3179865e-08\n",
      "Iteration 15750: loss = 9.570567e-10,1.3179539e-08\n",
      "Iteration 15755: loss = 9.564615e-10,1.317921e-08\n",
      "Iteration 15760: loss = 9.558682e-10,1.3178878e-08\n",
      "Iteration 15765: loss = 9.552769e-10,1.31785445e-08\n",
      "Iteration 15770: loss = 9.512422e-10,1.3178209e-08\n",
      "Iteration 15775: loss = 9.472176e-10,1.3177873e-08\n",
      "Iteration 15780: loss = 9.4663e-10,1.3177534e-08\n",
      "Iteration 15785: loss = 9.460449e-10,1.3177193e-08\n",
      "Iteration 15790: loss = 9.4546e-10,1.3176848e-08\n",
      "Iteration 15795: loss = 9.448762e-10,1.3176503e-08\n",
      "Iteration 15800: loss = 9.408683e-10,1.3176159e-08\n",
      "Iteration 15805: loss = 9.368738e-10,1.3175805e-08\n",
      "Iteration 15810: loss = 9.362952e-10,1.3175457e-08\n",
      "Iteration 15815: loss = 9.357184e-10,1.3175101e-08\n",
      "Iteration 15820: loss = 9.351445e-10,1.3174748e-08\n",
      "Iteration 15825: loss = 9.345688e-10,1.317439e-08\n",
      "Iteration 15830: loss = 9.2719304e-10,1.3174032e-08\n",
      "Iteration 15835: loss = 9.266235e-10,1.3173668e-08\n",
      "Iteration 15840: loss = 9.260541e-10,1.3173304e-08\n",
      "Iteration 15845: loss = 9.254861e-10,1.3172937e-08\n",
      "Iteration 15850: loss = 9.249193e-10,1.3172569e-08\n",
      "Iteration 15855: loss = 9.243551e-10,1.3172202e-08\n",
      "Iteration 15860: loss = 9.170255e-10,1.3171829e-08\n",
      "Iteration 15865: loss = 9.1646296e-10,1.3171455e-08\n",
      "Iteration 15870: loss = 9.1590335e-10,1.3171077e-08\n",
      "Iteration 15875: loss = 9.1534497e-10,1.3170699e-08\n",
      "Iteration 15880: loss = 9.1478686e-10,1.3170318e-08\n",
      "Iteration 15885: loss = 9.1423025e-10,1.3169937e-08\n",
      "Iteration 15890: loss = 9.06947e-10,1.3169551e-08\n",
      "Iteration 15895: loss = 9.063943e-10,1.3169165e-08\n",
      "Iteration 15900: loss = 9.0584257e-10,1.3168773e-08\n",
      "Iteration 15905: loss = 9.052928e-10,1.3168384e-08\n",
      "Iteration 15910: loss = 9.0474334e-10,1.3167991e-08\n",
      "Iteration 15915: loss = 9.041948e-10,1.3167595e-08\n",
      "Iteration 15920: loss = 8.969594e-10,1.31672e-08\n",
      "Iteration 15925: loss = 8.9641433e-10,1.3166798e-08\n",
      "Iteration 15930: loss = 8.9587204e-10,1.3166396e-08\n",
      "Iteration 15935: loss = 8.9532987e-10,1.3165993e-08\n",
      "Iteration 15940: loss = 8.947894e-10,1.3165588e-08\n",
      "Iteration 15945: loss = 8.909204e-10,1.3165179e-08\n",
      "Iteration 15950: loss = 8.9038205e-10,1.316477e-08\n",
      "Iteration 15955: loss = 8.865249e-10,1.31643585e-08\n",
      "Iteration 15960: loss = 8.8599034e-10,1.3163944e-08\n",
      "Iteration 15965: loss = 8.85457e-10,1.3163526e-08\n",
      "Iteration 15970: loss = 8.849251e-10,1.3163107e-08\n",
      "Iteration 15975: loss = 8.810823e-10,1.316269e-08\n",
      "Iteration 15980: loss = 8.805537e-10,1.3162266e-08\n",
      "Iteration 15985: loss = 8.7672286e-10,1.3161839e-08\n",
      "Iteration 15990: loss = 8.761965e-10,1.3161412e-08\n",
      "Iteration 15995: loss = 8.7567126e-10,1.3160982e-08\n",
      "Iteration 16000: loss = 8.7514646e-10,1.3160554e-08\n",
      "Iteration 16005: loss = 8.713323e-10,1.316012e-08\n",
      "Iteration 16010: loss = 8.708098e-10,1.3159685e-08\n",
      "Iteration 16015: loss = 8.7029023e-10,1.3159248e-08\n",
      "Iteration 16020: loss = 8.6648977e-10,1.3158807e-08\n",
      "Iteration 16025: loss = 8.659713e-10,1.3158367e-08\n",
      "Iteration 16030: loss = 8.654561e-10,1.3157924e-08\n",
      "Iteration 16035: loss = 8.6166757e-10,1.3157475e-08\n",
      "Iteration 16040: loss = 8.611549e-10,1.315703e-08\n",
      "Iteration 16045: loss = 8.6064195e-10,1.3156578e-08\n",
      "Iteration 16050: loss = 8.5686797e-10,1.3156127e-08\n",
      "Iteration 16055: loss = 8.5635654e-10,1.3155671e-08\n",
      "Iteration 16060: loss = 8.558502e-10,1.3155216e-08\n",
      "Iteration 16065: loss = 8.520887e-10,1.3154757e-08\n",
      "Iteration 16070: loss = 8.5158386e-10,1.3154295e-08\n",
      "Iteration 16075: loss = 8.510798e-10,1.3153833e-08\n",
      "Iteration 16080: loss = 8.505771e-10,1.3153366e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16085: loss = 8.468312e-10,1.3152898e-08\n",
      "Iteration 16090: loss = 8.4632995e-10,1.315243e-08\n",
      "Iteration 16095: loss = 8.4259555e-10,1.3151958e-08\n",
      "Iteration 16100: loss = 8.420988e-10,1.3151482e-08\n",
      "Iteration 16105: loss = 8.416023e-10,1.3151006e-08\n",
      "Iteration 16110: loss = 8.4110785e-10,1.3150527e-08\n",
      "Iteration 16115: loss = 8.40614e-10,1.3150046e-08\n",
      "Iteration 16120: loss = 8.368939e-10,1.3149565e-08\n",
      "Iteration 16125: loss = 8.364029e-10,1.3149082e-08\n",
      "Iteration 16130: loss = 8.326961e-10,1.3148593e-08\n",
      "Iteration 16135: loss = 8.32209e-10,1.3148101e-08\n",
      "Iteration 16140: loss = 8.317215e-10,1.3147609e-08\n",
      "Iteration 16145: loss = 8.312346e-10,1.3147117e-08\n",
      "Iteration 16150: loss = 8.3074964e-10,1.3146623e-08\n",
      "Iteration 16155: loss = 8.270577e-10,1.3146123e-08\n",
      "Iteration 16160: loss = 8.233787e-10,1.3145624e-08\n",
      "Iteration 16165: loss = 8.2289725e-10,1.3145122e-08\n",
      "Iteration 16170: loss = 8.224181e-10,1.3144614e-08\n",
      "Iteration 16175: loss = 8.2193957e-10,1.3144108e-08\n",
      "Iteration 16180: loss = 8.2146223e-10,1.3143597e-08\n",
      "Iteration 16185: loss = 8.2098506e-10,1.3143089e-08\n",
      "Iteration 16190: loss = 8.1414253e-10,1.3142576e-08\n",
      "Iteration 16195: loss = 8.136684e-10,1.3142059e-08\n",
      "Iteration 16200: loss = 8.131957e-10,1.3141542e-08\n",
      "Iteration 16205: loss = 8.1272433e-10,1.314102e-08\n",
      "Iteration 16210: loss = 8.1225493e-10,1.31405e-08\n",
      "Iteration 16215: loss = 8.1178575e-10,1.3139973e-08\n",
      "Iteration 16220: loss = 8.081471e-10,1.3139449e-08\n",
      "Iteration 16225: loss = 8.0452134e-10,1.313892e-08\n",
      "Iteration 16230: loss = 8.040552e-10,1.3138388e-08\n",
      "Iteration 16235: loss = 8.0359297e-10,1.31378535e-08\n",
      "Iteration 16240: loss = 8.0312884e-10,1.3137317e-08\n",
      "Iteration 16245: loss = 8.0266777e-10,1.3136776e-08\n",
      "Iteration 16250: loss = 8.0220675e-10,1.3136236e-08\n",
      "Iteration 16255: loss = 7.9859674e-10,1.3135697e-08\n",
      "Iteration 16260: loss = 7.949967e-10,1.313515e-08\n",
      "Iteration 16265: loss = 7.945409e-10,1.3134602e-08\n",
      "Iteration 16270: loss = 7.94087e-10,1.3134051e-08\n",
      "Iteration 16275: loss = 7.9363094e-10,1.31335e-08\n",
      "Iteration 16280: loss = 7.931777e-10,1.3132946e-08\n",
      "Iteration 16285: loss = 7.895948e-10,1.3132389e-08\n",
      "Iteration 16290: loss = 7.891427e-10,1.3131831e-08\n",
      "Iteration 16295: loss = 7.855718e-10,1.31312685e-08\n",
      "Iteration 16300: loss = 7.851235e-10,1.3130706e-08\n",
      "Iteration 16305: loss = 7.846768e-10,1.31301405e-08\n",
      "Iteration 16310: loss = 7.842309e-10,1.3129572e-08\n",
      "Iteration 16315: loss = 7.806737e-10,1.3129e-08\n",
      "Iteration 16320: loss = 7.802295e-10,1.3128428e-08\n",
      "Iteration 16325: loss = 7.797872e-10,1.3127853e-08\n",
      "Iteration 16330: loss = 7.762437e-10,1.3127275e-08\n",
      "Iteration 16335: loss = 7.758029e-10,1.3126696e-08\n",
      "Iteration 16340: loss = 7.753645e-10,1.3126115e-08\n",
      "Iteration 16345: loss = 7.7492573e-10,1.3125529e-08\n",
      "Iteration 16350: loss = 7.7139717e-10,1.3124943e-08\n",
      "Iteration 16355: loss = 7.709608e-10,1.3124355e-08\n",
      "Iteration 16360: loss = 7.705268e-10,1.3123763e-08\n",
      "Iteration 16365: loss = 7.7009316e-10,1.3123171e-08\n",
      "Iteration 16370: loss = 7.665772e-10,1.3122576e-08\n",
      "Iteration 16375: loss = 7.6614604e-10,1.3121975e-08\n",
      "Iteration 16380: loss = 7.626411e-10,1.3121374e-08\n",
      "Iteration 16385: loss = 7.622138e-10,1.3120772e-08\n",
      "Iteration 16390: loss = 7.617862e-10,1.3120167e-08\n",
      "Iteration 16395: loss = 7.6135925e-10,1.3119558e-08\n",
      "Iteration 16400: loss = 7.609345e-10,1.3118951e-08\n",
      "Iteration 16405: loss = 7.574444e-10,1.311834e-08\n",
      "Iteration 16410: loss = 7.570215e-10,1.3117724e-08\n",
      "Iteration 16415: loss = 7.5354417e-10,1.3117106e-08\n",
      "Iteration 16420: loss = 7.5312356e-10,1.31164875e-08\n",
      "Iteration 16425: loss = 7.527053e-10,1.3115862e-08\n",
      "Iteration 16430: loss = 7.5228695e-10,1.31152404e-08\n",
      "Iteration 16435: loss = 7.518684e-10,1.3114613e-08\n",
      "Iteration 16440: loss = 7.514525e-10,1.31139855e-08\n",
      "Iteration 16445: loss = 7.479907e-10,1.3113355e-08\n",
      "Iteration 16450: loss = 7.4454015e-10,1.3112722e-08\n",
      "Iteration 16455: loss = 7.441276e-10,1.31120865e-08\n",
      "Iteration 16460: loss = 7.437162e-10,1.3111444e-08\n",
      "Iteration 16465: loss = 7.433067e-10,1.3110804e-08\n",
      "Iteration 16470: loss = 7.4289747e-10,1.311016e-08\n",
      "Iteration 16475: loss = 7.4248735e-10,1.3109515e-08\n",
      "Iteration 16480: loss = 7.39052e-10,1.3108867e-08\n",
      "Iteration 16485: loss = 7.3562795e-10,1.31082185e-08\n",
      "Iteration 16490: loss = 7.3522294e-10,1.3107565e-08\n",
      "Iteration 16495: loss = 7.34818e-10,1.3106909e-08\n",
      "Iteration 16500: loss = 7.3441614e-10,1.3106251e-08\n",
      "Iteration 16505: loss = 7.3401374e-10,1.3105591e-08\n",
      "Iteration 16510: loss = 7.3361256e-10,1.3104929e-08\n",
      "Iteration 16515: loss = 7.3020406e-10,1.3104264e-08\n",
      "Iteration 16520: loss = 7.298044e-10,1.31036e-08\n",
      "Iteration 16525: loss = 7.264069e-10,1.3102932e-08\n",
      "Iteration 16530: loss = 7.260111e-10,1.3102261e-08\n",
      "Iteration 16535: loss = 7.2561573e-10,1.3101584e-08\n",
      "Iteration 16540: loss = 7.252216e-10,1.3100907e-08\n",
      "Iteration 16545: loss = 7.248269e-10,1.3100227e-08\n",
      "Iteration 16550: loss = 7.2144424e-10,1.3099548e-08\n",
      "Iteration 16555: loss = 7.210533e-10,1.3098863e-08\n",
      "Iteration 16560: loss = 7.2066236e-10,1.3098176e-08\n",
      "Iteration 16565: loss = 7.172926e-10,1.30974875e-08\n",
      "Iteration 16570: loss = 7.169045e-10,1.3096798e-08\n",
      "Iteration 16575: loss = 7.165166e-10,1.3096103e-08\n",
      "Iteration 16580: loss = 7.1613043e-10,1.3095407e-08\n",
      "Iteration 16585: loss = 7.12774e-10,1.309471e-08\n",
      "Iteration 16590: loss = 7.1238887e-10,1.30940085e-08\n",
      "Iteration 16595: loss = 7.120055e-10,1.3093305e-08\n",
      "Iteration 16600: loss = 7.116238e-10,1.3092601e-08\n",
      "Iteration 16605: loss = 7.082804e-10,1.3091892e-08\n",
      "Iteration 16610: loss = 7.0790035e-10,1.3091181e-08\n",
      "Iteration 16615: loss = 7.075207e-10,1.3090469e-08\n",
      "Iteration 16620: loss = 7.041907e-10,1.3089755e-08\n",
      "Iteration 16625: loss = 7.038142e-10,1.3089036e-08\n",
      "Iteration 16630: loss = 7.0343814e-10,1.3088314e-08\n",
      "Iteration 16635: loss = 7.030625e-10,1.30875915e-08\n",
      "Iteration 16640: loss = 7.0268763e-10,1.3086865e-08\n",
      "Iteration 16645: loss = 7.023134e-10,1.3086138e-08\n",
      "Iteration 16650: loss = 6.989986e-10,1.3085411e-08\n",
      "Iteration 16655: loss = 6.956942e-10,1.30846765e-08\n",
      "Iteration 16660: loss = 6.953245e-10,1.3083943e-08\n",
      "Iteration 16665: loss = 6.9495537e-10,1.3083204e-08\n",
      "Iteration 16670: loss = 6.945884e-10,1.3082462e-08\n",
      "Iteration 16675: loss = 6.9422024e-10,1.3081721e-08\n",
      "Iteration 16680: loss = 6.9385325e-10,1.3080973e-08\n",
      "Iteration 16685: loss = 6.93488e-10,1.3080226e-08\n",
      "Iteration 16690: loss = 6.8728606e-10,1.3079479e-08\n",
      "Iteration 16695: loss = 6.8692235e-10,1.3078726e-08\n",
      "Iteration 16700: loss = 6.865611e-10,1.3077968e-08\n",
      "Iteration 16705: loss = 6.8619976e-10,1.307721e-08\n",
      "Iteration 16710: loss = 6.858394e-10,1.3076449e-08\n",
      "Iteration 16715: loss = 6.8547995e-10,1.30756845e-08\n",
      "Iteration 16720: loss = 6.8512174e-10,1.307492e-08\n",
      "Iteration 16725: loss = 6.8185807e-10,1.3074153e-08\n",
      "Iteration 16730: loss = 6.8150113e-10,1.3073384e-08\n",
      "Iteration 16735: loss = 6.7825096e-10,1.3072612e-08\n",
      "Iteration 16740: loss = 6.77896e-10,1.3071836e-08\n",
      "Iteration 16745: loss = 6.775425e-10,1.3071058e-08\n",
      "Iteration 16750: loss = 6.771901e-10,1.30702755e-08\n",
      "Iteration 16755: loss = 6.768379e-10,1.3069492e-08\n",
      "Iteration 16760: loss = 6.764858e-10,1.3068707e-08\n",
      "Iteration 16765: loss = 6.732502e-10,1.3067923e-08\n",
      "Iteration 16770: loss = 6.729013e-10,1.3067131e-08\n",
      "Iteration 16775: loss = 6.7255285e-10,1.3066338e-08\n",
      "Iteration 16780: loss = 6.693306e-10,1.3065542e-08\n",
      "Iteration 16785: loss = 6.68984e-10,1.3064744e-08\n",
      "Iteration 16790: loss = 6.6864e-10,1.3063941e-08\n",
      "Iteration 16795: loss = 6.6829503e-10,1.3063138e-08\n",
      "Iteration 16800: loss = 6.6508665e-10,1.3062331e-08\n",
      "Iteration 16805: loss = 6.64743e-10,1.3061522e-08\n",
      "Iteration 16810: loss = 6.6440203e-10,1.306071e-08\n",
      "Iteration 16815: loss = 6.640602e-10,1.3059896e-08\n",
      "Iteration 16820: loss = 6.6372147e-10,1.305908e-08\n",
      "Iteration 16825: loss = 6.605254e-10,1.3058261e-08\n",
      "Iteration 16830: loss = 6.6018746e-10,1.3057439e-08\n",
      "Iteration 16835: loss = 6.5700306e-10,1.3056616e-08\n",
      "Iteration 16840: loss = 6.566679e-10,1.305579e-08\n",
      "Iteration 16845: loss = 6.563328e-10,1.3054958e-08\n",
      "Iteration 16850: loss = 6.5599887e-10,1.3054127e-08\n",
      "Iteration 16855: loss = 6.5566547e-10,1.3053292e-08\n",
      "Iteration 16860: loss = 6.5533395e-10,1.3052455e-08\n",
      "Iteration 16865: loss = 6.550003e-10,1.3051615e-08\n",
      "Iteration 16870: loss = 6.51832e-10,1.3050776e-08\n",
      "Iteration 16875: loss = 6.4867306e-10,1.3049931e-08\n",
      "Iteration 16880: loss = 6.483449e-10,1.30490845e-08\n",
      "Iteration 16885: loss = 6.4801636e-10,1.30482345e-08\n",
      "Iteration 16890: loss = 6.47691e-10,1.3047379e-08\n",
      "Iteration 16895: loss = 6.4736433e-10,1.3046524e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16900: loss = 6.470379e-10,1.3045667e-08\n",
      "Iteration 16905: loss = 6.467134e-10,1.3044808e-08\n",
      "Iteration 16910: loss = 6.4357036e-10,1.3043946e-08\n",
      "Iteration 16915: loss = 6.404379e-10,1.304308e-08\n",
      "Iteration 16920: loss = 6.4011685e-10,1.3042213e-08\n",
      "Iteration 16925: loss = 6.3979577e-10,1.3041341e-08\n",
      "Iteration 16930: loss = 6.3947736e-10,1.3040466e-08\n",
      "Iteration 16935: loss = 6.3915845e-10,1.3039587e-08\n",
      "Iteration 16940: loss = 6.388406e-10,1.3038708e-08\n",
      "Iteration 16945: loss = 6.38523e-10,1.3037825e-08\n",
      "Iteration 16950: loss = 6.354069e-10,1.3036943e-08\n",
      "Iteration 16955: loss = 6.350909e-10,1.3036054e-08\n",
      "Iteration 16960: loss = 6.3477584e-10,1.3035166e-08\n",
      "Iteration 16965: loss = 6.316716e-10,1.3034275e-08\n",
      "Iteration 16970: loss = 6.3135913e-10,1.3033379e-08\n",
      "Iteration 16975: loss = 6.3104705e-10,1.303248e-08\n",
      "Iteration 16980: loss = 6.307365e-10,1.3031579e-08\n",
      "Iteration 16985: loss = 6.3042593e-10,1.3030673e-08\n",
      "Iteration 16990: loss = 6.273357e-10,1.3029769e-08\n",
      "Iteration 16995: loss = 6.2702754e-10,1.302886e-08\n",
      "Iteration 17000: loss = 6.267195e-10,1.302795e-08\n",
      "Iteration 17005: loss = 6.2641253e-10,1.30270355e-08\n",
      "Iteration 17010: loss = 6.261056e-10,1.30261215e-08\n",
      "Iteration 17015: loss = 6.230289e-10,1.30252005e-08\n",
      "Iteration 17020: loss = 6.2272437e-10,1.3024278e-08\n",
      "Iteration 17025: loss = 6.1965894e-10,1.3023355e-08\n",
      "Iteration 17030: loss = 6.193565e-10,1.3022427e-08\n",
      "Iteration 17035: loss = 6.190549e-10,1.3021493e-08\n",
      "Iteration 17040: loss = 6.1875466e-10,1.3020562e-08\n",
      "Iteration 17045: loss = 6.184539e-10,1.3019627e-08\n",
      "Iteration 17050: loss = 6.1815436e-10,1.3018688e-08\n",
      "Iteration 17055: loss = 6.178547e-10,1.30177495e-08\n",
      "Iteration 17060: loss = 6.175558e-10,1.3016806e-08\n",
      "Iteration 17065: loss = 6.117629e-10,1.3015863e-08\n",
      "Iteration 17070: loss = 6.114668e-10,1.3014912e-08\n",
      "Iteration 17075: loss = 6.1117056e-10,1.3013961e-08\n",
      "Iteration 17080: loss = 6.108773e-10,1.3013006e-08\n",
      "Iteration 17085: loss = 6.105835e-10,1.3012049e-08\n",
      "Iteration 17090: loss = 6.1029143e-10,1.3011088e-08\n",
      "Iteration 17095: loss = 6.0999855e-10,1.3010126e-08\n",
      "Iteration 17100: loss = 6.097067e-10,1.3009162e-08\n",
      "Iteration 17105: loss = 6.0668126e-10,1.3008194e-08\n",
      "Iteration 17110: loss = 6.0639094e-10,1.3007225e-08\n",
      "Iteration 17115: loss = 6.033772e-10,1.3006252e-08\n",
      "Iteration 17120: loss = 6.0308925e-10,1.3005277e-08\n",
      "Iteration 17125: loss = 6.0280314e-10,1.3004296e-08\n",
      "Iteration 17130: loss = 6.0251637e-10,1.3003315e-08\n",
      "Iteration 17135: loss = 6.022307e-10,1.3002328e-08\n",
      "Iteration 17140: loss = 6.019466e-10,1.3001342e-08\n",
      "Iteration 17145: loss = 5.9894684e-10,1.3000352e-08\n",
      "Iteration 17150: loss = 5.986629e-10,1.2999361e-08\n",
      "Iteration 17155: loss = 5.9838007e-10,1.2998365e-08\n",
      "Iteration 17160: loss = 5.980971e-10,1.2997367e-08\n",
      "Iteration 17165: loss = 5.978158e-10,1.29963675e-08\n",
      "Iteration 17170: loss = 5.9483046e-10,1.2995365e-08\n",
      "Iteration 17175: loss = 5.9455046e-10,1.2994357e-08\n",
      "Iteration 17180: loss = 5.9427235e-10,1.2993349e-08\n",
      "Iteration 17185: loss = 5.9129845e-10,1.2992335e-08\n",
      "Iteration 17190: loss = 5.910214e-10,1.2991319e-08\n",
      "Iteration 17195: loss = 5.907453e-10,1.2990302e-08\n",
      "Iteration 17200: loss = 5.904704e-10,1.2989277e-08\n",
      "Iteration 17205: loss = 5.901954e-10,1.2988254e-08\n",
      "Iteration 17210: loss = 5.899211e-10,1.2987227e-08\n",
      "Iteration 17215: loss = 5.896473e-10,1.2986201e-08\n",
      "Iteration 17220: loss = 5.866875e-10,1.2985169e-08\n",
      "Iteration 17225: loss = 5.8641547e-10,1.2984137e-08\n",
      "Iteration 17230: loss = 5.834681e-10,1.2983099e-08\n",
      "Iteration 17235: loss = 5.8319805e-10,1.2982058e-08\n",
      "Iteration 17240: loss = 5.829294e-10,1.2981011e-08\n",
      "Iteration 17245: loss = 5.8266203e-10,1.29799655e-08\n",
      "Iteration 17250: loss = 5.8239485e-10,1.2978916e-08\n",
      "Iteration 17255: loss = 5.821275e-10,1.2977864e-08\n",
      "Iteration 17260: loss = 5.818607e-10,1.2976807e-08\n",
      "Iteration 17265: loss = 5.815944e-10,1.2975752e-08\n",
      "Iteration 17270: loss = 5.786615e-10,1.2974694e-08\n",
      "Iteration 17275: loss = 5.783962e-10,1.2973632e-08\n",
      "Iteration 17280: loss = 5.754745e-10,1.2972566e-08\n",
      "Iteration 17285: loss = 5.75212e-10,1.2971497e-08\n",
      "Iteration 17290: loss = 5.749511e-10,1.2970422e-08\n",
      "Iteration 17295: loss = 5.746904e-10,1.29693465e-08\n",
      "Iteration 17300: loss = 5.7443045e-10,1.2968268e-08\n",
      "Iteration 17305: loss = 5.741707e-10,1.296719e-08\n",
      "Iteration 17310: loss = 5.7126237e-10,1.2966108e-08\n",
      "Iteration 17315: loss = 5.710039e-10,1.2965022e-08\n",
      "Iteration 17320: loss = 5.707467e-10,1.2963932e-08\n",
      "Iteration 17325: loss = 5.7048927e-10,1.2962841e-08\n",
      "Iteration 17330: loss = 5.7023397e-10,1.2961745e-08\n",
      "Iteration 17335: loss = 5.673399e-10,1.2960647e-08\n",
      "Iteration 17340: loss = 5.6708443e-10,1.2959546e-08\n",
      "Iteration 17345: loss = 5.6683025e-10,1.2958446e-08\n",
      "Iteration 17350: loss = 5.665774e-10,1.295734e-08\n",
      "Iteration 17355: loss = 5.6369465e-10,1.295623e-08\n",
      "Iteration 17360: loss = 5.634436e-10,1.2955118e-08\n",
      "Iteration 17365: loss = 5.6319305e-10,1.2954001e-08\n",
      "Iteration 17370: loss = 5.6294275e-10,1.2952883e-08\n",
      "Iteration 17375: loss = 5.626928e-10,1.2951765e-08\n",
      "Iteration 17380: loss = 5.6244326e-10,1.2950641e-08\n",
      "Iteration 17385: loss = 5.621946e-10,1.2949514e-08\n",
      "Iteration 17390: loss = 5.6194627e-10,1.2948386e-08\n",
      "Iteration 17395: loss = 5.590784e-10,1.29472575e-08\n",
      "Iteration 17400: loss = 5.562208e-10,1.2946125e-08\n",
      "Iteration 17405: loss = 5.559755e-10,1.2944984e-08\n",
      "Iteration 17410: loss = 5.557316e-10,1.29438416e-08\n",
      "Iteration 17415: loss = 5.5548927e-10,1.2942697e-08\n",
      "Iteration 17420: loss = 5.552461e-10,1.2941548e-08\n",
      "Iteration 17425: loss = 5.5500327e-10,1.2940399e-08\n",
      "Iteration 17430: loss = 5.5476096e-10,1.2939246e-08\n",
      "Iteration 17435: loss = 5.545195e-10,1.293809e-08\n",
      "Iteration 17440: loss = 5.5167726e-10,1.2936932e-08\n",
      "Iteration 17445: loss = 5.514369e-10,1.2935773e-08\n",
      "Iteration 17450: loss = 5.5119714e-10,1.293461e-08\n",
      "Iteration 17455: loss = 5.483667e-10,1.2933442e-08\n",
      "Iteration 17460: loss = 5.481296e-10,1.2932269e-08\n",
      "Iteration 17465: loss = 5.4789334e-10,1.2931094e-08\n",
      "Iteration 17470: loss = 5.476581e-10,1.2929917e-08\n",
      "Iteration 17475: loss = 5.4742205e-10,1.2928737e-08\n",
      "Iteration 17480: loss = 5.4718746e-10,1.2927552e-08\n",
      "Iteration 17485: loss = 5.443715e-10,1.29263675e-08\n",
      "Iteration 17490: loss = 5.4413674e-10,1.292518e-08\n",
      "Iteration 17495: loss = 5.439048e-10,1.2923989e-08\n",
      "Iteration 17500: loss = 5.4367194e-10,1.2922792e-08\n",
      "Iteration 17505: loss = 5.4344057e-10,1.29215945e-08\n",
      "Iteration 17510: loss = 5.432094e-10,1.2920394e-08\n",
      "Iteration 17515: loss = 5.4040666e-10,1.2919191e-08\n",
      "Iteration 17520: loss = 5.4017635e-10,1.2917984e-08\n",
      "Iteration 17525: loss = 5.3994736e-10,1.2916775e-08\n",
      "Iteration 17530: loss = 5.371558e-10,1.29155655e-08\n",
      "Iteration 17535: loss = 5.369278e-10,1.291435e-08\n",
      "Iteration 17540: loss = 5.3670174e-10,1.2913129e-08\n",
      "Iteration 17545: loss = 5.364758e-10,1.2911906e-08\n",
      "Iteration 17550: loss = 5.3625027e-10,1.29106805e-08\n",
      "Iteration 17555: loss = 5.3602595e-10,1.290945e-08\n",
      "Iteration 17560: loss = 5.3580146e-10,1.290822e-08\n",
      "Iteration 17565: loss = 5.355764e-10,1.29069875e-08\n",
      "Iteration 17570: loss = 5.353518e-10,1.2905753e-08\n",
      "Iteration 17575: loss = 5.325746e-10,1.2904517e-08\n",
      "Iteration 17580: loss = 5.298077e-10,1.2903275e-08\n",
      "Iteration 17585: loss = 5.2958665e-10,1.290203e-08\n",
      "Iteration 17590: loss = 5.2936694e-10,1.2900779e-08\n",
      "Iteration 17595: loss = 5.291477e-10,1.2899527e-08\n",
      "Iteration 17600: loss = 5.28929e-10,1.2898267e-08\n",
      "Iteration 17605: loss = 5.287114e-10,1.28970115e-08\n",
      "Iteration 17610: loss = 5.2849386e-10,1.2895745e-08\n",
      "Iteration 17615: loss = 5.282763e-10,1.2894484e-08\n",
      "Iteration 17620: loss = 5.280593e-10,1.2893215e-08\n",
      "Iteration 17625: loss = 5.2530835e-10,1.2891944e-08\n",
      "Iteration 17630: loss = 5.250937e-10,1.2890673e-08\n",
      "Iteration 17635: loss = 5.2487864e-10,1.2889396e-08\n",
      "Iteration 17640: loss = 5.246641e-10,1.2888116e-08\n",
      "Iteration 17645: loss = 5.2445076e-10,1.2886832e-08\n",
      "Iteration 17650: loss = 5.217138e-10,1.2885549e-08\n",
      "Iteration 17655: loss = 5.215025e-10,1.2884258e-08\n",
      "Iteration 17660: loss = 5.2128996e-10,1.2882966e-08\n",
      "Iteration 17665: loss = 5.2107835e-10,1.2881673e-08\n",
      "Iteration 17670: loss = 5.18352e-10,1.2880371e-08\n",
      "Iteration 17675: loss = 5.18143e-10,1.287907e-08\n",
      "Iteration 17680: loss = 5.1793453e-10,1.28777655e-08\n",
      "Iteration 17685: loss = 5.1772614e-10,1.2876456e-08\n",
      "Iteration 17690: loss = 5.1751886e-10,1.2875144e-08\n",
      "Iteration 17695: loss = 5.1731136e-10,1.2873829e-08\n",
      "Iteration 17700: loss = 5.17105e-10,1.2872515e-08\n",
      "Iteration 17705: loss = 5.1689836e-10,1.2871195e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17710: loss = 5.166911e-10,1.2869872e-08\n",
      "Iteration 17715: loss = 5.139792e-10,1.286855e-08\n",
      "Iteration 17720: loss = 5.112785e-10,1.2867219e-08\n",
      "Iteration 17725: loss = 5.1107535e-10,1.2865886e-08\n",
      "Iteration 17730: loss = 5.108731e-10,1.2864549e-08\n",
      "Iteration 17735: loss = 5.1067167e-10,1.2863207e-08\n",
      "Iteration 17740: loss = 5.1047133e-10,1.2861863e-08\n",
      "Iteration 17745: loss = 5.102711e-10,1.2860514e-08\n",
      "Iteration 17750: loss = 5.1007115e-10,1.2859164e-08\n",
      "Iteration 17755: loss = 5.098719e-10,1.2857813e-08\n",
      "Iteration 17760: loss = 5.096728e-10,1.28564555e-08\n",
      "Iteration 17765: loss = 5.0698684e-10,1.28551e-08\n",
      "Iteration 17770: loss = 5.0678833e-10,1.2853739e-08\n",
      "Iteration 17775: loss = 5.065913e-10,1.2852373e-08\n",
      "Iteration 17780: loss = 5.0639304e-10,1.28510065e-08\n",
      "Iteration 17785: loss = 5.0619625e-10,1.2849638e-08\n",
      "Iteration 17790: loss = 5.035228e-10,1.2848265e-08\n",
      "Iteration 17795: loss = 5.0332694e-10,1.2846886e-08\n",
      "Iteration 17800: loss = 5.031329e-10,1.2845505e-08\n",
      "Iteration 17805: loss = 5.0293975e-10,1.2844121e-08\n",
      "Iteration 17810: loss = 5.0274634e-10,1.2842735e-08\n",
      "Iteration 17815: loss = 5.0008586e-10,1.2841344e-08\n",
      "Iteration 17820: loss = 4.9989474e-10,1.2839948e-08\n",
      "Iteration 17825: loss = 4.997039e-10,1.28385516e-08\n",
      "Iteration 17830: loss = 4.995139e-10,1.283715e-08\n",
      "Iteration 17835: loss = 4.993235e-10,1.2835745e-08\n",
      "Iteration 17840: loss = 4.9913434e-10,1.2834337e-08\n",
      "Iteration 17845: loss = 4.9894483e-10,1.2832929e-08\n",
      "Iteration 17850: loss = 4.987549e-10,1.28315145e-08\n",
      "Iteration 17855: loss = 4.9856674e-10,1.28301e-08\n",
      "Iteration 17860: loss = 4.98378e-10,1.2828686e-08\n",
      "Iteration 17865: loss = 4.9328286e-10,1.2827265e-08\n",
      "Iteration 17870: loss = 4.9309606e-10,1.2825838e-08\n",
      "Iteration 17875: loss = 4.9291143e-10,1.2824406e-08\n",
      "Iteration 17880: loss = 4.92727e-10,1.2822973e-08\n",
      "Iteration 17885: loss = 4.925435e-10,1.2821533e-08\n",
      "Iteration 17890: loss = 4.9236054e-10,1.2820093e-08\n",
      "Iteration 17895: loss = 4.921775e-10,1.281865e-08\n",
      "Iteration 17900: loss = 4.919944e-10,1.2817204e-08\n",
      "Iteration 17905: loss = 4.918121e-10,1.2815756e-08\n",
      "Iteration 17910: loss = 4.916291e-10,1.2814307e-08\n",
      "Iteration 17915: loss = 4.8900756e-10,1.281285e-08\n",
      "Iteration 17920: loss = 4.888269e-10,1.2811393e-08\n",
      "Iteration 17925: loss = 4.886479e-10,1.2809931e-08\n",
      "Iteration 17930: loss = 4.8846877e-10,1.2808464e-08\n",
      "Iteration 17935: loss = 4.882896e-10,1.2806994e-08\n",
      "Iteration 17940: loss = 4.881106e-10,1.2805522e-08\n",
      "Iteration 17945: loss = 4.855014e-10,1.2804048e-08\n",
      "Iteration 17950: loss = 4.85324e-10,1.28025714e-08\n",
      "Iteration 17955: loss = 4.8514665e-10,1.280109e-08\n",
      "Iteration 17960: loss = 4.849711e-10,1.2799602e-08\n",
      "Iteration 17965: loss = 4.823748e-10,1.2798115e-08\n",
      "Iteration 17970: loss = 4.8220095e-10,1.279662e-08\n",
      "Iteration 17975: loss = 4.820275e-10,1.2795123e-08\n",
      "Iteration 17980: loss = 4.8185456e-10,1.2793624e-08\n",
      "Iteration 17985: loss = 4.8168236e-10,1.27921185e-08\n",
      "Iteration 17990: loss = 4.815092e-10,1.2790611e-08\n",
      "Iteration 17995: loss = 4.8133725e-10,1.2789102e-08\n",
      "Iteration 18000: loss = 4.8116483e-10,1.27875905e-08\n",
      "Iteration 18005: loss = 4.8099297e-10,1.2786078e-08\n",
      "Iteration 18010: loss = 4.808211e-10,1.2784558e-08\n",
      "Iteration 18015: loss = 4.782383e-10,1.2783042e-08\n",
      "Iteration 18020: loss = 4.780667e-10,1.2781516e-08\n",
      "Iteration 18025: loss = 4.778976e-10,1.2779989e-08\n",
      "Iteration 18030: loss = 4.75328e-10,1.2778457e-08\n",
      "Iteration 18035: loss = 4.7516036e-10,1.2776918e-08\n",
      "Iteration 18040: loss = 4.74994e-10,1.2775378e-08\n",
      "Iteration 18045: loss = 4.748281e-10,1.27738335e-08\n",
      "Iteration 18050: loss = 4.746637e-10,1.2772284e-08\n",
      "Iteration 18055: loss = 4.744984e-10,1.2770732e-08\n",
      "Iteration 18060: loss = 4.7433335e-10,1.27691795e-08\n",
      "Iteration 18065: loss = 4.741688e-10,1.2767623e-08\n",
      "Iteration 18070: loss = 4.716129e-10,1.2766059e-08\n",
      "Iteration 18075: loss = 4.714491e-10,1.2764499e-08\n",
      "Iteration 18080: loss = 4.712862e-10,1.276293e-08\n",
      "Iteration 18085: loss = 4.7112375e-10,1.2761362e-08\n",
      "Iteration 18090: loss = 4.7096216e-10,1.2759788e-08\n",
      "Iteration 18095: loss = 4.708008e-10,1.2758211e-08\n",
      "Iteration 18100: loss = 4.706392e-10,1.2756631e-08\n",
      "Iteration 18105: loss = 4.704777e-10,1.2755048e-08\n",
      "Iteration 18110: loss = 4.703172e-10,1.2753463e-08\n",
      "Iteration 18115: loss = 4.6777326e-10,1.2751875e-08\n",
      "Iteration 18120: loss = 4.6761384e-10,1.2750282e-08\n",
      "Iteration 18125: loss = 4.6508117e-10,1.2748685e-08\n",
      "Iteration 18130: loss = 4.6492402e-10,1.2747082e-08\n",
      "Iteration 18135: loss = 4.6476756e-10,1.2745475e-08\n",
      "Iteration 18140: loss = 4.6461182e-10,1.27438655e-08\n",
      "Iteration 18145: loss = 4.6445617e-10,1.27422535e-08\n",
      "Iteration 18150: loss = 4.6430046e-10,1.2740639e-08\n",
      "Iteration 18155: loss = 4.641452e-10,1.2739021e-08\n",
      "Iteration 18160: loss = 4.639904e-10,1.2737399e-08\n",
      "Iteration 18165: loss = 4.63836e-10,1.27357715e-08\n",
      "Iteration 18170: loss = 4.636825e-10,1.2734145e-08\n",
      "Iteration 18175: loss = 4.6352777e-10,1.2732514e-08\n",
      "Iteration 18180: loss = 4.610102e-10,1.27308795e-08\n",
      "Iteration 18185: loss = 4.6085835e-10,1.2729239e-08\n",
      "Iteration 18190: loss = 4.6070545e-10,1.27275985e-08\n",
      "Iteration 18195: loss = 4.6055448e-10,1.2725951e-08\n",
      "Iteration 18200: loss = 4.6040247e-10,1.2724303e-08\n",
      "Iteration 18205: loss = 4.6025128e-10,1.272265e-08\n",
      "Iteration 18210: loss = 4.5774654e-10,1.2720997e-08\n",
      "Iteration 18215: loss = 4.5759666e-10,1.2719338e-08\n",
      "Iteration 18220: loss = 4.5744797e-10,1.2717673e-08\n",
      "Iteration 18225: loss = 4.5730061e-10,1.2716004e-08\n",
      "Iteration 18230: loss = 4.5715295e-10,1.271433e-08\n",
      "Iteration 18235: loss = 4.5466186e-10,1.2712654e-08\n",
      "Iteration 18240: loss = 4.545154e-10,1.27109745e-08\n",
      "Iteration 18245: loss = 4.5437062e-10,1.2709291e-08\n",
      "Iteration 18250: loss = 4.5422552e-10,1.27076e-08\n",
      "Iteration 18255: loss = 4.5408122e-10,1.2705913e-08\n",
      "Iteration 18260: loss = 4.539367e-10,1.270422e-08\n",
      "Iteration 18265: loss = 4.5379264e-10,1.2702522e-08\n",
      "Iteration 18270: loss = 4.5364887e-10,1.2700822e-08\n",
      "Iteration 18275: loss = 4.5350493e-10,1.269912e-08\n",
      "Iteration 18280: loss = 4.5336113e-10,1.26974165e-08\n",
      "Iteration 18285: loss = 4.5321755e-10,1.2695707e-08\n",
      "Iteration 18290: loss = 4.507388e-10,1.2693995e-08\n",
      "Iteration 18295: loss = 4.5059634e-10,1.2692282e-08\n",
      "Iteration 18300: loss = 4.5045556e-10,1.2690561e-08\n",
      "Iteration 18305: loss = 4.503148e-10,1.2688838e-08\n",
      "Iteration 18310: loss = 4.4784906e-10,1.268711e-08\n",
      "Iteration 18315: loss = 4.4770954e-10,1.2685376e-08\n",
      "Iteration 18320: loss = 4.4757054e-10,1.268364e-08\n",
      "Iteration 18325: loss = 4.474335e-10,1.2681897e-08\n",
      "Iteration 18330: loss = 4.4729606e-10,1.2680153e-08\n",
      "Iteration 18335: loss = 4.471591e-10,1.2678406e-08\n",
      "Iteration 18340: loss = 4.4702228e-10,1.2676654e-08\n",
      "Iteration 18345: loss = 4.4688506e-10,1.2674903e-08\n",
      "Iteration 18350: loss = 4.4443219e-10,1.2673144e-08\n",
      "Iteration 18355: loss = 4.4429713e-10,1.2671384e-08\n",
      "Iteration 18360: loss = 4.441619e-10,1.2669618e-08\n",
      "Iteration 18365: loss = 4.440283e-10,1.2667847e-08\n",
      "Iteration 18370: loss = 4.4389462e-10,1.2666075e-08\n",
      "Iteration 18375: loss = 4.437614e-10,1.2664296e-08\n",
      "Iteration 18380: loss = 4.4362802e-10,1.2662516e-08\n",
      "Iteration 18385: loss = 4.4349538e-10,1.2660733e-08\n",
      "Iteration 18390: loss = 4.4336293e-10,1.2658946e-08\n",
      "Iteration 18395: loss = 4.432309e-10,1.2657155e-08\n",
      "Iteration 18400: loss = 4.4309856e-10,1.26553665e-08\n",
      "Iteration 18405: loss = 4.4296578e-10,1.2653571e-08\n",
      "Iteration 18410: loss = 4.4052728e-10,1.2651772e-08\n",
      "Iteration 18415: loss = 4.403967e-10,1.2649968e-08\n",
      "Iteration 18420: loss = 4.37969e-10,1.2648162e-08\n",
      "Iteration 18425: loss = 4.3783951e-10,1.2646346e-08\n",
      "Iteration 18430: loss = 4.3771173e-10,1.2644529e-08\n",
      "Iteration 18435: loss = 4.3758488e-10,1.2642707e-08\n",
      "Iteration 18440: loss = 4.3745732e-10,1.2640881e-08\n",
      "Iteration 18445: loss = 4.3733053e-10,1.26390525e-08\n",
      "Iteration 18450: loss = 4.372053e-10,1.263722e-08\n",
      "Iteration 18455: loss = 4.370795e-10,1.2635384e-08\n",
      "Iteration 18460: loss = 4.369535e-10,1.2633546e-08\n",
      "Iteration 18465: loss = 4.368288e-10,1.2631701e-08\n",
      "Iteration 18470: loss = 4.3441584e-10,1.26298545e-08\n",
      "Iteration 18475: loss = 4.3429152e-10,1.2628005e-08\n",
      "Iteration 18480: loss = 4.3416792e-10,1.2626151e-08\n",
      "Iteration 18485: loss = 4.3404472e-10,1.2624291e-08\n",
      "Iteration 18490: loss = 4.339217e-10,1.2622431e-08\n",
      "Iteration 18495: loss = 4.3379852e-10,1.2620566e-08\n",
      "Iteration 18500: loss = 4.336758e-10,1.2618698e-08\n",
      "Iteration 18505: loss = 4.3355267e-10,1.26168285e-08\n",
      "Iteration 18510: loss = 4.3342965e-10,1.2614955e-08\n",
      "Iteration 18515: loss = 4.333068e-10,1.26130795e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18520: loss = 4.3318468e-10,1.2611204e-08\n",
      "Iteration 18525: loss = 4.3306292e-10,1.2609317e-08\n",
      "Iteration 18530: loss = 4.306624e-10,1.2607429e-08\n",
      "Iteration 18535: loss = 4.2827356e-10,1.2605535e-08\n",
      "Iteration 18540: loss = 4.2815465e-10,1.2603637e-08\n",
      "Iteration 18545: loss = 4.2803738e-10,1.2601734e-08\n",
      "Iteration 18550: loss = 4.2792028e-10,1.2599825e-08\n",
      "Iteration 18555: loss = 4.278042e-10,1.2597912e-08\n",
      "Iteration 18560: loss = 4.2768908e-10,1.2595997e-08\n",
      "Iteration 18565: loss = 4.2757217e-10,1.2594076e-08\n",
      "Iteration 18570: loss = 4.2745651e-10,1.2592154e-08\n",
      "Iteration 18575: loss = 4.273415e-10,1.2590229e-08\n",
      "Iteration 18580: loss = 4.2722506e-10,1.25883e-08\n",
      "Iteration 18585: loss = 4.2710943e-10,1.25863675e-08\n",
      "Iteration 18590: loss = 4.2473458e-10,1.2584435e-08\n",
      "Iteration 18595: loss = 4.2462026e-10,1.2582492e-08\n",
      "Iteration 18600: loss = 4.2450723e-10,1.2580551e-08\n",
      "Iteration 18605: loss = 4.243946e-10,1.2578599e-08\n",
      "Iteration 18610: loss = 4.2428294e-10,1.2576644e-08\n",
      "Iteration 18615: loss = 4.2417173e-10,1.2574686e-08\n",
      "Iteration 18620: loss = 4.240599e-10,1.2572726e-08\n",
      "Iteration 18625: loss = 4.2394918e-10,1.257076e-08\n",
      "Iteration 18630: loss = 4.2383816e-10,1.2568791e-08\n",
      "Iteration 18635: loss = 4.2372728e-10,1.25668205e-08\n",
      "Iteration 18640: loss = 4.236164e-10,1.256485e-08\n",
      "Iteration 18645: loss = 4.2350576e-10,1.2562871e-08\n",
      "Iteration 18650: loss = 4.2339465e-10,1.2560891e-08\n",
      "Iteration 18655: loss = 4.2103368e-10,1.2558907e-08\n",
      "Iteration 18660: loss = 4.2092338e-10,1.2556921e-08\n",
      "Iteration 18665: loss = 4.208144e-10,1.2554927e-08\n",
      "Iteration 18670: loss = 4.184659e-10,1.2552929e-08\n",
      "Iteration 18675: loss = 4.183598e-10,1.2550924e-08\n",
      "Iteration 18680: loss = 4.1825465e-10,1.2548915e-08\n",
      "Iteration 18685: loss = 4.1814915e-10,1.25469e-08\n",
      "Iteration 18690: loss = 4.1804427e-10,1.2544884e-08\n",
      "Iteration 18695: loss = 4.1794068e-10,1.25428645e-08\n",
      "Iteration 18700: loss = 4.178361e-10,1.254084e-08\n",
      "Iteration 18705: loss = 4.1773218e-10,1.2538811e-08\n",
      "Iteration 18710: loss = 4.176287e-10,1.2536778e-08\n",
      "Iteration 18715: loss = 4.1752568e-10,1.2534743e-08\n",
      "Iteration 18720: loss = 4.1519077e-10,1.25327055e-08\n",
      "Iteration 18725: loss = 4.1508774e-10,1.2530663e-08\n",
      "Iteration 18730: loss = 4.1498585e-10,1.2528615e-08\n",
      "Iteration 18735: loss = 4.1488357e-10,1.2526565e-08\n",
      "Iteration 18740: loss = 4.1478265e-10,1.2524508e-08\n",
      "Iteration 18745: loss = 4.1468204e-10,1.2522451e-08\n",
      "Iteration 18750: loss = 4.1458093e-10,1.2520387e-08\n",
      "Iteration 18755: loss = 4.1448067e-10,1.2518321e-08\n",
      "Iteration 18760: loss = 4.1438045e-10,1.2516252e-08\n",
      "Iteration 18765: loss = 4.1428036e-10,1.2514178e-08\n",
      "Iteration 18770: loss = 4.1418066e-10,1.2512103e-08\n",
      "Iteration 18775: loss = 4.1408024e-10,1.2510024e-08\n",
      "Iteration 18780: loss = 4.1397996e-10,1.25079405e-08\n",
      "Iteration 18785: loss = 4.138795e-10,1.2505857e-08\n",
      "Iteration 18790: loss = 4.1155823e-10,1.2503767e-08\n",
      "Iteration 18795: loss = 4.1146028e-10,1.250167e-08\n",
      "Iteration 18800: loss = 4.113636e-10,1.249957e-08\n",
      "Iteration 18805: loss = 4.112665e-10,1.2497463e-08\n",
      "Iteration 18810: loss = 4.1117057e-10,1.2495353e-08\n",
      "Iteration 18815: loss = 4.1107426e-10,1.2493241e-08\n",
      "Iteration 18820: loss = 4.087661e-10,1.2491125e-08\n",
      "Iteration 18825: loss = 4.0867168e-10,1.2489002e-08\n",
      "Iteration 18830: loss = 4.0857687e-10,1.2486874e-08\n",
      "Iteration 18835: loss = 4.0848355e-10,1.2484743e-08\n",
      "Iteration 18840: loss = 4.0839068e-10,1.2482608e-08\n",
      "Iteration 18845: loss = 4.0829762e-10,1.2480468e-08\n",
      "Iteration 18850: loss = 4.0820514e-10,1.2478326e-08\n",
      "Iteration 18855: loss = 4.0811218e-10,1.2476178e-08\n",
      "Iteration 18860: loss = 4.058179e-10,1.2474031e-08\n",
      "Iteration 18865: loss = 4.0572726e-10,1.2471871e-08\n",
      "Iteration 18870: loss = 4.0563783e-10,1.246971e-08\n",
      "Iteration 18875: loss = 4.0554834e-10,1.2467544e-08\n",
      "Iteration 18880: loss = 4.054583e-10,1.2465374e-08\n",
      "Iteration 18885: loss = 4.053703e-10,1.2463201e-08\n",
      "Iteration 18890: loss = 4.0528106e-10,1.2461022e-08\n",
      "Iteration 18895: loss = 4.051931e-10,1.2458839e-08\n",
      "Iteration 18900: loss = 4.0510528e-10,1.2456657e-08\n",
      "Iteration 18905: loss = 4.0501627e-10,1.2454467e-08\n",
      "Iteration 18910: loss = 4.0492865e-10,1.24522765e-08\n",
      "Iteration 18915: loss = 4.0484052e-10,1.2450084e-08\n",
      "Iteration 18920: loss = 4.047526e-10,1.2447885e-08\n",
      "Iteration 18925: loss = 4.0466377e-10,1.2445684e-08\n",
      "Iteration 18930: loss = 4.0238257e-10,1.2443479e-08\n",
      "Iteration 18935: loss = 4.0229609e-10,1.244127e-08\n",
      "Iteration 18940: loss = 4.022097e-10,1.2439053e-08\n",
      "Iteration 18945: loss = 4.0212503e-10,1.2436832e-08\n",
      "Iteration 18950: loss = 4.0203965e-10,1.2434608e-08\n",
      "Iteration 18955: loss = 4.019549e-10,1.24323805e-08\n",
      "Iteration 18960: loss = 4.0187031e-10,1.2430147e-08\n",
      "Iteration 18965: loss = 4.0178552e-10,1.2427913e-08\n",
      "Iteration 18970: loss = 4.0170026e-10,1.2425673e-08\n",
      "Iteration 18975: loss = 4.0161596e-10,1.2423433e-08\n",
      "Iteration 18980: loss = 4.0153156e-10,1.24211885e-08\n",
      "Iteration 18985: loss = 4.0144635e-10,1.24189405e-08\n",
      "Iteration 18990: loss = 4.013624e-10,1.2416688e-08\n",
      "Iteration 18995: loss = 3.9909454e-10,1.2414433e-08\n",
      "Iteration 19000: loss = 3.9683742e-10,1.2412169e-08\n",
      "Iteration 19005: loss = 3.967565e-10,1.2409895e-08\n",
      "Iteration 19010: loss = 3.9667744e-10,1.24076225e-08\n",
      "Iteration 19015: loss = 3.9659917e-10,1.2405339e-08\n",
      "Iteration 19020: loss = 3.9652112e-10,1.2403054e-08\n",
      "Iteration 19025: loss = 3.9644332e-10,1.2400762e-08\n",
      "Iteration 19030: loss = 3.9636602e-10,1.23984645e-08\n",
      "Iteration 19035: loss = 3.9628856e-10,1.23961685e-08\n",
      "Iteration 19040: loss = 3.9621165e-10,1.2393866e-08\n",
      "Iteration 19045: loss = 3.9613413e-10,1.23915616e-08\n",
      "Iteration 19050: loss = 3.9605716e-10,1.2389256e-08\n",
      "Iteration 19055: loss = 3.9598055e-10,1.23869395e-08\n",
      "Iteration 19060: loss = 3.9590378e-10,1.2384623e-08\n",
      "Iteration 19065: loss = 3.9582668e-10,1.2382304e-08\n",
      "Iteration 19070: loss = 3.9575065e-10,1.2379981e-08\n",
      "Iteration 19075: loss = 3.9350953e-10,1.23776545e-08\n",
      "Iteration 19080: loss = 3.9343398e-10,1.237532e-08\n",
      "Iteration 19085: loss = 3.9335946e-10,1.2372983e-08\n",
      "Iteration 19090: loss = 3.932847e-10,1.237064e-08\n",
      "Iteration 19095: loss = 3.9321088e-10,1.2368295e-08\n",
      "Iteration 19100: loss = 3.9313763e-10,1.236594e-08\n",
      "Iteration 19105: loss = 3.930648e-10,1.2363586e-08\n",
      "Iteration 19110: loss = 3.9299128e-10,1.2361227e-08\n",
      "Iteration 19115: loss = 3.9291917e-10,1.2358866e-08\n",
      "Iteration 19120: loss = 3.9284628e-10,1.2356496e-08\n",
      "Iteration 19125: loss = 3.927736e-10,1.2354126e-08\n",
      "Iteration 19130: loss = 3.9270073e-10,1.2351753e-08\n",
      "Iteration 19135: loss = 3.9262818e-10,1.2349377e-08\n",
      "Iteration 19140: loss = 3.9255543e-10,1.2346996e-08\n",
      "Iteration 19145: loss = 3.924833e-10,1.23446116e-08\n",
      "Iteration 19150: loss = 3.924098e-10,1.2342227e-08\n",
      "Iteration 19155: loss = 3.9018208e-10,1.2339834e-08\n",
      "Iteration 19160: loss = 3.9011197e-10,1.2337433e-08\n",
      "Iteration 19165: loss = 3.9004186e-10,1.2335032e-08\n",
      "Iteration 19170: loss = 3.899724e-10,1.2332622e-08\n",
      "Iteration 19175: loss = 3.8990342e-10,1.2330208e-08\n",
      "Iteration 19180: loss = 3.8983472e-10,1.2327793e-08\n",
      "Iteration 19185: loss = 3.8976586e-10,1.2325372e-08\n",
      "Iteration 19190: loss = 3.8969783e-10,1.2322946e-08\n",
      "Iteration 19195: loss = 3.8962963e-10,1.2320517e-08\n",
      "Iteration 19200: loss = 3.8956105e-10,1.2318082e-08\n",
      "Iteration 19205: loss = 3.894931e-10,1.2315648e-08\n",
      "Iteration 19210: loss = 3.894258e-10,1.23132065e-08\n",
      "Iteration 19215: loss = 3.8935796e-10,1.2310763e-08\n",
      "Iteration 19220: loss = 3.8929082e-10,1.2308313e-08\n",
      "Iteration 19225: loss = 3.8707848e-10,1.23058586e-08\n",
      "Iteration 19230: loss = 3.8487658e-10,1.2303399e-08\n",
      "Iteration 19235: loss = 3.848127e-10,1.2300931e-08\n",
      "Iteration 19240: loss = 3.8474993e-10,1.2298458e-08\n",
      "Iteration 19245: loss = 3.846887e-10,1.22959785e-08\n",
      "Iteration 19250: loss = 3.846267e-10,1.2293495e-08\n",
      "Iteration 19255: loss = 3.8456605e-10,1.2291008e-08\n",
      "Iteration 19260: loss = 3.8450507e-10,1.2288515e-08\n",
      "Iteration 19265: loss = 3.8444414e-10,1.2286018e-08\n",
      "Iteration 19270: loss = 3.843835e-10,1.228352e-08\n",
      "Iteration 19275: loss = 3.843228e-10,1.2281017e-08\n",
      "Iteration 19280: loss = 3.8426184e-10,1.2278513e-08\n",
      "Iteration 19285: loss = 3.8420114e-10,1.22760015e-08\n",
      "Iteration 19290: loss = 3.8414064e-10,1.2273487e-08\n",
      "Iteration 19295: loss = 3.8408024e-10,1.227097e-08\n",
      "Iteration 19300: loss = 3.8401934e-10,1.22684485e-08\n",
      "Iteration 19305: loss = 3.8395773e-10,1.2265925e-08\n",
      "Iteration 19310: loss = 3.838975e-10,1.22633965e-08\n",
      "Iteration 19315: loss = 3.817103e-10,1.2260866e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19320: loss = 3.8165135e-10,1.2258324e-08\n",
      "Iteration 19325: loss = 3.815933e-10,1.225578e-08\n",
      "Iteration 19330: loss = 3.8153616e-10,1.225323e-08\n",
      "Iteration 19335: loss = 3.8147885e-10,1.2250674e-08\n",
      "Iteration 19340: loss = 3.8142234e-10,1.22481145e-08\n",
      "Iteration 19345: loss = 3.8136602e-10,1.224555e-08\n",
      "Iteration 19350: loss = 3.8130968e-10,1.2242982e-08\n",
      "Iteration 19355: loss = 3.8125367e-10,1.2240413e-08\n",
      "Iteration 19360: loss = 3.8119866e-10,1.2237834e-08\n",
      "Iteration 19365: loss = 3.8114467e-10,1.2235252e-08\n",
      "Iteration 19370: loss = 3.8108958e-10,1.22326655e-08\n",
      "Iteration 19375: loss = 3.8103523e-10,1.22300765e-08\n",
      "Iteration 19380: loss = 3.8097994e-10,1.2227483e-08\n",
      "Iteration 19385: loss = 3.809261e-10,1.2224886e-08\n",
      "Iteration 19390: loss = 3.8087097e-10,1.2222284e-08\n",
      "Iteration 19395: loss = 3.808167e-10,1.22196795e-08\n",
      "Iteration 19400: loss = 3.786449e-10,1.221707e-08\n",
      "Iteration 19405: loss = 3.7859185e-10,1.2214456e-08\n",
      "Iteration 19410: loss = 3.7853953e-10,1.2211834e-08\n",
      "Iteration 19415: loss = 3.7848802e-10,1.2209208e-08\n",
      "Iteration 19420: loss = 3.784362e-10,1.2206578e-08\n",
      "Iteration 19425: loss = 3.7838496e-10,1.2203944e-08\n",
      "Iteration 19430: loss = 3.7833411e-10,1.2201304e-08\n",
      "Iteration 19435: loss = 3.7828293e-10,1.2198665e-08\n",
      "Iteration 19440: loss = 3.7823153e-10,1.2196018e-08\n",
      "Iteration 19445: loss = 3.781804e-10,1.219337e-08\n",
      "Iteration 19450: loss = 3.7812864e-10,1.2190715e-08\n",
      "Iteration 19455: loss = 3.78078e-10,1.2188059e-08\n",
      "Iteration 19460: loss = 3.7802614e-10,1.2185399e-08\n",
      "Iteration 19465: loss = 3.7797462e-10,1.2182733e-08\n",
      "Iteration 19470: loss = 3.7792366e-10,1.2180066e-08\n",
      "Iteration 19475: loss = 3.778725e-10,1.2177397e-08\n",
      "Iteration 19480: loss = 3.778212e-10,1.2174716e-08\n",
      "Iteration 19485: loss = 3.7566258e-10,1.2172038e-08\n",
      "Iteration 19490: loss = 3.7561323e-10,1.21693535e-08\n",
      "Iteration 19495: loss = 3.7556513e-10,1.216666e-08\n",
      "Iteration 19500: loss = 3.7551726e-10,1.21639605e-08\n",
      "Iteration 19505: loss = 3.7546924e-10,1.2161255e-08\n",
      "Iteration 19510: loss = 3.754226e-10,1.215855e-08\n",
      "Iteration 19515: loss = 3.7537576e-10,1.2155837e-08\n",
      "Iteration 19520: loss = 3.75329e-10,1.2153119e-08\n",
      "Iteration 19525: loss = 3.7528236e-10,1.21504e-08\n",
      "Iteration 19530: loss = 3.7523604e-10,1.2147676e-08\n",
      "Iteration 19535: loss = 3.751882e-10,1.2144952e-08\n",
      "Iteration 19540: loss = 3.7514172e-10,1.214222e-08\n",
      "Iteration 19545: loss = 3.750951e-10,1.2139482e-08\n",
      "Iteration 19550: loss = 3.7504877e-10,1.2136746e-08\n",
      "Iteration 19555: loss = 3.7500228e-10,1.2134003e-08\n",
      "Iteration 19560: loss = 3.7495507e-10,1.2131256e-08\n",
      "Iteration 19565: loss = 3.749084e-10,1.2128504e-08\n",
      "Iteration 19570: loss = 3.7486214e-10,1.2125753e-08\n",
      "Iteration 19575: loss = 3.7271697e-10,1.2122992e-08\n",
      "Iteration 19580: loss = 3.7267134e-10,1.21202275e-08\n",
      "Iteration 19585: loss = 3.7262746e-10,1.2117456e-08\n",
      "Iteration 19590: loss = 3.7258382e-10,1.2114677e-08\n",
      "Iteration 19595: loss = 3.725411e-10,1.2111897e-08\n",
      "Iteration 19600: loss = 3.7249845e-10,1.210911e-08\n",
      "Iteration 19605: loss = 3.7036788e-10,1.2106318e-08\n",
      "Iteration 19610: loss = 3.7032802e-10,1.21035155e-08\n",
      "Iteration 19615: loss = 3.7028822e-10,1.21007115e-08\n",
      "Iteration 19620: loss = 3.7025025e-10,1.20979005e-08\n",
      "Iteration 19625: loss = 3.7021242e-10,1.20950805e-08\n",
      "Iteration 19630: loss = 3.7017492e-10,1.209226e-08\n",
      "Iteration 19635: loss = 3.70138e-10,1.2089432e-08\n",
      "Iteration 19640: loss = 3.7010192e-10,1.2086598e-08\n",
      "Iteration 19645: loss = 3.7006642e-10,1.20837615e-08\n",
      "Iteration 19650: loss = 3.700303e-10,1.2080919e-08\n",
      "Iteration 19655: loss = 3.6999384e-10,1.20780745e-08\n",
      "Iteration 19660: loss = 3.6995815e-10,1.2075228e-08\n",
      "Iteration 19665: loss = 3.6992218e-10,1.2072372e-08\n",
      "Iteration 19670: loss = 3.678067e-10,1.2069518e-08\n",
      "Iteration 19675: loss = 3.6777162e-10,1.2066655e-08\n",
      "Iteration 19680: loss = 3.6773826e-10,1.2063785e-08\n",
      "Iteration 19685: loss = 3.6770487e-10,1.206091e-08\n",
      "Iteration 19690: loss = 3.6767256e-10,1.2058031e-08\n",
      "Iteration 19695: loss = 3.676404e-10,1.2055145e-08\n",
      "Iteration 19700: loss = 3.6760775e-10,1.2052257e-08\n",
      "Iteration 19705: loss = 3.6757572e-10,1.2049363e-08\n",
      "Iteration 19710: loss = 3.6754344e-10,1.2046467e-08\n",
      "Iteration 19715: loss = 3.6751124e-10,1.2043566e-08\n",
      "Iteration 19720: loss = 3.674794e-10,1.2040661e-08\n",
      "Iteration 19725: loss = 3.674466e-10,1.2037755e-08\n",
      "Iteration 19730: loss = 3.6741485e-10,1.2034839e-08\n",
      "Iteration 19735: loss = 3.673826e-10,1.2031923e-08\n",
      "Iteration 19740: loss = 3.673508e-10,1.20290045e-08\n",
      "Iteration 19745: loss = 3.6731898e-10,1.2026078e-08\n",
      "Iteration 19750: loss = 3.6728695e-10,1.2023149e-08\n",
      "Iteration 19755: loss = 3.6725503e-10,1.2020218e-08\n",
      "Iteration 19760: loss = 3.6722314e-10,1.2017283e-08\n",
      "Iteration 19765: loss = 3.6719083e-10,1.201434e-08\n",
      "Iteration 19770: loss = 3.6508907e-10,1.2011397e-08\n",
      "Iteration 19775: loss = 3.6505796e-10,1.2008448e-08\n",
      "Iteration 19780: loss = 3.6502845e-10,1.2005489e-08\n",
      "Iteration 19785: loss = 3.6499948e-10,1.2002525e-08\n",
      "Iteration 19790: loss = 3.6497058e-10,1.199956e-08\n",
      "Iteration 19795: loss = 3.6494233e-10,1.1996585e-08\n",
      "Iteration 19800: loss = 3.6491457e-10,1.1993608e-08\n",
      "Iteration 19805: loss = 3.6488693e-10,1.1990624e-08\n",
      "Iteration 19810: loss = 3.6486045e-10,1.1987636e-08\n",
      "Iteration 19815: loss = 3.6483372e-10,1.1984647e-08\n",
      "Iteration 19820: loss = 3.6480655e-10,1.1981647e-08\n",
      "Iteration 19825: loss = 3.6477996e-10,1.1978649e-08\n",
      "Iteration 19830: loss = 3.6475364e-10,1.1975643e-08\n",
      "Iteration 19835: loss = 3.647266e-10,1.1972634e-08\n",
      "Iteration 19840: loss = 3.6469994e-10,1.1969622e-08\n",
      "Iteration 19845: loss = 3.6467332e-10,1.19666055e-08\n",
      "Iteration 19850: loss = 3.64647e-10,1.1963587e-08\n",
      "Iteration 19855: loss = 3.6462022e-10,1.1960562e-08\n",
      "Iteration 19860: loss = 3.6459377e-10,1.1957532e-08\n",
      "Iteration 19865: loss = 3.6456738e-10,1.19545005e-08\n",
      "Iteration 19870: loss = 3.6454054e-10,1.1951462e-08\n",
      "Iteration 19875: loss = 3.6245373e-10,1.1948424e-08\n",
      "Iteration 19880: loss = 3.6242906e-10,1.1945373e-08\n",
      "Iteration 19885: loss = 3.6240586e-10,1.1942318e-08\n",
      "Iteration 19890: loss = 3.623839e-10,1.1939254e-08\n",
      "Iteration 19895: loss = 3.623618e-10,1.19361845e-08\n",
      "Iteration 19900: loss = 3.6234082e-10,1.1933116e-08\n",
      "Iteration 19905: loss = 3.6231937e-10,1.19300365e-08\n",
      "Iteration 19910: loss = 3.6229866e-10,1.1926956e-08\n",
      "Iteration 19915: loss = 3.622771e-10,1.1923872e-08\n",
      "Iteration 19920: loss = 3.622561e-10,1.1920779e-08\n",
      "Iteration 19925: loss = 3.6223535e-10,1.1917685e-08\n",
      "Iteration 19930: loss = 3.6221492e-10,1.1914587e-08\n",
      "Iteration 19935: loss = 3.621936e-10,1.1911485e-08\n",
      "Iteration 19940: loss = 3.6217304e-10,1.190838e-08\n",
      "Iteration 19945: loss = 3.6215184e-10,1.1905268e-08\n",
      "Iteration 19950: loss = 3.6213166e-10,1.1902156e-08\n",
      "Iteration 19955: loss = 3.6211023e-10,1.1899036e-08\n",
      "Iteration 19960: loss = 3.620896e-10,1.1895916e-08\n",
      "Iteration 19965: loss = 3.6206893e-10,1.1892788e-08\n",
      "Iteration 19970: loss = 3.6204847e-10,1.1889656e-08\n",
      "Iteration 19975: loss = 3.6202755e-10,1.1886522e-08\n",
      "Iteration 19980: loss = 3.620072e-10,1.1883383e-08\n",
      "Iteration 19985: loss = 3.5993641e-10,1.1880239e-08\n",
      "Iteration 19990: loss = 3.5991876e-10,1.1877087e-08\n",
      "Iteration 19995: loss = 3.599017e-10,1.1873922e-08\n",
      "Iteration 20000: loss = 3.5988632e-10,1.1870759e-08\n",
      "Iteration 20005: loss = 3.5987072e-10,1.1867587e-08\n",
      "Iteration 20010: loss = 3.5985562e-10,1.1864409e-08\n",
      "Iteration 20015: loss = 3.5983985e-10,1.1861231e-08\n",
      "Iteration 20020: loss = 3.598252e-10,1.1858043e-08\n",
      "Iteration 20025: loss = 3.598111e-10,1.1854854e-08\n",
      "Iteration 20030: loss = 3.597963e-10,1.1851661e-08\n",
      "Iteration 20035: loss = 3.59782e-10,1.1848463e-08\n",
      "Iteration 20040: loss = 3.597678e-10,1.1845262e-08\n",
      "Iteration 20045: loss = 3.5975267e-10,1.18420544e-08\n",
      "Iteration 20050: loss = 3.597378e-10,1.1838845e-08\n",
      "Iteration 20055: loss = 3.5972333e-10,1.1835633e-08\n",
      "Iteration 20060: loss = 3.5970904e-10,1.1832415e-08\n",
      "Iteration 20065: loss = 3.5969464e-10,1.182919e-08\n",
      "Iteration 20070: loss = 3.5968026e-10,1.1825966e-08\n",
      "Iteration 20075: loss = 3.5966596e-10,1.1822734e-08\n",
      "Iteration 20080: loss = 3.596514e-10,1.1819498e-08\n",
      "Iteration 20085: loss = 3.5963696e-10,1.1816262e-08\n",
      "Iteration 20090: loss = 3.596228e-10,1.18130155e-08\n",
      "Iteration 20095: loss = 3.5960893e-10,1.1809771e-08\n",
      "Iteration 20100: loss = 3.5755277e-10,1.1806518e-08\n",
      "Iteration 20105: loss = 3.5754041e-10,1.1803261e-08\n",
      "Iteration 20110: loss = 3.5752848e-10,1.1799995e-08\n",
      "Iteration 20115: loss = 3.5751746e-10,1.1796722e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20120: loss = 3.5750622e-10,1.1793446e-08\n",
      "Iteration 20125: loss = 3.5953665e-10,1.179017e-08\n",
      "Iteration 20130: loss = 3.595232e-10,1.178689e-08\n",
      "Iteration 20135: loss = 3.5951006e-10,1.1783611e-08\n",
      "Iteration 20140: loss = 3.5949588e-10,1.1780327e-08\n",
      "Iteration 20145: loss = 3.5948056e-10,1.1777041e-08\n",
      "Iteration 20150: loss = 3.594652e-10,1.1773752e-08\n",
      "Iteration 20155: loss = 3.594501e-10,1.1770459e-08\n",
      "Iteration 20160: loss = 3.5943493e-10,1.176716e-08\n",
      "Iteration 20165: loss = 3.5941974e-10,1.1763859e-08\n",
      "Iteration 20170: loss = 3.594044e-10,1.1760555e-08\n",
      "Iteration 20175: loss = 3.5938907e-10,1.1757244e-08\n",
      "Iteration 20180: loss = 3.5937378e-10,1.175393e-08\n",
      "Iteration 20185: loss = 3.5935835e-10,1.1750614e-08\n",
      "Iteration 20190: loss = 3.5934322e-10,1.1747292e-08\n",
      "Iteration 20195: loss = 3.5932937e-10,1.1743962e-08\n",
      "Iteration 20200: loss = 3.5931536e-10,1.1740629e-08\n",
      "Iteration 20205: loss = 3.5930162e-10,1.1737294e-08\n",
      "Iteration 20210: loss = 3.592878e-10,1.173395e-08\n",
      "Iteration 20215: loss = 3.592742e-10,1.1730604e-08\n",
      "Iteration 20220: loss = 3.5722006e-10,1.1727254e-08\n",
      "Iteration 20225: loss = 3.572079e-10,1.1723893e-08\n",
      "Iteration 20230: loss = 3.5719835e-10,1.17205285e-08\n",
      "Iteration 20235: loss = 3.5718806e-10,1.1717155e-08\n",
      "Iteration 20240: loss = 3.5717915e-10,1.1713777e-08\n",
      "Iteration 20245: loss = 3.571705e-10,1.1710397e-08\n",
      "Iteration 20250: loss = 3.5716163e-10,1.170701e-08\n",
      "Iteration 20255: loss = 3.5715397e-10,1.1703617e-08\n",
      "Iteration 20260: loss = 3.5714523e-10,1.1700221e-08\n",
      "Iteration 20265: loss = 3.5713735e-10,1.169682e-08\n",
      "Iteration 20270: loss = 3.5712924e-10,1.1693419e-08\n",
      "Iteration 20275: loss = 3.5712075e-10,1.169001e-08\n",
      "Iteration 20280: loss = 3.571127e-10,1.16865975e-08\n",
      "Iteration 20285: loss = 3.5710504e-10,1.16831815e-08\n",
      "Iteration 20290: loss = 3.5709713e-10,1.1679762e-08\n",
      "Iteration 20295: loss = 3.5708855e-10,1.16763355e-08\n",
      "Iteration 20300: loss = 3.57081e-10,1.1672906e-08\n",
      "Iteration 20305: loss = 3.5707326e-10,1.1669472e-08\n",
      "Iteration 20310: loss = 3.5706538e-10,1.1666034e-08\n",
      "Iteration 20315: loss = 3.5705772e-10,1.1662592e-08\n",
      "Iteration 20320: loss = 3.5705014e-10,1.16591465e-08\n",
      "Iteration 20325: loss = 3.5704287e-10,1.1655697e-08\n",
      "Iteration 20330: loss = 3.5703515e-10,1.1652242e-08\n",
      "Iteration 20335: loss = 3.5702719e-10,1.1648782e-08\n",
      "Iteration 20340: loss = 3.5702008e-10,1.1645321e-08\n",
      "Iteration 20345: loss = 3.570125e-10,1.16418555e-08\n",
      "Iteration 20350: loss = 3.5497336e-10,1.1638383e-08\n",
      "Iteration 20355: loss = 3.549682e-10,1.1634903e-08\n",
      "Iteration 20360: loss = 3.5496436e-10,1.1631415e-08\n",
      "Iteration 20365: loss = 3.5496087e-10,1.1627921e-08\n",
      "Iteration 20370: loss = 3.5495826e-10,1.1624422e-08\n",
      "Iteration 20375: loss = 3.5495606e-10,1.1620921e-08\n",
      "Iteration 20380: loss = 3.549541e-10,1.1617412e-08\n",
      "Iteration 20385: loss = 3.5495198e-10,1.1613898e-08\n",
      "Iteration 20390: loss = 3.549501e-10,1.1610384e-08\n",
      "Iteration 20395: loss = 3.5494763e-10,1.160686e-08\n",
      "Iteration 20400: loss = 3.5494582e-10,1.1603337e-08\n",
      "Iteration 20405: loss = 3.549443e-10,1.1599805e-08\n",
      "Iteration 20410: loss = 3.5494277e-10,1.1596274e-08\n",
      "Iteration 20415: loss = 3.5494083e-10,1.15927365e-08\n",
      "Iteration 20420: loss = 3.549392e-10,1.1589194e-08\n",
      "Iteration 20425: loss = 3.5493763e-10,1.1585646e-08\n",
      "Iteration 20430: loss = 3.549363e-10,1.1582097e-08\n",
      "Iteration 20435: loss = 3.5493583e-10,1.157854e-08\n",
      "Iteration 20440: loss = 3.5493622e-10,1.1574978e-08\n",
      "Iteration 20445: loss = 3.5493633e-10,1.15714105e-08\n",
      "Iteration 20450: loss = 3.5493677e-10,1.1567842e-08\n",
      "Iteration 20455: loss = 3.5493686e-10,1.1564267e-08\n",
      "Iteration 20460: loss = 3.5493739e-10,1.1560685e-08\n",
      "Iteration 20465: loss = 3.549385e-10,1.1557101e-08\n",
      "Iteration 20470: loss = 3.549392e-10,1.1553513e-08\n",
      "Iteration 20475: loss = 3.549404e-10,1.1549922e-08\n",
      "Iteration 20480: loss = 3.5494138e-10,1.1546324e-08\n",
      "Iteration 20485: loss = 3.54942e-10,1.1542724e-08\n",
      "Iteration 20490: loss = 3.5292158e-10,1.1539116e-08\n",
      "Iteration 20495: loss = 3.5292635e-10,1.1535497e-08\n",
      "Iteration 20500: loss = 3.5293227e-10,1.1531872e-08\n",
      "Iteration 20505: loss = 3.5293848e-10,1.1528244e-08\n",
      "Iteration 20510: loss = 3.529452e-10,1.1524609e-08\n",
      "Iteration 20515: loss = 3.5295153e-10,1.1520969e-08\n",
      "Iteration 20520: loss = 3.5295789e-10,1.1517327e-08\n",
      "Iteration 20525: loss = 3.5296469e-10,1.1513679e-08\n",
      "Iteration 20530: loss = 3.5297174e-10,1.15100285e-08\n",
      "Iteration 20535: loss = 3.5297898e-10,1.1506373e-08\n",
      "Iteration 20540: loss = 3.5298606e-10,1.1502715e-08\n",
      "Iteration 20545: loss = 3.5299316e-10,1.1499048e-08\n",
      "Iteration 20550: loss = 3.530003e-10,1.1495382e-08\n",
      "Iteration 20555: loss = 3.5300704e-10,1.1491707e-08\n",
      "Iteration 20560: loss = 3.530151e-10,1.1488034e-08\n",
      "Iteration 20565: loss = 3.5302214e-10,1.1484352e-08\n",
      "Iteration 20570: loss = 3.5302924e-10,1.1480665e-08\n",
      "Iteration 20575: loss = 3.530374e-10,1.1476979e-08\n",
      "Iteration 20580: loss = 3.5304468e-10,1.1473285e-08\n",
      "Iteration 20585: loss = 3.530527e-10,1.1469587e-08\n",
      "Iteration 20590: loss = 3.5306028e-10,1.1465886e-08\n",
      "Iteration 20595: loss = 3.5306805e-10,1.14621805e-08\n",
      "Iteration 20600: loss = 3.5307546e-10,1.145847e-08\n",
      "Iteration 20605: loss = 3.5308348e-10,1.1454757e-08\n",
      "Iteration 20610: loss = 3.5309092e-10,1.14510375e-08\n",
      "Iteration 20615: loss = 3.530987e-10,1.1447316e-08\n",
      "Iteration 20620: loss = 3.5310763e-10,1.144359e-08\n",
      "Iteration 20625: loss = 3.531153e-10,1.1439859e-08\n",
      "Iteration 20630: loss = 3.53124e-10,1.1436125e-08\n",
      "Iteration 20635: loss = 3.5313227e-10,1.1432386e-08\n",
      "Iteration 20640: loss = 3.531403e-10,1.1428644e-08\n",
      "Iteration 20645: loss = 3.5314943e-10,1.1424895e-08\n",
      "Iteration 20650: loss = 3.5315784e-10,1.1421143e-08\n",
      "Iteration 20655: loss = 3.531668e-10,1.1417385e-08\n",
      "Iteration 20660: loss = 3.5317557e-10,1.14136265e-08\n",
      "Iteration 20665: loss = 3.5318404e-10,1.1409864e-08\n",
      "Iteration 20670: loss = 3.5319317e-10,1.14060965e-08\n",
      "Iteration 20675: loss = 3.5320158e-10,1.14023235e-08\n",
      "Iteration 20680: loss = 3.532118e-10,1.139855e-08\n",
      "Iteration 20685: loss = 3.5322031e-10,1.1394768e-08\n",
      "Iteration 20690: loss = 3.532293e-10,1.1390983e-08\n",
      "Iteration 20695: loss = 3.5323788e-10,1.1387195e-08\n",
      "Iteration 20700: loss = 3.5324652e-10,1.1383406e-08\n",
      "Iteration 20705: loss = 3.5325443e-10,1.1379614e-08\n",
      "Iteration 20710: loss = 3.5326178e-10,1.1375817e-08\n",
      "Iteration 20715: loss = 3.5326905e-10,1.1372016e-08\n",
      "Iteration 20720: loss = 3.532773e-10,1.1368213e-08\n",
      "Iteration 20725: loss = 3.5328482e-10,1.1364402e-08\n",
      "Iteration 20730: loss = 3.5329425e-10,1.1360589e-08\n",
      "Iteration 20735: loss = 3.5330347e-10,1.135677e-08\n",
      "Iteration 20740: loss = 3.533135e-10,1.1352944e-08\n",
      "Iteration 20745: loss = 3.5332307e-10,1.1349115e-08\n",
      "Iteration 20750: loss = 3.5333317e-10,1.1345281e-08\n",
      "Iteration 20755: loss = 3.5334283e-10,1.1341444e-08\n",
      "Iteration 20760: loss = 3.533528e-10,1.1337602e-08\n",
      "Iteration 20765: loss = 3.5336312e-10,1.1333757e-08\n",
      "Iteration 20770: loss = 3.533732e-10,1.1329905e-08\n",
      "Iteration 20775: loss = 3.533838e-10,1.1326055e-08\n",
      "Iteration 20780: loss = 3.533942e-10,1.1322196e-08\n",
      "Iteration 20785: loss = 3.5340467e-10,1.1318334e-08\n",
      "Iteration 20790: loss = 3.5341488e-10,1.1314468e-08\n",
      "Iteration 20795: loss = 3.534257e-10,1.1310597e-08\n",
      "Iteration 20800: loss = 3.5343709e-10,1.13067244e-08\n",
      "Iteration 20805: loss = 3.5143533e-10,1.1302845e-08\n",
      "Iteration 20810: loss = 3.5144843e-10,1.1298955e-08\n",
      "Iteration 20815: loss = 3.5146439e-10,1.1295059e-08\n",
      "Iteration 20820: loss = 3.5148015e-10,1.1291156e-08\n",
      "Iteration 20825: loss = 3.5149694e-10,1.1287248e-08\n",
      "Iteration 20830: loss = 3.5151412e-10,1.1283335e-08\n",
      "Iteration 20835: loss = 3.5153178e-10,1.12794165e-08\n",
      "Iteration 20840: loss = 3.5154904e-10,1.1275497e-08\n",
      "Iteration 20845: loss = 3.515669e-10,1.1271571e-08\n",
      "Iteration 20850: loss = 3.515842e-10,1.126764e-08\n",
      "Iteration 20855: loss = 3.5160239e-10,1.1263704e-08\n",
      "Iteration 20860: loss = 3.516206e-10,1.12597665e-08\n",
      "Iteration 20865: loss = 3.5163852e-10,1.1255826e-08\n",
      "Iteration 20870: loss = 3.5165668e-10,1.1251879e-08\n",
      "Iteration 20875: loss = 3.516737e-10,1.12479315e-08\n",
      "Iteration 20880: loss = 3.516908e-10,1.1243983e-08\n",
      "Iteration 20885: loss = 3.517079e-10,1.12400285e-08\n",
      "Iteration 20890: loss = 3.517244e-10,1.12360725e-08\n",
      "Iteration 20895: loss = 3.5174105e-10,1.1232109e-08\n",
      "Iteration 20900: loss = 3.5175704e-10,1.12281455e-08\n",
      "Iteration 20905: loss = 3.5177336e-10,1.1224178e-08\n",
      "Iteration 20910: loss = 3.5178996e-10,1.1220204e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20915: loss = 3.5180736e-10,1.121623e-08\n",
      "Iteration 20920: loss = 3.5182354e-10,1.1212248e-08\n",
      "Iteration 20925: loss = 3.5183978e-10,1.12082645e-08\n",
      "Iteration 20930: loss = 3.518568e-10,1.1204278e-08\n",
      "Iteration 20935: loss = 3.518734e-10,1.1200287e-08\n",
      "Iteration 20940: loss = 3.5390338e-10,1.1196291e-08\n",
      "Iteration 20945: loss = 3.539167e-10,1.1192299e-08\n",
      "Iteration 20950: loss = 3.539292e-10,1.1188308e-08\n",
      "Iteration 20955: loss = 3.5394176e-10,1.1184314e-08\n",
      "Iteration 20960: loss = 3.5395256e-10,1.1180315e-08\n",
      "Iteration 20965: loss = 3.5396389e-10,1.1176314e-08\n",
      "Iteration 20970: loss = 3.5397507e-10,1.1172313e-08\n",
      "Iteration 20975: loss = 3.539862e-10,1.11683045e-08\n",
      "Iteration 20980: loss = 3.5399716e-10,1.1164293e-08\n",
      "Iteration 20985: loss = 3.5199502e-10,1.1160279e-08\n",
      "Iteration 20990: loss = 3.520079e-10,1.1156255e-08\n",
      "Iteration 20995: loss = 3.5202288e-10,1.1152224e-08\n",
      "Iteration 21000: loss = 3.5203981e-10,1.1148183e-08\n",
      "Iteration 21005: loss = 3.520565e-10,1.114414e-08\n",
      "Iteration 21010: loss = 3.520743e-10,1.114009e-08\n",
      "Iteration 21015: loss = 3.520926e-10,1.1136037e-08\n",
      "Iteration 21020: loss = 3.52111e-10,1.1131977e-08\n",
      "Iteration 21025: loss = 3.5212896e-10,1.1127912e-08\n",
      "Iteration 21030: loss = 3.5214798e-10,1.1123848e-08\n",
      "Iteration 21035: loss = 3.5216652e-10,1.1119777e-08\n",
      "Iteration 21040: loss = 3.5218484e-10,1.1115705e-08\n",
      "Iteration 21045: loss = 3.522024e-10,1.1111632e-08\n",
      "Iteration 21050: loss = 3.522195e-10,1.1107553e-08\n",
      "Iteration 21055: loss = 3.522366e-10,1.1103474e-08\n",
      "Iteration 21060: loss = 3.5225356e-10,1.1099388e-08\n",
      "Iteration 21065: loss = 3.522708e-10,1.1095299e-08\n",
      "Iteration 21070: loss = 3.5228756e-10,1.1091212e-08\n",
      "Iteration 21075: loss = 3.5230405e-10,1.1087115e-08\n",
      "Iteration 21080: loss = 3.5232126e-10,1.1083015e-08\n",
      "Iteration 21085: loss = 3.5233882e-10,1.1078915e-08\n",
      "Iteration 21090: loss = 3.5235603e-10,1.1074806e-08\n",
      "Iteration 21095: loss = 3.523746e-10,1.1070694e-08\n",
      "Iteration 21100: loss = 3.52394e-10,1.1066574e-08\n",
      "Iteration 21105: loss = 3.5241324e-10,1.1062452e-08\n",
      "Iteration 21110: loss = 3.5243297e-10,1.1058323e-08\n",
      "Iteration 21115: loss = 3.5245273e-10,1.1054192e-08\n",
      "Iteration 21120: loss = 3.524728e-10,1.1050058e-08\n",
      "Iteration 21125: loss = 3.5249295e-10,1.1045918e-08\n",
      "Iteration 21130: loss = 3.5251368e-10,1.10417755e-08\n",
      "Iteration 21135: loss = 3.525342e-10,1.1037628e-08\n",
      "Iteration 21140: loss = 3.5255474e-10,1.1033478e-08\n",
      "Iteration 21145: loss = 3.5257564e-10,1.1029325e-08\n",
      "Iteration 21150: loss = 3.525962e-10,1.1025167e-08\n",
      "Iteration 21155: loss = 3.5261694e-10,1.1021005e-08\n",
      "Iteration 21160: loss = 3.526377e-10,1.1016841e-08\n",
      "Iteration 21165: loss = 3.5265899e-10,1.1012673e-08\n",
      "Iteration 21170: loss = 3.5267986e-10,1.1008499e-08\n",
      "Iteration 21175: loss = 3.527015e-10,1.1004325e-08\n",
      "Iteration 21180: loss = 3.5272243e-10,1.1000142e-08\n",
      "Iteration 21185: loss = 3.5274317e-10,1.099596e-08\n",
      "Iteration 21190: loss = 3.5276468e-10,1.0991773e-08\n",
      "Iteration 21195: loss = 3.5078174e-10,1.0987584e-08\n",
      "Iteration 21200: loss = 3.5080472e-10,1.0983386e-08\n",
      "Iteration 21205: loss = 3.5082925e-10,1.0979182e-08\n",
      "Iteration 21210: loss = 3.5285808e-10,1.0974971e-08\n",
      "Iteration 21215: loss = 3.528817e-10,1.0970759e-08\n",
      "Iteration 21220: loss = 3.5290326e-10,1.0966554e-08\n",
      "Iteration 21225: loss = 3.5292436e-10,1.0962341e-08\n",
      "Iteration 21230: loss = 3.5294412e-10,1.095813e-08\n",
      "Iteration 21235: loss = 3.5296388e-10,1.0953916e-08\n",
      "Iteration 21240: loss = 3.5298356e-10,1.09497e-08\n",
      "Iteration 21245: loss = 3.530032e-10,1.0945479e-08\n",
      "Iteration 21250: loss = 3.5302342e-10,1.0941252e-08\n",
      "Iteration 21255: loss = 3.5304323e-10,1.0937026e-08\n",
      "Iteration 21260: loss = 3.5306244e-10,1.0932796e-08\n",
      "Iteration 21265: loss = 3.5308256e-10,1.092856e-08\n",
      "Iteration 21270: loss = 3.5310296e-10,1.0924323e-08\n",
      "Iteration 21275: loss = 3.5312234e-10,1.0920081e-08\n",
      "Iteration 21280: loss = 3.5314285e-10,1.0915835e-08\n",
      "Iteration 21285: loss = 3.5316292e-10,1.0911586e-08\n",
      "Iteration 21290: loss = 3.5318318e-10,1.0907334e-08\n",
      "Iteration 21295: loss = 3.5320336e-10,1.0903078e-08\n",
      "Iteration 21300: loss = 3.5322398e-10,1.0898818e-08\n",
      "Iteration 21305: loss = 3.5324496e-10,1.08945555e-08\n",
      "Iteration 21310: loss = 3.532655e-10,1.08902904e-08\n",
      "Iteration 21315: loss = 3.5328654e-10,1.0886018e-08\n",
      "Iteration 21320: loss = 3.5330747e-10,1.0881746e-08\n",
      "Iteration 21325: loss = 3.5332803e-10,1.0877468e-08\n",
      "Iteration 21330: loss = 3.5334877e-10,1.0873188e-08\n",
      "Iteration 21335: loss = 3.5337025e-10,1.0868904e-08\n",
      "Iteration 21340: loss = 3.53391e-10,1.0864617e-08\n",
      "Iteration 21345: loss = 3.5341075e-10,1.086033e-08\n",
      "Iteration 21350: loss = 3.534308e-10,1.0856041e-08\n",
      "Iteration 21355: loss = 3.534493e-10,1.0851746e-08\n",
      "Iteration 21360: loss = 3.5346814e-10,1.0847454e-08\n",
      "Iteration 21365: loss = 3.5348702e-10,1.0843153e-08\n",
      "Iteration 21370: loss = 3.535059e-10,1.0838852e-08\n",
      "Iteration 21375: loss = 3.5352485e-10,1.0834547e-08\n",
      "Iteration 21380: loss = 3.5354386e-10,1.0830241e-08\n",
      "Iteration 21385: loss = 3.5356182e-10,1.0825932e-08\n",
      "Iteration 21390: loss = 3.535822e-10,1.0821617e-08\n",
      "Iteration 21395: loss = 3.5360104e-10,1.0817297e-08\n",
      "Iteration 21400: loss = 3.5361994e-10,1.0812977e-08\n",
      "Iteration 21405: loss = 3.5363915e-10,1.0808653e-08\n",
      "Iteration 21410: loss = 3.5365896e-10,1.0804324e-08\n",
      "Iteration 21415: loss = 3.5367773e-10,1.0799994e-08\n",
      "Iteration 21420: loss = 3.5369754e-10,1.07956595e-08\n",
      "Iteration 21425: loss = 3.5371772e-10,1.07913225e-08\n",
      "Iteration 21430: loss = 3.5373685e-10,1.0786981e-08\n",
      "Iteration 21435: loss = 3.5175393e-10,1.0782636e-08\n",
      "Iteration 21440: loss = 3.517766e-10,1.0778279e-08\n",
      "Iteration 21445: loss = 3.5180203e-10,1.0773914e-08\n",
      "Iteration 21450: loss = 3.5182904e-10,1.0769539e-08\n",
      "Iteration 21455: loss = 3.5185668e-10,1.0765166e-08\n",
      "Iteration 21460: loss = 3.518846e-10,1.0760785e-08\n",
      "Iteration 21465: loss = 3.5191358e-10,1.0756399e-08\n",
      "Iteration 21470: loss = 3.5194214e-10,1.0752011e-08\n",
      "Iteration 21475: loss = 3.5397338e-10,1.0747624e-08\n",
      "Iteration 21480: loss = 3.5399803e-10,1.0743237e-08\n",
      "Iteration 21485: loss = 3.5402162e-10,1.0738853e-08\n",
      "Iteration 21490: loss = 3.540432e-10,1.0734472e-08\n",
      "Iteration 21495: loss = 3.5406367e-10,1.073009e-08\n",
      "Iteration 21500: loss = 3.5408218e-10,1.07257065e-08\n",
      "Iteration 21505: loss = 3.5410141e-10,1.07213225e-08\n",
      "Iteration 21510: loss = 3.5411965e-10,1.0716936e-08\n",
      "Iteration 21515: loss = 3.5413714e-10,1.0712548e-08\n",
      "Iteration 21520: loss = 3.5415437e-10,1.0708157e-08\n",
      "Iteration 21525: loss = 3.5417333e-10,1.0703761e-08\n",
      "Iteration 21530: loss = 3.5419034e-10,1.0699365e-08\n",
      "Iteration 21535: loss = 3.5420852e-10,1.0694965e-08\n",
      "Iteration 21540: loss = 3.5422673e-10,1.069056e-08\n",
      "Iteration 21545: loss = 3.5424477e-10,1.0686153e-08\n",
      "Iteration 21550: loss = 3.542628e-10,1.0681742e-08\n",
      "Iteration 21555: loss = 3.542806e-10,1.0677329e-08\n",
      "Iteration 21560: loss = 3.5429873e-10,1.0672914e-08\n",
      "Iteration 21565: loss = 3.5431744e-10,1.0668494e-08\n",
      "Iteration 21570: loss = 3.5433564e-10,1.0664074e-08\n",
      "Iteration 21575: loss = 3.5435413e-10,1.0659647e-08\n",
      "Iteration 21580: loss = 3.5437273e-10,1.065522e-08\n",
      "Iteration 21585: loss = 3.543916e-10,1.06507905e-08\n",
      "Iteration 21590: loss = 3.544102e-10,1.0646354e-08\n",
      "Iteration 21595: loss = 3.544294e-10,1.0641916e-08\n",
      "Iteration 21600: loss = 3.5444758e-10,1.0637476e-08\n",
      "Iteration 21605: loss = 3.5446737e-10,1.06330305e-08\n",
      "Iteration 21610: loss = 3.5448755e-10,1.062858e-08\n",
      "Iteration 21615: loss = 3.545091e-10,1.0624125e-08\n",
      "Iteration 21620: loss = 3.545304e-10,1.0619666e-08\n",
      "Iteration 21625: loss = 3.5455305e-10,1.0615204e-08\n",
      "Iteration 21630: loss = 3.54575e-10,1.0610737e-08\n",
      "Iteration 21635: loss = 3.5459746e-10,1.0606267e-08\n",
      "Iteration 21640: loss = 3.5462008e-10,1.0601795e-08\n",
      "Iteration 21645: loss = 3.5464287e-10,1.059732e-08\n",
      "Iteration 21650: loss = 3.5466552e-10,1.0592843e-08\n",
      "Iteration 21655: loss = 3.5468795e-10,1.0588362e-08\n",
      "Iteration 21660: loss = 3.547109e-10,1.0583878e-08\n",
      "Iteration 21665: loss = 3.5473288e-10,1.0579395e-08\n",
      "Iteration 21670: loss = 3.5475298e-10,1.0574912e-08\n",
      "Iteration 21675: loss = 3.5477318e-10,1.05704245e-08\n",
      "Iteration 21680: loss = 3.547936e-10,1.05659375e-08\n",
      "Iteration 21685: loss = 3.548136e-10,1.0561449e-08\n",
      "Iteration 21690: loss = 3.5483305e-10,1.05569535e-08\n",
      "Iteration 21695: loss = 3.5485312e-10,1.0552458e-08\n",
      "Iteration 21700: loss = 3.548729e-10,1.0547962e-08\n",
      "Iteration 21705: loss = 3.5489334e-10,1.05434586e-08\n",
      "Iteration 21710: loss = 3.5491268e-10,1.0538956e-08\n",
      "Iteration 21715: loss = 3.5493297e-10,1.0534449e-08\n",
      "Iteration 21720: loss = 3.5495343e-10,1.0529939e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21725: loss = 3.5497347e-10,1.0525429e-08\n",
      "Iteration 21730: loss = 3.5299041e-10,1.0520912e-08\n",
      "Iteration 21735: loss = 3.5301342e-10,1.0516383e-08\n",
      "Iteration 21740: loss = 3.5304004e-10,1.0511849e-08\n",
      "Iteration 21745: loss = 3.5306805e-10,1.0507309e-08\n",
      "Iteration 21750: loss = 3.530972e-10,1.0502763e-08\n",
      "Iteration 21755: loss = 3.5312656e-10,1.0498211e-08\n",
      "Iteration 21760: loss = 3.5515946e-10,1.0493661e-08\n",
      "Iteration 21765: loss = 3.5518496e-10,1.0489112e-08\n",
      "Iteration 21770: loss = 3.5520886e-10,1.0484569e-08\n",
      "Iteration 21775: loss = 3.5523115e-10,1.0480025e-08\n",
      "Iteration 21780: loss = 3.552522e-10,1.047548e-08\n",
      "Iteration 21785: loss = 3.5527384e-10,1.0470938e-08\n",
      "Iteration 21790: loss = 3.5529413e-10,1.0466389e-08\n",
      "Iteration 21795: loss = 3.553148e-10,1.046184e-08\n",
      "Iteration 21800: loss = 3.5533623e-10,1.0457288e-08\n",
      "Iteration 21805: loss = 3.5535644e-10,1.0452732e-08\n",
      "Iteration 21810: loss = 3.5537717e-10,1.0448176e-08\n",
      "Iteration 21815: loss = 3.5539824e-10,1.0443616e-08\n",
      "Iteration 21820: loss = 3.554188e-10,1.0439056e-08\n",
      "Iteration 21825: loss = 3.554386e-10,1.0434493e-08\n",
      "Iteration 21830: loss = 3.5545708e-10,1.0429932e-08\n",
      "Iteration 21835: loss = 3.5547557e-10,1.0425369e-08\n",
      "Iteration 21840: loss = 3.5549363e-10,1.04208056e-08\n",
      "Iteration 21845: loss = 3.5551093e-10,1.041624e-08\n",
      "Iteration 21850: loss = 3.5552872e-10,1.0411673e-08\n",
      "Iteration 21855: loss = 3.5554634e-10,1.0407103e-08\n",
      "Iteration 21860: loss = 3.5556344e-10,1.0402528e-08\n",
      "Iteration 21865: loss = 3.5558087e-10,1.0397953e-08\n",
      "Iteration 21870: loss = 3.555983e-10,1.0393375e-08\n",
      "Iteration 21875: loss = 3.5561662e-10,1.0388793e-08\n",
      "Iteration 21880: loss = 3.5563386e-10,1.03842135e-08\n",
      "Iteration 21885: loss = 3.5565142e-10,1.0379629e-08\n",
      "Iteration 21890: loss = 3.5566874e-10,1.037504e-08\n",
      "Iteration 21895: loss = 3.5568645e-10,1.0370449e-08\n",
      "Iteration 21900: loss = 3.5570358e-10,1.0365859e-08\n",
      "Iteration 21905: loss = 3.557216e-10,1.0361264e-08\n",
      "Iteration 21910: loss = 3.5574013e-10,1.0356668e-08\n",
      "Iteration 21915: loss = 3.5575742e-10,1.0352068e-08\n",
      "Iteration 21920: loss = 3.5577527e-10,1.0347465e-08\n",
      "Iteration 21925: loss = 3.5579287e-10,1.0342863e-08\n",
      "Iteration 21930: loss = 3.5581135e-10,1.0338255e-08\n",
      "Iteration 21935: loss = 3.5582906e-10,1.0333649e-08\n",
      "Iteration 21940: loss = 3.558471e-10,1.0329037e-08\n",
      "Iteration 21945: loss = 3.5586512e-10,1.0324424e-08\n",
      "Iteration 21950: loss = 3.5588296e-10,1.0319809e-08\n",
      "Iteration 21955: loss = 3.5590134e-10,1.031519e-08\n",
      "Iteration 21960: loss = 3.5591954e-10,1.031057e-08\n",
      "Iteration 21965: loss = 3.5593717e-10,1.030595e-08\n",
      "Iteration 21970: loss = 3.559554e-10,1.0301326e-08\n",
      "Iteration 21975: loss = 3.5597347e-10,1.02966995e-08\n",
      "Iteration 21980: loss = 3.559915e-10,1.0292069e-08\n",
      "Iteration 21985: loss = 3.560099e-10,1.028744e-08\n",
      "Iteration 21990: loss = 3.5602774e-10,1.0282806e-08\n",
      "Iteration 21995: loss = 3.5604675e-10,1.027817e-08\n",
      "Iteration 22000: loss = 3.5606496e-10,1.0273531e-08\n",
      "Iteration 22005: loss = 3.5608264e-10,1.0268893e-08\n",
      "Iteration 22010: loss = 3.5610126e-10,1.02642534e-08\n",
      "Iteration 22015: loss = 3.5611983e-10,1.0259609e-08\n",
      "Iteration 22020: loss = 3.5613765e-10,1.0254962e-08\n",
      "Iteration 22025: loss = 3.561539e-10,1.025032e-08\n",
      "Iteration 22030: loss = 3.5616884e-10,1.0245676e-08\n",
      "Iteration 22035: loss = 3.5618383e-10,1.0241033e-08\n",
      "Iteration 22040: loss = 3.561991e-10,1.02363895e-08\n",
      "Iteration 22045: loss = 3.562133e-10,1.0231741e-08\n",
      "Iteration 22050: loss = 3.5622785e-10,1.0227094e-08\n",
      "Iteration 22055: loss = 3.562424e-10,1.0222442e-08\n",
      "Iteration 22060: loss = 3.5625702e-10,1.0217791e-08\n",
      "Iteration 22065: loss = 3.5627168e-10,1.0213135e-08\n",
      "Iteration 22070: loss = 3.562858e-10,1.0208479e-08\n",
      "Iteration 22075: loss = 3.5630057e-10,1.0203824e-08\n",
      "Iteration 22080: loss = 3.563145e-10,1.0199161e-08\n",
      "Iteration 22085: loss = 3.563293e-10,1.01945e-08\n",
      "Iteration 22090: loss = 3.5634337e-10,1.0189835e-08\n",
      "Iteration 22095: loss = 3.5635825e-10,1.0185169e-08\n",
      "Iteration 22100: loss = 3.5637213e-10,1.0180503e-08\n",
      "Iteration 22105: loss = 3.5839973e-10,1.0175833e-08\n",
      "Iteration 22110: loss = 3.5841033e-10,1.0171171e-08\n",
      "Iteration 22115: loss = 3.5841752e-10,1.0166515e-08\n",
      "Iteration 22120: loss = 3.5842254e-10,1.01618625e-08\n",
      "Iteration 22125: loss = 3.5842598e-10,1.0157213e-08\n",
      "Iteration 22130: loss = 3.5843004e-10,1.015256e-08\n",
      "Iteration 22135: loss = 3.5843303e-10,1.0147907e-08\n",
      "Iteration 22140: loss = 3.584355e-10,1.0143253e-08\n",
      "Iteration 22145: loss = 3.5843797e-10,1.01386e-08\n",
      "Iteration 22150: loss = 3.5642897e-10,1.0133936e-08\n",
      "Iteration 22155: loss = 3.564372e-10,1.0129264e-08\n",
      "Iteration 22160: loss = 3.5644876e-10,1.0124584e-08\n",
      "Iteration 22165: loss = 3.564612e-10,1.0119894e-08\n",
      "Iteration 22170: loss = 3.5647474e-10,1.0115204e-08\n",
      "Iteration 22175: loss = 3.5648914e-10,1.011051e-08\n",
      "Iteration 22180: loss = 3.565038e-10,1.0105811e-08\n",
      "Iteration 22185: loss = 3.5651915e-10,1.0101111e-08\n",
      "Iteration 22190: loss = 3.5653366e-10,1.009641e-08\n",
      "Iteration 22195: loss = 3.565487e-10,1.0091707e-08\n",
      "Iteration 22200: loss = 3.565638e-10,1.00870015e-08\n",
      "Iteration 22205: loss = 3.56579e-10,1.0082293e-08\n",
      "Iteration 22210: loss = 3.5659478e-10,1.0077586e-08\n",
      "Iteration 22215: loss = 3.5660888e-10,1.0072874e-08\n",
      "Iteration 22220: loss = 3.5662448e-10,1.0068161e-08\n",
      "Iteration 22225: loss = 3.5663975e-10,1.0063449e-08\n",
      "Iteration 22230: loss = 3.5665515e-10,1.0058734e-08\n",
      "Iteration 22235: loss = 3.5667025e-10,1.0054015e-08\n",
      "Iteration 22240: loss = 3.566846e-10,1.0049298e-08\n",
      "Iteration 22245: loss = 3.567001e-10,1.00445785e-08\n",
      "Iteration 22250: loss = 3.5671502e-10,1.0039857e-08\n",
      "Iteration 22255: loss = 3.5672953e-10,1.0035134e-08\n",
      "Iteration 22260: loss = 3.5674508e-10,1.00304085e-08\n",
      "Iteration 22265: loss = 3.5675993e-10,1.00256825e-08\n",
      "Iteration 22270: loss = 3.5677475e-10,1.0020956e-08\n",
      "Iteration 22275: loss = 3.5678963e-10,1.0016225e-08\n",
      "Iteration 22280: loss = 3.5680459e-10,1.0011496e-08\n",
      "Iteration 22285: loss = 3.5681932e-10,1.0006765e-08\n",
      "Iteration 22290: loss = 3.568341e-10,1.0002031e-08\n",
      "Iteration 22295: loss = 3.5684664e-10,9.9973e-09\n",
      "Iteration 22300: loss = 3.5685846e-10,9.992572e-09\n",
      "Iteration 22305: loss = 3.5686923e-10,9.987843e-09\n",
      "Iteration 22310: loss = 3.5688028e-10,9.9831166e-09\n",
      "Iteration 22315: loss = 3.5689005e-10,9.978386e-09\n",
      "Iteration 22320: loss = 3.5690093e-10,9.973655e-09\n",
      "Iteration 22325: loss = 3.5691017e-10,9.968923e-09\n",
      "Iteration 22330: loss = 3.569203e-10,9.9641895e-09\n",
      "Iteration 22335: loss = 3.5693018e-10,9.9594555e-09\n",
      "Iteration 22340: loss = 3.569399e-10,9.954721e-09\n",
      "Iteration 22345: loss = 3.5694958e-10,9.949983e-09\n",
      "Iteration 22350: loss = 3.569588e-10,9.945245e-09\n",
      "Iteration 22355: loss = 3.5696882e-10,9.940505e-09\n",
      "Iteration 22360: loss = 3.5697892e-10,9.935764e-09\n",
      "Iteration 22365: loss = 3.5698822e-10,9.931023e-09\n",
      "Iteration 22370: loss = 3.5699807e-10,9.92628e-09\n",
      "Iteration 22375: loss = 3.5700798e-10,9.921536e-09\n",
      "Iteration 22380: loss = 3.5701742e-10,9.916787e-09\n",
      "Iteration 22385: loss = 3.5702688e-10,9.912043e-09\n",
      "Iteration 22390: loss = 3.5703585e-10,9.907294e-09\n",
      "Iteration 22395: loss = 3.5704587e-10,9.902545e-09\n",
      "Iteration 22400: loss = 3.570554e-10,9.897793e-09\n",
      "Iteration 22405: loss = 3.5706482e-10,9.893041e-09\n",
      "Iteration 22410: loss = 3.570739e-10,9.888289e-09\n",
      "Iteration 22415: loss = 3.5708358e-10,9.883535e-09\n",
      "Iteration 22420: loss = 3.570925e-10,9.87878e-09\n",
      "Iteration 22425: loss = 3.5710193e-10,9.874023e-09\n",
      "Iteration 22430: loss = 3.5711098e-10,9.869266e-09\n",
      "Iteration 22435: loss = 3.5712022e-10,9.864506e-09\n",
      "Iteration 22440: loss = 3.5712955e-10,9.859747e-09\n",
      "Iteration 22445: loss = 3.571378e-10,9.854986e-09\n",
      "Iteration 22450: loss = 3.5714776e-10,9.850225e-09\n",
      "Iteration 22455: loss = 3.571569e-10,9.84546e-09\n",
      "Iteration 22460: loss = 3.5716533e-10,9.840697e-09\n",
      "Iteration 22465: loss = 3.571743e-10,9.835932e-09\n",
      "Iteration 22470: loss = 3.5718326e-10,9.831165e-09\n",
      "Iteration 22475: loss = 3.571918e-10,9.826399e-09\n",
      "Iteration 22480: loss = 3.5720113e-10,9.8216315e-09\n",
      "Iteration 22485: loss = 3.5720918e-10,9.816862e-09\n",
      "Iteration 22490: loss = 3.5721834e-10,9.812092e-09\n",
      "Iteration 22495: loss = 3.572267e-10,9.807321e-09\n",
      "Iteration 22500: loss = 3.5723538e-10,9.802549e-09\n",
      "Iteration 22505: loss = 3.5724387e-10,9.797776e-09\n",
      "Iteration 22510: loss = 3.5725264e-10,9.793003e-09\n",
      "Iteration 22515: loss = 3.572609e-10,9.788229e-09\n",
      "Iteration 22520: loss = 3.5726885e-10,9.783453e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22525: loss = 3.5727696e-10,9.7786765e-09\n",
      "Iteration 22530: loss = 3.5728528e-10,9.773899e-09\n",
      "Iteration 22535: loss = 3.5729372e-10,9.769122e-09\n",
      "Iteration 22540: loss = 3.5730138e-10,9.764343e-09\n",
      "Iteration 22545: loss = 3.57309e-10,9.759562e-09\n",
      "Iteration 22550: loss = 3.5731704e-10,9.754784e-09\n",
      "Iteration 22555: loss = 3.5732473e-10,9.750002e-09\n",
      "Iteration 22560: loss = 3.573325e-10,9.7452215e-09\n",
      "Iteration 22565: loss = 3.5734035e-10,9.740439e-09\n",
      "Iteration 22570: loss = 3.5734768e-10,9.735655e-09\n",
      "Iteration 22575: loss = 3.5735523e-10,9.730869e-09\n",
      "Iteration 22580: loss = 3.573629e-10,9.726087e-09\n",
      "Iteration 22585: loss = 3.5737036e-10,9.7213e-09\n",
      "Iteration 22590: loss = 3.5737727e-10,9.716515e-09\n",
      "Iteration 22595: loss = 3.573848e-10,9.711727e-09\n",
      "Iteration 22600: loss = 3.573923e-10,9.706938e-09\n",
      "Iteration 22605: loss = 3.5739903e-10,9.702149e-09\n",
      "Iteration 22610: loss = 3.5740588e-10,9.697362e-09\n",
      "Iteration 22615: loss = 3.5741296e-10,9.692572e-09\n",
      "Iteration 22620: loss = 3.5742043e-10,9.687781e-09\n",
      "Iteration 22625: loss = 3.5742673e-10,9.682991e-09\n",
      "Iteration 22630: loss = 3.5743344e-10,9.678199e-09\n",
      "Iteration 22635: loss = 3.5743986e-10,9.673408e-09\n",
      "Iteration 22640: loss = 3.5744604e-10,9.668617e-09\n",
      "Iteration 22645: loss = 3.5745262e-10,9.663823e-09\n",
      "Iteration 22650: loss = 3.5745942e-10,9.659029e-09\n",
      "Iteration 22655: loss = 3.574652e-10,9.654235e-09\n",
      "Iteration 22660: loss = 3.5747125e-10,9.64944e-09\n",
      "Iteration 22665: loss = 3.5747771e-10,9.644646e-09\n",
      "Iteration 22670: loss = 3.574836e-10,9.63985e-09\n",
      "Iteration 22675: loss = 3.5748896e-10,9.635056e-09\n",
      "Iteration 22680: loss = 3.5749462e-10,9.630259e-09\n",
      "Iteration 22685: loss = 3.575009e-10,9.625463e-09\n",
      "Iteration 22690: loss = 3.5750614e-10,9.620665e-09\n",
      "Iteration 22695: loss = 3.575119e-10,9.615867e-09\n",
      "Iteration 22700: loss = 3.57517e-10,9.61107e-09\n",
      "Iteration 22705: loss = 3.5752226e-10,9.60627e-09\n",
      "Iteration 22710: loss = 3.5752792e-10,9.601472e-09\n",
      "Iteration 22715: loss = 3.575328e-10,9.596674e-09\n",
      "Iteration 22720: loss = 3.5753775e-10,9.591874e-09\n",
      "Iteration 22725: loss = 3.575424e-10,9.587074e-09\n",
      "Iteration 22730: loss = 3.5754713e-10,9.582274e-09\n",
      "Iteration 22735: loss = 3.5755213e-10,9.577473e-09\n",
      "Iteration 22740: loss = 3.5755635e-10,9.572672e-09\n",
      "Iteration 22745: loss = 3.5756123e-10,9.567872e-09\n",
      "Iteration 22750: loss = 3.5756523e-10,9.563071e-09\n",
      "Iteration 22755: loss = 3.575699e-10,9.558268e-09\n",
      "Iteration 22760: loss = 3.5757405e-10,9.553467e-09\n",
      "Iteration 22765: loss = 3.5757816e-10,9.548664e-09\n",
      "Iteration 22770: loss = 3.5758232e-10,9.543863e-09\n",
      "Iteration 22775: loss = 3.5758582e-10,9.539061e-09\n",
      "Iteration 22780: loss = 3.5758926e-10,9.534258e-09\n",
      "Iteration 22785: loss = 3.5759298e-10,9.529456e-09\n",
      "Iteration 22790: loss = 3.575966e-10,9.524653e-09\n",
      "Iteration 22795: loss = 3.5759765e-10,9.519856e-09\n",
      "Iteration 22800: loss = 3.5759742e-10,9.515059e-09\n",
      "Iteration 22805: loss = 3.575956e-10,9.510268e-09\n",
      "Iteration 22810: loss = 3.575957e-10,9.505473e-09\n",
      "Iteration 22815: loss = 3.5759615e-10,9.500676e-09\n",
      "Iteration 22820: loss = 3.5759576e-10,9.49588e-09\n",
      "Iteration 22825: loss = 3.5759598e-10,9.491082e-09\n",
      "Iteration 22830: loss = 3.5759684e-10,9.486284e-09\n",
      "Iteration 22835: loss = 3.5759687e-10,9.481483e-09\n",
      "Iteration 22840: loss = 3.575972e-10,9.476685e-09\n",
      "Iteration 22845: loss = 3.5759737e-10,9.4718855e-09\n",
      "Iteration 22850: loss = 3.5759776e-10,9.467085e-09\n",
      "Iteration 22855: loss = 3.575982e-10,9.462284e-09\n",
      "Iteration 22860: loss = 3.5759848e-10,9.457485e-09\n",
      "Iteration 22865: loss = 3.575996e-10,9.452684e-09\n",
      "Iteration 22870: loss = 3.5760014e-10,9.4478825e-09\n",
      "Iteration 22875: loss = 3.576005e-10,9.443079e-09\n",
      "Iteration 22880: loss = 3.576003e-10,9.4382795e-09\n",
      "Iteration 22885: loss = 3.576008e-10,9.433478e-09\n",
      "Iteration 22890: loss = 3.5760261e-10,9.428675e-09\n",
      "Iteration 22895: loss = 3.5760314e-10,9.4238715e-09\n",
      "Iteration 22900: loss = 3.5760372e-10,9.419068e-09\n",
      "Iteration 22905: loss = 3.5760486e-10,9.414263e-09\n",
      "Iteration 22910: loss = 3.5760628e-10,9.40946e-09\n",
      "Iteration 22915: loss = 3.576075e-10,9.404655e-09\n",
      "Iteration 22920: loss = 3.5760686e-10,9.3998525e-09\n",
      "Iteration 22925: loss = 3.5760792e-10,9.395048e-09\n",
      "Iteration 22930: loss = 3.5760783e-10,9.390244e-09\n",
      "Iteration 22935: loss = 3.576084e-10,9.3854435e-09\n",
      "Iteration 22940: loss = 3.5760875e-10,9.380639e-09\n",
      "Iteration 22945: loss = 3.5760858e-10,9.375836e-09\n",
      "Iteration 22950: loss = 3.5760864e-10,9.371034e-09\n",
      "Iteration 22955: loss = 3.5760803e-10,9.3662305e-09\n",
      "Iteration 22960: loss = 3.5760794e-10,9.361429e-09\n",
      "Iteration 22965: loss = 3.5760764e-10,9.3566275e-09\n",
      "Iteration 22970: loss = 3.5760686e-10,9.351826e-09\n",
      "Iteration 22975: loss = 3.5760583e-10,9.3470245e-09\n",
      "Iteration 22980: loss = 3.5760564e-10,9.342221e-09\n",
      "Iteration 22985: loss = 3.5760386e-10,9.3374215e-09\n",
      "Iteration 22990: loss = 3.576027e-10,9.332622e-09\n",
      "Iteration 22995: loss = 3.576018e-10,9.327821e-09\n",
      "Iteration 23000: loss = 3.5760014e-10,9.323022e-09\n",
      "Iteration 23005: loss = 3.575985e-10,9.3182235e-09\n",
      "Iteration 23010: loss = 3.5759695e-10,9.313423e-09\n",
      "Iteration 23015: loss = 3.5759484e-10,9.308625e-09\n",
      "Iteration 23020: loss = 3.5759276e-10,9.303828e-09\n",
      "Iteration 23025: loss = 3.575913e-10,9.299029e-09\n",
      "Iteration 23030: loss = 3.5758854e-10,9.294232e-09\n",
      "Iteration 23035: loss = 3.5758604e-10,9.289436e-09\n",
      "Iteration 23040: loss = 3.5758332e-10,9.284639e-09\n",
      "Iteration 23045: loss = 3.575804e-10,9.2798444e-09\n",
      "Iteration 23050: loss = 3.575775e-10,9.2750465e-09\n",
      "Iteration 23055: loss = 3.5757408e-10,9.270254e-09\n",
      "Iteration 23060: loss = 3.57571e-10,9.265458e-09\n",
      "Iteration 23065: loss = 3.575674e-10,9.260665e-09\n",
      "Iteration 23070: loss = 3.5756434e-10,9.255872e-09\n",
      "Iteration 23075: loss = 3.5756084e-10,9.251078e-09\n",
      "Iteration 23080: loss = 3.575571e-10,9.246287e-09\n",
      "Iteration 23085: loss = 3.5755307e-10,9.241495e-09\n",
      "Iteration 23090: loss = 3.5754946e-10,9.236704e-09\n",
      "Iteration 23095: loss = 3.57545e-10,9.231913e-09\n",
      "Iteration 23100: loss = 3.5754033e-10,9.227122e-09\n",
      "Iteration 23105: loss = 3.5753567e-10,9.222333e-09\n",
      "Iteration 23110: loss = 3.575313e-10,9.217544e-09\n",
      "Iteration 23115: loss = 3.5752623e-10,9.212756e-09\n",
      "Iteration 23120: loss = 3.5752143e-10,9.2079695e-09\n",
      "Iteration 23125: loss = 3.575161e-10,9.203182e-09\n",
      "Iteration 23130: loss = 3.5751058e-10,9.198398e-09\n",
      "Iteration 23135: loss = 3.5750558e-10,9.193611e-09\n",
      "Iteration 23140: loss = 3.5750017e-10,9.188826e-09\n",
      "Iteration 23145: loss = 3.5749448e-10,9.184042e-09\n",
      "Iteration 23150: loss = 3.5748857e-10,9.179258e-09\n",
      "Iteration 23155: loss = 3.5748215e-10,9.174475e-09\n",
      "Iteration 23160: loss = 3.5747627e-10,9.169694e-09\n",
      "Iteration 23165: loss = 3.5746991e-10,9.164912e-09\n",
      "Iteration 23170: loss = 3.5746386e-10,9.160131e-09\n",
      "Iteration 23175: loss = 3.5745684e-10,9.155353e-09\n",
      "Iteration 23180: loss = 3.5745032e-10,9.150574e-09\n",
      "Iteration 23185: loss = 3.574433e-10,9.145796e-09\n",
      "Iteration 23190: loss = 3.5743608e-10,9.141019e-09\n",
      "Iteration 23195: loss = 3.5742886e-10,9.136241e-09\n",
      "Iteration 23200: loss = 3.574212e-10,9.131467e-09\n",
      "Iteration 23205: loss = 3.574138e-10,9.126692e-09\n",
      "Iteration 23210: loss = 3.5740644e-10,9.1219166e-09\n",
      "Iteration 23215: loss = 3.5739833e-10,9.117144e-09\n",
      "Iteration 23220: loss = 3.5739053e-10,9.112373e-09\n",
      "Iteration 23225: loss = 3.5738223e-10,9.107601e-09\n",
      "Iteration 23230: loss = 3.5737424e-10,9.102832e-09\n",
      "Iteration 23235: loss = 3.5736544e-10,9.098061e-09\n",
      "Iteration 23240: loss = 3.5735648e-10,9.093293e-09\n",
      "Iteration 23245: loss = 3.57348e-10,9.088525e-09\n",
      "Iteration 23250: loss = 3.5733858e-10,9.083758e-09\n",
      "Iteration 23255: loss = 3.573305e-10,9.078992e-09\n",
      "Iteration 23260: loss = 3.573211e-10,9.074228e-09\n",
      "Iteration 23265: loss = 3.5731126e-10,9.069465e-09\n",
      "Iteration 23270: loss = 3.5730188e-10,9.0647e-09\n",
      "Iteration 23275: loss = 3.5729184e-10,9.0599395e-09\n",
      "Iteration 23280: loss = 3.5728212e-10,9.055178e-09\n",
      "Iteration 23285: loss = 3.5727268e-10,9.05042e-09\n",
      "Iteration 23290: loss = 3.5726186e-10,9.04566e-09\n",
      "Iteration 23295: loss = 3.5725115e-10,9.040903e-09\n",
      "Iteration 23300: loss = 3.572407e-10,9.036148e-09\n",
      "Iteration 23305: loss = 3.572306e-10,9.031391e-09\n",
      "Iteration 23310: loss = 3.5721923e-10,9.026637e-09\n",
      "Iteration 23315: loss = 3.5720837e-10,9.021885e-09\n",
      "Iteration 23320: loss = 3.5719705e-10,9.01713e-09\n",
      "Iteration 23325: loss = 3.5718603e-10,9.01238e-09\n",
      "Iteration 23330: loss = 3.571745e-10,9.00763e-09\n",
      "Iteration 23335: loss = 3.57163e-10,9.002881e-09\n",
      "Iteration 23340: loss = 3.571521e-10,8.998132e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23345: loss = 3.5713912e-10,8.9933865e-09\n",
      "Iteration 23350: loss = 3.5712722e-10,8.988641e-09\n",
      "Iteration 23355: loss = 3.5711564e-10,8.983898e-09\n",
      "Iteration 23360: loss = 3.57103e-10,8.979153e-09\n",
      "Iteration 23365: loss = 3.5709014e-10,8.974412e-09\n",
      "Iteration 23370: loss = 3.570776e-10,8.969673e-09\n",
      "Iteration 23375: loss = 3.570648e-10,8.964933e-09\n",
      "Iteration 23380: loss = 3.5705236e-10,8.9601935e-09\n",
      "Iteration 23385: loss = 3.5905093e-10,8.955459e-09\n",
      "Iteration 23390: loss = 3.5903067e-10,8.950742e-09\n",
      "Iteration 23395: loss = 3.5900327e-10,8.94604e-09\n",
      "Iteration 23400: loss = 3.589716e-10,8.941351e-09\n",
      "Iteration 23405: loss = 3.589391e-10,8.936667e-09\n",
      "Iteration 23410: loss = 3.5890735e-10,8.93198e-09\n",
      "Iteration 23415: loss = 3.5887684e-10,8.927291e-09\n",
      "Iteration 23420: loss = 3.588473e-10,8.9225995e-09\n",
      "Iteration 23425: loss = 3.588186e-10,8.917909e-09\n",
      "Iteration 23430: loss = 3.5878997e-10,8.913219e-09\n",
      "Iteration 23435: loss = 3.5876135e-10,8.90853e-09\n",
      "Iteration 23440: loss = 3.587328e-10,8.903839e-09\n",
      "Iteration 23445: loss = 3.5870454e-10,8.899151e-09\n",
      "Iteration 23450: loss = 3.5867584e-10,8.894464e-09\n",
      "Iteration 23455: loss = 3.5864847e-10,8.889778e-09\n",
      "Iteration 23460: loss = 3.5861983e-10,8.885093e-09\n",
      "Iteration 23465: loss = 3.5859124e-10,8.880407e-09\n",
      "Iteration 23470: loss = 3.585632e-10,8.875724e-09\n",
      "Iteration 23475: loss = 3.5853473e-10,8.871043e-09\n",
      "Iteration 23480: loss = 3.5850659e-10,8.866362e-09\n",
      "Iteration 23485: loss = 3.5847894e-10,8.861682e-09\n",
      "Iteration 23490: loss = 3.5844974e-10,8.857003e-09\n",
      "Iteration 23495: loss = 3.5640957e-10,8.8523215e-09\n",
      "Iteration 23500: loss = 3.5639036e-10,8.847622e-09\n",
      "Iteration 23505: loss = 3.5637784e-10,8.8429095e-09\n",
      "Iteration 23510: loss = 3.5636794e-10,8.838189e-09\n",
      "Iteration 23515: loss = 3.5635986e-10,8.8334655e-09\n",
      "Iteration 23520: loss = 3.5635303e-10,8.828742e-09\n",
      "Iteration 23525: loss = 3.5634673e-10,8.824015e-09\n",
      "Iteration 23530: loss = 3.5634076e-10,8.819292e-09\n",
      "Iteration 23535: loss = 3.5633432e-10,8.814569e-09\n",
      "Iteration 23540: loss = 3.5632794e-10,8.809847e-09\n",
      "Iteration 23545: loss = 3.5632128e-10,8.805127e-09\n",
      "Iteration 23550: loss = 3.563141e-10,8.80041e-09\n",
      "Iteration 23555: loss = 3.563073e-10,8.795693e-09\n",
      "Iteration 23560: loss = 3.563002e-10,8.790979e-09\n",
      "Iteration 23565: loss = 3.562924e-10,8.786265e-09\n",
      "Iteration 23570: loss = 3.562849e-10,8.781555e-09\n",
      "Iteration 23575: loss = 3.562766e-10,8.776848e-09\n",
      "Iteration 23580: loss = 3.5626813e-10,8.77214e-09\n",
      "Iteration 23585: loss = 3.5625944e-10,8.7674366e-09\n",
      "Iteration 23590: loss = 3.5625158e-10,8.762734e-09\n",
      "Iteration 23595: loss = 3.5624206e-10,8.758032e-09\n",
      "Iteration 23600: loss = 3.562325e-10,8.753332e-09\n",
      "Iteration 23605: loss = 3.5622286e-10,8.7486365e-09\n",
      "Iteration 23610: loss = 3.5621298e-10,8.743942e-09\n",
      "Iteration 23615: loss = 3.5620373e-10,8.739246e-09\n",
      "Iteration 23620: loss = 3.5619752e-10,8.7345455e-09\n",
      "Iteration 23625: loss = 3.561922e-10,8.7298435e-09\n",
      "Iteration 23630: loss = 3.5618908e-10,8.725138e-09\n",
      "Iteration 23635: loss = 3.5618544e-10,8.720434e-09\n",
      "Iteration 23640: loss = 3.5618217e-10,8.71573e-09\n",
      "Iteration 23645: loss = 3.5617864e-10,8.71103e-09\n",
      "Iteration 23650: loss = 3.5617495e-10,8.70633e-09\n",
      "Iteration 23655: loss = 3.5617098e-10,8.701633e-09\n",
      "Iteration 23660: loss = 3.5616676e-10,8.696939e-09\n",
      "Iteration 23665: loss = 3.5616243e-10,8.692246e-09\n",
      "Iteration 23670: loss = 3.5615755e-10,8.687556e-09\n",
      "Iteration 23675: loss = 3.561518e-10,8.682867e-09\n",
      "Iteration 23680: loss = 3.5614678e-10,8.678179e-09\n",
      "Iteration 23685: loss = 3.5614092e-10,8.673499e-09\n",
      "Iteration 23690: loss = 3.5613487e-10,8.668817e-09\n",
      "Iteration 23695: loss = 3.5612857e-10,8.664139e-09\n",
      "Iteration 23700: loss = 3.5612124e-10,8.659461e-09\n",
      "Iteration 23705: loss = 3.56114e-10,8.654787e-09\n",
      "Iteration 23710: loss = 3.5610623e-10,8.650116e-09\n",
      "Iteration 23715: loss = 3.5609873e-10,8.645447e-09\n",
      "Iteration 23720: loss = 3.560908e-10,8.6407805e-09\n",
      "Iteration 23725: loss = 3.5608208e-10,8.636116e-09\n",
      "Iteration 23730: loss = 3.5607317e-10,8.631454e-09\n",
      "Iteration 23735: loss = 3.560642e-10,8.626795e-09\n",
      "Iteration 23740: loss = 3.560546e-10,8.622137e-09\n",
      "Iteration 23745: loss = 3.5604494e-10,8.617483e-09\n",
      "Iteration 23750: loss = 3.5603487e-10,8.6128304e-09\n",
      "Iteration 23755: loss = 3.5602424e-10,8.608182e-09\n",
      "Iteration 23760: loss = 3.5601397e-10,8.603535e-09\n",
      "Iteration 23765: loss = 3.5600242e-10,8.598889e-09\n",
      "Iteration 23770: loss = 3.559916e-10,8.594247e-09\n",
      "Iteration 23775: loss = 3.5799222e-10,8.589608e-09\n",
      "Iteration 23780: loss = 3.579718e-10,8.584993e-09\n",
      "Iteration 23785: loss = 3.57943e-10,8.580398e-09\n",
      "Iteration 23790: loss = 3.5791026e-10,8.5758165e-09\n",
      "Iteration 23795: loss = 3.5787454e-10,8.571243e-09\n",
      "Iteration 23800: loss = 3.5783695e-10,8.566675e-09\n",
      "Iteration 23805: loss = 3.5779937e-10,8.562112e-09\n",
      "Iteration 23810: loss = 3.5776063e-10,8.557552e-09\n",
      "Iteration 23815: loss = 3.5772155e-10,8.552995e-09\n",
      "Iteration 23820: loss = 3.5768175e-10,8.548441e-09\n",
      "Iteration 23825: loss = 3.576428e-10,8.543887e-09\n",
      "Iteration 23830: loss = 3.576034e-10,8.539334e-09\n",
      "Iteration 23835: loss = 3.5756556e-10,8.534782e-09\n",
      "Iteration 23840: loss = 3.5753064e-10,8.530222e-09\n",
      "Iteration 23845: loss = 3.574987e-10,8.525659e-09\n",
      "Iteration 23850: loss = 3.574673e-10,8.521096e-09\n",
      "Iteration 23855: loss = 3.5743683e-10,8.5165315e-09\n",
      "Iteration 23860: loss = 3.574064e-10,8.511971e-09\n",
      "Iteration 23865: loss = 3.5737577e-10,8.507409e-09\n",
      "Iteration 23870: loss = 3.5734568e-10,8.502851e-09\n",
      "Iteration 23875: loss = 3.5731565e-10,8.498293e-09\n",
      "Iteration 23880: loss = 3.5527767e-10,8.493725e-09\n",
      "Iteration 23885: loss = 3.5526151e-10,8.489137e-09\n",
      "Iteration 23890: loss = 3.5525113e-10,8.4845375e-09\n",
      "Iteration 23895: loss = 3.5524397e-10,8.4799305e-09\n",
      "Iteration 23900: loss = 3.5523748e-10,8.4753236e-09\n",
      "Iteration 23905: loss = 3.552323e-10,8.470716e-09\n",
      "Iteration 23910: loss = 3.5522374e-10,8.466122e-09\n",
      "Iteration 23915: loss = 3.5521222e-10,8.461535e-09\n",
      "Iteration 23920: loss = 3.551999e-10,8.456952e-09\n",
      "Iteration 23925: loss = 3.55186e-10,8.452371e-09\n",
      "Iteration 23930: loss = 3.5517142e-10,8.447801e-09\n",
      "Iteration 23935: loss = 3.551568e-10,8.44323e-09\n",
      "Iteration 23940: loss = 3.5514067e-10,8.438664e-09\n",
      "Iteration 23945: loss = 3.5512504e-10,8.434099e-09\n",
      "Iteration 23950: loss = 3.5510872e-10,8.42954e-09\n",
      "Iteration 23955: loss = 3.5509165e-10,8.424982e-09\n",
      "Iteration 23960: loss = 3.5507433e-10,8.420427e-09\n",
      "Iteration 23965: loss = 3.550571e-10,8.415876e-09\n",
      "Iteration 23970: loss = 3.5503955e-10,8.411327e-09\n",
      "Iteration 23975: loss = 3.5502135e-10,8.406781e-09\n",
      "Iteration 23980: loss = 3.5500322e-10,8.402238e-09\n",
      "Iteration 23985: loss = 3.5498451e-10,8.3976985e-09\n",
      "Iteration 23990: loss = 3.5496583e-10,8.393161e-09\n",
      "Iteration 23995: loss = 3.54947e-10,8.388627e-09\n",
      "Iteration 24000: loss = 3.5492823e-10,8.384095e-09\n",
      "Iteration 24005: loss = 3.5491218e-10,8.379556e-09\n",
      "Iteration 24010: loss = 3.548992e-10,8.375014e-09\n",
      "Iteration 24015: loss = 3.5488648e-10,8.37047e-09\n",
      "Iteration 24020: loss = 3.5487493e-10,8.365928e-09\n",
      "Iteration 24025: loss = 3.5486394e-10,8.361387e-09\n",
      "Iteration 24030: loss = 3.5485223e-10,8.356849e-09\n",
      "Iteration 24035: loss = 3.5484135e-10,8.352314e-09\n",
      "Iteration 24040: loss = 3.548292e-10,8.347782e-09\n",
      "Iteration 24045: loss = 3.548171e-10,8.343252e-09\n",
      "Iteration 24050: loss = 3.5480427e-10,8.338727e-09\n",
      "Iteration 24055: loss = 3.5680323e-10,8.334209e-09\n",
      "Iteration 24060: loss = 3.567773e-10,8.32972e-09\n",
      "Iteration 24065: loss = 3.5674433e-10,8.325253e-09\n",
      "Iteration 24070: loss = 3.5670664e-10,8.320799e-09\n",
      "Iteration 24075: loss = 3.5666634e-10,8.316355e-09\n",
      "Iteration 24080: loss = 3.5662437e-10,8.3119165e-09\n",
      "Iteration 24085: loss = 3.565814e-10,8.307484e-09\n",
      "Iteration 24090: loss = 3.5653747e-10,8.303054e-09\n",
      "Iteration 24095: loss = 3.5649386e-10,8.298629e-09\n",
      "Iteration 24100: loss = 3.5644973e-10,8.294205e-09\n",
      "Iteration 24105: loss = 3.5640568e-10,8.289785e-09\n",
      "Iteration 24110: loss = 3.563617e-10,8.285367e-09\n",
      "Iteration 24115: loss = 3.563172e-10,8.280951e-09\n",
      "Iteration 24120: loss = 3.5627354e-10,8.276539e-09\n",
      "Iteration 24125: loss = 3.5622974e-10,8.272126e-09\n",
      "Iteration 24130: loss = 3.5619e-10,8.267707e-09\n",
      "Iteration 24135: loss = 3.5414008e-10,8.263285e-09\n",
      "Iteration 24140: loss = 3.5411307e-10,8.258839e-09\n",
      "Iteration 24145: loss = 3.540955e-10,8.2543705e-09\n",
      "Iteration 24150: loss = 3.5408418e-10,8.2498905e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24155: loss = 3.5407588e-10,8.245405e-09\n",
      "Iteration 24160: loss = 3.540702e-10,8.240917e-09\n",
      "Iteration 24165: loss = 3.5406514e-10,8.236429e-09\n",
      "Iteration 24170: loss = 3.540599e-10,8.231945e-09\n",
      "Iteration 24175: loss = 3.5405467e-10,8.227462e-09\n",
      "Iteration 24180: loss = 3.5404957e-10,8.222982e-09\n",
      "Iteration 24185: loss = 3.5404368e-10,8.218507e-09\n",
      "Iteration 24190: loss = 3.5403705e-10,8.214035e-09\n",
      "Iteration 24195: loss = 3.5403044e-10,8.2095655e-09\n",
      "Iteration 24200: loss = 3.5402337e-10,8.205101e-09\n",
      "Iteration 24205: loss = 3.5401607e-10,8.200641e-09\n",
      "Iteration 24210: loss = 3.5400805e-10,8.196183e-09\n",
      "Iteration 24215: loss = 3.5399952e-10,8.191731e-09\n",
      "Iteration 24220: loss = 3.5398995e-10,8.187282e-09\n",
      "Iteration 24225: loss = 3.5397987e-10,8.182837e-09\n",
      "Iteration 24230: loss = 3.5396966e-10,8.178397e-09\n",
      "Iteration 24235: loss = 3.539584e-10,8.17396e-09\n",
      "Iteration 24240: loss = 3.5394743e-10,8.169527e-09\n",
      "Iteration 24245: loss = 3.5393521e-10,8.165099e-09\n",
      "Iteration 24250: loss = 3.5392353e-10,8.160675e-09\n",
      "Iteration 24255: loss = 3.5391004e-10,8.156254e-09\n",
      "Iteration 24260: loss = 3.5389716e-10,8.151836e-09\n",
      "Iteration 24265: loss = 3.5388348e-10,8.147423e-09\n",
      "Iteration 24270: loss = 3.5386885e-10,8.143016e-09\n",
      "Iteration 24275: loss = 3.5385503e-10,8.13861e-09\n",
      "Iteration 24280: loss = 3.558488e-10,8.134219e-09\n",
      "Iteration 24285: loss = 3.5581801e-10,8.129861e-09\n",
      "Iteration 24290: loss = 3.5577938e-10,8.125523e-09\n",
      "Iteration 24295: loss = 3.5573633e-10,8.121201e-09\n",
      "Iteration 24300: loss = 3.5569095e-10,8.116887e-09\n",
      "Iteration 24305: loss = 3.5564804e-10,8.112573e-09\n",
      "Iteration 24310: loss = 3.55607e-10,8.108253e-09\n",
      "Iteration 24315: loss = 3.5556666e-10,8.103933e-09\n",
      "Iteration 24320: loss = 3.5552808e-10,8.099617e-09\n",
      "Iteration 24325: loss = 3.5548894e-10,8.095299e-09\n",
      "Iteration 24330: loss = 3.5545136e-10,8.090984e-09\n",
      "Iteration 24335: loss = 3.5541284e-10,8.0866736e-09\n",
      "Iteration 24340: loss = 3.553746e-10,8.082366e-09\n",
      "Iteration 24345: loss = 3.5533687e-10,8.078061e-09\n",
      "Iteration 24350: loss = 3.5529793e-10,8.0737586e-09\n",
      "Iteration 24355: loss = 3.5526007e-10,8.069459e-09\n",
      "Iteration 24360: loss = 3.5522096e-10,8.0651645e-09\n",
      "Iteration 24365: loss = 3.5518197e-10,8.060872e-09\n",
      "Iteration 24370: loss = 3.5514378e-10,8.056582e-09\n",
      "Iteration 24375: loss = 3.5510506e-10,8.052296e-09\n",
      "Iteration 24380: loss = 3.5305622e-10,8.048007e-09\n",
      "Iteration 24385: loss = 3.5303283e-10,8.043687e-09\n",
      "Iteration 24390: loss = 3.530153e-10,8.0393585e-09\n",
      "Iteration 24395: loss = 3.5299763e-10,8.035032e-09\n",
      "Iteration 24400: loss = 3.529799e-10,8.030707e-09\n",
      "Iteration 24405: loss = 3.5296632e-10,8.026379e-09\n",
      "Iteration 24410: loss = 3.529549e-10,8.022044e-09\n",
      "Iteration 24415: loss = 3.5294456e-10,8.01771e-09\n",
      "Iteration 24420: loss = 3.5293538e-10,8.0133775e-09\n",
      "Iteration 24425: loss = 3.52926e-10,8.009049e-09\n",
      "Iteration 24430: loss = 3.5291584e-10,8.004723e-09\n",
      "Iteration 24435: loss = 3.5290595e-10,8.000403e-09\n",
      "Iteration 24440: loss = 3.5289516e-10,7.9960865e-09\n",
      "Iteration 24445: loss = 3.528843e-10,7.9917735e-09\n",
      "Iteration 24450: loss = 3.5287306e-10,7.987467e-09\n",
      "Iteration 24455: loss = 3.5285996e-10,7.983163e-09\n",
      "Iteration 24460: loss = 3.5284722e-10,7.978864e-09\n",
      "Iteration 24465: loss = 3.528343e-10,7.974571e-09\n",
      "Iteration 24470: loss = 3.5282077e-10,7.97028e-09\n",
      "Iteration 24475: loss = 3.5280526e-10,7.965998e-09\n",
      "Iteration 24480: loss = 3.5480288e-10,7.9617175e-09\n",
      "Iteration 24485: loss = 3.5477576e-10,7.957472e-09\n",
      "Iteration 24490: loss = 3.5473804e-10,7.953255e-09\n",
      "Iteration 24495: loss = 3.546942e-10,7.949057e-09\n",
      "Iteration 24500: loss = 3.5464678e-10,7.944871e-09\n",
      "Iteration 24505: loss = 3.5459755e-10,7.940694e-09\n",
      "Iteration 24510: loss = 3.5454706e-10,7.936523e-09\n",
      "Iteration 24515: loss = 3.5449602e-10,7.932357e-09\n",
      "Iteration 24520: loss = 3.5444322e-10,7.928197e-09\n",
      "Iteration 24525: loss = 3.543915e-10,7.92404e-09\n",
      "Iteration 24530: loss = 3.543417e-10,7.919881e-09\n",
      "Iteration 24535: loss = 3.542963e-10,7.915714e-09\n",
      "Iteration 24540: loss = 3.54253e-10,7.911545e-09\n",
      "Iteration 24545: loss = 3.5421135e-10,7.907374e-09\n",
      "Iteration 24550: loss = 3.5417005e-10,7.903206e-09\n",
      "Iteration 24555: loss = 3.5412964e-10,7.89904e-09\n",
      "Iteration 24560: loss = 3.5408923e-10,7.894877e-09\n",
      "Iteration 24565: loss = 3.5404887e-10,7.890716e-09\n",
      "Iteration 24570: loss = 3.5400885e-10,7.88656e-09\n",
      "Iteration 24575: loss = 3.5195646e-10,7.882406e-09\n",
      "Iteration 24580: loss = 3.5192763e-10,7.878226e-09\n",
      "Iteration 24585: loss = 3.5190947e-10,7.874025e-09\n",
      "Iteration 24590: loss = 3.5189585e-10,7.869813e-09\n",
      "Iteration 24595: loss = 3.5188563e-10,7.865598e-09\n",
      "Iteration 24600: loss = 3.518774e-10,7.861383e-09\n",
      "Iteration 24605: loss = 3.5186945e-10,7.857166e-09\n",
      "Iteration 24610: loss = 3.5186096e-10,7.852955e-09\n",
      "Iteration 24615: loss = 3.518531e-10,7.848749e-09\n",
      "Iteration 24620: loss = 3.518442e-10,7.844546e-09\n",
      "Iteration 24625: loss = 3.518346e-10,7.840347e-09\n",
      "Iteration 24630: loss = 3.518251e-10,7.8361575e-09\n",
      "Iteration 24635: loss = 3.5181372e-10,7.83197e-09\n",
      "Iteration 24640: loss = 3.51803e-10,7.827789e-09\n",
      "Iteration 24645: loss = 3.5179037e-10,7.823614e-09\n",
      "Iteration 24650: loss = 3.5177786e-10,7.819441e-09\n",
      "Iteration 24655: loss = 3.5176906e-10,7.815264e-09\n",
      "Iteration 24660: loss = 3.517636e-10,7.811084e-09\n",
      "Iteration 24665: loss = 3.5376943e-10,7.806909e-09\n",
      "Iteration 24670: loss = 3.5374995e-10,7.802768e-09\n",
      "Iteration 24675: loss = 3.5372186e-10,7.798655e-09\n",
      "Iteration 24680: loss = 3.5368777e-10,7.794558e-09\n",
      "Iteration 24685: loss = 3.5365025e-10,7.790476e-09\n",
      "Iteration 24690: loss = 3.5360992e-10,7.786401e-09\n",
      "Iteration 24695: loss = 3.5356915e-10,7.782334e-09\n",
      "Iteration 24700: loss = 3.53527e-10,7.778272e-09\n",
      "Iteration 24705: loss = 3.5348438e-10,7.774218e-09\n",
      "Iteration 24710: loss = 3.5344164e-10,7.770166e-09\n",
      "Iteration 24715: loss = 3.5339787e-10,7.766118e-09\n",
      "Iteration 24720: loss = 3.5335415e-10,7.762079e-09\n",
      "Iteration 24725: loss = 3.5331046e-10,7.758041e-09\n",
      "Iteration 24730: loss = 3.5326653e-10,7.754007e-09\n",
      "Iteration 24735: loss = 3.5322192e-10,7.7499775e-09\n",
      "Iteration 24740: loss = 3.5317735e-10,7.745952e-09\n",
      "Iteration 24745: loss = 3.5313255e-10,7.741931e-09\n",
      "Iteration 24750: loss = 3.5308992e-10,7.737908e-09\n",
      "Iteration 24755: loss = 3.5305234e-10,7.7338775e-09\n",
      "Iteration 24760: loss = 3.5301354e-10,7.729854e-09\n",
      "Iteration 24765: loss = 3.529718e-10,7.7258395e-09\n",
      "Iteration 24770: loss = 3.509154e-10,7.721834e-09\n",
      "Iteration 24775: loss = 3.5088377e-10,7.717801e-09\n",
      "Iteration 24780: loss = 3.5086245e-10,7.71375e-09\n",
      "Iteration 24785: loss = 3.5084535e-10,7.709686e-09\n",
      "Iteration 24790: loss = 3.5083278e-10,7.705618e-09\n",
      "Iteration 24795: loss = 3.508208e-10,7.701551e-09\n",
      "Iteration 24800: loss = 3.5080908e-10,7.697487e-09\n",
      "Iteration 24805: loss = 3.507982e-10,7.693426e-09\n",
      "Iteration 24810: loss = 3.5078662e-10,7.689369e-09\n",
      "Iteration 24815: loss = 3.5077427e-10,7.68532e-09\n",
      "Iteration 24820: loss = 3.5076067e-10,7.681274e-09\n",
      "Iteration 24825: loss = 3.5074732e-10,7.677234e-09\n",
      "Iteration 24830: loss = 3.5073303e-10,7.673201e-09\n",
      "Iteration 24835: loss = 3.5071826e-10,7.669174e-09\n",
      "Iteration 24840: loss = 3.5271244e-10,7.6651565e-09\n",
      "Iteration 24845: loss = 3.526799e-10,7.66118e-09\n",
      "Iteration 24850: loss = 3.5263695e-10,7.657237e-09\n",
      "Iteration 24855: loss = 3.5258693e-10,7.653312e-09\n",
      "Iteration 24860: loss = 3.5253409e-10,7.649401e-09\n",
      "Iteration 24865: loss = 3.5248185e-10,7.6454905e-09\n",
      "Iteration 24870: loss = 3.5243355e-10,7.641574e-09\n",
      "Iteration 24875: loss = 3.5238648e-10,7.637659e-09\n",
      "Iteration 24880: loss = 3.5234016e-10,7.633745e-09\n",
      "Iteration 24885: loss = 3.5229467e-10,7.629833e-09\n",
      "Iteration 24890: loss = 3.5224976e-10,7.625927e-09\n",
      "Iteration 24895: loss = 3.522039e-10,7.622022e-09\n",
      "Iteration 24900: loss = 3.5215797e-10,7.618121e-09\n",
      "Iteration 24905: loss = 3.5211298e-10,7.6142275e-09\n",
      "Iteration 24910: loss = 3.5206696e-10,7.610338e-09\n",
      "Iteration 24915: loss = 3.520215e-10,7.60645e-09\n",
      "Iteration 24920: loss = 3.519753e-10,7.602568e-09\n",
      "Iteration 24925: loss = 3.519284e-10,7.598691e-09\n",
      "Iteration 24930: loss = 3.51882e-10,7.594818e-09\n",
      "Iteration 24935: loss = 3.498218e-10,7.59095e-09\n",
      "Iteration 24940: loss = 3.4978673e-10,7.5870545e-09\n",
      "Iteration 24945: loss = 3.4976969e-10,7.583119e-09\n",
      "Iteration 24950: loss = 3.4976247e-10,7.5791595e-09\n",
      "Iteration 24955: loss = 3.497612e-10,7.575193e-09\n",
      "Iteration 24960: loss = 3.4976286e-10,7.571221e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24965: loss = 3.4976558e-10,7.567247e-09\n",
      "Iteration 24970: loss = 3.4976813e-10,7.5632824e-09\n",
      "Iteration 24975: loss = 3.4977118e-10,7.5593185e-09\n",
      "Iteration 24980: loss = 3.4977254e-10,7.555361e-09\n",
      "Iteration 24985: loss = 3.4977288e-10,7.551412e-09\n",
      "Iteration 24990: loss = 3.4977252e-10,7.547468e-09\n",
      "Iteration 24995: loss = 3.4977135e-10,7.543532e-09\n",
      "Iteration 25000: loss = 3.497687e-10,7.539604e-09\n",
      "Iteration 25005: loss = 3.5177405e-10,7.535692e-09\n",
      "Iteration 25010: loss = 3.51749e-10,7.531827e-09\n",
      "Iteration 25015: loss = 3.5171419e-10,7.527993e-09\n",
      "Iteration 25020: loss = 3.5167358e-10,7.524179e-09\n",
      "Iteration 25025: loss = 3.5162817e-10,7.520379e-09\n",
      "Iteration 25030: loss = 3.5158118e-10,7.516588e-09\n",
      "Iteration 25035: loss = 3.515324e-10,7.512806e-09\n",
      "Iteration 25040: loss = 3.5148218e-10,7.509031e-09\n",
      "Iteration 25045: loss = 3.5143197e-10,7.505262e-09\n",
      "Iteration 25050: loss = 3.5138123e-10,7.501498e-09\n",
      "Iteration 25055: loss = 3.5132933e-10,7.497738e-09\n",
      "Iteration 25060: loss = 3.5127867e-10,7.493984e-09\n",
      "Iteration 25065: loss = 3.5123127e-10,7.490223e-09\n",
      "Iteration 25070: loss = 3.5118886e-10,7.486454e-09\n",
      "Iteration 25075: loss = 3.5114875e-10,7.482686e-09\n",
      "Iteration 25080: loss = 3.511049e-10,7.478927e-09\n",
      "Iteration 25085: loss = 3.510571e-10,7.475185e-09\n",
      "Iteration 25090: loss = 3.51007e-10,7.471452e-09\n",
      "Iteration 25095: loss = 3.5095463e-10,7.46773e-09\n",
      "Iteration 25100: loss = 3.5090128e-10,7.4640125e-09\n",
      "Iteration 25105: loss = 3.508472e-10,7.460304e-09\n",
      "Iteration 25110: loss = 3.487823e-10,7.456593e-09\n",
      "Iteration 25115: loss = 3.4874562e-10,7.452848e-09\n",
      "Iteration 25120: loss = 3.4871886e-10,7.4490814e-09\n",
      "Iteration 25125: loss = 3.4869804e-10,7.445305e-09\n",
      "Iteration 25130: loss = 3.4868033e-10,7.4415247e-09\n",
      "Iteration 25135: loss = 3.4866324e-10,7.4377424e-09\n",
      "Iteration 25140: loss = 3.4864747e-10,7.4339654e-09\n",
      "Iteration 25145: loss = 3.4863148e-10,7.4301933e-09\n",
      "Iteration 25150: loss = 3.4861503e-10,7.4264244e-09\n",
      "Iteration 25155: loss = 3.48604e-10,7.4226465e-09\n",
      "Iteration 25160: loss = 3.485964e-10,7.418865e-09\n",
      "Iteration 25165: loss = 3.4858963e-10,7.4150837e-09\n",
      "Iteration 25170: loss = 3.5059353e-10,7.411314e-09\n",
      "Iteration 25175: loss = 3.5056924e-10,7.4075883e-09\n",
      "Iteration 25180: loss = 3.5053282e-10,7.403897e-09\n",
      "Iteration 25185: loss = 3.5048978e-10,7.4002284e-09\n",
      "Iteration 25190: loss = 3.5044312e-10,7.396574e-09\n",
      "Iteration 25195: loss = 3.5039263e-10,7.3929325e-09\n",
      "Iteration 25200: loss = 3.5034042e-10,7.389298e-09\n",
      "Iteration 25205: loss = 3.502877e-10,7.385671e-09\n",
      "Iteration 25210: loss = 3.5023373e-10,7.3820514e-09\n",
      "Iteration 25215: loss = 3.5017955e-10,7.378437e-09\n",
      "Iteration 25220: loss = 3.5012412e-10,7.3748305e-09\n",
      "Iteration 25225: loss = 3.5006945e-10,7.3712263e-09\n",
      "Iteration 25230: loss = 3.500136e-10,7.3676296e-09\n",
      "Iteration 25235: loss = 3.499586e-10,7.364034e-09\n",
      "Iteration 25240: loss = 3.4990844e-10,7.3604323e-09\n",
      "Iteration 25245: loss = 3.49862e-10,7.3568236e-09\n",
      "Iteration 25250: loss = 3.4981815e-10,7.3532123e-09\n",
      "Iteration 25255: loss = 3.4977543e-10,7.3496036e-09\n",
      "Iteration 25260: loss = 3.477215e-10,7.345996e-09\n",
      "Iteration 25265: loss = 3.4769423e-10,7.3423547e-09\n",
      "Iteration 25270: loss = 3.4767958e-10,7.338685e-09\n",
      "Iteration 25275: loss = 3.4767164e-10,7.335004e-09\n",
      "Iteration 25280: loss = 3.4766764e-10,7.3313196e-09\n",
      "Iteration 25285: loss = 3.476646e-10,7.327633e-09\n",
      "Iteration 25290: loss = 3.4766215e-10,7.3239494e-09\n",
      "Iteration 25295: loss = 3.4765923e-10,7.320273e-09\n",
      "Iteration 25300: loss = 3.476556e-10,7.316604e-09\n",
      "Iteration 25305: loss = 3.4765069e-10,7.312941e-09\n",
      "Iteration 25310: loss = 3.47645e-10,7.309286e-09\n",
      "Iteration 25315: loss = 3.4763759e-10,7.305637e-09\n",
      "Iteration 25320: loss = 3.4762895e-10,7.3019972e-09\n",
      "Iteration 25325: loss = 3.4761874e-10,7.2983655e-09\n",
      "Iteration 25330: loss = 3.496179e-10,7.294748e-09\n",
      "Iteration 25335: loss = 3.4958672e-10,7.2911823e-09\n",
      "Iteration 25340: loss = 3.4954248e-10,7.287651e-09\n",
      "Iteration 25345: loss = 3.4949058e-10,7.2841435e-09\n",
      "Iteration 25350: loss = 3.4943462e-10,7.2806556e-09\n",
      "Iteration 25355: loss = 3.4938044e-10,7.277163e-09\n",
      "Iteration 25360: loss = 3.4932993e-10,7.2736674e-09\n",
      "Iteration 25365: loss = 3.4928074e-10,7.270173e-09\n",
      "Iteration 25370: loss = 3.4923264e-10,7.266681e-09\n",
      "Iteration 25375: loss = 3.4918457e-10,7.2631936e-09\n",
      "Iteration 25380: loss = 3.4912984e-10,7.2597244e-09\n",
      "Iteration 25385: loss = 3.490713e-10,7.256274e-09\n",
      "Iteration 25390: loss = 3.4900868e-10,7.2528357e-09\n",
      "Iteration 25395: loss = 3.4894532e-10,7.249406e-09\n",
      "Iteration 25400: loss = 3.4888017e-10,7.2459843e-09\n",
      "Iteration 25405: loss = 3.4881462e-10,7.2425683e-09\n",
      "Iteration 25410: loss = 3.4874828e-10,7.2391604e-09\n",
      "Iteration 25415: loss = 3.466701e-10,7.2357533e-09\n",
      "Iteration 25420: loss = 3.466256e-10,7.2322965e-09\n",
      "Iteration 25425: loss = 3.4659844e-10,7.2287993e-09\n",
      "Iteration 25430: loss = 3.4658057e-10,7.2252795e-09\n",
      "Iteration 25435: loss = 3.4656925e-10,7.221752e-09\n",
      "Iteration 25440: loss = 3.4656056e-10,7.2182225e-09\n",
      "Iteration 25445: loss = 3.4655187e-10,7.2146937e-09\n",
      "Iteration 25450: loss = 3.4654327e-10,7.2111703e-09\n",
      "Iteration 25455: loss = 3.4653516e-10,7.2076523e-09\n",
      "Iteration 25460: loss = 3.4652506e-10,7.204141e-09\n",
      "Iteration 25465: loss = 3.4651396e-10,7.2006383e-09\n",
      "Iteration 25470: loss = 3.4650185e-10,7.1971424e-09\n",
      "Iteration 25475: loss = 3.4648875e-10,7.1936563e-09\n",
      "Iteration 25480: loss = 3.4647366e-10,7.1901747e-09\n",
      "Iteration 25485: loss = 3.4645756e-10,7.1867046e-09\n",
      "Iteration 25490: loss = 3.4644e-10,7.18324e-09\n",
      "Iteration 25495: loss = 3.4842937e-10,7.1797994e-09\n",
      "Iteration 25500: loss = 3.483858e-10,7.1764106e-09\n",
      "Iteration 25505: loss = 3.4833134e-10,7.1730577e-09\n",
      "Iteration 25510: loss = 3.482692e-10,7.169728e-09\n",
      "Iteration 25515: loss = 3.4820283e-10,7.166416e-09\n",
      "Iteration 25520: loss = 3.481334e-10,7.1631145e-09\n",
      "Iteration 25525: loss = 3.4806788e-10,7.1598087e-09\n",
      "Iteration 25530: loss = 3.4800643e-10,7.156497e-09\n",
      "Iteration 25535: loss = 3.4794775e-10,7.1531825e-09\n",
      "Iteration 25540: loss = 3.4788972e-10,7.149869e-09\n",
      "Iteration 25545: loss = 3.4783373e-10,7.1465593e-09\n",
      "Iteration 25550: loss = 3.4777703e-10,7.1432527e-09\n",
      "Iteration 25555: loss = 3.4772027e-10,7.1399513e-09\n",
      "Iteration 25560: loss = 3.4766326e-10,7.136655e-09\n",
      "Iteration 25565: loss = 3.4559858e-10,7.1333486e-09\n",
      "Iteration 25570: loss = 3.4556427e-10,7.1300033e-09\n",
      "Iteration 25575: loss = 3.4554073e-10,7.1266353e-09\n",
      "Iteration 25580: loss = 3.455222e-10,7.1232558e-09\n",
      "Iteration 25585: loss = 3.455071e-10,7.119873e-09\n",
      "Iteration 25590: loss = 3.4549288e-10,7.1164936e-09\n",
      "Iteration 25595: loss = 3.4547934e-10,7.1131168e-09\n",
      "Iteration 25600: loss = 3.4546463e-10,7.109748e-09\n",
      "Iteration 25605: loss = 3.4545114e-10,7.106379e-09\n",
      "Iteration 25610: loss = 3.45443e-10,7.1030026e-09\n",
      "Iteration 25615: loss = 3.4543698e-10,7.0996213e-09\n",
      "Iteration 25620: loss = 3.4543268e-10,7.096244e-09\n",
      "Iteration 25625: loss = 3.4542771e-10,7.0928725e-09\n",
      "Iteration 25630: loss = 3.4542272e-10,7.089506e-09\n",
      "Iteration 25635: loss = 3.4541578e-10,7.0861472e-09\n",
      "Iteration 25640: loss = 3.454075e-10,7.082798e-09\n",
      "Iteration 25645: loss = 3.4539852e-10,7.0794557e-09\n",
      "Iteration 25650: loss = 3.453873e-10,7.076123e-09\n",
      "Iteration 25655: loss = 3.4537415e-10,7.0727983e-09\n",
      "Iteration 25660: loss = 3.473637e-10,7.0695085e-09\n",
      "Iteration 25665: loss = 3.473211e-10,7.066271e-09\n",
      "Iteration 25670: loss = 3.472621e-10,7.063082e-09\n",
      "Iteration 25675: loss = 3.471902e-10,7.0599304e-09\n",
      "Iteration 25680: loss = 3.4711103e-10,7.056804e-09\n",
      "Iteration 25685: loss = 3.4702785e-10,7.0536945e-09\n",
      "Iteration 25690: loss = 3.469417e-10,7.050594e-09\n",
      "Iteration 25695: loss = 3.468538e-10,7.0475044e-09\n",
      "Iteration 25700: loss = 3.4676947e-10,7.044411e-09\n",
      "Iteration 25705: loss = 3.4669e-10,7.0413066e-09\n",
      "Iteration 25710: loss = 3.466156e-10,7.0381976e-09\n",
      "Iteration 25715: loss = 3.445386e-10,7.0350628e-09\n",
      "Iteration 25720: loss = 3.4449388e-10,7.031884e-09\n",
      "Iteration 25725: loss = 3.4445957e-10,7.028682e-09\n",
      "Iteration 25730: loss = 3.4443123e-10,7.025468e-09\n",
      "Iteration 25735: loss = 3.4440617e-10,7.022251e-09\n",
      "Iteration 25740: loss = 3.4438186e-10,7.019037e-09\n",
      "Iteration 25745: loss = 3.443581e-10,7.0158257e-09\n",
      "Iteration 25750: loss = 3.4433412e-10,7.012621e-09\n",
      "Iteration 25755: loss = 3.4430883e-10,7.0094224e-09\n",
      "Iteration 25760: loss = 3.4428305e-10,7.0062307e-09\n",
      "Iteration 25765: loss = 3.4425554e-10,7.0030484e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25770: loss = 3.4422684e-10,6.9998736e-09\n",
      "Iteration 25775: loss = 3.4419725e-10,6.9967063e-09\n",
      "Iteration 25780: loss = 3.441668e-10,6.993543e-09\n",
      "Iteration 25785: loss = 3.441403e-10,6.9903767e-09\n",
      "Iteration 25790: loss = 3.4411915e-10,6.987201e-09\n",
      "Iteration 25795: loss = 3.440995e-10,6.9840267e-09\n",
      "Iteration 25800: loss = 3.440794e-10,6.980856e-09\n",
      "Iteration 25805: loss = 3.4406e-10,6.9776913e-09\n",
      "Iteration 25810: loss = 3.4403913e-10,6.974531e-09\n",
      "Iteration 25815: loss = 3.4401726e-10,6.971381e-09\n",
      "Iteration 25820: loss = 3.439944e-10,6.968237e-09\n",
      "Iteration 25825: loss = 3.4396977e-10,6.965102e-09\n",
      "Iteration 25830: loss = 3.439441e-10,6.961976e-09\n",
      "Iteration 25835: loss = 3.4391648e-10,6.9588566e-09\n",
      "Iteration 25840: loss = 3.4589107e-10,6.9557737e-09\n",
      "Iteration 25845: loss = 3.458329e-10,6.9527455e-09\n",
      "Iteration 25850: loss = 3.4576322e-10,6.9497545e-09\n",
      "Iteration 25855: loss = 3.4568584e-10,6.9467867e-09\n",
      "Iteration 25860: loss = 3.4560454e-10,6.9438326e-09\n",
      "Iteration 25865: loss = 3.43514e-10,6.9408763e-09\n",
      "Iteration 25870: loss = 3.4345796e-10,6.9378676e-09\n",
      "Iteration 25875: loss = 3.434191e-10,6.9348154e-09\n",
      "Iteration 25880: loss = 3.433905e-10,6.9317436e-09\n",
      "Iteration 25885: loss = 3.433672e-10,6.9286625e-09\n",
      "Iteration 25890: loss = 3.4334632e-10,6.9255774e-09\n",
      "Iteration 25895: loss = 3.4332623e-10,6.9224972e-09\n",
      "Iteration 25900: loss = 3.4330538e-10,6.919421e-09\n",
      "Iteration 25905: loss = 3.4328476e-10,6.91635e-09\n",
      "Iteration 25910: loss = 3.432632e-10,6.9132886e-09\n",
      "Iteration 25915: loss = 3.4324013e-10,6.910235e-09\n",
      "Iteration 25920: loss = 3.4321523e-10,6.9071886e-09\n",
      "Iteration 25925: loss = 3.4318892e-10,6.904151e-09\n",
      "Iteration 25930: loss = 3.431606e-10,6.901123e-09\n",
      "Iteration 25935: loss = 3.4313205e-10,6.8981048e-09\n",
      "Iteration 25940: loss = 3.431019e-10,6.8950916e-09\n",
      "Iteration 25945: loss = 3.430699e-10,6.892088e-09\n",
      "Iteration 25950: loss = 3.430363e-10,6.889093e-09\n",
      "Iteration 25955: loss = 3.4300152e-10,6.886107e-09\n",
      "Iteration 25960: loss = 3.4296554e-10,6.8831283e-09\n",
      "Iteration 25965: loss = 3.429279e-10,6.8801587e-09\n",
      "Iteration 25970: loss = 3.4289208e-10,6.8771895e-09\n",
      "Iteration 25975: loss = 3.4286154e-10,6.8742088e-09\n",
      "Iteration 25980: loss = 3.4283432e-10,6.871227e-09\n",
      "Iteration 25985: loss = 3.428062e-10,6.8682495e-09\n",
      "Iteration 25990: loss = 3.4277037e-10,6.8652977e-09\n",
      "Iteration 25995: loss = 3.4273062e-10,6.8623645e-09\n",
      "Iteration 26000: loss = 3.4268655e-10,6.859443e-09\n",
      "Iteration 26005: loss = 3.4263958e-10,6.8565345e-09\n",
      "Iteration 26010: loss = 3.4259154e-10,6.853635e-09\n",
      "Iteration 26015: loss = 3.4254208e-10,6.850744e-09\n",
      "Iteration 26020: loss = 3.404976e-10,6.8478334e-09\n",
      "Iteration 26025: loss = 3.424776e-10,6.844883e-09\n",
      "Iteration 26030: loss = 3.424455e-10,6.8419594e-09\n",
      "Iteration 26035: loss = 3.4240336e-10,6.839069e-09\n",
      "Iteration 26040: loss = 3.4235437e-10,6.8361965e-09\n",
      "Iteration 26045: loss = 3.4230116e-10,6.833344e-09\n",
      "Iteration 26050: loss = 3.4224487e-10,6.830503e-09\n",
      "Iteration 26055: loss = 3.4218897e-10,6.827665e-09\n",
      "Iteration 26060: loss = 3.4213868e-10,6.8248163e-09\n",
      "Iteration 26065: loss = 3.420927e-10,6.8219643e-09\n",
      "Iteration 26070: loss = 3.4204742e-10,6.8191124e-09\n",
      "Iteration 26075: loss = 3.4200287e-10,6.8162618e-09\n",
      "Iteration 26080: loss = 3.4195813e-10,6.813418e-09\n",
      "Iteration 26085: loss = 3.419128e-10,6.8105797e-09\n",
      "Iteration 26090: loss = 3.4186654e-10,6.80775e-09\n",
      "Iteration 26095: loss = 3.418197e-10,6.8049255e-09\n",
      "Iteration 26100: loss = 3.4177175e-10,6.8021087e-09\n",
      "Iteration 26105: loss = 3.4172254e-10,6.7993007e-09\n",
      "Iteration 26110: loss = 3.416725e-10,6.7964985e-09\n",
      "Iteration 26115: loss = 3.4162126e-10,6.793704e-09\n",
      "Iteration 26120: loss = 3.4156952e-10,6.790916e-09\n",
      "Iteration 26125: loss = 3.4151604e-10,6.7881363e-09\n",
      "Iteration 26130: loss = 3.4146427e-10,6.7853567e-09\n",
      "Iteration 26135: loss = 3.4141986e-10,6.7825643e-09\n",
      "Iteration 26140: loss = 3.4137826e-10,6.779768e-09\n",
      "Iteration 26145: loss = 3.3934566e-10,6.776941e-09\n",
      "Iteration 26150: loss = 3.3933645e-10,6.774067e-09\n",
      "Iteration 26155: loss = 3.3933836e-10,6.771168e-09\n",
      "Iteration 26160: loss = 3.3934586e-10,6.768259e-09\n",
      "Iteration 26165: loss = 3.3935502e-10,6.7653496e-09\n",
      "Iteration 26170: loss = 3.3936476e-10,6.762444e-09\n",
      "Iteration 26175: loss = 3.3937372e-10,6.759545e-09\n",
      "Iteration 26180: loss = 3.3938097e-10,6.7566557e-09\n",
      "Iteration 26185: loss = 3.3938571e-10,6.7537775e-09\n",
      "Iteration 26190: loss = 3.3938832e-10,6.7509096e-09\n",
      "Iteration 26195: loss = 3.3938888e-10,6.748051e-09\n",
      "Iteration 26200: loss = 3.3938596e-10,6.7452053e-09\n",
      "Iteration 26205: loss = 3.3938116e-10,6.7423693e-09\n",
      "Iteration 26210: loss = 3.3937428e-10,6.739546e-09\n",
      "Iteration 26215: loss = 3.4136674e-10,6.7367343e-09\n",
      "Iteration 26220: loss = 3.413343e-10,6.7339867e-09\n",
      "Iteration 26225: loss = 3.412839e-10,6.731291e-09\n",
      "Iteration 26230: loss = 3.4122147e-10,6.7286323e-09\n",
      "Iteration 26235: loss = 3.411528e-10,6.7259958e-09\n",
      "Iteration 26240: loss = 3.4107908e-10,6.7233743e-09\n",
      "Iteration 26245: loss = 3.410028e-10,6.7207644e-09\n",
      "Iteration 26250: loss = 3.4092543e-10,6.718163e-09\n",
      "Iteration 26255: loss = 3.4085365e-10,6.7155512e-09\n",
      "Iteration 26260: loss = 3.4078698e-10,6.7129298e-09\n",
      "Iteration 26265: loss = 3.4072378e-10,6.710306e-09\n",
      "Iteration 26270: loss = 3.4066108e-10,6.7076833e-09\n",
      "Iteration 26275: loss = 3.4059935e-10,6.7050623e-09\n",
      "Iteration 26280: loss = 3.4053813e-10,6.702447e-09\n",
      "Iteration 26285: loss = 3.4047587e-10,6.699837e-09\n",
      "Iteration 26290: loss = 3.4041336e-10,6.697234e-09\n",
      "Iteration 26295: loss = 3.403499e-10,6.694637e-09\n",
      "Iteration 26300: loss = 3.3828537e-10,6.6920367e-09\n",
      "Iteration 26305: loss = 3.382453e-10,6.689386e-09\n",
      "Iteration 26310: loss = 3.3821956e-10,6.6867027e-09\n",
      "Iteration 26315: loss = 3.3820113e-10,6.684003e-09\n",
      "Iteration 26320: loss = 3.381856e-10,6.6812995e-09\n",
      "Iteration 26325: loss = 3.3817615e-10,6.6785866e-09\n",
      "Iteration 26330: loss = 3.381733e-10,6.67586e-09\n",
      "Iteration 26335: loss = 3.3817357e-10,6.6731265e-09\n",
      "Iteration 26340: loss = 3.381676e-10,6.6704167e-09\n",
      "Iteration 26345: loss = 3.381551e-10,6.6677286e-09\n",
      "Iteration 26350: loss = 3.381374e-10,6.6650587e-09\n",
      "Iteration 26355: loss = 3.3811534e-10,6.662403e-09\n",
      "Iteration 26360: loss = 3.3809233e-10,6.6597607e-09\n",
      "Iteration 26365: loss = 3.380649e-10,6.6571264e-09\n",
      "Iteration 26370: loss = 3.380353e-10,6.654507e-09\n",
      "Iteration 26375: loss = 3.380044e-10,6.651897e-09\n",
      "Iteration 26380: loss = 3.379709e-10,6.649295e-09\n",
      "Iteration 26385: loss = 3.379349e-10,6.646704e-09\n",
      "Iteration 26390: loss = 3.3789768e-10,6.6441217e-09\n",
      "Iteration 26395: loss = 3.378596e-10,6.6415486e-09\n",
      "Iteration 26400: loss = 3.3781825e-10,6.638983e-09\n",
      "Iteration 26405: loss = 3.3777606e-10,6.6364287e-09\n",
      "Iteration 26410: loss = 3.3773281e-10,6.6338828e-09\n",
      "Iteration 26415: loss = 3.3768766e-10,6.6313444e-09\n",
      "Iteration 26420: loss = 3.3764078e-10,6.6288135e-09\n",
      "Iteration 26425: loss = 3.3759265e-10,6.626293e-09\n",
      "Iteration 26430: loss = 3.3754324e-10,6.623779e-09\n",
      "Iteration 26435: loss = 3.3749833e-10,6.621257e-09\n",
      "Iteration 26440: loss = 3.374596e-10,6.6187233e-09\n",
      "Iteration 26445: loss = 3.3742295e-10,6.6161885e-09\n",
      "Iteration 26450: loss = 3.3738753e-10,6.613655e-09\n",
      "Iteration 26455: loss = 3.3735126e-10,6.6111263e-09\n",
      "Iteration 26460: loss = 3.3931494e-10,6.608614e-09\n",
      "Iteration 26465: loss = 3.3724937e-10,6.6061654e-09\n",
      "Iteration 26470: loss = 3.371908e-10,6.6037105e-09\n",
      "Iteration 26475: loss = 3.371398e-10,6.6012418e-09\n",
      "Iteration 26480: loss = 3.3709224e-10,6.598767e-09\n",
      "Iteration 26485: loss = 3.3704628e-10,6.5962933e-09\n",
      "Iteration 26490: loss = 3.3700057e-10,6.5938206e-09\n",
      "Iteration 26495: loss = 3.3695483e-10,6.591355e-09\n",
      "Iteration 26500: loss = 3.3690772e-10,6.5888957e-09\n",
      "Iteration 26505: loss = 3.368599e-10,6.586445e-09\n",
      "Iteration 26510: loss = 3.3681072e-10,6.583999e-09\n",
      "Iteration 26515: loss = 3.367632e-10,6.5815557e-09\n",
      "Iteration 26520: loss = 3.367225e-10,6.5790973e-09\n",
      "Iteration 26525: loss = 3.3668554e-10,6.576633e-09\n",
      "Iteration 26530: loss = 3.3665057e-10,6.5741688e-09\n",
      "Iteration 26535: loss = 3.366155e-10,6.571709e-09\n",
      "Iteration 26540: loss = 3.3657968e-10,6.5692545e-09\n",
      "Iteration 26545: loss = 3.3654246e-10,6.5668093e-09\n",
      "Iteration 26550: loss = 3.3650394e-10,6.5643704e-09\n",
      "Iteration 26555: loss = 3.364642e-10,6.5619394e-09\n",
      "Iteration 26560: loss = 3.3642303e-10,6.559519e-09\n",
      "Iteration 26565: loss = 3.363798e-10,6.557105e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26570: loss = 3.36335e-10,6.5547017e-09\n",
      "Iteration 26575: loss = 3.3628914e-10,6.552306e-09\n",
      "Iteration 26580: loss = 3.3624104e-10,6.5499193e-09\n",
      "Iteration 26585: loss = 3.3619227e-10,6.547539e-09\n",
      "Iteration 26590: loss = 3.3614178e-10,6.5451684e-09\n",
      "Iteration 26595: loss = 3.3608935e-10,6.5428054e-09\n",
      "Iteration 26600: loss = 3.3404948e-10,6.5404304e-09\n",
      "Iteration 26605: loss = 3.3403094e-10,6.537985e-09\n",
      "Iteration 26610: loss = 3.3403236e-10,6.5354917e-09\n",
      "Iteration 26615: loss = 3.3404468e-10,6.5329724e-09\n",
      "Iteration 26620: loss = 3.3406222e-10,6.5304446e-09\n",
      "Iteration 26625: loss = 3.3408046e-10,6.527916e-09\n",
      "Iteration 26630: loss = 3.3409872e-10,6.525396e-09\n",
      "Iteration 26635: loss = 3.3411496e-10,6.5228836e-09\n",
      "Iteration 26640: loss = 3.341286e-10,6.520382e-09\n",
      "Iteration 26645: loss = 3.3413913e-10,6.5178924e-09\n",
      "Iteration 26650: loss = 3.3414715e-10,6.5154153e-09\n",
      "Iteration 26655: loss = 3.3415115e-10,6.512951e-09\n",
      "Iteration 26660: loss = 3.3415312e-10,6.5104993e-09\n",
      "Iteration 26665: loss = 3.3415126e-10,6.508061e-09\n",
      "Iteration 26670: loss = 3.3414693e-10,6.5056343e-09\n",
      "Iteration 26675: loss = 3.3413874e-10,6.5032206e-09\n",
      "Iteration 26680: loss = 3.3412825e-10,6.500818e-09\n",
      "Iteration 26685: loss = 3.3411443e-10,6.4984276e-09\n",
      "Iteration 26690: loss = 3.34098e-10,6.49605e-09\n",
      "Iteration 26695: loss = 3.3407868e-10,6.493682e-09\n",
      "Iteration 26700: loss = 3.340576e-10,6.4913257e-09\n",
      "Iteration 26705: loss = 3.3403338e-10,6.4889814e-09\n",
      "Iteration 26710: loss = 3.3598893e-10,6.486681e-09\n",
      "Iteration 26715: loss = 3.3592507e-10,6.484448e-09\n",
      "Iteration 26720: loss = 3.358461e-10,6.4822636e-09\n",
      "Iteration 26725: loss = 3.3575778e-10,6.480108e-09\n",
      "Iteration 26730: loss = 3.356642e-10,6.477969e-09\n",
      "Iteration 26735: loss = 3.3556738e-10,6.4758443e-09\n",
      "Iteration 26740: loss = 3.3546924e-10,6.473729e-09\n",
      "Iteration 26745: loss = 3.3537328e-10,6.4716073e-09\n",
      "Iteration 26750: loss = 3.352866e-10,6.4694703e-09\n",
      "Iteration 26755: loss = 3.3520364e-10,6.467325e-09\n",
      "Iteration 26760: loss = 3.351236e-10,6.465177e-09\n",
      "Iteration 26765: loss = 3.3504408e-10,6.4630297e-09\n",
      "Iteration 26770: loss = 3.3297146e-10,6.4608847e-09\n",
      "Iteration 26775: loss = 3.3291114e-10,6.4586962e-09\n",
      "Iteration 26780: loss = 3.3287104e-10,6.4564567e-09\n",
      "Iteration 26785: loss = 3.328415e-10,6.4541905e-09\n",
      "Iteration 26790: loss = 3.328179e-10,6.4519163e-09\n",
      "Iteration 26795: loss = 3.3278857e-10,6.4496586e-09\n",
      "Iteration 26800: loss = 3.3275305e-10,6.447423e-09\n",
      "Iteration 26805: loss = 3.3271264e-10,6.4452053e-09\n",
      "Iteration 26810: loss = 3.3266867e-10,6.443001e-09\n",
      "Iteration 26815: loss = 3.326225e-10,6.4408066e-09\n",
      "Iteration 26820: loss = 3.3257988e-10,6.4386048e-09\n",
      "Iteration 26825: loss = 3.3254346e-10,6.4363928e-09\n",
      "Iteration 26830: loss = 3.3250888e-10,6.4341785e-09\n",
      "Iteration 26835: loss = 3.3247546e-10,6.431965e-09\n",
      "Iteration 26840: loss = 3.3244127e-10,6.4297594e-09\n",
      "Iteration 26845: loss = 3.324061e-10,6.427559e-09\n",
      "Iteration 26850: loss = 3.3237038e-10,6.4253656e-09\n",
      "Iteration 26855: loss = 3.3233136e-10,6.4231824e-09\n",
      "Iteration 26860: loss = 3.3229133e-10,6.421007e-09\n",
      "Iteration 26865: loss = 3.3224956e-10,6.4188423e-09\n",
      "Iteration 26870: loss = 3.3220593e-10,6.416685e-09\n",
      "Iteration 26875: loss = 3.321601e-10,6.4145373e-09\n",
      "Iteration 26880: loss = 3.321128e-10,6.4123986e-09\n",
      "Iteration 26885: loss = 3.3206338e-10,6.4102674e-09\n",
      "Iteration 26890: loss = 3.3201275e-10,6.408147e-09\n",
      "Iteration 26895: loss = 3.3195993e-10,6.406035e-09\n",
      "Iteration 26900: loss = 3.319064e-10,6.4039303e-09\n",
      "Iteration 26905: loss = 3.3185044e-10,6.4018324e-09\n",
      "Iteration 26910: loss = 3.317966e-10,6.3997354e-09\n",
      "Iteration 26915: loss = 3.2976552e-10,6.397623e-09\n",
      "Iteration 26920: loss = 3.297411e-10,6.395452e-09\n",
      "Iteration 26925: loss = 3.2974e-10,6.3932233e-09\n",
      "Iteration 26930: loss = 3.2975098e-10,6.390967e-09\n",
      "Iteration 26935: loss = 3.2976746e-10,6.3886976e-09\n",
      "Iteration 26940: loss = 3.297864e-10,6.3864287e-09\n",
      "Iteration 26945: loss = 3.2980405e-10,6.3841643e-09\n",
      "Iteration 26950: loss = 3.2981995e-10,6.38191e-09\n",
      "Iteration 26955: loss = 3.2983247e-10,6.379667e-09\n",
      "Iteration 26960: loss = 3.2984251e-10,6.377437e-09\n",
      "Iteration 26965: loss = 3.298494e-10,6.375219e-09\n",
      "Iteration 26970: loss = 3.2985326e-10,6.3730132e-09\n",
      "Iteration 26975: loss = 3.298524e-10,6.3708208e-09\n",
      "Iteration 26980: loss = 3.2984926e-10,6.3686425e-09\n",
      "Iteration 26985: loss = 3.2984168e-10,6.366477e-09\n",
      "Iteration 26990: loss = 3.298317e-10,6.364326e-09\n",
      "Iteration 26995: loss = 3.2981853e-10,6.3621854e-09\n",
      "Iteration 27000: loss = 3.2980163e-10,6.3600565e-09\n",
      "Iteration 27005: loss = 3.2978253e-10,6.357941e-09\n",
      "Iteration 27010: loss = 3.2975986e-10,6.3558363e-09\n",
      "Iteration 27015: loss = 3.2973466e-10,6.3537433e-09\n",
      "Iteration 27020: loss = 3.2970648e-10,6.351661e-09\n",
      "Iteration 27025: loss = 3.296764e-10,6.3495893e-09\n",
      "Iteration 27030: loss = 3.296435e-10,6.3475296e-09\n",
      "Iteration 27035: loss = 3.2960815e-10,6.3454793e-09\n",
      "Iteration 27040: loss = 3.2957057e-10,6.3434387e-09\n",
      "Iteration 27045: loss = 3.2953065e-10,6.3414087e-09\n",
      "Iteration 27050: loss = 3.2948858e-10,6.3393872e-09\n",
      "Iteration 27055: loss = 3.2944433e-10,6.3373764e-09\n",
      "Iteration 27060: loss = 3.2939843e-10,6.335375e-09\n",
      "Iteration 27065: loss = 3.2934966e-10,6.333382e-09\n",
      "Iteration 27070: loss = 3.2930034e-10,6.3313963e-09\n",
      "Iteration 27075: loss = 3.2924885e-10,6.3294214e-09\n",
      "Iteration 27080: loss = 3.2919809e-10,6.3274452e-09\n",
      "Iteration 27085: loss = 3.2915548e-10,6.3254526e-09\n",
      "Iteration 27090: loss = 3.291172e-10,6.3234515e-09\n",
      "Iteration 27095: loss = 3.2908032e-10,6.321452e-09\n",
      "Iteration 27100: loss = 3.2904393e-10,6.3194543e-09\n",
      "Iteration 27105: loss = 3.290066e-10,6.317462e-09\n",
      "Iteration 27110: loss = 3.2896694e-10,6.31548e-09\n",
      "Iteration 27115: loss = 3.2892647e-10,6.313504e-09\n",
      "Iteration 27120: loss = 3.288839e-10,6.311538e-09\n",
      "Iteration 27125: loss = 3.2883932e-10,6.309581e-09\n",
      "Iteration 27130: loss = 3.2879288e-10,6.3076318e-09\n",
      "Iteration 27135: loss = 3.287447e-10,6.305692e-09\n",
      "Iteration 27140: loss = 3.267193e-10,6.3037615e-09\n",
      "Iteration 27145: loss = 3.2669392e-10,6.301767e-09\n",
      "Iteration 27150: loss = 3.2867095e-10,6.2997034e-09\n",
      "Iteration 27155: loss = 3.2865474e-10,6.29769e-09\n",
      "Iteration 27160: loss = 3.2861946e-10,6.295733e-09\n",
      "Iteration 27165: loss = 3.2857206e-10,6.2938144e-09\n",
      "Iteration 27170: loss = 3.2851588e-10,6.291922e-09\n",
      "Iteration 27175: loss = 3.2845435e-10,6.2900476e-09\n",
      "Iteration 27180: loss = 3.2839e-10,6.288186e-09\n",
      "Iteration 27185: loss = 3.283305e-10,6.2863115e-09\n",
      "Iteration 27190: loss = 3.2827693e-10,6.2844263e-09\n",
      "Iteration 27195: loss = 3.2822603e-10,6.2825376e-09\n",
      "Iteration 27200: loss = 3.281758e-10,6.280652e-09\n",
      "Iteration 27205: loss = 3.281255e-10,6.27877e-09\n",
      "Iteration 27210: loss = 3.2807426e-10,6.2768946e-09\n",
      "Iteration 27215: loss = 3.280214e-10,6.275026e-09\n",
      "Iteration 27220: loss = 3.2796707e-10,6.2731633e-09\n",
      "Iteration 27225: loss = 3.2791178e-10,6.27131e-09\n",
      "Iteration 27230: loss = 3.2785508e-10,6.269463e-09\n",
      "Iteration 27235: loss = 3.277969e-10,6.2676233e-09\n",
      "Iteration 27240: loss = 3.2773637e-10,6.265793e-09\n",
      "Iteration 27245: loss = 3.2767533e-10,6.2639702e-09\n",
      "Iteration 27250: loss = 3.2761263e-10,6.2621544e-09\n",
      "Iteration 27255: loss = 3.2754832e-10,6.2603456e-09\n",
      "Iteration 27260: loss = 3.274844e-10,6.258539e-09\n",
      "Iteration 27265: loss = 3.274291e-10,6.2567143e-09\n",
      "Iteration 27270: loss = 3.2737876e-10,6.2548766e-09\n",
      "Iteration 27275: loss = 3.25356e-10,6.2530385e-09\n",
      "Iteration 27280: loss = 3.2533645e-10,6.2511254e-09\n",
      "Iteration 27285: loss = 3.253475e-10,6.2491345e-09\n",
      "Iteration 27290: loss = 3.2537462e-10,6.247101e-09\n",
      "Iteration 27295: loss = 3.273852e-10,6.245051e-09\n",
      "Iteration 27300: loss = 3.273957e-10,6.2430714e-09\n",
      "Iteration 27305: loss = 3.254109e-10,6.241149e-09\n",
      "Iteration 27310: loss = 3.2541822e-10,6.239187e-09\n",
      "Iteration 27315: loss = 3.2543437e-10,6.2372e-09\n",
      "Iteration 27320: loss = 3.254551e-10,6.2352066e-09\n",
      "Iteration 27325: loss = 3.25459e-10,6.233262e-09\n",
      "Iteration 27330: loss = 3.254602e-10,6.231329e-09\n",
      "Iteration 27335: loss = 3.25467e-10,6.2293837e-09\n",
      "Iteration 27340: loss = 3.25475e-10,6.227439e-09\n",
      "Iteration 27345: loss = 3.254825e-10,6.2254997e-09\n",
      "Iteration 27350: loss = 3.2548805e-10,6.223568e-09\n",
      "Iteration 27355: loss = 3.254905e-10,6.221649e-09\n",
      "Iteration 27360: loss = 3.2548966e-10,6.219741e-09\n",
      "Iteration 27365: loss = 3.2548578e-10,6.217845e-09\n",
      "Iteration 27370: loss = 3.2547795e-10,6.2159637e-09\n",
      "Iteration 27375: loss = 3.2546774e-10,6.214093e-09\n",
      "Iteration 27380: loss = 3.254529e-10,6.212237e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27385: loss = 3.2543587e-10,6.210392e-09\n",
      "Iteration 27390: loss = 3.2541467e-10,6.20856e-09\n",
      "Iteration 27395: loss = 3.2539105e-10,6.2067396e-09\n",
      "Iteration 27400: loss = 3.2536426e-10,6.20493e-09\n",
      "Iteration 27405: loss = 3.253341e-10,6.203133e-09\n",
      "Iteration 27410: loss = 3.253016e-10,6.201347e-09\n",
      "Iteration 27415: loss = 3.252665e-10,6.199571e-09\n",
      "Iteration 27420: loss = 3.2522873e-10,6.1978054e-09\n",
      "Iteration 27425: loss = 3.2518807e-10,6.196048e-09\n",
      "Iteration 27430: loss = 3.2514658e-10,6.194304e-09\n",
      "Iteration 27435: loss = 3.251014e-10,6.1925673e-09\n",
      "Iteration 27440: loss = 3.250543e-10,6.19084e-09\n",
      "Iteration 27445: loss = 3.2500472e-10,6.189122e-09\n",
      "Iteration 27450: loss = 3.2495387e-10,6.187413e-09\n",
      "Iteration 27455: loss = 3.2490113e-10,6.1857133e-09\n",
      "Iteration 27460: loss = 3.2484562e-10,6.184022e-09\n",
      "Iteration 27465: loss = 3.24796e-10,6.182317e-09\n",
      "Iteration 27470: loss = 3.2475442e-10,6.1805956e-09\n",
      "Iteration 27475: loss = 3.2471534e-10,6.1788703e-09\n",
      "Iteration 27480: loss = 3.2467787e-10,6.177145e-09\n",
      "Iteration 27485: loss = 3.2463968e-10,6.1754237e-09\n",
      "Iteration 27490: loss = 3.2460012e-10,6.17371e-09\n",
      "Iteration 27495: loss = 3.2455919e-10,6.172004e-09\n",
      "Iteration 27500: loss = 3.2451608e-10,6.170305e-09\n",
      "Iteration 27505: loss = 3.244708e-10,6.1686154e-09\n",
      "Iteration 27510: loss = 3.2442427e-10,6.1669363e-09\n",
      "Iteration 27515: loss = 3.2437553e-10,6.165263e-09\n",
      "Iteration 27520: loss = 3.243246e-10,6.1635994e-09\n",
      "Iteration 27525: loss = 3.2427225e-10,6.1619447e-09\n",
      "Iteration 27530: loss = 3.2421776e-10,6.1603e-09\n",
      "Iteration 27535: loss = 3.2219963e-10,6.1586474e-09\n",
      "Iteration 27540: loss = 3.2217348e-10,6.156931e-09\n",
      "Iteration 27545: loss = 3.2216504e-10,6.155169e-09\n",
      "Iteration 27550: loss = 3.2216477e-10,6.153388e-09\n",
      "Iteration 27555: loss = 3.2216815e-10,6.1515992e-09\n",
      "Iteration 27560: loss = 3.2217237e-10,6.1498135e-09\n",
      "Iteration 27565: loss = 3.2217548e-10,6.148034e-09\n",
      "Iteration 27570: loss = 3.2217573e-10,6.146263e-09\n",
      "Iteration 27575: loss = 3.2217298e-10,6.1445053e-09\n",
      "Iteration 27580: loss = 3.22167e-10,6.1427587e-09\n",
      "Iteration 27585: loss = 3.2215774e-10,6.1410255e-09\n",
      "Iteration 27590: loss = 3.2214476e-10,6.139304e-09\n",
      "Iteration 27595: loss = 3.221288e-10,6.1375953e-09\n",
      "Iteration 27600: loss = 3.221089e-10,6.135899e-09\n",
      "Iteration 27605: loss = 3.2208594e-10,6.1342145e-09\n",
      "Iteration 27610: loss = 3.2206005e-10,6.132542e-09\n",
      "Iteration 27615: loss = 3.2203154e-10,6.130879e-09\n",
      "Iteration 27620: loss = 3.2200942e-10,6.129202e-09\n",
      "Iteration 27625: loss = 3.21993e-10,6.1275136e-09\n",
      "Iteration 27630: loss = 3.2197844e-10,6.1258234e-09\n",
      "Iteration 27635: loss = 3.219629e-10,6.124136e-09\n",
      "Iteration 27640: loss = 3.2194658e-10,6.122459e-09\n",
      "Iteration 27645: loss = 3.219275e-10,6.1207888e-09\n",
      "Iteration 27650: loss = 3.2190609e-10,6.1191274e-09\n",
      "Iteration 27655: loss = 3.2188197e-10,6.11748e-09\n",
      "Iteration 27660: loss = 3.2185424e-10,6.115841e-09\n",
      "Iteration 27665: loss = 3.2182434e-10,6.1142145e-09\n",
      "Iteration 27670: loss = 3.2179118e-10,6.1125993e-09\n",
      "Iteration 27675: loss = 3.2175582e-10,6.1109935e-09\n",
      "Iteration 27680: loss = 3.217173e-10,6.1093974e-09\n",
      "Iteration 27685: loss = 3.2167635e-10,6.107813e-09\n",
      "Iteration 27690: loss = 3.2163275e-10,6.1062373e-09\n",
      "Iteration 27695: loss = 3.2158756e-10,6.1046705e-09\n",
      "Iteration 27700: loss = 3.2153957e-10,6.103114e-09\n",
      "Iteration 27705: loss = 3.2148983e-10,6.1015664e-09\n",
      "Iteration 27710: loss = 3.2143802e-10,6.1000263e-09\n",
      "Iteration 27715: loss = 3.2138345e-10,6.0984964e-09\n",
      "Iteration 27720: loss = 3.2132816e-10,6.096974e-09\n",
      "Iteration 27725: loss = 3.212701e-10,6.0954592e-09\n",
      "Iteration 27730: loss = 3.2121042e-10,6.093952e-09\n",
      "Iteration 27735: loss = 3.2114914e-10,6.0924523e-09\n",
      "Iteration 27740: loss = 3.2108824e-10,6.0909566e-09\n",
      "Iteration 27745: loss = 3.210362e-10,6.0894383e-09\n",
      "Iteration 27750: loss = 3.2099004e-10,6.087907e-09\n",
      "Iteration 27755: loss = 3.209464e-10,6.0863727e-09\n",
      "Iteration 27760: loss = 3.209036e-10,6.0848393e-09\n",
      "Iteration 27765: loss = 3.1891287e-10,6.0832837e-09\n",
      "Iteration 27770: loss = 3.1890632e-10,6.081658e-09\n",
      "Iteration 27775: loss = 3.18916e-10,6.0799907e-09\n",
      "Iteration 27780: loss = 3.1893319e-10,6.078305e-09\n",
      "Iteration 27785: loss = 3.1895306e-10,6.0766143e-09\n",
      "Iteration 27790: loss = 3.1896294e-10,6.0749556e-09\n",
      "Iteration 27795: loss = 3.1896366e-10,6.073323e-09\n",
      "Iteration 27800: loss = 3.189691e-10,6.071683e-09\n",
      "Iteration 27805: loss = 3.1897437e-10,6.0700445e-09\n",
      "Iteration 27810: loss = 3.1897843e-10,6.0684138e-09\n",
      "Iteration 27815: loss = 3.189796e-10,6.0667924e-09\n",
      "Iteration 27820: loss = 3.1897815e-10,6.0651812e-09\n",
      "Iteration 27825: loss = 3.1897276e-10,6.0635843e-09\n",
      "Iteration 27830: loss = 3.1896397e-10,6.062e-09\n",
      "Iteration 27835: loss = 3.1895164e-10,6.0604264e-09\n",
      "Iteration 27840: loss = 3.189359e-10,6.058867e-09\n",
      "Iteration 27845: loss = 3.1891614e-10,6.0573204e-09\n",
      "Iteration 27850: loss = 3.188936e-10,6.055786e-09\n",
      "Iteration 27855: loss = 3.1886763e-10,6.0542624e-09\n",
      "Iteration 27860: loss = 3.188383e-10,6.0527494e-09\n",
      "Iteration 27865: loss = 3.1880606e-10,6.05125e-09\n",
      "Iteration 27870: loss = 3.187712e-10,6.049759e-09\n",
      "Iteration 27875: loss = 3.1873304e-10,6.04828e-09\n",
      "Iteration 27880: loss = 3.1869216e-10,6.046811e-09\n",
      "Iteration 27885: loss = 3.18649e-10,6.045351e-09\n",
      "Iteration 27890: loss = 3.186037e-10,6.043903e-09\n",
      "Iteration 27895: loss = 3.1855574e-10,6.0424616e-09\n",
      "Iteration 27900: loss = 3.1850542e-10,6.041028e-09\n",
      "Iteration 27905: loss = 3.1845304e-10,6.0396053e-09\n",
      "Iteration 27910: loss = 3.183995e-10,6.0381904e-09\n",
      "Iteration 27915: loss = 3.183428e-10,6.0367835e-09\n",
      "Iteration 27920: loss = 3.1828473e-10,6.0353846e-09\n",
      "Iteration 27925: loss = 3.182288e-10,6.033984e-09\n",
      "Iteration 27930: loss = 3.1818173e-10,6.0325602e-09\n",
      "Iteration 27935: loss = 3.1814043e-10,6.031126e-09\n",
      "Iteration 27940: loss = 3.181008e-10,6.029688e-09\n",
      "Iteration 27945: loss = 3.1806133e-10,6.0282526e-09\n",
      "Iteration 27950: loss = 3.1802036e-10,6.0268244e-09\n",
      "Iteration 27955: loss = 3.1797878e-10,6.0254006e-09\n",
      "Iteration 27960: loss = 3.1793512e-10,6.0239853e-09\n",
      "Iteration 27965: loss = 3.1788938e-10,6.0225775e-09\n",
      "Iteration 27970: loss = 3.1784167e-10,6.0211796e-09\n",
      "Iteration 27975: loss = 3.1779224e-10,6.0197896e-09\n",
      "Iteration 27980: loss = 3.177395e-10,6.018407e-09\n",
      "Iteration 27985: loss = 3.1768602e-10,6.017035e-09\n",
      "Iteration 27990: loss = 3.1763003e-10,6.0156697e-09\n",
      "Iteration 27995: loss = 3.1757266e-10,6.0143113e-09\n",
      "Iteration 28000: loss = 3.1556668e-10,6.012961e-09\n",
      "Iteration 28005: loss = 3.1552813e-10,6.0115566e-09\n",
      "Iteration 28010: loss = 3.155133e-10,6.0100884e-09\n",
      "Iteration 28015: loss = 3.1551184e-10,6.008589e-09\n",
      "Iteration 28020: loss = 3.1551664e-10,6.0070757e-09\n",
      "Iteration 28025: loss = 3.155226e-10,6.0055614e-09\n",
      "Iteration 28030: loss = 3.1552796e-10,6.0040506e-09\n",
      "Iteration 28035: loss = 3.1553135e-10,6.002548e-09\n",
      "Iteration 28040: loss = 3.1553116e-10,6.0010588e-09\n",
      "Iteration 28045: loss = 3.155278e-10,5.9995813e-09\n",
      "Iteration 28050: loss = 3.1552036e-10,5.998116e-09\n",
      "Iteration 28055: loss = 3.1550992e-10,5.9966636e-09\n",
      "Iteration 28060: loss = 3.1549485e-10,5.995224e-09\n",
      "Iteration 28065: loss = 3.1547695e-10,5.9937966e-09\n",
      "Iteration 28070: loss = 3.1545458e-10,5.9923835e-09\n",
      "Iteration 28075: loss = 3.154295e-10,5.990981e-09\n",
      "Iteration 28080: loss = 3.15401e-10,5.9895897e-09\n",
      "Iteration 28085: loss = 3.153694e-10,5.988209e-09\n",
      "Iteration 28090: loss = 3.1533867e-10,5.98683e-09\n",
      "Iteration 28095: loss = 3.1531636e-10,5.9854304e-09\n",
      "Iteration 28100: loss = 3.1529768e-10,5.984022e-09\n",
      "Iteration 28105: loss = 3.1528102e-10,5.9826135e-09\n",
      "Iteration 28110: loss = 3.1526312e-10,5.981209e-09\n",
      "Iteration 28115: loss = 3.152436e-10,5.9798126e-09\n",
      "Iteration 28120: loss = 3.1522152e-10,5.9784253e-09\n",
      "Iteration 28125: loss = 3.1519642e-10,5.9770473e-09\n",
      "Iteration 28130: loss = 3.1516878e-10,5.9756813e-09\n",
      "Iteration 28135: loss = 3.1513803e-10,5.974324e-09\n",
      "Iteration 28140: loss = 3.151044e-10,5.9729794e-09\n",
      "Iteration 28145: loss = 3.1506822e-10,5.9716445e-09\n",
      "Iteration 28150: loss = 3.1502925e-10,5.97032e-09\n",
      "Iteration 28155: loss = 3.149877e-10,5.969004e-09\n",
      "Iteration 28160: loss = 3.1494363e-10,5.9676974e-09\n",
      "Iteration 28165: loss = 3.1489666e-10,5.9664016e-09\n",
      "Iteration 28170: loss = 3.1484745e-10,5.9651137e-09\n",
      "Iteration 28175: loss = 3.1479633e-10,5.9638348e-09\n",
      "Iteration 28180: loss = 3.1474323e-10,5.962564e-09\n",
      "Iteration 28185: loss = 3.1468764e-10,5.9613012e-09\n",
      "Iteration 28190: loss = 3.146312e-10,5.960046e-09\n",
      "Iteration 28195: loss = 3.1457162e-10,5.9587983e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28200: loss = 3.1451128e-10,5.9575584e-09\n",
      "Iteration 28205: loss = 3.1444827e-10,5.956325e-09\n",
      "Iteration 28210: loss = 3.1438419e-10,5.955099e-09\n",
      "Iteration 28215: loss = 3.1431954e-10,5.953878e-09\n",
      "Iteration 28220: loss = 3.1426364e-10,5.9526353e-09\n",
      "Iteration 28225: loss = 3.1421576e-10,5.951373e-09\n",
      "Iteration 28230: loss = 3.1417113e-10,5.950104e-09\n",
      "Iteration 28235: loss = 3.141273e-10,5.9488334e-09\n",
      "Iteration 28240: loss = 3.1408376e-10,5.9475687e-09\n",
      "Iteration 28245: loss = 3.1211186e-10,5.94628e-09\n",
      "Iteration 28250: loss = 3.1210626e-10,5.944913e-09\n",
      "Iteration 28255: loss = 3.1211886e-10,5.9435017e-09\n",
      "Iteration 28260: loss = 3.1213873e-10,5.9420717e-09\n",
      "Iteration 28265: loss = 3.1408268e-10,5.9406804e-09\n",
      "Iteration 28270: loss = 3.1213232e-10,5.939346e-09\n",
      "Iteration 28275: loss = 3.1213088e-10,5.9379817e-09\n",
      "Iteration 28280: loss = 3.1213587e-10,5.936602e-09\n",
      "Iteration 28285: loss = 3.1214278e-10,5.9352177e-09\n",
      "Iteration 28290: loss = 3.1214611e-10,5.933849e-09\n",
      "Iteration 28295: loss = 3.121442e-10,5.932496e-09\n",
      "Iteration 28300: loss = 3.1214306e-10,5.931144e-09\n",
      "Iteration 28305: loss = 3.1213862e-10,5.9298015e-09\n",
      "Iteration 28310: loss = 3.121321e-10,5.928469e-09\n",
      "Iteration 28315: loss = 3.121222e-10,5.927148e-09\n",
      "Iteration 28320: loss = 3.1210812e-10,5.9258394e-09\n",
      "Iteration 28325: loss = 3.1209116e-10,5.924543e-09\n",
      "Iteration 28330: loss = 3.120705e-10,5.9232588e-09\n",
      "Iteration 28335: loss = 3.120464e-10,5.9219847e-09\n",
      "Iteration 28340: loss = 3.1201883e-10,5.9207244e-09\n",
      "Iteration 28345: loss = 3.119883e-10,5.919474e-09\n",
      "Iteration 28350: loss = 3.119549e-10,5.9182335e-09\n",
      "Iteration 28355: loss = 3.119186e-10,5.9170047e-09\n",
      "Iteration 28360: loss = 3.118785e-10,5.9157856e-09\n",
      "Iteration 28365: loss = 3.1183678e-10,5.9145746e-09\n",
      "Iteration 28370: loss = 3.117925e-10,5.9133756e-09\n",
      "Iteration 28375: loss = 3.117452e-10,5.9121845e-09\n",
      "Iteration 28380: loss = 3.1169578e-10,5.9110015e-09\n",
      "Iteration 28385: loss = 3.1164393e-10,5.909828e-09\n",
      "Iteration 28390: loss = 3.1159064e-10,5.908662e-09\n",
      "Iteration 28395: loss = 3.1153477e-10,5.907504e-09\n",
      "Iteration 28400: loss = 3.1147648e-10,5.906355e-09\n",
      "Iteration 28405: loss = 3.1141778e-10,5.9052123e-09\n",
      "Iteration 28410: loss = 3.1135702e-10,5.904076e-09\n",
      "Iteration 28415: loss = 3.1130307e-10,5.9029226e-09\n",
      "Iteration 28420: loss = 3.112579e-10,5.9017458e-09\n",
      "Iteration 28425: loss = 3.112173e-10,5.9005605e-09\n",
      "Iteration 28430: loss = 3.1117853e-10,5.899376e-09\n",
      "Iteration 28435: loss = 3.092122e-10,5.898188e-09\n",
      "Iteration 28440: loss = 3.0920314e-10,5.8969256e-09\n",
      "Iteration 28445: loss = 3.0921568e-10,5.8956027e-09\n",
      "Iteration 28450: loss = 3.1115102e-10,5.894298e-09\n",
      "Iteration 28455: loss = 3.0920697e-10,5.893061e-09\n",
      "Iteration 28460: loss = 3.0920314e-10,5.891791e-09\n",
      "Iteration 28465: loss = 3.1113925e-10,5.8904925e-09\n",
      "Iteration 28470: loss = 3.0920072e-10,5.8892455e-09\n",
      "Iteration 28475: loss = 3.0919972e-10,5.8879763e-09\n",
      "Iteration 28480: loss = 3.1113187e-10,5.8866947e-09\n",
      "Iteration 28485: loss = 3.0919808e-10,5.8854432e-09\n",
      "Iteration 28490: loss = 3.0919833e-10,5.884175e-09\n",
      "Iteration 28495: loss = 3.0919947e-10,5.8829084e-09\n",
      "Iteration 28500: loss = 3.0919417e-10,5.8816614e-09\n",
      "Iteration 28505: loss = 3.0918718e-10,5.8804224e-09\n",
      "Iteration 28510: loss = 3.0918032e-10,5.8791843e-09\n",
      "Iteration 28515: loss = 3.0917083e-10,5.877955e-09\n",
      "Iteration 28520: loss = 3.0915917e-10,5.876736e-09\n",
      "Iteration 28525: loss = 3.0914368e-10,5.875528e-09\n",
      "Iteration 28530: loss = 3.091248e-10,5.8743312e-09\n",
      "Iteration 28535: loss = 3.0910297e-10,5.8731464e-09\n",
      "Iteration 28540: loss = 3.090775e-10,5.8719727e-09\n",
      "Iteration 28545: loss = 3.0904906e-10,5.8708096e-09\n",
      "Iteration 28550: loss = 3.0901745e-10,5.8696594e-09\n",
      "Iteration 28555: loss = 3.0898267e-10,5.8685172e-09\n",
      "Iteration 28560: loss = 3.0894473e-10,5.8673852e-09\n",
      "Iteration 28565: loss = 3.0890437e-10,5.8662635e-09\n",
      "Iteration 28570: loss = 3.0886124e-10,5.8651515e-09\n",
      "Iteration 28575: loss = 3.0881567e-10,5.8640492e-09\n",
      "Iteration 28580: loss = 3.0876743e-10,5.8629546e-09\n",
      "Iteration 28585: loss = 3.0871738e-10,5.8618683e-09\n",
      "Iteration 28590: loss = 3.0866512e-10,5.8607914e-09\n",
      "Iteration 28595: loss = 3.066977e-10,5.8597047e-09\n",
      "Iteration 28600: loss = 3.0667882e-10,5.8585403e-09\n",
      "Iteration 28605: loss = 3.0668074e-10,5.857321e-09\n",
      "Iteration 28610: loss = 3.0669303e-10,5.856074e-09\n",
      "Iteration 28615: loss = 3.0670994e-10,5.854819e-09\n",
      "Iteration 28620: loss = 3.0863953e-10,5.8535807e-09\n",
      "Iteration 28625: loss = 3.0670846e-10,5.8524092e-09\n",
      "Iteration 28630: loss = 3.0670325e-10,5.8512195e-09\n",
      "Iteration 28635: loss = 3.0670216e-10,5.8500236e-09\n",
      "Iteration 28640: loss = 3.0670178e-10,5.848825e-09\n",
      "Iteration 28645: loss = 3.0669978e-10,5.8476335e-09\n",
      "Iteration 28650: loss = 3.0669547e-10,5.8464504e-09\n",
      "Iteration 28655: loss = 3.0668842e-10,5.845278e-09\n",
      "Iteration 28660: loss = 3.0667702e-10,5.8441167e-09\n",
      "Iteration 28665: loss = 3.0666278e-10,5.8429688e-09\n",
      "Iteration 28670: loss = 3.066439e-10,5.841833e-09\n",
      "Iteration 28675: loss = 3.066218e-10,5.840709e-09\n",
      "Iteration 28680: loss = 3.0659708e-10,5.839596e-09\n",
      "Iteration 28685: loss = 3.0656813e-10,5.8384937e-09\n",
      "Iteration 28690: loss = 3.0653638e-10,5.8374034e-09\n",
      "Iteration 28695: loss = 3.0650085e-10,5.836323e-09\n",
      "Iteration 28700: loss = 3.0646355e-10,5.835253e-09\n",
      "Iteration 28705: loss = 3.0642305e-10,5.834192e-09\n",
      "Iteration 28710: loss = 3.0638908e-10,5.8331144e-09\n",
      "Iteration 28715: loss = 3.0636374e-10,5.8320175e-09\n",
      "Iteration 28720: loss = 3.0634215e-10,5.8309118e-09\n",
      "Iteration 28725: loss = 3.0632055e-10,5.8298055e-09\n",
      "Iteration 28730: loss = 3.0629835e-10,5.828706e-09\n",
      "Iteration 28735: loss = 3.0627384e-10,5.8276144e-09\n",
      "Iteration 28740: loss = 3.0624744e-10,5.8265286e-09\n",
      "Iteration 28745: loss = 3.0621827e-10,5.825453e-09\n",
      "Iteration 28750: loss = 3.0618658e-10,5.8243876e-09\n",
      "Iteration 28755: loss = 3.0615163e-10,5.8233316e-09\n",
      "Iteration 28760: loss = 3.0611416e-10,5.822285e-09\n",
      "Iteration 28765: loss = 3.0607383e-10,5.8212484e-09\n",
      "Iteration 28770: loss = 3.0603123e-10,5.82022e-09\n",
      "Iteration 28775: loss = 3.0598574e-10,5.819202e-09\n",
      "Iteration 28780: loss = 3.0593797e-10,5.8181913e-09\n",
      "Iteration 28785: loss = 3.0588804e-10,5.817189e-09\n",
      "Iteration 28790: loss = 3.0583644e-10,5.816195e-09\n",
      "Iteration 28795: loss = 3.0578187e-10,5.8152074e-09\n",
      "Iteration 28800: loss = 3.0572556e-10,5.814229e-09\n",
      "Iteration 28805: loss = 3.05668e-10,5.8132557e-09\n",
      "Iteration 28810: loss = 3.056088e-10,5.81229e-09\n",
      "Iteration 28815: loss = 3.0554734e-10,5.811331e-09\n",
      "Iteration 28820: loss = 3.054849e-10,5.8103775e-09\n",
      "Iteration 28825: loss = 3.0542058e-10,5.8094316e-09\n",
      "Iteration 28830: loss = 3.0535494e-10,5.8084892e-09\n",
      "Iteration 28835: loss = 3.052902e-10,5.807548e-09\n",
      "Iteration 28840: loss = 3.0523667e-10,5.8065788e-09\n",
      "Iteration 28845: loss = 3.0519073e-10,5.80559e-09\n",
      "Iteration 28850: loss = 3.0514827e-10,5.804592e-09\n",
      "Iteration 28855: loss = 3.051066e-10,5.8035963e-09\n",
      "Iteration 28860: loss = 3.0315936e-10,5.802593e-09\n",
      "Iteration 28865: loss = 3.031614e-10,5.801481e-09\n",
      "Iteration 28870: loss = 3.0508238e-10,5.8003424e-09\n",
      "Iteration 28875: loss = 3.0316097e-10,5.799273e-09\n",
      "Iteration 28880: loss = 3.0507516e-10,5.7981593e-09\n",
      "Iteration 28885: loss = 3.0315658e-10,5.797087e-09\n",
      "Iteration 28890: loss = 3.0507408e-10,5.795966e-09\n",
      "Iteration 28895: loss = 3.0315503e-10,5.7948997e-09\n",
      "Iteration 28900: loss = 3.0506864e-10,5.7937926e-09\n",
      "Iteration 28905: loss = 3.031515e-10,5.7927254e-09\n",
      "Iteration 28910: loss = 3.050666e-10,5.7916205e-09\n",
      "Iteration 28915: loss = 3.0314537e-10,5.7905645e-09\n",
      "Iteration 28920: loss = 3.0506067e-10,5.789463e-09\n",
      "Iteration 28925: loss = 3.0314878e-10,5.7883875e-09\n",
      "Iteration 28930: loss = 3.0314473e-10,5.7873164e-09\n",
      "Iteration 28935: loss = 3.031409e-10,5.7862497e-09\n",
      "Iteration 28940: loss = 3.050549e-10,5.7851577e-09\n",
      "Iteration 28945: loss = 3.031385e-10,5.7841016e-09\n",
      "Iteration 28950: loss = 3.0314132e-10,5.78302e-09\n",
      "Iteration 28955: loss = 3.0313893e-10,5.7819554e-09\n",
      "Iteration 28960: loss = 3.05049e-10,5.780882e-09\n",
      "Iteration 28965: loss = 3.0313127e-10,5.779838e-09\n",
      "Iteration 28970: loss = 3.0312622e-10,5.7787855e-09\n",
      "Iteration 28975: loss = 3.031219e-10,5.7777325e-09\n",
      "Iteration 28980: loss = 3.031165e-10,5.7766867e-09\n",
      "Iteration 28985: loss = 3.0310834e-10,5.7756475e-09\n",
      "Iteration 28990: loss = 3.0309713e-10,5.77462e-09\n",
      "Iteration 28995: loss = 3.0308275e-10,5.7736034e-09\n",
      "Iteration 29000: loss = 3.030645e-10,5.7725975e-09\n",
      "Iteration 29005: loss = 3.0304315e-10,5.7716036e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29010: loss = 3.0301844e-10,5.77062e-09\n",
      "Iteration 29015: loss = 3.0298958e-10,5.7696483e-09\n",
      "Iteration 29020: loss = 3.0295857e-10,5.768687e-09\n",
      "Iteration 29025: loss = 3.0292388e-10,5.7677347e-09\n",
      "Iteration 29030: loss = 3.028866e-10,5.7667933e-09\n",
      "Iteration 29035: loss = 3.028468e-10,5.7658607e-09\n",
      "Iteration 29040: loss = 3.0280398e-10,5.764937e-09\n",
      "Iteration 29045: loss = 3.0275912e-10,5.764023e-09\n",
      "Iteration 29050: loss = 3.0271138e-10,5.7631167e-09\n",
      "Iteration 29055: loss = 3.026636e-10,5.7622094e-09\n",
      "Iteration 29060: loss = 3.026274e-10,5.761277e-09\n",
      "Iteration 29065: loss = 3.0259797e-10,5.7603278e-09\n",
      "Iteration 29070: loss = 3.0257086e-10,5.7593716e-09\n",
      "Iteration 29075: loss = 3.0254468e-10,5.7584155e-09\n",
      "Iteration 29080: loss = 3.0251732e-10,5.757465e-09\n",
      "Iteration 29085: loss = 3.0248834e-10,5.7565224e-09\n",
      "Iteration 29090: loss = 3.0245648e-10,5.755586e-09\n",
      "Iteration 29095: loss = 3.0242256e-10,5.7546576e-09\n",
      "Iteration 29100: loss = 3.004982e-10,5.7537077e-09\n",
      "Iteration 29105: loss = 3.005056e-10,5.752672e-09\n",
      "Iteration 29110: loss = 3.0053146e-10,5.7515854e-09\n",
      "Iteration 29115: loss = 3.0245942e-10,5.750496e-09\n",
      "Iteration 29120: loss = 3.0055664e-10,5.749494e-09\n",
      "Iteration 29125: loss = 3.005555e-10,5.7484892e-09\n",
      "Iteration 29130: loss = 3.0056743e-10,5.747446e-09\n",
      "Iteration 29135: loss = 3.0058592e-10,5.7463914e-09\n",
      "Iteration 29140: loss = 3.0059719e-10,5.7453566e-09\n",
      "Iteration 29145: loss = 3.0060418e-10,5.744337e-09\n",
      "Iteration 29150: loss = 3.0061353e-10,5.74331e-09\n",
      "Iteration 29155: loss = 3.0062183e-10,5.7422884e-09\n",
      "Iteration 29160: loss = 3.0062866e-10,5.7412737e-09\n",
      "Iteration 29165: loss = 3.006325e-10,5.7402674e-09\n",
      "Iteration 29170: loss = 3.0063255e-10,5.7392744e-09\n",
      "Iteration 29175: loss = 3.0062805e-10,5.7382947e-09\n",
      "Iteration 29180: loss = 3.0062028e-10,5.737327e-09\n",
      "Iteration 29185: loss = 3.00608e-10,5.7363714e-09\n",
      "Iteration 29190: loss = 3.0059197e-10,5.7354295e-09\n",
      "Iteration 29195: loss = 3.0057215e-10,5.734499e-09\n",
      "Iteration 29200: loss = 3.0054878e-10,5.73358e-09\n",
      "Iteration 29205: loss = 3.0052158e-10,5.7326717e-09\n",
      "Iteration 29210: loss = 3.0049177e-10,5.731774e-09\n",
      "Iteration 29215: loss = 3.0045824e-10,5.7308864e-09\n",
      "Iteration 29220: loss = 3.0042246e-10,5.7300102e-09\n",
      "Iteration 29225: loss = 3.00383e-10,5.7291416e-09\n",
      "Iteration 29230: loss = 3.0034158e-10,5.7282814e-09\n",
      "Iteration 29235: loss = 3.0029743e-10,5.7274314e-09\n",
      "Iteration 29240: loss = 3.0026123e-10,5.7265606e-09\n",
      "Iteration 29245: loss = 3.002335e-10,5.7256675e-09\n",
      "Iteration 29250: loss = 3.0021097e-10,5.724762e-09\n",
      "Iteration 29255: loss = 3.00189e-10,5.723859e-09\n",
      "Iteration 29260: loss = 3.0016614e-10,5.722958e-09\n",
      "Iteration 29265: loss = 3.001425e-10,5.7220633e-09\n",
      "Iteration 29270: loss = 3.001163e-10,5.7211746e-09\n",
      "Iteration 29275: loss = 3.0008757e-10,5.720295e-09\n",
      "Iteration 29280: loss = 3.0005534e-10,5.7194263e-09\n",
      "Iteration 29285: loss = 3.0002106e-10,5.7185656e-09\n",
      "Iteration 29290: loss = 2.9998412e-10,5.7177143e-09\n",
      "Iteration 29295: loss = 2.999445e-10,5.716871e-09\n",
      "Iteration 29300: loss = 2.999023e-10,5.7160365e-09\n",
      "Iteration 29305: loss = 2.9985756e-10,5.7152105e-09\n",
      "Iteration 29310: loss = 2.9981054e-10,5.714392e-09\n",
      "Iteration 29315: loss = 2.9976185e-10,5.7135807e-09\n",
      "Iteration 29320: loss = 2.9971056e-10,5.7127783e-09\n",
      "Iteration 29325: loss = 2.996575e-10,5.71198e-09\n",
      "Iteration 29330: loss = 2.9960298e-10,5.7111915e-09\n",
      "Iteration 29335: loss = 2.9954675e-10,5.710407e-09\n",
      "Iteration 29340: loss = 2.9760452e-10,5.709614e-09\n",
      "Iteration 29345: loss = 2.9758657e-10,5.7087277e-09\n",
      "Iteration 29350: loss = 2.9759076e-10,5.7077822e-09\n",
      "Iteration 29355: loss = 2.9760752e-10,5.706804e-09\n",
      "Iteration 29360: loss = 2.995132e-10,5.7058336e-09\n",
      "Iteration 29365: loss = 2.976095e-10,5.7049414e-09\n",
      "Iteration 29370: loss = 2.976048e-10,5.7040266e-09\n",
      "Iteration 29375: loss = 2.9760858e-10,5.703094e-09\n",
      "Iteration 29380: loss = 2.9760452e-10,5.70218e-09\n",
      "Iteration 29385: loss = 2.9759875e-10,5.701276e-09\n",
      "Iteration 29390: loss = 2.9759503e-10,5.700364e-09\n",
      "Iteration 29395: loss = 2.9759287e-10,5.699451e-09\n",
      "Iteration 29400: loss = 2.9758898e-10,5.6985465e-09\n",
      "Iteration 29405: loss = 2.9758265e-10,5.6976464e-09\n",
      "Iteration 29410: loss = 2.975737e-10,5.6967586e-09\n",
      "Iteration 29415: loss = 2.9756075e-10,5.695881e-09\n",
      "Iteration 29420: loss = 2.9754457e-10,5.695016e-09\n",
      "Iteration 29425: loss = 2.9752442e-10,5.694162e-09\n",
      "Iteration 29430: loss = 2.9750072e-10,5.6933174e-09\n",
      "Iteration 29435: loss = 2.9747377e-10,5.6924843e-09\n",
      "Iteration 29440: loss = 2.9744435e-10,5.691661e-09\n",
      "Iteration 29445: loss = 2.9741143e-10,5.6908487e-09\n",
      "Iteration 29450: loss = 2.9737565e-10,5.690044e-09\n",
      "Iteration 29455: loss = 2.9733724e-10,5.689249e-09\n",
      "Iteration 29460: loss = 2.97296e-10,5.6884635e-09\n",
      "Iteration 29465: loss = 2.972527e-10,5.687686e-09\n",
      "Iteration 29470: loss = 2.9721098e-10,5.6869034e-09\n",
      "Iteration 29475: loss = 2.9718114e-10,5.6860916e-09\n",
      "Iteration 29480: loss = 2.9715777e-10,5.685263e-09\n",
      "Iteration 29485: loss = 2.971368e-10,5.68443e-09\n",
      "Iteration 29490: loss = 2.9711644e-10,5.683596e-09\n",
      "Iteration 29495: loss = 2.9709393e-10,5.6827703e-09\n",
      "Iteration 29500: loss = 2.9706956e-10,5.6819505e-09\n",
      "Iteration 29505: loss = 2.9704286e-10,5.6811387e-09\n",
      "Iteration 29510: loss = 2.9701244e-10,5.6803375e-09\n",
      "Iteration 29515: loss = 2.969805e-10,5.679543e-09\n",
      "Iteration 29520: loss = 2.9694555e-10,5.6787592e-09\n",
      "Iteration 29525: loss = 2.9690853e-10,5.6779825e-09\n",
      "Iteration 29530: loss = 2.9686797e-10,5.6772156e-09\n",
      "Iteration 29535: loss = 2.9682526e-10,5.6764557e-09\n",
      "Iteration 29540: loss = 2.9678068e-10,5.675703e-09\n",
      "Iteration 29545: loss = 2.9673305e-10,5.6749596e-09\n",
      "Iteration 29550: loss = 2.9668445e-10,5.6742224e-09\n",
      "Iteration 29555: loss = 2.9663294e-10,5.673493e-09\n",
      "Iteration 29560: loss = 2.9658023e-10,5.672769e-09\n",
      "Iteration 29565: loss = 2.9652558e-10,5.6720513e-09\n",
      "Iteration 29570: loss = 2.964691e-10,5.6713403e-09\n",
      "Iteration 29575: loss = 2.9641153e-10,5.6706337e-09\n",
      "Iteration 29580: loss = 2.963517e-10,5.6699343e-09\n",
      "Iteration 29585: loss = 2.9629124e-10,5.6692397e-09\n",
      "Iteration 29590: loss = 2.9622935e-10,5.668549e-09\n",
      "Iteration 29595: loss = 2.9428796e-10,5.6678577e-09\n",
      "Iteration 29600: loss = 2.9425914e-10,5.66708e-09\n",
      "Iteration 29605: loss = 2.9425742e-10,5.6662297e-09\n",
      "Iteration 29610: loss = 2.9426975e-10,5.665342e-09\n",
      "Iteration 29615: loss = 2.942888e-10,5.664439e-09\n",
      "Iteration 29620: loss = 2.9430966e-10,5.663531e-09\n",
      "Iteration 29625: loss = 2.9432853e-10,5.6626304e-09\n",
      "Iteration 29630: loss = 2.943447e-10,5.6617386e-09\n",
      "Iteration 29635: loss = 2.9435746e-10,5.6608576e-09\n",
      "Iteration 29640: loss = 2.9436573e-10,5.659991e-09\n",
      "Iteration 29645: loss = 2.9436914e-10,5.659138e-09\n",
      "Iteration 29650: loss = 2.943686e-10,5.658298e-09\n",
      "Iteration 29655: loss = 2.943635e-10,5.657471e-09\n",
      "Iteration 29660: loss = 2.9435396e-10,5.6566583e-09\n",
      "Iteration 29665: loss = 2.943403e-10,5.6558562e-09\n",
      "Iteration 29670: loss = 2.9432348e-10,5.6550675e-09\n",
      "Iteration 29675: loss = 2.943025e-10,5.6542895e-09\n",
      "Iteration 29680: loss = 2.9427813e-10,5.6535234e-09\n",
      "Iteration 29685: loss = 2.942503e-10,5.652767e-09\n",
      "Iteration 29690: loss = 2.9421968e-10,5.6520215e-09\n",
      "Iteration 29695: loss = 2.941857e-10,5.6512834e-09\n",
      "Iteration 29700: loss = 2.941512e-10,5.6505502e-09\n",
      "Iteration 29705: loss = 2.9412833e-10,5.649786e-09\n",
      "Iteration 29710: loss = 2.941128e-10,5.649004e-09\n",
      "Iteration 29715: loss = 2.9409947e-10,5.6482143e-09\n",
      "Iteration 29720: loss = 2.940871e-10,5.6474274e-09\n",
      "Iteration 29725: loss = 2.9407263e-10,5.646646e-09\n",
      "Iteration 29730: loss = 2.9405592e-10,5.645871e-09\n",
      "Iteration 29735: loss = 2.940371e-10,5.6451044e-09\n",
      "Iteration 29740: loss = 2.940148e-10,5.644349e-09\n",
      "Iteration 29745: loss = 2.9398994e-10,5.643601e-09\n",
      "Iteration 29750: loss = 2.9210254e-10,5.6428315e-09\n",
      "Iteration 29755: loss = 2.939857e-10,5.6419855e-09\n",
      "Iteration 29760: loss = 2.939807e-10,5.6411884e-09\n",
      "Iteration 29765: loss = 2.9209998e-10,5.6404037e-09\n",
      "Iteration 29770: loss = 2.9397654e-10,5.63958e-09\n",
      "Iteration 29775: loss = 2.9209457e-10,5.6388036e-09\n",
      "Iteration 29780: loss = 2.93969e-10,5.6379887e-09\n",
      "Iteration 29785: loss = 2.9396605e-10,5.637192e-09\n",
      "Iteration 29790: loss = 2.9209124e-10,5.636401e-09\n",
      "Iteration 29795: loss = 2.939618e-10,5.635599e-09\n",
      "Iteration 29800: loss = 2.9396008e-10,5.634804e-09\n",
      "Iteration 29805: loss = 2.920841e-10,5.6340186e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29810: loss = 2.9208094e-10,5.63323e-09\n",
      "Iteration 29815: loss = 2.9207994e-10,5.632436e-09\n",
      "Iteration 29820: loss = 2.939505e-10,5.631643e-09\n",
      "Iteration 29825: loss = 2.9207384e-10,5.6308638e-09\n",
      "Iteration 29830: loss = 2.920727e-10,5.6300755e-09\n",
      "Iteration 29835: loss = 2.920676e-10,5.629297e-09\n",
      "Iteration 29840: loss = 2.9206584e-10,5.628513e-09\n",
      "Iteration 29845: loss = 2.9206318e-10,5.627732e-09\n",
      "Iteration 29850: loss = 2.920587e-10,5.626956e-09\n",
      "Iteration 29855: loss = 2.9205172e-10,5.62619e-09\n",
      "Iteration 29860: loss = 2.9204164e-10,5.6254335e-09\n",
      "Iteration 29865: loss = 2.9202823e-10,5.6246874e-09\n",
      "Iteration 29870: loss = 2.9201128e-10,5.6239524e-09\n",
      "Iteration 29875: loss = 2.9199101e-10,5.6232246e-09\n",
      "Iteration 29880: loss = 2.9196717e-10,5.6225105e-09\n",
      "Iteration 29885: loss = 2.919397e-10,5.6218075e-09\n",
      "Iteration 29890: loss = 2.919098e-10,5.621113e-09\n",
      "Iteration 29895: loss = 2.9187724e-10,5.6204277e-09\n",
      "Iteration 29900: loss = 2.918408e-10,5.619751e-09\n",
      "Iteration 29905: loss = 2.918026e-10,5.6190816e-09\n",
      "Iteration 29910: loss = 2.9176225e-10,5.61842e-09\n",
      "Iteration 29915: loss = 2.9171868e-10,5.6177676e-09\n",
      "Iteration 29920: loss = 2.9167394e-10,5.617121e-09\n",
      "Iteration 29925: loss = 2.9162647e-10,5.6164824e-09\n",
      "Iteration 29930: loss = 2.915773e-10,5.6158496e-09\n",
      "Iteration 29935: loss = 2.9152633e-10,5.6152225e-09\n",
      "Iteration 29940: loss = 2.9147393e-10,5.614602e-09\n",
      "Iteration 29945: loss = 2.9141914e-10,5.6139866e-09\n",
      "Iteration 29950: loss = 2.913746e-10,5.613347e-09\n",
      "Iteration 29955: loss = 2.913411e-10,5.6126797e-09\n",
      "Iteration 29960: loss = 2.9131297e-10,5.611995e-09\n",
      "Iteration 29965: loss = 2.912872e-10,5.611308e-09\n",
      "Iteration 29970: loss = 2.9126118e-10,5.6106235e-09\n",
      "Iteration 29975: loss = 2.912339e-10,5.6099423e-09\n",
      "Iteration 29980: loss = 2.9120475e-10,5.609267e-09\n",
      "Iteration 29985: loss = 2.9117342e-10,5.608599e-09\n",
      "Iteration 29990: loss = 2.9113975e-10,5.607938e-09\n",
      "Iteration 29995: loss = 2.9110445e-10,5.6072857e-09\n",
      "Iteration 30000: loss = 2.9106875e-10,5.6066343e-09\n",
      "Iteration 30005: loss = 2.9104483e-10,5.6059513e-09\n",
      "Iteration 30010: loss = 2.910284e-10,5.605249e-09\n",
      "Iteration 30015: loss = 2.9101474e-10,5.6045395e-09\n",
      "Iteration 30020: loss = 2.9100242e-10,5.6038303e-09\n",
      "Iteration 30025: loss = 2.909891e-10,5.603124e-09\n",
      "Iteration 30030: loss = 2.9097308e-10,5.602425e-09\n",
      "Iteration 30035: loss = 2.9095426e-10,5.601736e-09\n",
      "Iteration 30040: loss = 2.9093308e-10,5.601055e-09\n",
      "Iteration 30045: loss = 2.9090866e-10,5.6003824e-09\n",
      "Iteration 30050: loss = 2.9088135e-10,5.599719e-09\n",
      "Iteration 30055: loss = 2.9085162e-10,5.599065e-09\n",
      "Iteration 30060: loss = 2.908185e-10,5.5984204e-09\n",
      "Iteration 30065: loss = 2.9078295e-10,5.5977845e-09\n",
      "Iteration 30070: loss = 2.9074462e-10,5.597156e-09\n",
      "Iteration 30075: loss = 2.9070438e-10,5.5965343e-09\n",
      "Iteration 30080: loss = 2.906618e-10,5.595921e-09\n",
      "Iteration 30085: loss = 2.9061686e-10,5.5953135e-09\n",
      "Iteration 30090: loss = 2.905701e-10,5.594713e-09\n",
      "Iteration 30095: loss = 2.9052225e-10,5.5941185e-09\n",
      "Iteration 30100: loss = 2.9047195e-10,5.593531e-09\n",
      "Iteration 30105: loss = 2.9041963e-10,5.5929474e-09\n",
      "Iteration 30110: loss = 2.885117e-10,5.5923524e-09\n",
      "Iteration 30115: loss = 2.8850108e-10,5.59166e-09\n",
      "Iteration 30120: loss = 2.8851607e-10,5.590898e-09\n",
      "Iteration 30125: loss = 2.9039204e-10,5.59014e-09\n",
      "Iteration 30130: loss = 2.885139e-10,5.5894667e-09\n",
      "Iteration 30135: loss = 2.8851024e-10,5.58876e-09\n",
      "Iteration 30140: loss = 2.903803e-10,5.58802e-09\n",
      "Iteration 30145: loss = 2.8851221e-10,5.587324e-09\n",
      "Iteration 30150: loss = 2.8851022e-10,5.586615e-09\n",
      "Iteration 30155: loss = 2.8850564e-10,5.585915e-09\n",
      "Iteration 30160: loss = 2.9036953e-10,5.5851985e-09\n",
      "Iteration 30165: loss = 2.884976e-10,5.584516e-09\n",
      "Iteration 30170: loss = 2.8849528e-10,5.583811e-09\n",
      "Iteration 30175: loss = 2.8849684e-10,5.5831015e-09\n",
      "Iteration 30180: loss = 2.884972e-10,5.582394e-09\n",
      "Iteration 30185: loss = 2.884966e-10,5.5816907e-09\n",
      "Iteration 30190: loss = 2.8849897e-10,5.5809797e-09\n",
      "Iteration 30195: loss = 2.8849814e-10,5.5802776e-09\n",
      "Iteration 30200: loss = 2.8663885e-10,5.579594e-09\n",
      "Iteration 30205: loss = 2.866362e-10,5.5789022e-09\n",
      "Iteration 30210: loss = 2.8848815e-10,5.5782023e-09\n",
      "Iteration 30215: loss = 2.8663097e-10,5.577516e-09\n",
      "Iteration 30220: loss = 2.8848415e-10,5.5768172e-09\n",
      "Iteration 30225: loss = 2.8848215e-10,5.5761267e-09\n",
      "Iteration 30230: loss = 2.8847905e-10,5.5754397e-09\n",
      "Iteration 30235: loss = 2.8662472e-10,5.574752e-09\n",
      "Iteration 30240: loss = 2.8662217e-10,5.5740665e-09\n",
      "Iteration 30245: loss = 2.884732e-10,5.5733773e-09\n",
      "Iteration 30250: loss = 2.8847283e-10,5.572687e-09\n",
      "Iteration 30255: loss = 2.8661512e-10,5.5720126e-09\n",
      "Iteration 30260: loss = 2.8661445e-10,5.571326e-09\n",
      "Iteration 30265: loss = 2.8846747e-10,5.5706373e-09\n",
      "Iteration 30270: loss = 2.8660774e-10,5.5699707e-09\n",
      "Iteration 30275: loss = 2.884619e-10,5.5692784e-09\n",
      "Iteration 30280: loss = 2.8660296e-10,5.568613e-09\n",
      "Iteration 30285: loss = 2.8660443e-10,5.5679257e-09\n",
      "Iteration 30290: loss = 2.8660296e-10,5.567248e-09\n",
      "Iteration 30295: loss = 2.8659677e-10,5.5665845e-09\n",
      "Iteration 30300: loss = 2.8659497e-10,5.5659104e-09\n",
      "Iteration 30305: loss = 2.8659805e-10,5.565222e-09\n",
      "Iteration 30310: loss = 2.8658967e-10,5.564567e-09\n",
      "Iteration 30315: loss = 2.8658098e-10,5.563912e-09\n",
      "Iteration 30320: loss = 2.8657646e-10,5.563251e-09\n",
      "Iteration 30325: loss = 2.8657224e-10,5.5625886e-09\n",
      "Iteration 30330: loss = 2.8656696e-10,5.5619287e-09\n",
      "Iteration 30335: loss = 2.8655953e-10,5.5612785e-09\n",
      "Iteration 30340: loss = 2.865483e-10,5.5606364e-09\n",
      "Iteration 30345: loss = 2.8653477e-10,5.5600045e-09\n",
      "Iteration 30350: loss = 2.865174e-10,5.5593814e-09\n",
      "Iteration 30355: loss = 2.8649713e-10,5.5587694e-09\n",
      "Iteration 30360: loss = 2.8647335e-10,5.5581664e-09\n",
      "Iteration 30365: loss = 2.8644706e-10,5.5575717e-09\n",
      "Iteration 30370: loss = 2.8641686e-10,5.5569873e-09\n",
      "Iteration 30375: loss = 2.8638503e-10,5.5564113e-09\n",
      "Iteration 30380: loss = 2.8635053e-10,5.5558416e-09\n",
      "Iteration 30385: loss = 2.8631283e-10,5.5552816e-09\n",
      "Iteration 30390: loss = 2.8627326e-10,5.5547265e-09\n",
      "Iteration 30395: loss = 2.8623184e-10,5.554181e-09\n",
      "Iteration 30400: loss = 2.8618805e-10,5.55364e-09\n",
      "Iteration 30405: loss = 2.8614242e-10,5.5531073e-09\n",
      "Iteration 30410: loss = 2.861e-10,5.5525655e-09\n",
      "Iteration 30415: loss = 2.8607075e-10,5.5519873e-09\n",
      "Iteration 30420: loss = 2.8605004e-10,5.5513913e-09\n",
      "Iteration 30425: loss = 2.8603178e-10,5.5507865e-09\n",
      "Iteration 30430: loss = 2.8601455e-10,5.5501808e-09\n",
      "Iteration 30435: loss = 2.859961e-10,5.5495804e-09\n",
      "Iteration 30440: loss = 2.8597572e-10,5.5489835e-09\n",
      "Iteration 30445: loss = 2.8595284e-10,5.548395e-09\n",
      "Iteration 30450: loss = 2.8592806e-10,5.5478155e-09\n",
      "Iteration 30455: loss = 2.8590044e-10,5.5472418e-09\n",
      "Iteration 30460: loss = 2.8586986e-10,5.5466796e-09\n",
      "Iteration 30465: loss = 2.8583744e-10,5.5461213e-09\n",
      "Iteration 30470: loss = 2.85802e-10,5.5455733e-09\n",
      "Iteration 30475: loss = 2.8576427e-10,5.545031e-09\n",
      "Iteration 30480: loss = 2.857245e-10,5.5444964e-09\n",
      "Iteration 30485: loss = 2.856824e-10,5.543969e-09\n",
      "Iteration 30490: loss = 2.856394e-10,5.5434466e-09\n",
      "Iteration 30495: loss = 2.8559333e-10,5.5429314e-09\n",
      "Iteration 30500: loss = 2.855464e-10,5.54242e-09\n",
      "Iteration 30505: loss = 2.854972e-10,5.5419163e-09\n",
      "Iteration 30510: loss = 2.854475e-10,5.541416e-09\n",
      "Iteration 30515: loss = 2.8539526e-10,5.5409206e-09\n",
      "Iteration 30520: loss = 2.8534186e-10,5.540432e-09\n",
      "Iteration 30525: loss = 2.8528815e-10,5.5399445e-09\n",
      "Iteration 30530: loss = 2.852326e-10,5.539463e-09\n",
      "Iteration 30535: loss = 2.8517655e-10,5.538984e-09\n",
      "Iteration 30540: loss = 2.851183e-10,5.538512e-09\n",
      "Iteration 30545: loss = 2.850601e-10,5.538041e-09\n",
      "Iteration 30550: loss = 2.8500027e-10,5.5375744e-09\n",
      "Iteration 30555: loss = 2.8311123e-10,5.537073e-09\n",
      "Iteration 30560: loss = 2.8311334e-10,5.5364393e-09\n",
      "Iteration 30565: loss = 2.8498212e-10,5.5357416e-09\n",
      "Iteration 30570: loss = 2.8498762e-10,5.5351017e-09\n",
      "Iteration 30575: loss = 2.8497754e-10,5.534506e-09\n",
      "Iteration 30580: loss = 2.8311323e-10,5.53394e-09\n",
      "Iteration 30585: loss = 2.8495795e-10,5.533314e-09\n",
      "Iteration 30590: loss = 2.849576e-10,5.5326925e-09\n",
      "Iteration 30595: loss = 2.8495148e-10,5.5320903e-09\n",
      "Iteration 30600: loss = 2.8310546e-10,5.53148e-09\n",
      "Iteration 30605: loss = 2.8495054e-10,5.530854e-09\n",
      "Iteration 30610: loss = 2.8310296e-10,5.530249e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30615: loss = 2.830991e-10,5.5296434e-09\n",
      "Iteration 30620: loss = 2.8310002e-10,5.529025e-09\n",
      "Iteration 30625: loss = 2.8309763e-10,5.5284173e-09\n",
      "Iteration 30630: loss = 2.8309638e-10,5.5278075e-09\n",
      "Iteration 30635: loss = 2.8309194e-10,5.527207e-09\n",
      "Iteration 30640: loss = 2.8309274e-10,5.5265916e-09\n",
      "Iteration 30645: loss = 2.8308694e-10,5.525998e-09\n",
      "Iteration 30650: loss = 2.8308358e-10,5.525396e-09\n",
      "Iteration 30655: loss = 2.849331e-10,5.524768e-09\n",
      "Iteration 30660: loss = 2.8308444e-10,5.524177e-09\n",
      "Iteration 30665: loss = 2.8308497e-10,5.5235687e-09\n",
      "Iteration 30670: loss = 2.8308242e-10,5.5229674e-09\n",
      "Iteration 30675: loss = 2.8308075e-10,5.5223683e-09\n",
      "Iteration 30680: loss = 2.8308003e-10,5.521765e-09\n",
      "Iteration 30685: loss = 2.8307898e-10,5.5211635e-09\n",
      "Iteration 30690: loss = 2.8307592e-10,5.5205702e-09\n",
      "Iteration 30695: loss = 2.8306996e-10,5.519984e-09\n",
      "Iteration 30700: loss = 2.8306044e-10,5.519408e-09\n",
      "Iteration 30705: loss = 2.8304856e-10,5.5188423e-09\n",
      "Iteration 30710: loss = 2.830324e-10,5.518287e-09\n",
      "Iteration 30715: loss = 2.8301342e-10,5.517741e-09\n",
      "Iteration 30720: loss = 2.829915e-10,5.517203e-09\n",
      "Iteration 30725: loss = 2.829663e-10,5.516675e-09\n",
      "Iteration 30730: loss = 2.8293803e-10,5.5161555e-09\n",
      "Iteration 30735: loss = 2.8290745e-10,5.515645e-09\n",
      "Iteration 30740: loss = 2.8287422e-10,5.5151412e-09\n",
      "Iteration 30745: loss = 2.828386e-10,5.5146447e-09\n",
      "Iteration 30750: loss = 2.8280092e-10,5.5141562e-09\n",
      "Iteration 30755: loss = 2.827609e-10,5.513673e-09\n",
      "Iteration 30760: loss = 2.8271882e-10,5.513196e-09\n",
      "Iteration 30765: loss = 2.8267563e-10,5.5127254e-09\n",
      "Iteration 30770: loss = 2.8263e-10,5.51226e-09\n",
      "Iteration 30775: loss = 2.8258332e-10,5.511799e-09\n",
      "Iteration 30780: loss = 2.8253452e-10,5.5113447e-09\n",
      "Iteration 30785: loss = 2.8248515e-10,5.510895e-09\n",
      "Iteration 30790: loss = 2.8243416e-10,5.5104463e-09\n",
      "Iteration 30795: loss = 2.805702e-10,5.509947e-09\n",
      "Iteration 30800: loss = 2.805772e-10,5.5093454e-09\n",
      "Iteration 30805: loss = 2.8060498e-10,5.508688e-09\n",
      "Iteration 30810: loss = 2.8246885e-10,5.5080225e-09\n",
      "Iteration 30815: loss = 2.806292e-10,5.5074607e-09\n",
      "Iteration 30820: loss = 2.8062483e-10,5.506892e-09\n",
      "Iteration 30825: loss = 2.8063826e-10,5.506276e-09\n",
      "Iteration 30830: loss = 2.8249095e-10,5.505645e-09\n",
      "Iteration 30835: loss = 2.8249741e-10,5.5050506e-09\n",
      "Iteration 30840: loss = 2.8066813e-10,5.504464e-09\n",
      "Iteration 30845: loss = 2.825122e-10,5.503859e-09\n",
      "Iteration 30850: loss = 2.8068198e-10,5.5032765e-09\n",
      "Iteration 30855: loss = 2.8068906e-10,5.502684e-09\n",
      "Iteration 30860: loss = 2.8069602e-10,5.5020903e-09\n",
      "Iteration 30865: loss = 2.8070138e-10,5.501505e-09\n",
      "Iteration 30870: loss = 2.8070393e-10,5.500927e-09\n",
      "Iteration 30875: loss = 2.8070332e-10,5.5003575e-09\n",
      "Iteration 30880: loss = 2.8069905e-10,5.4998006e-09\n",
      "Iteration 30885: loss = 2.806911e-10,5.4992535e-09\n",
      "Iteration 30890: loss = 2.8068006e-10,5.498717e-09\n",
      "Iteration 30895: loss = 2.8066485e-10,5.4981903e-09\n",
      "Iteration 30900: loss = 2.8064603e-10,5.497674e-09\n",
      "Iteration 30905: loss = 2.8062538e-10,5.4971663e-09\n",
      "Iteration 30910: loss = 2.8060004e-10,5.496669e-09\n",
      "Iteration 30915: loss = 2.8057318e-10,5.496179e-09\n",
      "Iteration 30920: loss = 2.8054314e-10,5.495697e-09\n",
      "Iteration 30925: loss = 2.8051061e-10,5.4952234e-09\n",
      "Iteration 30930: loss = 2.80476e-10,5.4947566e-09\n",
      "Iteration 30935: loss = 2.8043884e-10,5.4942966e-09\n",
      "Iteration 30940: loss = 2.804002e-10,5.493842e-09\n",
      "Iteration 30945: loss = 2.803592e-10,5.4933946e-09\n",
      "Iteration 30950: loss = 2.8031688e-10,5.4929514e-09\n",
      "Iteration 30955: loss = 2.8027214e-10,5.4925144e-09\n",
      "Iteration 30960: loss = 2.802264e-10,5.492083e-09\n",
      "Iteration 30965: loss = 2.801793e-10,5.491657e-09\n",
      "Iteration 30970: loss = 2.8013053e-10,5.4912332e-09\n",
      "Iteration 30975: loss = 2.8009364e-10,5.49078e-09\n",
      "Iteration 30980: loss = 2.8008293e-10,5.4902545e-09\n",
      "Iteration 30985: loss = 2.8009084e-10,5.4896807e-09\n",
      "Iteration 30990: loss = 2.8010771e-10,5.489082e-09\n",
      "Iteration 30995: loss = 2.8012845e-10,5.4884755e-09\n",
      "Iteration 31000: loss = 2.801294e-10,5.4879234e-09\n",
      "Iteration 31005: loss = 2.8012173e-10,5.487394e-09\n",
      "Iteration 31010: loss = 2.8012367e-10,5.4868416e-09\n",
      "Iteration 31015: loss = 2.8012653e-10,5.486285e-09\n",
      "Iteration 31020: loss = 2.801205e-10,5.485755e-09\n",
      "Iteration 31025: loss = 2.8011612e-10,5.4852203e-09\n",
      "Iteration 31030: loss = 2.8011282e-10,5.4846847e-09\n",
      "Iteration 31035: loss = 2.801081e-10,5.484152e-09\n",
      "Iteration 31040: loss = 2.8010116e-10,5.483627e-09\n",
      "Iteration 31045: loss = 2.800918e-10,5.4831095e-09\n",
      "Iteration 31050: loss = 2.8007918e-10,5.482601e-09\n",
      "Iteration 31055: loss = 2.8006353e-10,5.4821028e-09\n",
      "Iteration 31060: loss = 2.8004507e-10,5.4816125e-09\n",
      "Iteration 31065: loss = 2.8002295e-10,5.4811307e-09\n",
      "Iteration 31070: loss = 2.7999827e-10,5.4806586e-09\n",
      "Iteration 31075: loss = 2.7997166e-10,5.4801927e-09\n",
      "Iteration 31080: loss = 2.7994132e-10,5.479737e-09\n",
      "Iteration 31085: loss = 2.799094e-10,5.4792872e-09\n",
      "Iteration 31090: loss = 2.7987498e-10,5.4788445e-09\n",
      "Iteration 31095: loss = 2.798388e-10,5.478408e-09\n",
      "Iteration 31100: loss = 2.798002e-10,5.4779776e-09\n",
      "Iteration 31105: loss = 2.7975974e-10,5.4775535e-09\n",
      "Iteration 31110: loss = 2.7971767e-10,5.4771343e-09\n",
      "Iteration 31115: loss = 2.796738e-10,5.4767204e-09\n",
      "Iteration 31120: loss = 2.796284e-10,5.476312e-09\n",
      "Iteration 31125: loss = 2.7776945e-10,5.475877e-09\n",
      "Iteration 31130: loss = 2.7776628e-10,5.4753553e-09\n",
      "Iteration 31135: loss = 2.7778255e-10,5.4747824e-09\n",
      "Iteration 31140: loss = 2.7961386e-10,5.4742326e-09\n",
      "Iteration 31145: loss = 2.7777453e-10,5.473747e-09\n",
      "Iteration 31150: loss = 2.77774e-10,5.473221e-09\n",
      "Iteration 31155: loss = 2.796077e-10,5.47267e-09\n",
      "Iteration 31160: loss = 2.7777225e-10,5.4721743e-09\n",
      "Iteration 31165: loss = 2.7776972e-10,5.4716556e-09\n",
      "Iteration 31170: loss = 2.7777528e-10,5.471117e-09\n",
      "Iteration 31175: loss = 2.777746e-10,5.4705955e-09\n",
      "Iteration 31180: loss = 2.777704e-10,5.4700857e-09\n",
      "Iteration 31185: loss = 2.7776995e-10,5.469564e-09\n",
      "Iteration 31190: loss = 2.7777017e-10,5.469042e-09\n",
      "Iteration 31195: loss = 2.7776875e-10,5.4685256e-09\n",
      "Iteration 31200: loss = 2.7776575e-10,5.4680136e-09\n",
      "Iteration 31205: loss = 2.7775965e-10,5.467511e-09\n",
      "Iteration 31210: loss = 2.7774974e-10,5.467018e-09\n",
      "Iteration 31215: loss = 2.777378e-10,5.4665352e-09\n",
      "Iteration 31220: loss = 2.7772218e-10,5.46606e-09\n",
      "Iteration 31225: loss = 2.777034e-10,5.465594e-09\n",
      "Iteration 31230: loss = 2.776814e-10,5.465138e-09\n",
      "Iteration 31235: loss = 2.776567e-10,5.4646896e-09\n",
      "Iteration 31240: loss = 2.7762975e-10,5.46425e-09\n",
      "Iteration 31245: loss = 2.7760025e-10,5.463816e-09\n",
      "Iteration 31250: loss = 2.7756822e-10,5.46339e-09\n",
      "Iteration 31255: loss = 2.775344e-10,5.46297e-09\n",
      "Iteration 31260: loss = 2.774978e-10,5.4625584e-09\n",
      "Iteration 31265: loss = 2.774602e-10,5.462149e-09\n",
      "Iteration 31270: loss = 2.7742483e-10,5.461737e-09\n",
      "Iteration 31275: loss = 2.7739502e-10,5.4613083e-09\n",
      "Iteration 31280: loss = 2.773686e-10,5.4608713e-09\n",
      "Iteration 31285: loss = 2.773424e-10,5.460434e-09\n",
      "Iteration 31290: loss = 2.7731514e-10,5.4600013e-09\n",
      "Iteration 31295: loss = 2.7728694e-10,5.45957e-09\n",
      "Iteration 31300: loss = 2.7725758e-10,5.459145e-09\n",
      "Iteration 31305: loss = 2.7722602e-10,5.4587246e-09\n",
      "Iteration 31310: loss = 2.7719277e-10,5.4583102e-09\n",
      "Iteration 31315: loss = 2.7715769e-10,5.457903e-09\n",
      "Iteration 31320: loss = 2.7712144e-10,5.4575002e-09\n",
      "Iteration 31325: loss = 2.7708655e-10,5.45709e-09\n",
      "Iteration 31330: loss = 2.7705827e-10,5.4566662e-09\n",
      "Iteration 31335: loss = 2.770331e-10,5.4562337e-09\n",
      "Iteration 31340: loss = 2.7700875e-10,5.4558003e-09\n",
      "Iteration 31345: loss = 2.7698246e-10,5.4553695e-09\n",
      "Iteration 31350: loss = 2.7695607e-10,5.454944e-09\n",
      "Iteration 31355: loss = 2.7692723e-10,5.454523e-09\n",
      "Iteration 31360: loss = 2.768976e-10,5.4541065e-09\n",
      "Iteration 31365: loss = 2.7687333e-10,5.4536753e-09\n",
      "Iteration 31370: loss = 2.768535e-10,5.4532334e-09\n",
      "Iteration 31375: loss = 2.7683475e-10,5.4527884e-09\n",
      "Iteration 31380: loss = 2.768166e-10,5.4523426e-09\n",
      "Iteration 31385: loss = 2.7679628e-10,5.4519047e-09\n",
      "Iteration 31390: loss = 2.7677435e-10,5.4514695e-09\n",
      "Iteration 31395: loss = 2.7675032e-10,5.451042e-09\n",
      "Iteration 31400: loss = 2.7672434e-10,5.4506217e-09\n",
      "Iteration 31405: loss = 2.7669614e-10,5.4502056e-09\n",
      "Iteration 31410: loss = 2.7666544e-10,5.449798e-09\n",
      "Iteration 31415: loss = 2.7663286e-10,5.449395e-09\n",
      "Iteration 31420: loss = 2.765986e-10,5.4490004e-09\n",
      "Iteration 31425: loss = 2.7656183e-10,5.4486105e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31430: loss = 2.7652353e-10,5.448226e-09\n",
      "Iteration 31435: loss = 2.764842e-10,5.4478457e-09\n",
      "Iteration 31440: loss = 2.764424e-10,5.447472e-09\n",
      "Iteration 31445: loss = 2.7639985e-10,5.4471014e-09\n",
      "Iteration 31450: loss = 2.7635552e-10,5.4467364e-09\n",
      "Iteration 31455: loss = 2.7631644e-10,5.446358e-09\n",
      "Iteration 31460: loss = 2.762827e-10,5.4459655e-09\n",
      "Iteration 31465: loss = 2.762523e-10,5.4455636e-09\n",
      "Iteration 31470: loss = 2.7622152e-10,5.4451625e-09\n",
      "Iteration 31475: loss = 2.7439143e-10,5.444725e-09\n",
      "Iteration 31480: loss = 2.7622377e-10,5.444194e-09\n",
      "Iteration 31485: loss = 2.7622357e-10,5.443712e-09\n",
      "Iteration 31490: loss = 2.743942e-10,5.4432716e-09\n",
      "Iteration 31495: loss = 2.7621877e-10,5.4427653e-09\n",
      "Iteration 31500: loss = 2.7621502e-10,5.442295e-09\n",
      "Iteration 31505: loss = 2.762137e-10,5.4418203e-09\n",
      "Iteration 31510: loss = 2.7439592e-10,5.441352e-09\n",
      "Iteration 31515: loss = 2.743955e-10,5.4408757e-09\n",
      "Iteration 31520: loss = 2.7439578e-10,5.4403966e-09\n",
      "Iteration 31525: loss = 2.7621144e-10,5.4399165e-09\n",
      "Iteration 31530: loss = 2.743924e-10,5.439454e-09\n",
      "Iteration 31535: loss = 2.7439323e-10,5.438978e-09\n",
      "Iteration 31540: loss = 2.7439157e-10,5.438506e-09\n",
      "Iteration 31545: loss = 2.7620828e-10,5.4380265e-09\n",
      "Iteration 31550: loss = 2.7439254e-10,5.437557e-09\n",
      "Iteration 31555: loss = 2.743911e-10,5.437089e-09\n",
      "Iteration 31560: loss = 2.7439015e-10,5.43662e-09\n",
      "Iteration 31565: loss = 2.743897e-10,5.4361498e-09\n",
      "Iteration 31570: loss = 2.743871e-10,5.4356857e-09\n",
      "Iteration 31575: loss = 2.7438418e-10,5.435223e-09\n",
      "Iteration 31580: loss = 2.743891e-10,5.4347393e-09\n",
      "Iteration 31585: loss = 2.7438848e-10,5.4342735e-09\n",
      "Iteration 31590: loss = 2.762045e-10,5.4338e-09\n",
      "Iteration 31595: loss = 2.743821e-10,5.433355e-09\n",
      "Iteration 31600: loss = 2.7437863e-10,5.432896e-09\n",
      "Iteration 31605: loss = 2.7437538e-10,5.43244e-09\n",
      "Iteration 31610: loss = 2.7437067e-10,5.4319846e-09\n",
      "Iteration 31615: loss = 2.743645e-10,5.431537e-09\n",
      "Iteration 31620: loss = 2.743554e-10,5.431097e-09\n",
      "Iteration 31625: loss = 2.7434377e-10,5.430665e-09\n",
      "Iteration 31630: loss = 2.7432925e-10,5.4302394e-09\n",
      "Iteration 31635: loss = 2.743117e-10,5.4298264e-09\n",
      "Iteration 31640: loss = 2.7429173e-10,5.4294174e-09\n",
      "Iteration 31645: loss = 2.7426864e-10,5.429018e-09\n",
      "Iteration 31650: loss = 2.7244593e-10,5.4286065e-09\n",
      "Iteration 31655: loss = 2.7426536e-10,5.428106e-09\n",
      "Iteration 31660: loss = 2.7426453e-10,5.4276486e-09\n",
      "Iteration 31665: loss = 2.7245742e-10,5.4271956e-09\n",
      "Iteration 31670: loss = 2.742692e-10,5.426717e-09\n",
      "Iteration 31675: loss = 2.724568e-10,5.4262808e-09\n",
      "Iteration 31680: loss = 2.742678e-10,5.4258047e-09\n",
      "Iteration 31685: loss = 2.7246047e-10,5.4253553e-09\n",
      "Iteration 31690: loss = 2.7246175e-10,5.424895e-09\n",
      "Iteration 31695: loss = 2.7427194e-10,5.4244254e-09\n",
      "Iteration 31700: loss = 2.7246136e-10,5.4239853e-09\n",
      "Iteration 31705: loss = 2.7427322e-10,5.4235123e-09\n",
      "Iteration 31710: loss = 2.7246147e-10,5.423076e-09\n",
      "Iteration 31715: loss = 2.7427235e-10,5.4226046e-09\n",
      "Iteration 31720: loss = 2.7246697e-10,5.422155e-09\n",
      "Iteration 31725: loss = 2.7246738e-10,5.421701e-09\n",
      "Iteration 31730: loss = 2.724669e-10,5.421252e-09\n",
      "Iteration 31735: loss = 2.7427496e-10,5.42079e-09\n",
      "Iteration 31740: loss = 2.7246602e-10,5.4203526e-09\n",
      "Iteration 31745: loss = 2.7246988e-10,5.419891e-09\n",
      "Iteration 31750: loss = 2.724667e-10,5.419451e-09\n",
      "Iteration 31755: loss = 2.7246905e-10,5.418995e-09\n",
      "Iteration 31760: loss = 2.724683e-10,5.418549e-09\n",
      "Iteration 31765: loss = 2.7427574e-10,5.418094e-09\n",
      "Iteration 31770: loss = 2.7246558e-10,5.417662e-09\n",
      "Iteration 31775: loss = 2.724642e-10,5.4172187e-09\n",
      "Iteration 31780: loss = 2.7246386e-10,5.4167724e-09\n",
      "Iteration 31785: loss = 2.7246286e-10,5.416331e-09\n",
      "Iteration 31790: loss = 2.7245925e-10,5.4158957e-09\n",
      "Iteration 31795: loss = 2.7245306e-10,5.415467e-09\n",
      "Iteration 31800: loss = 2.724441e-10,5.4150475e-09\n",
      "Iteration 31805: loss = 2.724323e-10,5.414635e-09\n",
      "Iteration 31810: loss = 2.7241798e-10,5.414232e-09\n",
      "Iteration 31815: loss = 2.7240027e-10,5.4138383e-09\n",
      "Iteration 31820: loss = 2.723798e-10,5.4134515e-09\n",
      "Iteration 31825: loss = 2.7235725e-10,5.413072e-09\n",
      "Iteration 31830: loss = 2.7233163e-10,5.412701e-09\n",
      "Iteration 31835: loss = 2.723036e-10,5.4123355e-09\n",
      "Iteration 31840: loss = 2.7227387e-10,5.4119766e-09\n",
      "Iteration 31845: loss = 2.722418e-10,5.411625e-09\n",
      "Iteration 31850: loss = 2.7220823e-10,5.4112776e-09\n",
      "Iteration 31855: loss = 2.721732e-10,5.4109357e-09\n",
      "Iteration 31860: loss = 2.7214445e-10,5.4105755e-09\n",
      "Iteration 31865: loss = 2.7211944e-10,5.410205e-09\n",
      "Iteration 31870: loss = 2.7209698e-10,5.40983e-09\n",
      "Iteration 31875: loss = 2.720749e-10,5.4094538e-09\n",
      "Iteration 31880: loss = 2.7205224e-10,5.4090803e-09\n",
      "Iteration 31885: loss = 2.7202876e-10,5.408709e-09\n",
      "Iteration 31890: loss = 2.7200267e-10,5.408346e-09\n",
      "Iteration 31895: loss = 2.7197514e-10,5.407986e-09\n",
      "Iteration 31900: loss = 2.7194585e-10,5.407632e-09\n",
      "Iteration 31905: loss = 2.7191452e-10,5.4072844e-09\n",
      "Iteration 31910: loss = 2.7188163e-10,5.4069407e-09\n",
      "Iteration 31915: loss = 2.7184696e-10,5.406604e-09\n",
      "Iteration 31920: loss = 2.7181069e-10,5.4062714e-09\n",
      "Iteration 31925: loss = 2.7177313e-10,5.4059424e-09\n",
      "Iteration 31930: loss = 2.717336e-10,5.4056195e-09\n",
      "Iteration 31935: loss = 2.7169372e-10,5.405299e-09\n",
      "Iteration 31940: loss = 2.7165156e-10,5.4049836e-09\n",
      "Iteration 31945: loss = 2.7161248e-10,5.4046594e-09\n",
      "Iteration 31950: loss = 2.715815e-10,5.404317e-09\n",
      "Iteration 31955: loss = 2.7155353e-10,5.403965e-09\n",
      "Iteration 31960: loss = 2.715265e-10,5.40361e-09\n",
      "Iteration 31965: loss = 2.7149988e-10,5.403257e-09\n",
      "Iteration 31970: loss = 2.7147168e-10,5.4029075e-09\n",
      "Iteration 31975: loss = 2.7144328e-10,5.402558e-09\n",
      "Iteration 31980: loss = 2.71413e-10,5.4022156e-09\n",
      "Iteration 31985: loss = 2.71381e-10,5.40188e-09\n",
      "Iteration 31990: loss = 2.7134756e-10,5.401545e-09\n",
      "Iteration 31995: loss = 2.7131247e-10,5.401217e-09\n",
      "Iteration 32000: loss = 2.712759e-10,5.400894e-09\n",
      "Iteration 32005: loss = 2.7123848e-10,5.4005724e-09\n",
      "Iteration 32010: loss = 2.711987e-10,5.400259e-09\n",
      "Iteration 32015: loss = 2.7115846e-10,5.399947e-09\n",
      "Iteration 32020: loss = 2.7111666e-10,5.39964e-09\n",
      "Iteration 32025: loss = 2.7107813e-10,5.3993237e-09\n",
      "Iteration 32030: loss = 2.692744e-10,5.398927e-09\n",
      "Iteration 32035: loss = 2.7109245e-10,5.3984452e-09\n",
      "Iteration 32040: loss = 2.7109562e-10,5.398017e-09\n",
      "Iteration 32045: loss = 2.710845e-10,5.397629e-09\n",
      "Iteration 32050: loss = 2.6927502e-10,5.3972498e-09\n",
      "Iteration 32055: loss = 2.7107852e-10,5.396808e-09\n",
      "Iteration 32060: loss = 2.7107544e-10,5.3964015e-09\n",
      "Iteration 32065: loss = 2.7107572e-10,5.3959845e-09\n",
      "Iteration 32070: loss = 2.7107647e-10,5.395565e-09\n",
      "Iteration 32075: loss = 2.710762e-10,5.3951505e-09\n",
      "Iteration 32080: loss = 2.6927852e-10,5.394742e-09\n",
      "Iteration 32085: loss = 2.6927874e-10,5.3943263e-09\n",
      "Iteration 32090: loss = 2.6928024e-10,5.3939093e-09\n",
      "Iteration 32095: loss = 2.7107686e-10,5.3934914e-09\n",
      "Iteration 32100: loss = 2.7107663e-10,5.3930793e-09\n",
      "Iteration 32105: loss = 2.6927735e-10,5.392677e-09\n",
      "Iteration 32110: loss = 2.710778e-10,5.392251e-09\n",
      "Iteration 32115: loss = 2.6928063e-10,5.3918434e-09\n",
      "Iteration 32120: loss = 2.69277e-10,5.3914433e-09\n",
      "Iteration 32125: loss = 2.692799e-10,5.3910236e-09\n",
      "Iteration 32130: loss = 2.6928076e-10,5.390611e-09\n",
      "Iteration 32135: loss = 2.6928024e-10,5.3902034e-09\n",
      "Iteration 32140: loss = 2.6928035e-10,5.389795e-09\n",
      "Iteration 32145: loss = 2.6927893e-10,5.3893885e-09\n",
      "Iteration 32150: loss = 2.6927804e-10,5.3889826e-09\n",
      "Iteration 32155: loss = 2.6928262e-10,5.3885616e-09\n",
      "Iteration 32160: loss = 2.710791e-10,5.388152e-09\n",
      "Iteration 32165: loss = 2.6927863e-10,5.3877582e-09\n",
      "Iteration 32170: loss = 2.6927902e-10,5.3873506e-09\n",
      "Iteration 32175: loss = 2.6928157e-10,5.3869385e-09\n",
      "Iteration 32180: loss = 2.692835e-10,5.3865286e-09\n",
      "Iteration 32185: loss = 2.692833e-10,5.3861235e-09\n",
      "Iteration 32190: loss = 2.6928096e-10,5.3857248e-09\n",
      "Iteration 32195: loss = 2.6927596e-10,5.385335e-09\n",
      "Iteration 32200: loss = 2.692679e-10,5.3849516e-09\n",
      "Iteration 32205: loss = 2.692566e-10,5.384579e-09\n",
      "Iteration 32210: loss = 2.692427e-10,5.3842144e-09\n",
      "Iteration 32215: loss = 2.692262e-10,5.383858e-09\n",
      "Iteration 32220: loss = 2.692066e-10,5.383508e-09\n",
      "Iteration 32225: loss = 2.691847e-10,5.383166e-09\n",
      "Iteration 32230: loss = 2.6916067e-10,5.3828297e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32235: loss = 2.6913397e-10,5.382501e-09\n",
      "Iteration 32240: loss = 2.6910593e-10,5.3821774e-09\n",
      "Iteration 32245: loss = 2.6907523e-10,5.3818603e-09\n",
      "Iteration 32250: loss = 2.6904318e-10,5.3815485e-09\n",
      "Iteration 32255: loss = 2.6900973e-10,5.3812412e-09\n",
      "Iteration 32260: loss = 2.6897407e-10,5.3809375e-09\n",
      "Iteration 32265: loss = 2.689376e-10,5.380639e-09\n",
      "Iteration 32270: loss = 2.688996e-10,5.3803455e-09\n",
      "Iteration 32275: loss = 2.6886007e-10,5.380055e-09\n",
      "Iteration 32280: loss = 2.688196e-10,5.3797677e-09\n",
      "Iteration 32285: loss = 2.6878513e-10,5.379465e-09\n",
      "Iteration 32290: loss = 2.6876554e-10,5.379124e-09\n",
      "Iteration 32295: loss = 2.6875485e-10,5.3787574e-09\n",
      "Iteration 32300: loss = 2.6874858e-10,5.3783786e-09\n",
      "Iteration 32305: loss = 2.6874422e-10,5.377998e-09\n",
      "Iteration 32310: loss = 2.687387e-10,5.3776175e-09\n",
      "Iteration 32315: loss = 2.687321e-10,5.377242e-09\n",
      "Iteration 32320: loss = 2.68723e-10,5.376873e-09\n",
      "Iteration 32325: loss = 2.6871158e-10,5.376512e-09\n",
      "Iteration 32330: loss = 2.6869784e-10,5.3761573e-09\n",
      "Iteration 32335: loss = 2.6868127e-10,5.37581e-09\n",
      "Iteration 32340: loss = 2.6866243e-10,5.3754694e-09\n",
      "Iteration 32345: loss = 2.6864105e-10,5.3751377e-09\n",
      "Iteration 32350: loss = 2.6861732e-10,5.374811e-09\n",
      "Iteration 32355: loss = 2.6859118e-10,5.374492e-09\n",
      "Iteration 32360: loss = 2.685637e-10,5.374176e-09\n",
      "Iteration 32365: loss = 2.685342e-10,5.373867e-09\n",
      "Iteration 32370: loss = 2.6850236e-10,5.373565e-09\n",
      "Iteration 32375: loss = 2.6846972e-10,5.373266e-09\n",
      "Iteration 32380: loss = 2.6843486e-10,5.3729723e-09\n",
      "Iteration 32385: loss = 2.6839905e-10,5.3726823e-09\n",
      "Iteration 32390: loss = 2.6836186e-10,5.3723945e-09\n",
      "Iteration 32395: loss = 2.6832356e-10,5.3721125e-09\n",
      "Iteration 32400: loss = 2.682832e-10,5.371835e-09\n",
      "Iteration 32405: loss = 2.682455e-10,5.371553e-09\n",
      "Iteration 32410: loss = 2.6821584e-10,5.3712492e-09\n",
      "Iteration 32415: loss = 2.6819e-10,5.370933e-09\n",
      "Iteration 32420: loss = 2.681657e-10,5.370613e-09\n",
      "Iteration 32425: loss = 2.681425e-10,5.3702935e-09\n",
      "Iteration 32430: loss = 2.681178e-10,5.3699756e-09\n",
      "Iteration 32435: loss = 2.6809238e-10,5.369662e-09\n",
      "Iteration 32440: loss = 2.6806546e-10,5.3693516e-09\n",
      "Iteration 32445: loss = 2.6803698e-10,5.369047e-09\n",
      "Iteration 32450: loss = 2.680073e-10,5.3687463e-09\n",
      "Iteration 32455: loss = 2.6797584e-10,5.368449e-09\n",
      "Iteration 32460: loss = 2.6794225e-10,5.3681593e-09\n",
      "Iteration 32465: loss = 2.6790792e-10,5.3678724e-09\n",
      "Iteration 32470: loss = 2.6787203e-10,5.367588e-09\n",
      "Iteration 32475: loss = 2.6783545e-10,5.3673093e-09\n",
      "Iteration 32480: loss = 2.660189e-10,5.3670126e-09\n",
      "Iteration 32485: loss = 2.678176e-10,5.3665974e-09\n",
      "Iteration 32490: loss = 2.6782607e-10,5.366197e-09\n",
      "Iteration 32495: loss = 2.6602737e-10,5.3658527e-09\n",
      "Iteration 32500: loss = 2.6781835e-10,5.3654605e-09\n",
      "Iteration 32505: loss = 2.6602998e-10,5.3650884e-09\n",
      "Iteration 32510: loss = 2.6781832e-10,5.3647065e-09\n",
      "Iteration 32515: loss = 2.678197e-10,5.364325e-09\n",
      "Iteration 32520: loss = 2.6603153e-10,5.3639555e-09\n",
      "Iteration 32525: loss = 2.6603206e-10,5.363577e-09\n",
      "Iteration 32530: loss = 2.6603333e-10,5.3631983e-09\n",
      "Iteration 32535: loss = 2.6782113e-10,5.362817e-09\n",
      "Iteration 32540: loss = 2.6603358e-10,5.3624456e-09\n",
      "Iteration 32545: loss = 2.6603258e-10,5.3620743e-09\n",
      "Iteration 32550: loss = 2.67822e-10,5.3616906e-09\n",
      "Iteration 32555: loss = 2.6603578e-10,5.3613176e-09\n",
      "Iteration 32560: loss = 2.6603558e-10,5.360944e-09\n",
      "Iteration 32565: loss = 2.6603714e-10,5.360567e-09\n",
      "Iteration 32570: loss = 2.6603245e-10,5.360208e-09\n",
      "Iteration 32575: loss = 2.66036e-10,5.359826e-09\n",
      "Iteration 32580: loss = 2.660359e-10,5.359454e-09\n",
      "Iteration 32585: loss = 2.660348e-10,5.3590856e-09\n",
      "Iteration 32590: loss = 2.6782718e-10,5.3586966e-09\n",
      "Iteration 32595: loss = 2.6603114e-10,5.3583533e-09\n",
      "Iteration 32600: loss = 2.6602856e-10,5.357991e-09\n",
      "Iteration 32605: loss = 2.6602945e-10,5.357619e-09\n",
      "Iteration 32610: loss = 2.660316e-10,5.3572435e-09\n",
      "Iteration 32615: loss = 2.6603364e-10,5.356869e-09\n",
      "Iteration 32620: loss = 2.6603394e-10,5.3564992e-09\n",
      "Iteration 32625: loss = 2.6603145e-10,5.356137e-09\n",
      "Iteration 32630: loss = 2.6602667e-10,5.3557816e-09\n",
      "Iteration 32635: loss = 2.6601935e-10,5.3554348e-09\n",
      "Iteration 32640: loss = 2.660084e-10,5.3550964e-09\n",
      "Iteration 32645: loss = 2.659952e-10,5.354764e-09\n",
      "Iteration 32650: loss = 2.659796e-10,5.354442e-09\n",
      "Iteration 32655: loss = 2.659639e-10,5.354118e-09\n",
      "Iteration 32660: loss = 2.6595456e-10,5.353777e-09\n",
      "Iteration 32665: loss = 2.6594757e-10,5.35343e-09\n",
      "Iteration 32670: loss = 2.659423e-10,5.3530798e-09\n",
      "Iteration 32675: loss = 2.659355e-10,5.352734e-09\n",
      "Iteration 32680: loss = 2.6592759e-10,5.3523905e-09\n",
      "Iteration 32685: loss = 2.659172e-10,5.3520544e-09\n",
      "Iteration 32690: loss = 2.6590438e-10,5.3517244e-09\n",
      "Iteration 32695: loss = 2.6588964e-10,5.351403e-09\n",
      "Iteration 32700: loss = 2.6587205e-10,5.3510867e-09\n",
      "Iteration 32705: loss = 2.6585298e-10,5.3507785e-09\n",
      "Iteration 32710: loss = 2.6583047e-10,5.350475e-09\n",
      "Iteration 32715: loss = 2.6580652e-10,5.350178e-09\n",
      "Iteration 32720: loss = 2.6578048e-10,5.3498876e-09\n",
      "Iteration 32725: loss = 2.65753e-10,5.3496008e-09\n",
      "Iteration 32730: loss = 2.6572358e-10,5.3493214e-09\n",
      "Iteration 32735: loss = 2.656926e-10,5.349044e-09\n",
      "Iteration 32740: loss = 2.6566033e-10,5.348773e-09\n",
      "Iteration 32745: loss = 2.6562633e-10,5.3485048e-09\n",
      "Iteration 32750: loss = 2.6559097e-10,5.348241e-09\n",
      "Iteration 32755: loss = 2.655551e-10,5.34798e-09\n",
      "Iteration 32760: loss = 2.6551775e-10,5.3477223e-09\n",
      "Iteration 32765: loss = 2.6547875e-10,5.3474705e-09\n",
      "Iteration 32770: loss = 2.6544097e-10,5.347216e-09\n",
      "Iteration 32775: loss = 2.6541064e-10,5.34694e-09\n",
      "Iteration 32780: loss = 2.6538613e-10,5.3466493e-09\n",
      "Iteration 32785: loss = 2.6536393e-10,5.346354e-09\n",
      "Iteration 32790: loss = 2.6534203e-10,5.346056e-09\n",
      "Iteration 32795: loss = 2.6532054e-10,5.3457594e-09\n",
      "Iteration 32800: loss = 2.652963e-10,5.3454685e-09\n",
      "Iteration 32805: loss = 2.6527194e-10,5.34518e-09\n",
      "Iteration 32810: loss = 2.6524546e-10,5.3448943e-09\n",
      "Iteration 32815: loss = 2.6521813e-10,5.3446154e-09\n",
      "Iteration 32820: loss = 2.651886e-10,5.3443405e-09\n",
      "Iteration 32825: loss = 2.6515848e-10,5.3440696e-09\n",
      "Iteration 32830: loss = 2.6512637e-10,5.3438027e-09\n",
      "Iteration 32835: loss = 2.6509303e-10,5.343541e-09\n",
      "Iteration 32840: loss = 2.650586e-10,5.34328e-09\n",
      "Iteration 32845: loss = 2.6502292e-10,5.3430247e-09\n",
      "Iteration 32850: loss = 2.6498587e-10,5.342774e-09\n",
      "Iteration 32855: loss = 2.631798e-10,5.3425038e-09\n",
      "Iteration 32860: loss = 2.6496727e-10,5.3421214e-09\n",
      "Iteration 32865: loss = 2.6496716e-10,5.3417706e-09\n",
      "Iteration 32870: loss = 2.63173e-10,5.3414664e-09\n",
      "Iteration 32875: loss = 2.6495403e-10,5.3411027e-09\n",
      "Iteration 32880: loss = 2.6494806e-10,5.3407683e-09\n",
      "Iteration 32885: loss = 2.6494298e-10,5.3404325e-09\n",
      "Iteration 32890: loss = 2.631618e-10,5.3400973e-09\n",
      "Iteration 32895: loss = 2.6493832e-10,5.3397473e-09\n",
      "Iteration 32900: loss = 2.6493044e-10,5.3394196e-09\n",
      "Iteration 32905: loss = 2.6314823e-10,5.3390856e-09\n",
      "Iteration 32910: loss = 2.631436e-10,5.33875e-09\n",
      "Iteration 32915: loss = 2.6491662e-10,5.3384115e-09\n",
      "Iteration 32920: loss = 2.6491445e-10,5.3380695e-09\n",
      "Iteration 32925: loss = 2.6312869e-10,5.337748e-09\n",
      "Iteration 32930: loss = 2.6490418e-10,5.3374034e-09\n",
      "Iteration 32935: loss = 2.6311908e-10,5.337081e-09\n",
      "Iteration 32940: loss = 2.6489294e-10,5.336741e-09\n",
      "Iteration 32945: loss = 2.648886e-10,5.336407e-09\n",
      "Iteration 32950: loss = 2.6310373e-10,5.3360836e-09\n",
      "Iteration 32955: loss = 2.6310015e-10,5.3357474e-09\n",
      "Iteration 32960: loss = 2.630958e-10,5.3354148e-09\n",
      "Iteration 32965: loss = 2.6308955e-10,5.335088e-09\n",
      "Iteration 32970: loss = 2.6486416e-10,5.334748e-09\n",
      "Iteration 32975: loss = 2.6485839e-10,5.334418e-09\n",
      "Iteration 32980: loss = 2.630728e-10,5.3340994e-09\n",
      "Iteration 32985: loss = 2.6307018e-10,5.3337637e-09\n",
      "Iteration 32990: loss = 2.630633e-10,5.333438e-09\n",
      "Iteration 32995: loss = 2.6483754e-10,5.3331024e-09\n",
      "Iteration 33000: loss = 2.630541e-10,5.3327787e-09\n",
      "Iteration 33005: loss = 2.63045e-10,5.3324607e-09\n",
      "Iteration 33010: loss = 2.6482003e-10,5.3321214e-09\n",
      "Iteration 33015: loss = 2.6303545e-10,5.331804e-09\n",
      "Iteration 33020: loss = 2.6480906e-10,5.3314704e-09\n",
      "Iteration 33025: loss = 2.630249e-10,5.331149e-09\n",
      "Iteration 33030: loss = 2.6301616e-10,5.3308327e-09\n",
      "Iteration 33035: loss = 2.6300762e-10,5.330514e-09\n",
      "Iteration 33040: loss = 2.6300825e-10,5.330173e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33045: loss = 2.6300045e-10,5.3298557e-09\n",
      "Iteration 33050: loss = 2.629891e-10,5.329547e-09\n",
      "Iteration 33055: loss = 2.629845e-10,5.3292197e-09\n",
      "Iteration 33060: loss = 2.6298258e-10,5.328886e-09\n",
      "Iteration 33065: loss = 2.6298097e-10,5.328552e-09\n",
      "Iteration 33070: loss = 2.6296929e-10,5.3282445e-09\n",
      "Iteration 33075: loss = 2.6295774e-10,5.3279394e-09\n",
      "Iteration 33080: loss = 2.629472e-10,5.3276312e-09\n",
      "Iteration 33085: loss = 2.629361e-10,5.327323e-09\n",
      "Iteration 33090: loss = 2.629244e-10,5.327018e-09\n",
      "Iteration 33095: loss = 2.6291055e-10,5.3267177e-09\n",
      "Iteration 33100: loss = 2.6289548e-10,5.3264237e-09\n",
      "Iteration 33105: loss = 2.628782e-10,5.3261355e-09\n",
      "Iteration 33110: loss = 2.6109442e-10,5.325846e-09\n",
      "Iteration 33115: loss = 2.628764e-10,5.325468e-09\n",
      "Iteration 33120: loss = 2.628792e-10,5.3251243e-09\n",
      "Iteration 33125: loss = 2.6110106e-10,5.3248206e-09\n",
      "Iteration 33130: loss = 2.6287458e-10,5.324469e-09\n",
      "Iteration 33135: loss = 2.61104e-10,5.324144e-09\n",
      "Iteration 33140: loss = 2.6110455e-10,5.323809e-09\n",
      "Iteration 33145: loss = 2.6110622e-10,5.3234706e-09\n",
      "Iteration 33150: loss = 2.628754e-10,5.323131e-09\n",
      "Iteration 33155: loss = 2.628732e-10,5.322805e-09\n",
      "Iteration 33160: loss = 2.628681e-10,5.3224856e-09\n",
      "Iteration 33165: loss = 2.6109218e-10,5.3221765e-09\n",
      "Iteration 33170: loss = 2.6285377e-10,5.3218585e-09\n",
      "Iteration 33175: loss = 2.6285027e-10,5.321537e-09\n",
      "Iteration 33180: loss = 2.628411e-10,5.3212315e-09\n",
      "Iteration 33185: loss = 2.6283745e-10,5.320911e-09\n",
      "Iteration 33190: loss = 2.610572e-10,5.3206155e-09\n",
      "Iteration 33195: loss = 2.628224e-10,5.3202895e-09\n",
      "Iteration 33200: loss = 2.610457e-10,5.3199845e-09\n",
      "Iteration 33205: loss = 2.6103966e-10,5.319671e-09\n",
      "Iteration 33210: loss = 2.6103278e-10,5.3193587e-09\n",
      "Iteration 33215: loss = 2.627967e-10,5.3190385e-09\n",
      "Iteration 33220: loss = 2.610185e-10,5.31874e-09\n",
      "Iteration 33225: loss = 2.6101302e-10,5.318425e-09\n",
      "Iteration 33230: loss = 2.6277716e-10,5.318105e-09\n",
      "Iteration 33235: loss = 2.6099736e-10,5.31781e-09\n",
      "Iteration 33240: loss = 2.6276356e-10,5.317485e-09\n",
      "Iteration 33245: loss = 2.60982e-10,5.317196e-09\n",
      "Iteration 33250: loss = 2.627488e-10,5.3168687e-09\n",
      "Iteration 33255: loss = 2.609662e-10,5.316585e-09\n",
      "Iteration 33260: loss = 2.6096578e-10,5.3162585e-09\n",
      "Iteration 33265: loss = 2.6095429e-10,5.315964e-09\n",
      "Iteration 33270: loss = 2.6094488e-10,5.315662e-09\n",
      "Iteration 33275: loss = 2.6271466e-10,5.315329e-09\n",
      "Iteration 33280: loss = 2.609375e-10,5.3150306e-09\n",
      "Iteration 33285: loss = 2.626996e-10,5.314719e-09\n",
      "Iteration 33290: loss = 2.609242e-10,5.314417e-09\n",
      "Iteration 33295: loss = 2.6091634e-10,5.3141127e-09\n",
      "Iteration 33300: loss = 2.6267688e-10,5.313805e-09\n",
      "Iteration 33305: loss = 2.6266958e-10,5.3134994e-09\n",
      "Iteration 33310: loss = 2.6089045e-10,5.3132094e-09\n",
      "Iteration 33315: loss = 2.6265695e-10,5.3128857e-09\n",
      "Iteration 33320: loss = 2.6087027e-10,5.3126166e-09\n",
      "Iteration 33325: loss = 2.6086455e-10,5.3123075e-09\n",
      "Iteration 33330: loss = 2.6263516e-10,5.3119744e-09\n",
      "Iteration 33335: loss = 2.608485e-10,5.3117057e-09\n",
      "Iteration 33340: loss = 2.6083663e-10,5.311417e-09\n",
      "Iteration 33345: loss = 2.6083324e-10,5.3111027e-09\n",
      "Iteration 33350: loss = 2.6083363e-10,5.3107785e-09\n",
      "Iteration 33355: loss = 2.6082328e-10,5.3104863e-09\n",
      "Iteration 33360: loss = 2.6080782e-10,5.3102065e-09\n",
      "Iteration 33365: loss = 2.6079838e-10,5.3099107e-09\n",
      "Iteration 33370: loss = 2.607912e-10,5.30961e-09\n",
      "Iteration 33375: loss = 2.6078448e-10,5.309307e-09\n",
      "Iteration 33380: loss = 2.6077765e-10,5.3090057e-09\n",
      "Iteration 33385: loss = 2.6076916e-10,5.308709e-09\n",
      "Iteration 33390: loss = 2.607582e-10,5.308419e-09\n",
      "Iteration 33395: loss = 2.607454e-10,5.3081335e-09\n",
      "Iteration 33400: loss = 2.607303e-10,5.3078546e-09\n",
      "Iteration 33405: loss = 2.6071342e-10,5.3075815e-09\n",
      "Iteration 33410: loss = 2.606943e-10,5.307314e-09\n",
      "Iteration 33415: loss = 2.6067334e-10,5.3070535e-09\n",
      "Iteration 33420: loss = 2.6065014e-10,5.306797e-09\n",
      "Iteration 33425: loss = 2.606265e-10,5.306544e-09\n",
      "Iteration 33430: loss = 2.605996e-10,5.3062985e-09\n",
      "Iteration 33435: loss = 2.6057229e-10,5.306056e-09\n",
      "Iteration 33440: loss = 2.6054311e-10,5.3058153e-09\n",
      "Iteration 33445: loss = 2.6051286e-10,5.3055818e-09\n",
      "Iteration 33450: loss = 2.6048091e-10,5.3053517e-09\n",
      "Iteration 33455: loss = 2.6044855e-10,5.3051226e-09\n",
      "Iteration 33460: loss = 2.6042168e-10,5.304879e-09\n",
      "Iteration 33465: loss = 2.6040178e-10,5.3046176e-09\n",
      "Iteration 33470: loss = 2.6038513e-10,5.3043467e-09\n",
      "Iteration 33475: loss = 2.603699e-10,5.304074e-09\n",
      "Iteration 33480: loss = 2.6035388e-10,5.303801e-09\n",
      "Iteration 33485: loss = 2.603373e-10,5.303531e-09\n",
      "Iteration 33490: loss = 2.60319e-10,5.3032663e-09\n",
      "Iteration 33495: loss = 2.6029984e-10,5.303004e-09\n",
      "Iteration 33500: loss = 2.6027863e-10,5.3027467e-09\n",
      "Iteration 33505: loss = 2.602562e-10,5.3024958e-09\n",
      "Iteration 33510: loss = 2.602316e-10,5.3022475e-09\n",
      "Iteration 33515: loss = 2.602062e-10,5.3020037e-09\n",
      "Iteration 33520: loss = 2.6017863e-10,5.301765e-09\n",
      "Iteration 33525: loss = 2.6014974e-10,5.3015303e-09\n",
      "Iteration 33530: loss = 2.6012048e-10,5.301298e-09\n",
      "Iteration 33535: loss = 2.6008923e-10,5.301069e-09\n",
      "Iteration 33540: loss = 2.6005684e-10,5.300845e-09\n",
      "Iteration 33545: loss = 2.6002353e-10,5.300623e-09\n",
      "Iteration 33550: loss = 2.5998964e-10,5.300404e-09\n",
      "Iteration 33555: loss = 2.5995503e-10,5.300188e-09\n",
      "Iteration 33560: loss = 2.5992192e-10,5.299966e-09\n",
      "Iteration 33565: loss = 2.598977e-10,5.2997207e-09\n",
      "Iteration 33570: loss = 2.5987754e-10,5.299465e-09\n",
      "Iteration 33575: loss = 2.5985944e-10,5.2992037e-09\n",
      "Iteration 33580: loss = 2.5984168e-10,5.298941e-09\n",
      "Iteration 33585: loss = 2.5982394e-10,5.2986797e-09\n",
      "Iteration 33590: loss = 2.5980548e-10,5.2984213e-09\n",
      "Iteration 33595: loss = 2.5978583e-10,5.2981646e-09\n",
      "Iteration 33600: loss = 2.597639e-10,5.297915e-09\n",
      "Iteration 33605: loss = 2.579857e-10,5.2976623e-09\n",
      "Iteration 33610: loss = 2.5975297e-10,5.297327e-09\n",
      "Iteration 33615: loss = 2.5975555e-10,5.2970104e-09\n",
      "Iteration 33620: loss = 2.5799227e-10,5.2967177e-09\n",
      "Iteration 33625: loss = 2.5975982e-10,5.2963838e-09\n",
      "Iteration 33630: loss = 2.5799546e-10,5.2960942e-09\n",
      "Iteration 33635: loss = 2.597604e-10,5.2957656e-09\n",
      "Iteration 33640: loss = 2.5799599e-10,5.2954787e-09\n",
      "Iteration 33645: loss = 2.5976224e-10,5.2951465e-09\n",
      "Iteration 33650: loss = 2.5800076e-10,5.2948512e-09\n",
      "Iteration 33655: loss = 2.5976224e-10,5.294531e-09\n",
      "Iteration 33660: loss = 2.5976357e-10,5.294223e-09\n",
      "Iteration 33665: loss = 2.5976474e-10,5.293914e-09\n",
      "Iteration 33670: loss = 2.5976762e-10,5.2936002e-09\n",
      "Iteration 33675: loss = 2.5800648e-10,5.2933045e-09\n",
      "Iteration 33680: loss = 2.5977e-10,5.292982e-09\n",
      "Iteration 33685: loss = 2.5801106e-10,5.2926805e-09\n",
      "Iteration 33690: loss = 2.5801136e-10,5.292375e-09\n",
      "Iteration 33695: loss = 2.5800978e-10,5.2920752e-09\n",
      "Iteration 33700: loss = 2.5977598e-10,5.2917475e-09\n",
      "Iteration 33705: loss = 2.5801777e-10,5.291445e-09\n",
      "Iteration 33710: loss = 2.5801863e-10,5.291138e-09\n",
      "Iteration 33715: loss = 2.5801966e-10,5.2908318e-09\n",
      "Iteration 33720: loss = 2.597811e-10,5.2905165e-09\n",
      "Iteration 33725: loss = 2.5801503e-10,5.290237e-09\n",
      "Iteration 33730: loss = 2.5801258e-10,5.289942e-09\n",
      "Iteration 33735: loss = 2.5801353e-10,5.2896345e-09\n",
      "Iteration 33740: loss = 2.5801575e-10,5.2893263e-09\n",
      "Iteration 33745: loss = 2.5801755e-10,5.2890186e-09\n",
      "Iteration 33750: loss = 2.5801847e-10,5.2887144e-09\n",
      "Iteration 33755: loss = 2.580172e-10,5.2884155e-09\n",
      "Iteration 33760: loss = 2.580132e-10,5.2881246e-09\n",
      "Iteration 33765: loss = 2.5800728e-10,5.28784e-09\n",
      "Iteration 33770: loss = 2.579981e-10,5.2875633e-09\n",
      "Iteration 33775: loss = 2.5798705e-10,5.287291e-09\n",
      "Iteration 33780: loss = 2.579734e-10,5.2870277e-09\n",
      "Iteration 33785: loss = 2.5795757e-10,5.2867697e-09\n",
      "Iteration 33790: loss = 2.5793997e-10,5.286517e-09\n",
      "Iteration 33795: loss = 2.5791988e-10,5.2862705e-09\n",
      "Iteration 33800: loss = 2.5789834e-10,5.2860303e-09\n",
      "Iteration 33805: loss = 2.578751e-10,5.285793e-09\n",
      "Iteration 33810: loss = 2.5785046e-10,5.2855613e-09\n",
      "Iteration 33815: loss = 2.578243e-10,5.2853335e-09\n",
      "Iteration 33820: loss = 2.577971e-10,5.2851057e-09\n",
      "Iteration 33825: loss = 2.5776759e-10,5.2848863e-09\n",
      "Iteration 33830: loss = 2.5773803e-10,5.2846687e-09\n",
      "Iteration 33835: loss = 2.5770838e-10,5.28445e-09\n",
      "Iteration 33840: loss = 2.576867e-10,5.284211e-09\n",
      "Iteration 33845: loss = 2.5767088e-10,5.2839555e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33850: loss = 2.576573e-10,5.283695e-09\n",
      "Iteration 33855: loss = 2.576441e-10,5.2834324e-09\n",
      "Iteration 33860: loss = 2.576309e-10,5.283171e-09\n",
      "Iteration 33865: loss = 2.5761615e-10,5.2829146e-09\n",
      "Iteration 33870: loss = 2.5759964e-10,5.2826614e-09\n",
      "Iteration 33875: loss = 2.5758168e-10,5.2824127e-09\n",
      "Iteration 33880: loss = 2.5756272e-10,5.282168e-09\n",
      "Iteration 33885: loss = 2.5754143e-10,5.28193e-09\n",
      "Iteration 33890: loss = 2.575191e-10,5.281694e-09\n",
      "Iteration 33895: loss = 2.5749505e-10,5.281464e-09\n",
      "Iteration 33900: loss = 2.5746902e-10,5.281239e-09\n",
      "Iteration 33905: loss = 2.5744204e-10,5.2810165e-09\n",
      "Iteration 33910: loss = 2.5741403e-10,5.2807962e-09\n",
      "Iteration 33915: loss = 2.57385e-10,5.28058e-09\n",
      "Iteration 33920: loss = 2.573544e-10,5.2803686e-09\n",
      "Iteration 33925: loss = 2.5732305e-10,5.28016e-09\n",
      "Iteration 33930: loss = 2.5729097e-10,5.2799507e-09\n",
      "Iteration 33935: loss = 2.572583e-10,5.2797455e-09\n",
      "Iteration 33940: loss = 2.572248e-10,5.2795435e-09\n",
      "Iteration 33945: loss = 2.5719257e-10,5.2793347e-09\n",
      "Iteration 33950: loss = 2.5717037e-10,5.279105e-09\n",
      "Iteration 33955: loss = 2.5715152e-10,5.278861e-09\n",
      "Iteration 33960: loss = 2.5713612e-10,5.2786118e-09\n",
      "Iteration 33965: loss = 2.5712035e-10,5.2783586e-09\n",
      "Iteration 33970: loss = 2.5710425e-10,5.2781104e-09\n",
      "Iteration 33975: loss = 2.5708743e-10,5.2778635e-09\n",
      "Iteration 33980: loss = 2.570695e-10,5.2776197e-09\n",
      "Iteration 33985: loss = 2.570501e-10,5.2773803e-09\n",
      "Iteration 33990: loss = 2.5702887e-10,5.277146e-09\n",
      "Iteration 33995: loss = 2.5700655e-10,5.276916e-09\n",
      "Iteration 34000: loss = 2.5698257e-10,5.276688e-09\n",
      "Iteration 34005: loss = 2.5695698e-10,5.276465e-09\n",
      "Iteration 34010: loss = 2.5693045e-10,5.2762474e-09\n",
      "Iteration 34015: loss = 2.5690305e-10,5.2760303e-09\n",
      "Iteration 34020: loss = 2.568744e-10,5.275818e-09\n",
      "Iteration 34025: loss = 2.5684446e-10,5.275609e-09\n",
      "Iteration 34030: loss = 2.5681335e-10,5.2754023e-09\n",
      "Iteration 34035: loss = 2.567816e-10,5.275199e-09\n",
      "Iteration 34040: loss = 2.5674915e-10,5.274997e-09\n",
      "Iteration 34045: loss = 2.5671612e-10,5.2747966e-09\n",
      "Iteration 34050: loss = 2.5669017e-10,5.274578e-09\n",
      "Iteration 34055: loss = 2.566812e-10,5.2743134e-09\n",
      "Iteration 34060: loss = 2.5668204e-10,5.2740217e-09\n",
      "Iteration 34065: loss = 2.566853e-10,5.2737237e-09\n",
      "Iteration 34070: loss = 2.5668093e-10,5.2734452e-09\n",
      "Iteration 34075: loss = 2.5667016e-10,5.2731868e-09\n",
      "Iteration 34080: loss = 2.5490948e-10,5.2729328e-09\n",
      "Iteration 34085: loss = 2.566692e-10,5.2726126e-09\n",
      "Iteration 34090: loss = 2.566663e-10,5.272332e-09\n",
      "Iteration 34095: loss = 2.5666777e-10,5.272039e-09\n",
      "Iteration 34100: loss = 2.5667302e-10,5.2717377e-09\n",
      "Iteration 34105: loss = 2.5666852e-10,5.271461e-09\n",
      "Iteration 34110: loss = 2.5491972e-10,5.2711773e-09\n",
      "Iteration 34115: loss = 2.566789e-10,5.270859e-09\n",
      "Iteration 34120: loss = 2.5492067e-10,5.2705977e-09\n",
      "Iteration 34125: loss = 2.5667662e-10,5.270291e-09\n",
      "Iteration 34130: loss = 2.5492664e-10,5.270009e-09\n",
      "Iteration 34135: loss = 2.5492883e-10,5.2697176e-09\n",
      "Iteration 34140: loss = 2.566795e-10,5.2694227e-09\n",
      "Iteration 34145: loss = 2.5667996e-10,5.2691354e-09\n",
      "Iteration 34150: loss = 2.5493338e-10,5.2688467e-09\n",
      "Iteration 34155: loss = 2.566843e-10,5.268552e-09\n",
      "Iteration 34160: loss = 2.566855e-10,5.2682623e-09\n",
      "Iteration 34165: loss = 2.5668676e-10,5.2679727e-09\n",
      "Iteration 34170: loss = 2.5493857e-10,5.2676885e-09\n",
      "Iteration 34175: loss = 2.5493943e-10,5.2674016e-09\n",
      "Iteration 34180: loss = 2.5669217e-10,5.2671028e-09\n",
      "Iteration 34185: loss = 2.5494393e-10,5.266819e-09\n",
      "Iteration 34190: loss = 2.5494257e-10,5.2665388e-09\n",
      "Iteration 34195: loss = 2.5669813e-10,5.266233e-09\n",
      "Iteration 34200: loss = 2.5494606e-10,5.265961e-09\n",
      "Iteration 34205: loss = 2.5670047e-10,5.2656595e-09\n",
      "Iteration 34210: loss = 2.5670124e-10,5.2653735e-09\n",
      "Iteration 34215: loss = 2.5670255e-10,5.2650857e-09\n",
      "Iteration 34220: loss = 2.5495087e-10,5.2648126e-09\n",
      "Iteration 34225: loss = 2.5670657e-10,5.2645075e-09\n",
      "Iteration 34230: loss = 2.5495558e-10,5.2642335e-09\n",
      "Iteration 34235: loss = 2.5670976e-10,5.2639324e-09\n",
      "Iteration 34240: loss = 2.549558e-10,5.2636677e-09\n",
      "Iteration 34245: loss = 2.5496025e-10,5.263372e-09\n",
      "Iteration 34250: loss = 2.5496505e-10,5.263077e-09\n",
      "Iteration 34255: loss = 2.5671595e-10,5.262787e-09\n",
      "Iteration 34260: loss = 2.5496322e-10,5.2625166e-09\n",
      "Iteration 34265: loss = 2.5496477e-10,5.262231e-09\n",
      "Iteration 34270: loss = 2.5496813e-10,5.2619415e-09\n",
      "Iteration 34275: loss = 2.5497168e-10,5.2616507e-09\n",
      "Iteration 34280: loss = 2.5497437e-10,5.261362e-09\n",
      "Iteration 34285: loss = 2.5497482e-10,5.2610787e-09\n",
      "Iteration 34290: loss = 2.5497277e-10,5.260803e-09\n",
      "Iteration 34295: loss = 2.5496885e-10,5.260533e-09\n",
      "Iteration 34300: loss = 2.549623e-10,5.26027e-09\n",
      "Iteration 34305: loss = 2.549538e-10,5.2600133e-09\n",
      "Iteration 34310: loss = 2.5494212e-10,5.2597637e-09\n",
      "Iteration 34315: loss = 2.54929e-10,5.2595204e-09\n",
      "Iteration 34320: loss = 2.5491367e-10,5.259281e-09\n",
      "Iteration 34325: loss = 2.5489616e-10,5.259049e-09\n",
      "Iteration 34330: loss = 2.548772e-10,5.2588214e-09\n",
      "Iteration 34335: loss = 2.5485675e-10,5.2585967e-09\n",
      "Iteration 34340: loss = 2.54834e-10,5.2583795e-09\n",
      "Iteration 34345: loss = 2.5481042e-10,5.258163e-09\n",
      "Iteration 34350: loss = 2.547855e-10,5.257951e-09\n",
      "Iteration 34355: loss = 2.5475982e-10,5.2577445e-09\n",
      "Iteration 34360: loss = 2.5473232e-10,5.25754e-09\n",
      "Iteration 34365: loss = 2.5470426e-10,5.2573377e-09\n",
      "Iteration 34370: loss = 2.5467478e-10,5.257139e-09\n",
      "Iteration 34375: loss = 2.5464478e-10,5.2569415e-09\n",
      "Iteration 34380: loss = 2.546137e-10,5.2567484e-09\n",
      "Iteration 34385: loss = 2.5458194e-10,5.2565574e-09\n",
      "Iteration 34390: loss = 2.5454922e-10,5.2563687e-09\n",
      "Iteration 34395: loss = 2.5451832e-10,5.256176e-09\n",
      "Iteration 34400: loss = 2.5449554e-10,5.2559606e-09\n",
      "Iteration 34405: loss = 2.544784e-10,5.255731e-09\n",
      "Iteration 34410: loss = 2.5446367e-10,5.2554934e-09\n",
      "Iteration 34415: loss = 2.544496e-10,5.2552545e-09\n",
      "Iteration 34420: loss = 2.5443572e-10,5.2550155e-09\n",
      "Iteration 34425: loss = 2.5442168e-10,5.254779e-09\n",
      "Iteration 34430: loss = 2.5440564e-10,5.2545444e-09\n",
      "Iteration 34435: loss = 2.5438837e-10,5.254315e-09\n",
      "Iteration 34440: loss = 2.5436958e-10,5.2540905e-09\n",
      "Iteration 34445: loss = 2.543495e-10,5.2538693e-09\n",
      "Iteration 34450: loss = 2.5432828e-10,5.2536517e-09\n",
      "Iteration 34455: loss = 2.5430555e-10,5.253438e-09\n",
      "Iteration 34460: loss = 2.54281e-10,5.2532294e-09\n",
      "Iteration 34465: loss = 2.5425576e-10,5.2530233e-09\n",
      "Iteration 34470: loss = 2.542296e-10,5.25282e-09\n",
      "Iteration 34475: loss = 2.5420205e-10,5.2526197e-09\n",
      "Iteration 34480: loss = 2.541766e-10,5.2524145e-09\n",
      "Iteration 34485: loss = 2.5415955e-10,5.252186e-09\n",
      "Iteration 34490: loss = 2.5414695e-10,5.2519464e-09\n",
      "Iteration 34495: loss = 2.5413632e-10,5.2517017e-09\n",
      "Iteration 34500: loss = 2.5412536e-10,5.251456e-09\n",
      "Iteration 34505: loss = 2.5411442e-10,5.2512124e-09\n",
      "Iteration 34510: loss = 2.5410277e-10,5.2509717e-09\n",
      "Iteration 34515: loss = 2.5409e-10,5.2507314e-09\n",
      "Iteration 34520: loss = 2.540753e-10,5.2504987e-09\n",
      "Iteration 34525: loss = 2.5405894e-10,5.250271e-09\n",
      "Iteration 34530: loss = 2.5404062e-10,5.250046e-09\n",
      "Iteration 34535: loss = 2.5402844e-10,5.249807e-09\n",
      "Iteration 34540: loss = 2.5402205e-10,5.249552e-09\n",
      "Iteration 34545: loss = 2.5401925e-10,5.2492863e-09\n",
      "Iteration 34550: loss = 2.5401722e-10,5.249019e-09\n",
      "Iteration 34555: loss = 2.5401395e-10,5.248756e-09\n",
      "Iteration 34560: loss = 2.5400956e-10,5.2484963e-09\n",
      "Iteration 34565: loss = 2.54003e-10,5.248241e-09\n",
      "Iteration 34570: loss = 2.539948e-10,5.2479914e-09\n",
      "Iteration 34575: loss = 2.5398453e-10,5.2477485e-09\n",
      "Iteration 34580: loss = 2.5397257e-10,5.247509e-09\n",
      "Iteration 34585: loss = 2.5395816e-10,5.2472764e-09\n",
      "Iteration 34590: loss = 2.5394206e-10,5.247049e-09\n",
      "Iteration 34595: loss = 2.5392474e-10,5.246826e-09\n",
      "Iteration 34600: loss = 2.5390576e-10,5.2466067e-09\n",
      "Iteration 34605: loss = 2.5388447e-10,5.2463927e-09\n",
      "Iteration 34610: loss = 2.5386246e-10,5.2461826e-09\n",
      "Iteration 34615: loss = 2.5383887e-10,5.2459757e-09\n",
      "Iteration 34620: loss = 2.5381378e-10,5.245774e-09\n",
      "Iteration 34625: loss = 2.537878e-10,5.2455746e-09\n",
      "Iteration 34630: loss = 2.5376096e-10,5.245379e-09\n",
      "Iteration 34635: loss = 2.5373298e-10,5.245184e-09\n",
      "Iteration 34640: loss = 2.5370453e-10,5.244991e-09\n",
      "Iteration 34645: loss = 2.5367483e-10,5.244802e-09\n",
      "Iteration 34650: loss = 2.519059e-10,5.2446163e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34655: loss = 2.5364985e-10,5.244332e-09\n",
      "Iteration 34660: loss = 2.5365907e-10,5.244038e-09\n",
      "Iteration 34665: loss = 2.5191046e-10,5.243796e-09\n",
      "Iteration 34670: loss = 2.5365493e-10,5.2435114e-09\n",
      "Iteration 34675: loss = 2.5365418e-10,5.243244e-09\n",
      "Iteration 34680: loss = 2.5365784e-10,5.242965e-09\n",
      "Iteration 34685: loss = 2.5191635e-10,5.242705e-09\n",
      "Iteration 34690: loss = 2.5366573e-10,5.242406e-09\n",
      "Iteration 34695: loss = 2.5366167e-10,5.24215e-09\n",
      "Iteration 34700: loss = 2.5192284e-10,5.241883e-09\n",
      "Iteration 34705: loss = 2.536683e-10,5.2415965e-09\n",
      "Iteration 34710: loss = 2.5192354e-10,5.241345e-09\n",
      "Iteration 34715: loss = 2.5367167e-10,5.2410507e-09\n",
      "Iteration 34720: loss = 2.5192826e-10,5.2407967e-09\n",
      "Iteration 34725: loss = 2.536744e-10,5.2405076e-09\n",
      "Iteration 34730: loss = 2.5192856e-10,5.240262e-09\n",
      "Iteration 34735: loss = 2.5367777e-10,5.239964e-09\n",
      "Iteration 34740: loss = 2.5193558e-10,5.239709e-09\n",
      "Iteration 34745: loss = 2.5193905e-10,5.239432e-09\n",
      "Iteration 34750: loss = 2.536829e-10,5.23915e-09\n",
      "Iteration 34755: loss = 2.5193952e-10,5.2388973e-09\n",
      "Iteration 34760: loss = 2.51944e-10,5.2386193e-09\n",
      "Iteration 34765: loss = 2.519421e-10,5.2383573e-09\n",
      "Iteration 34770: loss = 2.5193833e-10,5.238102e-09\n",
      "Iteration 34775: loss = 2.5194194e-10,5.2378253e-09\n",
      "Iteration 34780: loss = 2.5194904e-10,5.237541e-09\n",
      "Iteration 34785: loss = 2.5195449e-10,5.2372604e-09\n",
      "Iteration 34790: loss = 2.5195185e-10,5.2370033e-09\n",
      "Iteration 34795: loss = 2.5195118e-10,5.236737e-09\n",
      "Iteration 34800: loss = 2.5195215e-10,5.236471e-09\n",
      "Iteration 34805: loss = 2.5195204e-10,5.2362052e-09\n",
      "Iteration 34810: loss = 2.519505e-10,5.235945e-09\n",
      "Iteration 34815: loss = 2.5194666e-10,5.2356897e-09\n",
      "Iteration 34820: loss = 2.5194116e-10,5.2354405e-09\n",
      "Iteration 34825: loss = 2.519332e-10,5.2351976e-09\n",
      "Iteration 34830: loss = 2.5192323e-10,5.23496e-09\n",
      "Iteration 34835: loss = 2.5191113e-10,5.234726e-09\n",
      "Iteration 34840: loss = 2.5189736e-10,5.2345017e-09\n",
      "Iteration 34845: loss = 2.518817e-10,5.2342806e-09\n",
      "Iteration 34850: loss = 2.5186944e-10,5.2340496e-09\n",
      "Iteration 34855: loss = 2.51864e-10,5.2338e-09\n",
      "Iteration 34860: loss = 2.5186273e-10,5.2335416e-09\n",
      "Iteration 34865: loss = 2.5186128e-10,5.2332805e-09\n",
      "Iteration 34870: loss = 2.518607e-10,5.2330185e-09\n",
      "Iteration 34875: loss = 2.5185792e-10,5.2327622e-09\n",
      "Iteration 34880: loss = 2.5185404e-10,5.2325095e-09\n",
      "Iteration 34885: loss = 2.518483e-10,5.232263e-09\n",
      "Iteration 34890: loss = 2.518398e-10,5.2320233e-09\n",
      "Iteration 34895: loss = 2.518296e-10,5.2317874e-09\n",
      "Iteration 34900: loss = 2.5181793e-10,5.231558e-09\n",
      "Iteration 34905: loss = 2.518034e-10,5.231333e-09\n",
      "Iteration 34910: loss = 2.5178773e-10,5.2311138e-09\n",
      "Iteration 34915: loss = 2.5177013e-10,5.2308993e-09\n",
      "Iteration 34920: loss = 2.5175098e-10,5.230689e-09\n",
      "Iteration 34925: loss = 2.5173053e-10,5.230483e-09\n",
      "Iteration 34930: loss = 2.5170846e-10,5.23028e-09\n",
      "Iteration 34935: loss = 2.516859e-10,5.2300804e-09\n",
      "Iteration 34940: loss = 2.5166166e-10,5.2298854e-09\n",
      "Iteration 34945: loss = 2.5163657e-10,5.2296913e-09\n",
      "Iteration 34950: loss = 2.516099e-10,5.2295026e-09\n",
      "Iteration 34955: loss = 2.5158312e-10,5.2293156e-09\n",
      "Iteration 34960: loss = 2.5155542e-10,5.2291282e-09\n",
      "Iteration 34965: loss = 2.5152705e-10,5.2289444e-09\n",
      "Iteration 34970: loss = 2.5149746e-10,5.2287645e-09\n",
      "Iteration 34975: loss = 2.5146885e-10,5.2285825e-09\n",
      "Iteration 34980: loss = 2.5144967e-10,5.2283746e-09\n",
      "Iteration 34985: loss = 2.514353e-10,5.2281526e-09\n",
      "Iteration 34990: loss = 2.5142413e-10,5.2279225e-09\n",
      "Iteration 34995: loss = 2.5141444e-10,5.227688e-09\n",
      "Iteration 35000: loss = 2.5140437e-10,5.2274567e-09\n",
      "Iteration 35005: loss = 2.5139574e-10,5.2272195e-09\n",
      "Iteration 35010: loss = 2.5139532e-10,5.22696e-09\n",
      "Iteration 35015: loss = 2.513992e-10,5.22669e-09\n",
      "Iteration 35020: loss = 2.513972e-10,5.226436e-09\n",
      "Iteration 35025: loss = 2.5138733e-10,5.2262044e-09\n",
      "Iteration 35030: loss = 2.5137054e-10,5.2259894e-09\n",
      "Iteration 35035: loss = 2.5135144e-10,5.2257825e-09\n",
      "Iteration 35040: loss = 2.513374e-10,5.225561e-09\n",
      "Iteration 35045: loss = 2.5132904e-10,5.2253255e-09\n",
      "Iteration 35050: loss = 2.513224e-10,5.2250853e-09\n",
      "Iteration 35055: loss = 2.513131e-10,5.224852e-09\n",
      "Iteration 35060: loss = 2.5130104e-10,5.224626e-09\n",
      "Iteration 35065: loss = 2.5129132e-10,5.2243943e-09\n",
      "Iteration 35070: loss = 2.5128066e-10,5.2241638e-09\n",
      "Iteration 35075: loss = 2.5126906e-10,5.2239386e-09\n",
      "Iteration 35080: loss = 2.5125604e-10,5.223715e-09\n",
      "Iteration 35085: loss = 2.5124214e-10,5.2234954e-09\n",
      "Iteration 35090: loss = 2.5122662e-10,5.223279e-09\n",
      "Iteration 35095: loss = 2.512095e-10,5.2230686e-09\n",
      "Iteration 35100: loss = 2.5119112e-10,5.2228604e-09\n",
      "Iteration 35105: loss = 2.5117117e-10,5.2226556e-09\n",
      "Iteration 35110: loss = 2.5115032e-10,5.222456e-09\n",
      "Iteration 35115: loss = 2.511279e-10,5.2222595e-09\n",
      "Iteration 35120: loss = 2.511044e-10,5.2220672e-09\n",
      "Iteration 35125: loss = 2.5107996e-10,5.2218754e-09\n",
      "Iteration 35130: loss = 2.51055e-10,5.221688e-09\n",
      "Iteration 35135: loss = 2.510288e-10,5.2215015e-09\n",
      "Iteration 35140: loss = 2.510019e-10,5.2213194e-09\n",
      "Iteration 35145: loss = 2.509736e-10,5.2211386e-09\n",
      "Iteration 35150: loss = 2.5094526e-10,5.2209614e-09\n",
      "Iteration 35155: loss = 2.509161e-10,5.220783e-09\n",
      "Iteration 35160: loss = 2.5089328e-10,5.2205893e-09\n",
      "Iteration 35165: loss = 2.5087832e-10,5.2203744e-09\n",
      "Iteration 35170: loss = 2.508669e-10,5.2201496e-09\n",
      "Iteration 35175: loss = 2.5085659e-10,5.219921e-09\n",
      "Iteration 35180: loss = 2.5084662e-10,5.219693e-09\n",
      "Iteration 35185: loss = 2.5083627e-10,5.219466e-09\n",
      "Iteration 35190: loss = 2.5082486e-10,5.219241e-09\n",
      "Iteration 35195: loss = 2.508124e-10,5.21902e-09\n",
      "Iteration 35200: loss = 2.5079763e-10,5.2188045e-09\n",
      "Iteration 35205: loss = 2.5078212e-10,5.2185922e-09\n",
      "Iteration 35210: loss = 2.5076494e-10,5.218383e-09\n",
      "Iteration 35215: loss = 2.507464e-10,5.2181783e-09\n",
      "Iteration 35220: loss = 2.5072647e-10,5.217979e-09\n",
      "Iteration 35225: loss = 2.5070518e-10,5.217782e-09\n",
      "Iteration 35230: loss = 2.50683e-10,5.217588e-09\n",
      "Iteration 35235: loss = 2.4894634e-10,5.217354e-09\n",
      "Iteration 35240: loss = 2.5069e-10,5.2170592e-09\n",
      "Iteration 35245: loss = 2.506859e-10,5.2168154e-09\n",
      "Iteration 35250: loss = 2.506862e-10,5.2165614e-09\n",
      "Iteration 35255: loss = 2.5069058e-10,5.216296e-09\n",
      "Iteration 35260: loss = 2.4895677e-10,5.2160534e-09\n",
      "Iteration 35265: loss = 2.5069335e-10,5.2157803e-09\n",
      "Iteration 35270: loss = 2.5069283e-10,5.215526e-09\n",
      "Iteration 35275: loss = 2.5069655e-10,5.2152602e-09\n",
      "Iteration 35280: loss = 2.4896368e-10,5.2150195e-09\n",
      "Iteration 35285: loss = 2.5069902e-10,5.214749e-09\n",
      "Iteration 35290: loss = 2.507008e-10,5.2144906e-09\n",
      "Iteration 35295: loss = 2.5070154e-10,5.2142353e-09\n",
      "Iteration 35300: loss = 2.507067e-10,5.2139675e-09\n",
      "Iteration 35305: loss = 2.4897562e-10,5.2137192e-09\n",
      "Iteration 35310: loss = 2.489778e-10,5.2134608e-09\n",
      "Iteration 35315: loss = 2.489802e-10,5.213201e-09\n",
      "Iteration 35320: loss = 2.489809e-10,5.2129483e-09\n",
      "Iteration 35325: loss = 2.5071442e-10,5.2126814e-09\n",
      "Iteration 35330: loss = 2.4898567e-10,5.2124283e-09\n",
      "Iteration 35335: loss = 2.4898472e-10,5.2121782e-09\n",
      "Iteration 35340: loss = 2.4898886e-10,5.2119136e-09\n",
      "Iteration 35345: loss = 2.4898786e-10,5.211665e-09\n",
      "Iteration 35350: loss = 2.489874e-10,5.211414e-09\n",
      "Iteration 35355: loss = 2.5072724e-10,5.2111324e-09\n",
      "Iteration 35360: loss = 2.4899374e-10,5.210893e-09\n",
      "Iteration 35365: loss = 2.4899757e-10,5.210631e-09\n",
      "Iteration 35370: loss = 2.4899957e-10,5.2103735e-09\n",
      "Iteration 35375: loss = 2.4899238e-10,5.2101408e-09\n",
      "Iteration 35380: loss = 2.4899424e-10,5.2098845e-09\n",
      "Iteration 35385: loss = 2.4900024e-10,5.2096167e-09\n",
      "Iteration 35390: loss = 2.490081e-10,5.209344e-09\n",
      "Iteration 35395: loss = 2.4901e-10,5.209088e-09\n",
      "Iteration 35400: loss = 2.490064e-10,5.2088467e-09\n",
      "Iteration 35405: loss = 2.4900437e-10,5.208601e-09\n",
      "Iteration 35410: loss = 2.490033e-10,5.2083537e-09\n",
      "Iteration 35415: loss = 2.490013e-10,5.2081077e-09\n",
      "Iteration 35420: loss = 2.4899788e-10,5.207866e-09\n",
      "Iteration 35425: loss = 2.4899285e-10,5.2076294e-09\n",
      "Iteration 35430: loss = 2.4898575e-10,5.207399e-09\n",
      "Iteration 35435: loss = 2.4897723e-10,5.207172e-09\n",
      "Iteration 35440: loss = 2.489664e-10,5.206951e-09\n",
      "Iteration 35445: loss = 2.4895383e-10,5.2067355e-09\n",
      "Iteration 35450: loss = 2.4893929e-10,5.206525e-09\n",
      "Iteration 35455: loss = 2.4892385e-10,5.2063163e-09\n",
      "Iteration 35460: loss = 2.489066e-10,5.206113e-09\n",
      "Iteration 35465: loss = 2.488875e-10,5.2059153e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35470: loss = 2.4886745e-10,5.2057194e-09\n",
      "Iteration 35475: loss = 2.488465e-10,5.2055276e-09\n",
      "Iteration 35480: loss = 2.4882404e-10,5.2053393e-09\n",
      "Iteration 35485: loss = 2.4880026e-10,5.2051545e-09\n",
      "Iteration 35490: loss = 2.487768e-10,5.2049702e-09\n",
      "Iteration 35495: loss = 2.4876068e-10,5.2047637e-09\n",
      "Iteration 35500: loss = 2.4875094e-10,5.2045404e-09\n",
      "Iteration 35505: loss = 2.487442e-10,5.2043094e-09\n",
      "Iteration 35510: loss = 2.487385e-10,5.2040763e-09\n",
      "Iteration 35515: loss = 2.487321e-10,5.203846e-09\n",
      "Iteration 35520: loss = 2.4872543e-10,5.2036153e-09\n",
      "Iteration 35525: loss = 2.4871724e-10,5.2033893e-09\n",
      "Iteration 35530: loss = 2.4870747e-10,5.203166e-09\n",
      "Iteration 35535: loss = 2.4869634e-10,5.202948e-09\n",
      "Iteration 35540: loss = 2.486832e-10,5.2027356e-09\n",
      "Iteration 35545: loss = 2.4866942e-10,5.202526e-09\n",
      "Iteration 35550: loss = 2.4865335e-10,5.20232e-09\n",
      "Iteration 35555: loss = 2.4863564e-10,5.2021205e-09\n",
      "Iteration 35560: loss = 2.486167e-10,5.2019242e-09\n",
      "Iteration 35565: loss = 2.4859673e-10,5.201731e-09\n",
      "Iteration 35570: loss = 2.4857602e-10,5.2015396e-09\n",
      "Iteration 35575: loss = 2.485536e-10,5.201352e-09\n",
      "Iteration 35580: loss = 2.4853028e-10,5.201169e-09\n",
      "Iteration 35585: loss = 2.485057e-10,5.200988e-09\n",
      "Iteration 35590: loss = 2.4848093e-10,5.2008087e-09\n",
      "Iteration 35595: loss = 2.4845512e-10,5.200632e-09\n",
      "Iteration 35600: loss = 2.484288e-10,5.2004574e-09\n",
      "Iteration 35605: loss = 2.4840477e-10,5.2002758e-09\n",
      "Iteration 35610: loss = 2.483894e-10,5.200069e-09\n",
      "Iteration 35615: loss = 2.483795e-10,5.199848e-09\n",
      "Iteration 35620: loss = 2.4837202e-10,5.199623e-09\n",
      "Iteration 35625: loss = 2.4836425e-10,5.1993965e-09\n",
      "Iteration 35630: loss = 2.4835714e-10,5.199171e-09\n",
      "Iteration 35635: loss = 2.4834662e-10,5.1989515e-09\n",
      "Iteration 35640: loss = 2.4833316e-10,5.1987423e-09\n",
      "Iteration 35645: loss = 2.4831937e-10,5.1985327e-09\n",
      "Iteration 35650: loss = 2.48306e-10,5.1983227e-09\n",
      "Iteration 35655: loss = 2.4830052e-10,5.1980926e-09\n",
      "Iteration 35660: loss = 2.482978e-10,5.1978524e-09\n",
      "Iteration 35665: loss = 2.4828847e-10,5.1976317e-09\n",
      "Iteration 35670: loss = 2.4827387e-10,5.197426e-09\n",
      "Iteration 35675: loss = 2.4825733e-10,5.197225e-09\n",
      "Iteration 35680: loss = 2.4824748e-10,5.197006e-09\n",
      "Iteration 35685: loss = 2.4823898e-10,5.196784e-09\n",
      "Iteration 35690: loss = 2.4822625e-10,5.196573e-09\n",
      "Iteration 35695: loss = 2.482157e-10,5.1963562e-09\n",
      "Iteration 35700: loss = 2.482044e-10,5.1961426e-09\n",
      "Iteration 35705: loss = 2.481942e-10,5.195924e-09\n",
      "Iteration 35710: loss = 2.4818328e-10,5.1957083e-09\n",
      "Iteration 35715: loss = 2.481703e-10,5.1954987e-09\n",
      "Iteration 35720: loss = 2.481587e-10,5.1952855e-09\n",
      "Iteration 35725: loss = 2.4814722e-10,5.1950733e-09\n",
      "Iteration 35730: loss = 2.4813487e-10,5.194861e-09\n",
      "Iteration 35735: loss = 2.4812158e-10,5.194652e-09\n",
      "Iteration 35740: loss = 2.4810737e-10,5.1944453e-09\n",
      "Iteration 35745: loss = 2.4809158e-10,5.194244e-09\n",
      "Iteration 35750: loss = 2.4807453e-10,5.194046e-09\n",
      "Iteration 35755: loss = 2.4805605e-10,5.1938516e-09\n",
      "Iteration 35760: loss = 2.4803712e-10,5.1936597e-09\n",
      "Iteration 35765: loss = 2.4801708e-10,5.1934697e-09\n",
      "Iteration 35770: loss = 2.4799537e-10,5.193284e-09\n",
      "Iteration 35775: loss = 2.479725e-10,5.1931033e-09\n",
      "Iteration 35780: loss = 2.4794902e-10,5.1929225e-09\n",
      "Iteration 35785: loss = 2.4792493e-10,5.1927453e-09\n",
      "Iteration 35790: loss = 2.4790003e-10,5.192569e-09\n",
      "Iteration 35795: loss = 2.4787508e-10,5.192394e-09\n",
      "Iteration 35800: loss = 2.4784824e-10,5.192222e-09\n",
      "Iteration 35805: loss = 2.478226e-10,5.1920495e-09\n",
      "Iteration 35810: loss = 2.478058e-10,5.191852e-09\n",
      "Iteration 35815: loss = 2.4779523e-10,5.191637e-09\n",
      "Iteration 35820: loss = 2.4778726e-10,5.1914157e-09\n",
      "Iteration 35825: loss = 2.4778055e-10,5.19119e-09\n",
      "Iteration 35830: loss = 2.477734e-10,5.1909663e-09\n",
      "Iteration 35835: loss = 2.4776597e-10,5.1907443e-09\n",
      "Iteration 35840: loss = 2.4775612e-10,5.190528e-09\n",
      "Iteration 35845: loss = 2.477631e-10,5.190267e-09\n",
      "Iteration 35850: loss = 2.4776603e-10,5.1900138e-09\n",
      "Iteration 35855: loss = 2.4604077e-10,5.1897855e-09\n",
      "Iteration 35860: loss = 2.4777044e-10,5.1895155e-09\n",
      "Iteration 35865: loss = 2.4776872e-10,5.1892775e-09\n",
      "Iteration 35870: loss = 2.4777072e-10,5.18903e-09\n",
      "Iteration 35875: loss = 2.4777028e-10,5.188789e-09\n",
      "Iteration 35880: loss = 2.4777536e-10,5.1885327e-09\n",
      "Iteration 35885: loss = 2.4605415e-10,5.1882947e-09\n",
      "Iteration 35890: loss = 2.4777888e-10,5.1880393e-09\n",
      "Iteration 35895: loss = 2.4777927e-10,5.1877946e-09\n",
      "Iteration 35900: loss = 2.4778207e-10,5.1875437e-09\n",
      "Iteration 35905: loss = 2.4778266e-10,5.1873013e-09\n",
      "Iteration 35910: loss = 2.4778546e-10,5.1870526e-09\n",
      "Iteration 35915: loss = 2.4778773e-10,5.186803e-09\n",
      "Iteration 35920: loss = 2.4606742e-10,5.186564e-09\n",
      "Iteration 35925: loss = 2.460701e-10,5.1863145e-09\n",
      "Iteration 35930: loss = 2.477938e-10,5.1860596e-09\n",
      "Iteration 35935: loss = 2.4607502e-10,5.185818e-09\n",
      "Iteration 35940: loss = 2.4607652e-10,5.1855724e-09\n",
      "Iteration 35945: loss = 2.4607724e-10,5.1853277e-09\n",
      "Iteration 35950: loss = 2.460801e-10,5.185079e-09\n",
      "Iteration 35955: loss = 2.4780564e-10,5.184821e-09\n",
      "Iteration 35960: loss = 2.4607924e-10,5.1845994e-09\n",
      "Iteration 35965: loss = 2.4608668e-10,5.1843374e-09\n",
      "Iteration 35970: loss = 2.4609045e-10,5.184087e-09\n",
      "Iteration 35975: loss = 2.4609279e-10,5.1838387e-09\n",
      "Iteration 35980: loss = 2.460887e-10,5.1836073e-09\n",
      "Iteration 35985: loss = 2.4609487e-10,5.1833515e-09\n",
      "Iteration 35990: loss = 2.4609767e-10,5.1831037e-09\n",
      "Iteration 35995: loss = 2.4610053e-10,5.182854e-09\n",
      "Iteration 36000: loss = 2.4610328e-10,5.1826063e-09\n",
      "Iteration 36005: loss = 2.4610253e-10,5.1823674e-09\n",
      "Iteration 36010: loss = 2.4782815e-10,5.1821094e-09\n",
      "Iteration 36015: loss = 2.4610844e-10,5.18187e-09\n",
      "Iteration 36020: loss = 2.4611238e-10,5.18162e-09\n",
      "Iteration 36025: loss = 2.4611083e-10,5.1813833e-09\n",
      "Iteration 36030: loss = 2.4611452e-10,5.181133e-09\n",
      "Iteration 36035: loss = 2.4783978e-10,5.180877e-09\n",
      "Iteration 36040: loss = 2.4611463e-10,5.1806524e-09\n",
      "Iteration 36045: loss = 2.4611288e-10,5.1804174e-09\n",
      "Iteration 36050: loss = 2.4611244e-10,5.1801785e-09\n",
      "Iteration 36055: loss = 2.4611205e-10,5.1799396e-09\n",
      "Iteration 36060: loss = 2.4611102e-10,5.1797024e-09\n",
      "Iteration 36065: loss = 2.4610877e-10,5.179469e-09\n",
      "Iteration 36070: loss = 2.4610444e-10,5.179242e-09\n",
      "Iteration 36075: loss = 2.4609825e-10,5.179019e-09\n",
      "Iteration 36080: loss = 2.4609012e-10,5.1788014e-09\n",
      "Iteration 36085: loss = 2.4608074e-10,5.178587e-09\n",
      "Iteration 36090: loss = 2.4606886e-10,5.1783813e-09\n",
      "Iteration 36095: loss = 2.4605593e-10,5.178176e-09\n",
      "Iteration 36100: loss = 2.460414e-10,5.1779763e-09\n",
      "Iteration 36105: loss = 2.46025e-10,5.1777804e-09\n",
      "Iteration 36110: loss = 2.460077e-10,5.177589e-09\n",
      "Iteration 36115: loss = 2.4598865e-10,5.1774025e-09\n",
      "Iteration 36120: loss = 2.4596894e-10,5.1772173e-09\n",
      "Iteration 36125: loss = 2.459488e-10,5.1770352e-09\n",
      "Iteration 36130: loss = 2.459273e-10,5.176854e-09\n",
      "Iteration 36135: loss = 2.4590413e-10,5.1766778e-09\n",
      "Iteration 36140: loss = 2.4588073e-10,5.176502e-09\n",
      "Iteration 36145: loss = 2.4585622e-10,5.176331e-09\n",
      "Iteration 36150: loss = 2.458314e-10,5.176161e-09\n",
      "Iteration 36155: loss = 2.4580657e-10,5.175991e-09\n",
      "Iteration 36160: loss = 2.4578056e-10,5.1758238e-09\n",
      "Iteration 36165: loss = 2.457539e-10,5.1756577e-09\n",
      "Iteration 36170: loss = 2.4572747e-10,5.175491e-09\n",
      "Iteration 36175: loss = 2.4570704e-10,5.1753077e-09\n",
      "Iteration 36180: loss = 2.45695e-10,5.175105e-09\n",
      "Iteration 36185: loss = 2.4568603e-10,5.1748894e-09\n",
      "Iteration 36190: loss = 2.4567884e-10,5.1746722e-09\n",
      "Iteration 36195: loss = 2.456722e-10,5.1744515e-09\n",
      "Iteration 36200: loss = 2.4566485e-10,5.1742344e-09\n",
      "Iteration 36205: loss = 2.45657e-10,5.1740185e-09\n",
      "Iteration 36210: loss = 2.4564775e-10,5.173806e-09\n",
      "Iteration 36215: loss = 2.4563637e-10,5.1735984e-09\n",
      "Iteration 36220: loss = 2.456236e-10,5.1733955e-09\n",
      "Iteration 36225: loss = 2.4560967e-10,5.1731956e-09\n",
      "Iteration 36230: loss = 2.4559496e-10,5.1730007e-09\n",
      "Iteration 36235: loss = 2.4557806e-10,5.1728084e-09\n",
      "Iteration 36240: loss = 2.455601e-10,5.1726206e-09\n",
      "Iteration 36245: loss = 2.455414e-10,5.172435e-09\n",
      "Iteration 36250: loss = 2.455217e-10,5.172251e-09\n",
      "Iteration 36255: loss = 2.4550098e-10,5.17207e-09\n",
      "Iteration 36260: loss = 2.4547955e-10,5.1718922e-09\n",
      "Iteration 36265: loss = 2.4545666e-10,5.171718e-09\n",
      "Iteration 36270: loss = 2.4543298e-10,5.1715454e-09\n",
      "Iteration 36275: loss = 2.4540892e-10,5.1713744e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36280: loss = 2.4538385e-10,5.1712057e-09\n",
      "Iteration 36285: loss = 2.4535876e-10,5.1710387e-09\n",
      "Iteration 36290: loss = 2.4533345e-10,5.170871e-09\n",
      "Iteration 36295: loss = 2.453072e-10,5.1707056e-09\n",
      "Iteration 36300: loss = 2.4528055e-10,5.1705413e-09\n",
      "Iteration 36305: loss = 2.45253e-10,5.170382e-09\n",
      "Iteration 36310: loss = 2.4523006e-10,5.1702074e-09\n",
      "Iteration 36315: loss = 2.452159e-10,5.1700098e-09\n",
      "Iteration 36320: loss = 2.452066e-10,5.1697997e-09\n",
      "Iteration 36325: loss = 2.451994e-10,5.169583e-09\n",
      "Iteration 36330: loss = 2.4519273e-10,5.1693654e-09\n",
      "Iteration 36335: loss = 2.45186e-10,5.169148e-09\n",
      "Iteration 36340: loss = 2.4517766e-10,5.1689337e-09\n",
      "Iteration 36345: loss = 2.4516875e-10,5.168723e-09\n",
      "Iteration 36350: loss = 2.4515756e-10,5.168517e-09\n",
      "Iteration 36355: loss = 2.451455e-10,5.1683147e-09\n",
      "Iteration 36360: loss = 2.4513244e-10,5.1681153e-09\n",
      "Iteration 36365: loss = 2.451173e-10,5.1679203e-09\n",
      "Iteration 36370: loss = 2.4510124e-10,5.1677267e-09\n",
      "Iteration 36375: loss = 2.4508354e-10,5.1675406e-09\n",
      "Iteration 36380: loss = 2.450656e-10,5.1673554e-09\n",
      "Iteration 36385: loss = 2.4504612e-10,5.167173e-09\n",
      "Iteration 36390: loss = 2.4502614e-10,5.166992e-09\n",
      "Iteration 36395: loss = 2.4500493e-10,5.166816e-09\n",
      "Iteration 36400: loss = 2.4498212e-10,5.1666422e-09\n",
      "Iteration 36405: loss = 2.449587e-10,5.166471e-09\n",
      "Iteration 36410: loss = 2.449349e-10,5.1663016e-09\n",
      "Iteration 36415: loss = 2.4491112e-10,5.166131e-09\n",
      "Iteration 36420: loss = 2.4488625e-10,5.165965e-09\n",
      "Iteration 36425: loss = 2.4486038e-10,5.1657993e-09\n",
      "Iteration 36430: loss = 2.431454e-10,5.1655804e-09\n",
      "Iteration 36435: loss = 2.4487448e-10,5.1652913e-09\n",
      "Iteration 36440: loss = 2.4487465e-10,5.1650586e-09\n",
      "Iteration 36445: loss = 2.4315208e-10,5.1648557e-09\n",
      "Iteration 36450: loss = 2.4487062e-10,5.1645976e-09\n",
      "Iteration 36455: loss = 2.431572e-10,5.164372e-09\n",
      "Iteration 36460: loss = 2.448733e-10,5.1641225e-09\n",
      "Iteration 36465: loss = 2.448744e-10,5.1638844e-09\n",
      "Iteration 36470: loss = 2.4316515e-10,5.163646e-09\n",
      "Iteration 36475: loss = 2.4316682e-10,5.1634084e-09\n",
      "Iteration 36480: loss = 2.4316962e-10,5.163167e-09\n",
      "Iteration 36485: loss = 2.4317087e-10,5.1629296e-09\n",
      "Iteration 36490: loss = 2.4317343e-10,5.162687e-09\n",
      "Iteration 36495: loss = 2.4317662e-10,5.1624447e-09\n",
      "Iteration 36500: loss = 2.4317875e-10,5.1622044e-09\n",
      "Iteration 36505: loss = 2.431794e-10,5.161969e-09\n",
      "Iteration 36510: loss = 2.4318264e-10,5.161726e-09\n",
      "Iteration 36515: loss = 2.4489658e-10,5.161481e-09\n",
      "Iteration 36520: loss = 2.4318386e-10,5.1612554e-09\n",
      "Iteration 36525: loss = 2.4490235e-10,5.1609974e-09\n",
      "Iteration 36530: loss = 2.4318467e-10,5.160785e-09\n",
      "Iteration 36535: loss = 2.44905e-10,5.160524e-09\n",
      "Iteration 36540: loss = 2.431929e-10,5.160295e-09\n",
      "Iteration 36545: loss = 2.4490895e-10,5.160045e-09\n",
      "Iteration 36550: loss = 2.4319927e-10,5.159811e-09\n",
      "Iteration 36555: loss = 2.4320165e-10,5.1595714e-09\n",
      "Iteration 36560: loss = 2.4491578e-10,5.1593263e-09\n",
      "Iteration 36565: loss = 2.4320526e-10,5.1590945e-09\n",
      "Iteration 36570: loss = 2.4320893e-10,5.1588493e-09\n",
      "Iteration 36575: loss = 2.432119e-10,5.158609e-09\n",
      "Iteration 36580: loss = 2.4321203e-10,5.158376e-09\n",
      "Iteration 36585: loss = 2.4492727e-10,5.158128e-09\n",
      "Iteration 36590: loss = 2.4321598e-10,5.157897e-09\n",
      "Iteration 36595: loss = 2.4322044e-10,5.157654e-09\n",
      "Iteration 36600: loss = 2.4322047e-10,5.1574203e-09\n",
      "Iteration 36605: loss = 2.4322036e-10,5.1571876e-09\n",
      "Iteration 36610: loss = 2.4322283e-10,5.1569478e-09\n",
      "Iteration 36615: loss = 2.4322536e-10,5.156708e-09\n",
      "Iteration 36620: loss = 2.4322694e-10,5.1564704e-09\n",
      "Iteration 36625: loss = 2.432268e-10,5.156238e-09\n",
      "Iteration 36630: loss = 2.4322497e-10,5.156011e-09\n",
      "Iteration 36635: loss = 2.4322175e-10,5.155788e-09\n",
      "Iteration 36640: loss = 2.4321667e-10,5.155569e-09\n",
      "Iteration 36645: loss = 2.432091e-10,5.155357e-09\n",
      "Iteration 36650: loss = 2.4320015e-10,5.1551488e-09\n",
      "Iteration 36655: loss = 2.4318955e-10,5.154944e-09\n",
      "Iteration 36660: loss = 2.431777e-10,5.154745e-09\n",
      "Iteration 36665: loss = 2.4316374e-10,5.15455e-09\n",
      "Iteration 36670: loss = 2.4315025e-10,5.154355e-09\n",
      "Iteration 36675: loss = 2.431453e-10,5.154137e-09\n",
      "Iteration 36680: loss = 2.4314573e-10,5.153903e-09\n",
      "Iteration 36685: loss = 2.4314759e-10,5.1536655e-09\n",
      "Iteration 36690: loss = 2.4314958e-10,5.153427e-09\n",
      "Iteration 36695: loss = 2.4315208e-10,5.15319e-09\n",
      "Iteration 36700: loss = 2.4315247e-10,5.1529563e-09\n",
      "Iteration 36705: loss = 2.4315192e-10,5.152726e-09\n",
      "Iteration 36710: loss = 2.431488e-10,5.1525024e-09\n",
      "Iteration 36715: loss = 2.4314376e-10,5.1522844e-09\n",
      "Iteration 36720: loss = 2.431382e-10,5.152069e-09\n",
      "Iteration 36725: loss = 2.431293e-10,5.1518603e-09\n",
      "Iteration 36730: loss = 2.431193e-10,5.151656e-09\n",
      "Iteration 36735: loss = 2.431073e-10,5.151456e-09\n",
      "Iteration 36740: loss = 2.430949e-10,5.15126e-09\n",
      "Iteration 36745: loss = 2.4308e-10,5.151068e-09\n",
      "Iteration 36750: loss = 2.4306449e-10,5.15088e-09\n",
      "Iteration 36755: loss = 2.43047e-10,5.150696e-09\n",
      "Iteration 36760: loss = 2.430292e-10,5.150514e-09\n",
      "Iteration 36765: loss = 2.4301056e-10,5.150333e-09\n",
      "Iteration 36770: loss = 2.4299082e-10,5.150156e-09\n",
      "Iteration 36775: loss = 2.429696e-10,5.1499818e-09\n",
      "Iteration 36780: loss = 2.429478e-10,5.14981e-09\n",
      "Iteration 36785: loss = 2.4292449e-10,5.1496416e-09\n",
      "Iteration 36790: loss = 2.4290184e-10,5.149474e-09\n",
      "Iteration 36795: loss = 2.4287852e-10,5.1493076e-09\n",
      "Iteration 36800: loss = 2.4285432e-10,5.149142e-09\n",
      "Iteration 36805: loss = 2.4282984e-10,5.1489777e-09\n",
      "Iteration 36810: loss = 2.4280494e-10,5.1488143e-09\n",
      "Iteration 36815: loss = 2.4278005e-10,5.148652e-09\n",
      "Iteration 36820: loss = 2.4275326e-10,5.148495e-09\n",
      "Iteration 36825: loss = 2.42726e-10,5.148338e-09\n",
      "Iteration 36830: loss = 2.4269906e-10,5.148181e-09\n",
      "Iteration 36835: loss = 2.4267402e-10,5.1480216e-09\n",
      "Iteration 36840: loss = 2.426576e-10,5.1478333e-09\n",
      "Iteration 36845: loss = 2.426477e-10,5.1476308e-09\n",
      "Iteration 36850: loss = 2.42641e-10,5.1474185e-09\n",
      "Iteration 36855: loss = 2.426351e-10,5.147203e-09\n",
      "Iteration 36860: loss = 2.4262964e-10,5.1469886e-09\n",
      "Iteration 36865: loss = 2.4262317e-10,5.146776e-09\n",
      "Iteration 36870: loss = 2.4261584e-10,5.146566e-09\n",
      "Iteration 36875: loss = 2.4260685e-10,5.1463593e-09\n",
      "Iteration 36880: loss = 2.4259644e-10,5.146158e-09\n",
      "Iteration 36885: loss = 2.425848e-10,5.14596e-09\n",
      "Iteration 36890: loss = 2.4257182e-10,5.145766e-09\n",
      "Iteration 36895: loss = 2.4255745e-10,5.1455733e-09\n",
      "Iteration 36900: loss = 2.425421e-10,5.1453855e-09\n",
      "Iteration 36905: loss = 2.4252483e-10,5.1452025e-09\n",
      "Iteration 36910: loss = 2.4250738e-10,5.145022e-09\n",
      "Iteration 36915: loss = 2.4248856e-10,5.144843e-09\n",
      "Iteration 36920: loss = 2.424694e-10,5.1446647e-09\n",
      "Iteration 36925: loss = 2.4244926e-10,5.14449e-09\n",
      "Iteration 36930: loss = 2.4242786e-10,5.14432e-09\n",
      "Iteration 36935: loss = 2.424054e-10,5.1441518e-09\n",
      "Iteration 36940: loss = 2.4238236e-10,5.1439843e-09\n",
      "Iteration 36945: loss = 2.4235924e-10,5.1438187e-09\n",
      "Iteration 36950: loss = 2.4233515e-10,5.1436553e-09\n",
      "Iteration 36955: loss = 2.4231103e-10,5.143492e-09\n",
      "Iteration 36960: loss = 2.4228708e-10,5.143329e-09\n",
      "Iteration 36965: loss = 2.4226196e-10,5.1431677e-09\n",
      "Iteration 36970: loss = 2.422363e-10,5.1430065e-09\n",
      "Iteration 36975: loss = 2.42211e-10,5.142848e-09\n",
      "Iteration 36980: loss = 2.4218946e-10,5.142677e-09\n",
      "Iteration 36985: loss = 2.4217714e-10,5.1424824e-09\n",
      "Iteration 36990: loss = 2.4216948e-10,5.1422746e-09\n",
      "Iteration 36995: loss = 2.4216365e-10,5.142061e-09\n",
      "Iteration 37000: loss = 2.4215804e-10,5.141846e-09\n",
      "Iteration 37005: loss = 2.4215274e-10,5.141632e-09\n",
      "Iteration 37010: loss = 2.4214641e-10,5.1414215e-09\n",
      "Iteration 37015: loss = 2.4213906e-10,5.141213e-09\n",
      "Iteration 37020: loss = 2.421307e-10,5.1410054e-09\n",
      "Iteration 37025: loss = 2.4211977e-10,5.140806e-09\n",
      "Iteration 37030: loss = 2.4210847e-10,5.140609e-09\n",
      "Iteration 37035: loss = 2.4209557e-10,5.140416e-09\n",
      "Iteration 37040: loss = 2.4208122e-10,5.140226e-09\n",
      "Iteration 37045: loss = 2.4206623e-10,5.140038e-09\n",
      "Iteration 37050: loss = 2.4204902e-10,5.1398557e-09\n",
      "Iteration 37055: loss = 2.4203142e-10,5.1396762e-09\n",
      "Iteration 37060: loss = 2.42013e-10,5.1394986e-09\n",
      "Iteration 37065: loss = 2.420101e-10,5.139279e-09\n",
      "Iteration 37070: loss = 2.4201746e-10,5.139028e-09\n",
      "Iteration 37075: loss = 2.4031158e-10,5.1388156e-09\n",
      "Iteration 37080: loss = 2.420187e-10,5.138568e-09\n",
      "Iteration 37085: loss = 2.4031543e-10,5.138347e-09\n",
      "Iteration 37090: loss = 2.4202168e-10,5.1381033e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37095: loss = 2.4202426e-10,5.1378657e-09\n",
      "Iteration 37100: loss = 2.4202415e-10,5.1376388e-09\n",
      "Iteration 37105: loss = 2.420284e-10,5.137399e-09\n",
      "Iteration 37110: loss = 2.403263e-10,5.137174e-09\n",
      "Iteration 37115: loss = 2.4032942e-10,5.136937e-09\n",
      "Iteration 37120: loss = 2.4203306e-10,5.1367017e-09\n",
      "Iteration 37125: loss = 2.4203925e-10,5.1364553e-09\n",
      "Iteration 37130: loss = 2.4033628e-10,5.1362328e-09\n",
      "Iteration 37135: loss = 2.4204097e-10,5.1359934e-09\n",
      "Iteration 37140: loss = 2.420448e-10,5.1357567e-09\n",
      "Iteration 37145: loss = 2.4034372e-10,5.1355293e-09\n",
      "Iteration 37150: loss = 2.4204685e-10,5.135295e-09\n",
      "Iteration 37155: loss = 2.420499e-10,5.1350577e-09\n",
      "Iteration 37160: loss = 2.4034916e-10,5.13483e-09\n",
      "Iteration 37165: loss = 2.4205368e-10,5.1345914e-09\n",
      "Iteration 37170: loss = 2.4205635e-10,5.1343556e-09\n",
      "Iteration 37175: loss = 2.4205907e-10,5.1341207e-09\n",
      "Iteration 37180: loss = 2.4206312e-10,5.1338827e-09\n",
      "Iteration 37185: loss = 2.4036148e-10,5.133658e-09\n",
      "Iteration 37190: loss = 2.4206576e-10,5.133419e-09\n",
      "Iteration 37195: loss = 2.42069e-10,5.1331823e-09\n",
      "Iteration 37200: loss = 2.420715e-10,5.132947e-09\n",
      "Iteration 37205: loss = 2.4037097e-10,5.1327196e-09\n",
      "Iteration 37210: loss = 2.4037114e-10,5.1324927e-09\n",
      "Iteration 37215: loss = 2.403757e-10,5.132252e-09\n",
      "Iteration 37220: loss = 2.403768e-10,5.1320206e-09\n",
      "Iteration 37225: loss = 2.4208482e-10,5.131774e-09\n",
      "Iteration 37230: loss = 2.4037955e-10,5.1315596e-09\n",
      "Iteration 37235: loss = 2.42088e-10,5.1313096e-09\n",
      "Iteration 37240: loss = 2.4209051e-10,5.1310747e-09\n",
      "Iteration 37245: loss = 2.4038802e-10,5.1308535e-09\n",
      "Iteration 37250: loss = 2.4039337e-10,5.1306106e-09\n",
      "Iteration 37255: loss = 2.4209815e-10,5.1303726e-09\n",
      "Iteration 37260: loss = 2.4039296e-10,5.1301585e-09\n",
      "Iteration 37265: loss = 2.4210373e-10,5.129903e-09\n",
      "Iteration 37270: loss = 2.404004e-10,5.1296833e-09\n",
      "Iteration 37275: loss = 2.4210714e-10,5.1294387e-09\n",
      "Iteration 37280: loss = 2.4040123e-10,5.129227e-09\n",
      "Iteration 37285: loss = 2.4040844e-10,5.12898e-09\n",
      "Iteration 37290: loss = 2.4040922e-10,5.12875e-09\n",
      "Iteration 37295: loss = 2.4040825e-10,5.128526e-09\n",
      "Iteration 37300: loss = 2.4041602e-10,5.128279e-09\n",
      "Iteration 37305: loss = 2.404195e-10,5.128043e-09\n",
      "Iteration 37310: loss = 2.421241e-10,5.1278035e-09\n",
      "Iteration 37315: loss = 2.404192e-10,5.1275886e-09\n",
      "Iteration 37320: loss = 2.40421e-10,5.1273563e-09\n",
      "Iteration 37325: loss = 2.4042554e-10,5.127118e-09\n",
      "Iteration 37330: loss = 2.4042995e-10,5.1268776e-09\n",
      "Iteration 37335: loss = 2.4043467e-10,5.1266396e-09\n",
      "Iteration 37340: loss = 2.4213878e-10,5.1264037e-09\n",
      "Iteration 37345: loss = 2.404278e-10,5.1262052e-09\n",
      "Iteration 37350: loss = 2.404214e-10,5.1259965e-09\n",
      "Iteration 37355: loss = 2.4041608e-10,5.125782e-09\n",
      "Iteration 37360: loss = 2.4041158e-10,5.1255693e-09\n",
      "Iteration 37365: loss = 2.4040545e-10,5.1253597e-09\n",
      "Iteration 37370: loss = 2.4039767e-10,5.125154e-09\n",
      "Iteration 37375: loss = 2.4038932e-10,5.124951e-09\n",
      "Iteration 37380: loss = 2.4037997e-10,5.1247495e-09\n",
      "Iteration 37385: loss = 2.4036936e-10,5.1245532e-09\n",
      "Iteration 37390: loss = 2.4035673e-10,5.1243605e-09\n",
      "Iteration 37395: loss = 2.4034302e-10,5.1241713e-09\n",
      "Iteration 37400: loss = 2.4032795e-10,5.123986e-09\n",
      "Iteration 37405: loss = 2.4031208e-10,5.1238036e-09\n",
      "Iteration 37410: loss = 2.4029528e-10,5.1236233e-09\n",
      "Iteration 37415: loss = 2.4027738e-10,5.123446e-09\n",
      "Iteration 37420: loss = 2.402589e-10,5.1232703e-09\n",
      "Iteration 37425: loss = 2.402387e-10,5.1231e-09\n",
      "Iteration 37430: loss = 2.4021835e-10,5.1229287e-09\n",
      "Iteration 37435: loss = 2.4019736e-10,5.1227604e-09\n",
      "Iteration 37440: loss = 2.401758e-10,5.122593e-09\n",
      "Iteration 37445: loss = 2.401635e-10,5.1224007e-09\n",
      "Iteration 37450: loss = 2.4015745e-10,5.1221916e-09\n",
      "Iteration 37455: loss = 2.401552e-10,5.121972e-09\n",
      "Iteration 37460: loss = 2.4015392e-10,5.1217484e-09\n",
      "Iteration 37465: loss = 2.4015334e-10,5.1215254e-09\n",
      "Iteration 37470: loss = 2.401516e-10,5.1213043e-09\n",
      "Iteration 37475: loss = 2.4014765e-10,5.1210884e-09\n",
      "Iteration 37480: loss = 2.401425e-10,5.120877e-09\n",
      "Iteration 37485: loss = 2.401363e-10,5.1206674e-09\n",
      "Iteration 37490: loss = 2.4012875e-10,5.120463e-09\n",
      "Iteration 37495: loss = 2.401188e-10,5.120265e-09\n",
      "Iteration 37500: loss = 2.4010738e-10,5.1200697e-09\n",
      "Iteration 37505: loss = 2.4009525e-10,5.1198783e-09\n",
      "Iteration 37510: loss = 2.4008157e-10,5.119689e-09\n",
      "Iteration 37515: loss = 2.4006672e-10,5.119504e-09\n",
      "Iteration 37520: loss = 2.400508e-10,5.1193227e-09\n",
      "Iteration 37525: loss = 2.400339e-10,5.1191438e-09\n",
      "Iteration 37530: loss = 2.4001554e-10,5.1189684e-09\n",
      "Iteration 37535: loss = 2.3999674e-10,5.118794e-09\n",
      "Iteration 37540: loss = 2.399773e-10,5.1186224e-09\n",
      "Iteration 37545: loss = 2.3995786e-10,5.1184497e-09\n",
      "Iteration 37550: loss = 2.3993693e-10,5.118282e-09\n",
      "Iteration 37555: loss = 2.3991495e-10,5.1181157e-09\n",
      "Iteration 37560: loss = 2.3989258e-10,5.1179523e-09\n",
      "Iteration 37565: loss = 2.3986957e-10,5.11779e-09\n",
      "Iteration 37570: loss = 2.3984573e-10,5.1176303e-09\n",
      "Iteration 37575: loss = 2.3982208e-10,5.1174687e-09\n",
      "Iteration 37580: loss = 2.397986e-10,5.1173092e-09\n",
      "Iteration 37585: loss = 2.3977384e-10,5.117152e-09\n",
      "Iteration 37590: loss = 2.397493e-10,5.116995e-09\n",
      "Iteration 37595: loss = 2.3972407e-10,5.1168385e-09\n",
      "Iteration 37600: loss = 2.3970206e-10,5.116672e-09\n",
      "Iteration 37605: loss = 2.3968963e-10,5.1164823e-09\n",
      "Iteration 37610: loss = 2.3968233e-10,5.116277e-09\n",
      "Iteration 37615: loss = 2.396783e-10,5.1160636e-09\n",
      "Iteration 37620: loss = 2.396751e-10,5.1158473e-09\n",
      "Iteration 37625: loss = 2.3967192e-10,5.1156306e-09\n",
      "Iteration 37630: loss = 2.3966704e-10,5.1154174e-09\n",
      "Iteration 37635: loss = 2.3966132e-10,5.1152105e-09\n",
      "Iteration 37640: loss = 2.3965357e-10,5.1150053e-09\n",
      "Iteration 37645: loss = 2.3964528e-10,5.1148037e-09\n",
      "Iteration 37650: loss = 2.3963606e-10,5.1146047e-09\n",
      "Iteration 37655: loss = 2.39625e-10,5.1144107e-09\n",
      "Iteration 37660: loss = 2.3961175e-10,5.114222e-09\n",
      "Iteration 37665: loss = 2.395984e-10,5.114035e-09\n",
      "Iteration 37670: loss = 2.3958344e-10,5.1138502e-09\n",
      "Iteration 37675: loss = 2.395679e-10,5.113668e-09\n",
      "Iteration 37680: loss = 2.395515e-10,5.113489e-09\n",
      "Iteration 37685: loss = 2.3953342e-10,5.1133138e-09\n",
      "Iteration 37690: loss = 2.3951457e-10,5.113142e-09\n",
      "Iteration 37695: loss = 2.3949498e-10,5.1129714e-09\n",
      "Iteration 37700: loss = 2.3947375e-10,5.1128026e-09\n",
      "Iteration 37705: loss = 2.3945354e-10,5.112635e-09\n",
      "Iteration 37710: loss = 2.3943206e-10,5.1124696e-09\n",
      "Iteration 37715: loss = 2.394105e-10,5.1123052e-09\n",
      "Iteration 37720: loss = 2.393885e-10,5.11214e-09\n",
      "Iteration 37725: loss = 2.3936578e-10,5.1119775e-09\n",
      "Iteration 37730: loss = 2.3934268e-10,5.1118163e-09\n",
      "Iteration 37735: loss = 2.3931906e-10,5.111657e-09\n",
      "Iteration 37740: loss = 2.3929478e-10,5.1115e-09\n",
      "Iteration 37745: loss = 2.3927002e-10,5.111343e-09\n",
      "Iteration 37750: loss = 2.392445e-10,5.111189e-09\n",
      "Iteration 37755: loss = 2.3922478e-10,5.111019e-09\n",
      "Iteration 37760: loss = 2.392221e-10,5.1108033e-09\n",
      "Iteration 37765: loss = 2.3923588e-10,5.1105404e-09\n",
      "Iteration 37770: loss = 2.392431e-10,5.110297e-09\n",
      "Iteration 37775: loss = 2.3924496e-10,5.1100675e-09\n",
      "Iteration 37780: loss = 2.392432e-10,5.109848e-09\n",
      "Iteration 37785: loss = 2.3923888e-10,5.1096367e-09\n",
      "Iteration 37790: loss = 2.3754343e-10,5.109423e-09\n",
      "Iteration 37795: loss = 2.3924782e-10,5.109162e-09\n",
      "Iteration 37800: loss = 2.3924937e-10,5.1089346e-09\n",
      "Iteration 37805: loss = 2.3924301e-10,5.1087277e-09\n",
      "Iteration 37810: loss = 2.3924962e-10,5.108485e-09\n",
      "Iteration 37815: loss = 2.3925104e-10,5.1082596e-09\n",
      "Iteration 37820: loss = 2.3925037e-10,5.1080367e-09\n",
      "Iteration 37825: loss = 2.3925487e-10,5.1077995e-09\n",
      "Iteration 37830: loss = 2.392547e-10,5.1075766e-09\n",
      "Iteration 37835: loss = 2.3925775e-10,5.107344e-09\n",
      "Iteration 37840: loss = 2.3926003e-10,5.107115e-09\n",
      "Iteration 37845: loss = 2.3756924e-10,5.106889e-09\n",
      "Iteration 37850: loss = 2.3926713e-10,5.106646e-09\n",
      "Iteration 37855: loss = 2.3926885e-10,5.1064184e-09\n",
      "Iteration 37860: loss = 2.3756574e-10,5.1062283e-09\n",
      "Iteration 37865: loss = 2.3925154e-10,5.1060187e-09\n",
      "Iteration 37870: loss = 2.3754e-10,5.105851e-09\n",
      "Iteration 37875: loss = 2.3922256e-10,5.1056497e-09\n",
      "Iteration 37880: loss = 2.3920932e-10,5.1054623e-09\n",
      "Iteration 37885: loss = 2.3750524e-10,5.1052744e-09\n",
      "Iteration 37890: loss = 2.3749236e-10,5.1050866e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37895: loss = 2.3917796e-10,5.104878e-09\n",
      "Iteration 37900: loss = 2.374678e-10,5.1047064e-09\n",
      "Iteration 37905: loss = 2.3915778e-10,5.1044857e-09\n",
      "Iteration 37910: loss = 2.3913818e-10,5.104317e-09\n",
      "Iteration 37915: loss = 2.3912813e-10,5.10412e-09\n",
      "Iteration 37920: loss = 2.374217e-10,5.1039386e-09\n",
      "Iteration 37925: loss = 2.3910304e-10,5.1037423e-09\n",
      "Iteration 37930: loss = 2.3909202e-10,5.103548e-09\n",
      "Iteration 37935: loss = 2.3908167e-10,5.1033533e-09\n",
      "Iteration 37940: loss = 2.390683e-10,5.1031672e-09\n",
      "Iteration 37945: loss = 2.3905491e-10,5.1029803e-09\n",
      "Iteration 37950: loss = 2.3904573e-10,5.102781e-09\n",
      "Iteration 37955: loss = 2.373369e-10,5.102608e-09\n",
      "Iteration 37960: loss = 2.39021e-10,5.1024034e-09\n",
      "Iteration 37965: loss = 2.3731453e-10,5.1022235e-09\n",
      "Iteration 37970: loss = 2.3899646e-10,5.102025e-09\n",
      "Iteration 37975: loss = 2.372908e-10,5.1018407e-09\n",
      "Iteration 37980: loss = 2.3897234e-10,5.1016436e-09\n",
      "Iteration 37985: loss = 2.372654e-10,5.1014637e-09\n",
      "Iteration 37990: loss = 2.3894978e-10,5.1012594e-09\n",
      "Iteration 37995: loss = 2.389353e-10,5.101075e-09\n",
      "Iteration 38000: loss = 2.389252e-10,5.100881e-09\n",
      "Iteration 38005: loss = 2.3891328e-10,5.1006905e-09\n",
      "Iteration 38010: loss = 2.372086e-10,5.100504e-09\n",
      "Iteration 38015: loss = 2.3888821e-10,5.1003126e-09\n",
      "Iteration 38020: loss = 2.3887667e-10,5.1001217e-09\n",
      "Iteration 38025: loss = 2.3717142e-10,5.0999365e-09\n",
      "Iteration 38030: loss = 2.3885346e-10,5.0997393e-09\n",
      "Iteration 38035: loss = 2.3884342e-10,5.099543e-09\n",
      "Iteration 38040: loss = 2.3713545e-10,5.0993676e-09\n",
      "Iteration 38045: loss = 2.3882174e-10,5.0991575e-09\n",
      "Iteration 38050: loss = 2.3711477e-10,5.0989795e-09\n",
      "Iteration 38055: loss = 2.3710298e-10,5.098787e-09\n",
      "Iteration 38060: loss = 2.387844e-10,5.098591e-09\n",
      "Iteration 38065: loss = 2.3707916e-10,5.0984053e-09\n",
      "Iteration 38070: loss = 2.37067e-10,5.0982174e-09\n",
      "Iteration 38075: loss = 2.3874774e-10,5.098023e-09\n",
      "Iteration 38080: loss = 2.3704172e-10,5.097841e-09\n",
      "Iteration 38085: loss = 2.370314e-10,5.0976463e-09\n",
      "Iteration 38090: loss = 2.387146e-10,5.097446e-09\n",
      "Iteration 38095: loss = 2.3870148e-10,5.0972577e-09\n",
      "Iteration 38100: loss = 2.3699534e-10,5.097077e-09\n",
      "Iteration 38105: loss = 2.3868066e-10,5.0968705e-09\n",
      "Iteration 38110: loss = 2.3697141e-10,5.096696e-09\n",
      "Iteration 38115: loss = 2.3696137e-10,5.0965023e-09\n",
      "Iteration 38120: loss = 2.3864186e-10,5.0963083e-09\n",
      "Iteration 38125: loss = 2.3863087e-10,5.096116e-09\n",
      "Iteration 38130: loss = 2.3861912e-10,5.095926e-09\n",
      "Iteration 38135: loss = 2.3860852e-10,5.095732e-09\n",
      "Iteration 38140: loss = 2.369006e-10,5.095556e-09\n",
      "Iteration 38145: loss = 2.3858626e-10,5.095349e-09\n",
      "Iteration 38150: loss = 2.3687582e-10,5.0951794e-09\n",
      "Iteration 38155: loss = 2.3856275e-10,5.094967e-09\n",
      "Iteration 38160: loss = 2.3685667e-10,5.094787e-09\n",
      "Iteration 38165: loss = 2.3853636e-10,5.094594e-09\n",
      "Iteration 38170: loss = 2.385255e-10,5.0944013e-09\n",
      "Iteration 38175: loss = 2.368216e-10,5.0942144e-09\n",
      "Iteration 38180: loss = 2.385019e-10,5.094021e-09\n",
      "Iteration 38185: loss = 2.367965e-10,5.0938374e-09\n",
      "Iteration 38190: loss = 2.3678637e-10,5.093643e-09\n",
      "Iteration 38195: loss = 2.3846644e-10,5.0934514e-09\n",
      "Iteration 38200: loss = 2.3845617e-10,5.0932574e-09\n",
      "Iteration 38205: loss = 2.3844318e-10,5.09307e-09\n",
      "Iteration 38210: loss = 2.3843322e-10,5.0928755e-09\n",
      "Iteration 38215: loss = 2.3672445e-10,5.0927027e-09\n",
      "Iteration 38220: loss = 2.3841226e-10,5.092488e-09\n",
      "Iteration 38225: loss = 2.3670385e-10,5.092314e-09\n",
      "Iteration 38230: loss = 2.3838578e-10,5.0921165e-09\n",
      "Iteration 38235: loss = 2.366821e-10,5.0919287e-09\n",
      "Iteration 38240: loss = 2.3836408e-10,5.091732e-09\n",
      "Iteration 38245: loss = 2.366576e-10,5.091551e-09\n",
      "Iteration 38250: loss = 2.3834176e-10,5.091349e-09\n",
      "Iteration 38255: loss = 2.366356e-10,5.0911675e-09\n",
      "Iteration 38260: loss = 2.383194e-10,5.0909654e-09\n",
      "Iteration 38265: loss = 2.3661023e-10,5.0907927e-09\n",
      "Iteration 38270: loss = 2.366006e-10,5.0905973e-09\n",
      "Iteration 38275: loss = 2.3658853e-10,5.0904085e-09\n",
      "Iteration 38280: loss = 2.3657556e-10,5.090221e-09\n",
      "Iteration 38285: loss = 2.3825839e-10,5.0900217e-09\n",
      "Iteration 38290: loss = 2.3824662e-10,5.0898312e-09\n",
      "Iteration 38295: loss = 2.3823565e-10,5.089639e-09\n",
      "Iteration 38300: loss = 2.3652813e-10,5.089463e-09\n",
      "Iteration 38305: loss = 2.3821392e-10,5.0892552e-09\n",
      "Iteration 38310: loss = 2.3650798e-10,5.089075e-09\n",
      "Iteration 38315: loss = 2.3649618e-10,5.088885e-09\n",
      "Iteration 38320: loss = 2.381806e-10,5.088681e-09\n",
      "Iteration 38325: loss = 2.3647115e-10,5.0885096e-09\n",
      "Iteration 38330: loss = 2.3646116e-10,5.0883147e-09\n",
      "Iteration 38335: loss = 2.3814245e-10,5.0881197e-09\n",
      "Iteration 38340: loss = 2.38132e-10,5.087927e-09\n",
      "Iteration 38345: loss = 2.3642185e-10,5.087757e-09\n",
      "Iteration 38350: loss = 2.3810884e-10,5.0875464e-09\n",
      "Iteration 38355: loss = 2.380969e-10,5.0873576e-09\n",
      "Iteration 38360: loss = 2.3809485e-10,5.0871405e-09\n",
      "Iteration 38365: loss = 2.364033e-10,5.0869207e-09\n",
      "Iteration 38370: loss = 2.3640404e-10,5.0866955e-09\n",
      "Iteration 38375: loss = 2.364041e-10,5.0864744e-09\n",
      "Iteration 38380: loss = 2.3810418e-10,5.0862257e-09\n",
      "Iteration 38385: loss = 2.3640867e-10,5.086018e-09\n",
      "Iteration 38390: loss = 2.3811017e-10,5.085766e-09\n",
      "Iteration 38395: loss = 2.364097e-10,5.0855706e-09\n",
      "Iteration 38400: loss = 2.3641697e-10,5.0853273e-09\n",
      "Iteration 38405: loss = 2.3642152e-10,5.0850932e-09\n",
      "Iteration 38410: loss = 2.3642646e-10,5.0848583e-09\n",
      "Iteration 38415: loss = 2.3642377e-10,5.084644e-09\n",
      "Iteration 38420: loss = 2.3642718e-10,5.084413e-09\n",
      "Iteration 38425: loss = 2.364311e-10,5.0841797e-09\n",
      "Iteration 38430: loss = 2.3642735e-10,5.083968e-09\n",
      "Iteration 38435: loss = 2.3643415e-10,5.083727e-09\n",
      "Iteration 38440: loss = 2.3643762e-10,5.0834954e-09\n",
      "Iteration 38445: loss = 2.364314e-10,5.0832907e-09\n",
      "Iteration 38450: loss = 2.3643423e-10,5.0830615e-09\n",
      "Iteration 38455: loss = 2.3644117e-10,5.0828195e-09\n",
      "Iteration 38460: loss = 2.3644983e-10,5.0825744e-09\n",
      "Iteration 38465: loss = 2.36451e-10,5.082351e-09\n",
      "Iteration 38470: loss = 2.3644986e-10,5.0821307e-09\n",
      "Iteration 38475: loss = 2.36451e-10,5.081906e-09\n",
      "Iteration 38480: loss = 2.3645252e-10,5.08168e-09\n",
      "Iteration 38485: loss = 2.3645372e-10,5.0814553e-09\n",
      "Iteration 38490: loss = 2.3645458e-10,5.0812323e-09\n",
      "Iteration 38495: loss = 2.3645372e-10,5.081011e-09\n",
      "Iteration 38500: loss = 2.3645144e-10,5.080796e-09\n",
      "Iteration 38505: loss = 2.364467e-10,5.0805857e-09\n",
      "Iteration 38510: loss = 2.364414e-10,5.0803806e-09\n",
      "Iteration 38515: loss = 2.3643396e-10,5.080177e-09\n",
      "Iteration 38520: loss = 2.3642613e-10,5.079979e-09\n",
      "Iteration 38525: loss = 2.3641572e-10,5.0797846e-09\n",
      "Iteration 38530: loss = 2.3640415e-10,5.079596e-09\n",
      "Iteration 38535: loss = 2.3639157e-10,5.0794084e-09\n",
      "Iteration 38540: loss = 2.3637792e-10,5.0792233e-09\n",
      "Iteration 38545: loss = 2.3636315e-10,5.0790425e-09\n",
      "Iteration 38550: loss = 2.3634836e-10,5.0788613e-09\n",
      "Iteration 38555: loss = 2.3633162e-10,5.078687e-09\n",
      "Iteration 38560: loss = 2.3631383e-10,5.078514e-09\n",
      "Iteration 38565: loss = 2.3629632e-10,5.078341e-09\n",
      "Iteration 38570: loss = 2.3628752e-10,5.0781432e-09\n",
      "Iteration 38575: loss = 2.3628496e-10,5.077928e-09\n",
      "Iteration 38580: loss = 2.3628632e-10,5.077704e-09\n",
      "Iteration 38585: loss = 2.3628835e-10,5.0774753e-09\n",
      "Iteration 38590: loss = 2.3629057e-10,5.077249e-09\n",
      "Iteration 38595: loss = 2.362918e-10,5.0770237e-09\n",
      "Iteration 38600: loss = 2.3629157e-10,5.076801e-09\n",
      "Iteration 38605: loss = 2.362899e-10,5.076586e-09\n",
      "Iteration 38610: loss = 2.3628632e-10,5.076374e-09\n",
      "Iteration 38615: loss = 2.362817e-10,5.076166e-09\n",
      "Iteration 38620: loss = 2.3627494e-10,5.075961e-09\n",
      "Iteration 38625: loss = 2.362669e-10,5.0757625e-09\n",
      "Iteration 38630: loss = 2.3625715e-10,5.0755675e-09\n",
      "Iteration 38635: loss = 2.3624644e-10,5.075376e-09\n",
      "Iteration 38640: loss = 2.36235e-10,5.075187e-09\n",
      "Iteration 38645: loss = 2.3622168e-10,5.0750004e-09\n",
      "Iteration 38650: loss = 2.3620791e-10,5.0748183e-09\n",
      "Iteration 38655: loss = 2.361927e-10,5.0746385e-09\n",
      "Iteration 38660: loss = 2.3617586e-10,5.0744626e-09\n",
      "Iteration 38665: loss = 2.3615912e-10,5.0742877e-09\n",
      "Iteration 38670: loss = 2.361411e-10,5.074116e-09\n",
      "Iteration 38675: loss = 2.3612307e-10,5.073944e-09\n",
      "Iteration 38680: loss = 2.3610452e-10,5.0737743e-09\n",
      "Iteration 38685: loss = 2.3608468e-10,5.0736064e-09\n",
      "Iteration 38690: loss = 2.3606536e-10,5.0734403e-09\n",
      "Iteration 38695: loss = 2.3604466e-10,5.0732742e-09\n",
      "Iteration 38700: loss = 2.3602367e-10,5.0731104e-09\n",
      "Iteration 38705: loss = 2.3600183e-10,5.072949e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38710: loss = 2.359806e-10,5.072789e-09\n",
      "Iteration 38715: loss = 2.3595764e-10,5.07263e-09\n",
      "Iteration 38720: loss = 2.359348e-10,5.0724713e-09\n",
      "Iteration 38725: loss = 2.3591165e-10,5.0723137e-09\n",
      "Iteration 38730: loss = 2.3588864e-10,5.072156e-09\n",
      "Iteration 38735: loss = 2.3586552e-10,5.0719993e-09\n",
      "Iteration 38740: loss = 2.3584198e-10,5.0718425e-09\n",
      "Iteration 38745: loss = 2.3582325e-10,5.0716733e-09\n",
      "Iteration 38750: loss = 2.358134e-10,5.0714783e-09\n",
      "Iteration 38755: loss = 2.358092e-10,5.0712687e-09\n",
      "Iteration 38760: loss = 2.3580674e-10,5.0710542e-09\n",
      "Iteration 38765: loss = 2.3580507e-10,5.070837e-09\n",
      "Iteration 38770: loss = 2.3580327e-10,5.0706213e-09\n",
      "Iteration 38775: loss = 2.3580043e-10,5.0704085e-09\n",
      "Iteration 38780: loss = 2.357966e-10,5.0701963e-09\n",
      "Iteration 38785: loss = 2.3579183e-10,5.069989e-09\n",
      "Iteration 38790: loss = 2.3578475e-10,5.069787e-09\n",
      "Iteration 38795: loss = 2.357768e-10,5.069589e-09\n",
      "Iteration 38800: loss = 2.3576727e-10,5.069394e-09\n",
      "Iteration 38805: loss = 2.357568e-10,5.0692006e-09\n",
      "Iteration 38810: loss = 2.3574545e-10,5.0690114e-09\n",
      "Iteration 38815: loss = 2.3573224e-10,5.068826e-09\n",
      "Iteration 38820: loss = 2.357184e-10,5.068644e-09\n",
      "Iteration 38825: loss = 2.3570315e-10,5.0684643e-09\n",
      "Iteration 38830: loss = 2.3568716e-10,5.0682876e-09\n",
      "Iteration 38835: loss = 2.3567034e-10,5.0681126e-09\n",
      "Iteration 38840: loss = 2.356537e-10,5.067938e-09\n",
      "Iteration 38845: loss = 2.356354e-10,5.0677658e-09\n",
      "Iteration 38850: loss = 2.3561725e-10,5.067596e-09\n",
      "Iteration 38855: loss = 2.3559807e-10,5.0674274e-09\n",
      "Iteration 38860: loss = 2.3557814e-10,5.067262e-09\n",
      "Iteration 38865: loss = 2.355575e-10,5.0670987e-09\n",
      "Iteration 38870: loss = 2.355359e-10,5.066936e-09\n",
      "Iteration 38875: loss = 2.3551497e-10,5.066773e-09\n",
      "Iteration 38880: loss = 2.3549337e-10,5.066611e-09\n",
      "Iteration 38885: loss = 2.3547156e-10,5.0664513e-09\n",
      "Iteration 38890: loss = 2.3544916e-10,5.0662914e-09\n",
      "Iteration 38895: loss = 2.3542654e-10,5.066133e-09\n",
      "Iteration 38900: loss = 2.3540395e-10,5.0659748e-09\n",
      "Iteration 38905: loss = 2.353813e-10,5.0658167e-09\n",
      "Iteration 38910: loss = 2.3535995e-10,5.0656546e-09\n",
      "Iteration 38915: loss = 2.3534766e-10,5.065467e-09\n",
      "Iteration 38920: loss = 2.3534216e-10,5.065262e-09\n",
      "Iteration 38925: loss = 2.353396e-10,5.0650475e-09\n",
      "Iteration 38930: loss = 2.353381e-10,5.064831e-09\n",
      "Iteration 38935: loss = 2.3533658e-10,5.064613e-09\n",
      "Iteration 38940: loss = 2.3533442e-10,5.0643973e-09\n",
      "Iteration 38945: loss = 2.353319e-10,5.064185e-09\n",
      "Iteration 38950: loss = 2.3532695e-10,5.0639763e-09\n",
      "Iteration 38955: loss = 2.3532112e-10,5.0637725e-09\n",
      "Iteration 38960: loss = 2.3531352e-10,5.0635722e-09\n",
      "Iteration 38965: loss = 2.3530525e-10,5.0633733e-09\n",
      "Iteration 38970: loss = 2.352957e-10,5.06318e-09\n",
      "Iteration 38975: loss = 2.3530258e-10,5.0629394e-09\n",
      "Iteration 38980: loss = 2.353063e-10,5.0627063e-09\n",
      "Iteration 38985: loss = 2.3362007e-10,5.0624998e-09\n",
      "Iteration 38990: loss = 2.3531296e-10,5.0622475e-09\n",
      "Iteration 38995: loss = 2.353123e-10,5.062027e-09\n",
      "Iteration 39000: loss = 2.353113e-10,5.0618087e-09\n",
      "Iteration 39005: loss = 2.3531607e-10,5.061575e-09\n",
      "Iteration 39010: loss = 2.3531607e-10,5.0613536e-09\n",
      "Iteration 39015: loss = 2.3531932e-10,5.061124e-09\n",
      "Iteration 39020: loss = 2.35322e-10,5.0608944e-09\n",
      "Iteration 39025: loss = 2.3363925e-10,5.060677e-09\n",
      "Iteration 39030: loss = 2.3532787e-10,5.0604374e-09\n",
      "Iteration 39035: loss = 2.3364508e-10,5.060218e-09\n",
      "Iteration 39040: loss = 2.3533783e-10,5.0599667e-09\n",
      "Iteration 39045: loss = 2.3364918e-10,5.059766e-09\n",
      "Iteration 39050: loss = 2.3534003e-10,5.0595212e-09\n",
      "Iteration 39055: loss = 2.336552e-10,5.0593068e-09\n",
      "Iteration 39060: loss = 2.3534405e-10,5.0590665e-09\n",
      "Iteration 39065: loss = 2.3534433e-10,5.0588445e-09\n",
      "Iteration 39070: loss = 2.336613e-10,5.0586273e-09\n",
      "Iteration 39075: loss = 2.353505e-10,5.0583844e-09\n",
      "Iteration 39080: loss = 2.353517e-10,5.05816e-09\n",
      "Iteration 39085: loss = 2.3535315e-10,5.0579354e-09\n",
      "Iteration 39090: loss = 2.336718e-10,5.0577134e-09\n",
      "Iteration 39095: loss = 2.3535754e-10,5.0574824e-09\n",
      "Iteration 39100: loss = 2.3536412e-10,5.0572426e-09\n",
      "Iteration 39105: loss = 2.336803e-10,5.0570264e-09\n",
      "Iteration 39110: loss = 2.3368268e-10,5.0568e-09\n",
      "Iteration 39115: loss = 2.3368507e-10,5.0565734e-09\n",
      "Iteration 39120: loss = 2.3537303e-10,5.056335e-09\n",
      "Iteration 39125: loss = 2.3368776e-10,5.0561226e-09\n",
      "Iteration 39130: loss = 2.353782e-10,5.0558775e-09\n",
      "Iteration 39135: loss = 2.3369587e-10,5.0556586e-09\n",
      "Iteration 39140: loss = 2.3538255e-10,5.0554227e-09\n",
      "Iteration 39145: loss = 2.336988e-10,5.0552087e-09\n",
      "Iteration 39150: loss = 2.3538613e-10,5.054973e-09\n",
      "Iteration 39155: loss = 2.3370175e-10,5.0547593e-09\n",
      "Iteration 39160: loss = 2.3370794e-10,5.05452e-09\n",
      "Iteration 39165: loss = 2.353953e-10,5.054284e-09\n",
      "Iteration 39170: loss = 2.3539676e-10,5.0540576e-09\n",
      "Iteration 39175: loss = 2.353994e-10,5.0538307e-09\n",
      "Iteration 39180: loss = 2.337178e-10,5.0536086e-09\n",
      "Iteration 39185: loss = 2.3372074e-10,5.05338e-09\n",
      "Iteration 39190: loss = 2.337221e-10,5.0531552e-09\n",
      "Iteration 39195: loss = 2.3372554e-10,5.0529247e-09\n",
      "Iteration 39200: loss = 2.3372618e-10,5.052703e-09\n",
      "Iteration 39205: loss = 2.354167e-10,5.0524567e-09\n",
      "Iteration 39210: loss = 2.337295e-10,5.05225e-09\n",
      "Iteration 39215: loss = 2.3542054e-10,5.052003e-09\n",
      "Iteration 39220: loss = 2.337383e-10,5.0517834e-09\n",
      "Iteration 39225: loss = 2.354256e-10,5.0515467e-09\n",
      "Iteration 39230: loss = 2.3374463e-10,5.0513256e-09\n",
      "Iteration 39235: loss = 2.33749e-10,5.051093e-09\n",
      "Iteration 39240: loss = 2.3374933e-10,5.0508717e-09\n",
      "Iteration 39245: loss = 2.354363e-10,5.050634e-09\n",
      "Iteration 39250: loss = 2.3374733e-10,5.0504334e-09\n",
      "Iteration 39255: loss = 2.3375216e-10,5.0502003e-09\n",
      "Iteration 39260: loss = 2.3544514e-10,5.049947e-09\n",
      "Iteration 39265: loss = 2.337561e-10,5.049748e-09\n",
      "Iteration 39270: loss = 2.3375615e-10,5.049526e-09\n",
      "Iteration 39275: loss = 2.3376265e-10,5.049287e-09\n",
      "Iteration 39280: loss = 2.3377128e-10,5.049042e-09\n",
      "Iteration 39285: loss = 2.3377278e-10,5.0488165e-09\n",
      "Iteration 39290: loss = 2.3377353e-10,5.0485958e-09\n",
      "Iteration 39295: loss = 2.3377641e-10,5.0483653e-09\n",
      "Iteration 39300: loss = 2.3378063e-10,5.0481335e-09\n",
      "Iteration 39305: loss = 2.3378402e-10,5.0479025e-09\n",
      "Iteration 39310: loss = 2.3378607e-10,5.0476765e-09\n",
      "Iteration 39315: loss = 2.3378618e-10,5.047453e-09\n",
      "Iteration 39320: loss = 2.3378563e-10,5.0472355e-09\n",
      "Iteration 39325: loss = 2.337839e-10,5.0470206e-09\n",
      "Iteration 39330: loss = 2.337795e-10,5.0468105e-09\n",
      "Iteration 39335: loss = 2.3377397e-10,5.046606e-09\n",
      "Iteration 39340: loss = 2.337665e-10,5.046404e-09\n",
      "Iteration 39345: loss = 2.3375812e-10,5.046206e-09\n",
      "Iteration 39350: loss = 2.3374863e-10,5.0460103e-09\n",
      "Iteration 39355: loss = 2.3373725e-10,5.045821e-09\n",
      "Iteration 39360: loss = 2.3372473e-10,5.0456346e-09\n",
      "Iteration 39365: loss = 2.3371186e-10,5.04545e-09\n",
      "Iteration 39370: loss = 2.3369762e-10,5.0452678e-09\n",
      "Iteration 39375: loss = 2.3368277e-10,5.0450883e-09\n",
      "Iteration 39380: loss = 2.336671e-10,5.0449094e-09\n",
      "Iteration 39385: loss = 2.3365107e-10,5.044733e-09\n",
      "Iteration 39390: loss = 2.3363386e-10,5.044559e-09\n",
      "Iteration 39395: loss = 2.3361582e-10,5.0443885e-09\n",
      "Iteration 39400: loss = 2.335969e-10,5.0442197e-09\n",
      "Iteration 39405: loss = 2.335774e-10,5.0440514e-09\n",
      "Iteration 39410: loss = 2.3355798e-10,5.043885e-09\n",
      "Iteration 39415: loss = 2.33538e-10,5.0437183e-09\n",
      "Iteration 39420: loss = 2.3351737e-10,5.0435545e-09\n",
      "Iteration 39425: loss = 2.334968e-10,5.043391e-09\n",
      "Iteration 39430: loss = 2.3347582e-10,5.043228e-09\n",
      "Iteration 39435: loss = 2.3345456e-10,5.043065e-09\n",
      "Iteration 39440: loss = 2.3343327e-10,5.042903e-09\n",
      "Iteration 39445: loss = 2.3341185e-10,5.0427413e-09\n",
      "Iteration 39450: loss = 2.3339922e-10,5.0425557e-09\n",
      "Iteration 39455: loss = 2.3339355e-10,5.0423496e-09\n",
      "Iteration 39460: loss = 2.3339178e-10,5.0421334e-09\n",
      "Iteration 39465: loss = 2.333918e-10,5.0419118e-09\n",
      "Iteration 39470: loss = 2.3339244e-10,5.0416897e-09\n",
      "Iteration 39475: loss = 2.333917e-10,5.0414712e-09\n",
      "Iteration 39480: loss = 2.333901e-10,5.0412536e-09\n",
      "Iteration 39485: loss = 2.3338703e-10,5.0410414e-09\n",
      "Iteration 39490: loss = 2.3338234e-10,5.0408335e-09\n",
      "Iteration 39495: loss = 2.3337646e-10,5.040629e-09\n",
      "Iteration 39500: loss = 2.3336946e-10,5.040428e-09\n",
      "Iteration 39505: loss = 2.33361e-10,5.040229e-09\n",
      "Iteration 39510: loss = 2.333512e-10,5.0400355e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39515: loss = 2.3333993e-10,5.0398454e-09\n",
      "Iteration 39520: loss = 2.3332786e-10,5.039658e-09\n",
      "Iteration 39525: loss = 2.3331467e-10,5.0394733e-09\n",
      "Iteration 39530: loss = 2.3330093e-10,5.0392908e-09\n",
      "Iteration 39535: loss = 2.3328608e-10,5.0391105e-09\n",
      "Iteration 39540: loss = 2.332707e-10,5.0389315e-09\n",
      "Iteration 39545: loss = 2.332549e-10,5.038755e-09\n",
      "Iteration 39550: loss = 2.3323726e-10,5.0385816e-09\n",
      "Iteration 39555: loss = 2.3321933e-10,5.0384106e-09\n",
      "Iteration 39560: loss = 2.3320054e-10,5.0382414e-09\n",
      "Iteration 39565: loss = 2.3318145e-10,5.038073e-09\n",
      "Iteration 39570: loss = 2.3316205e-10,5.037906e-09\n",
      "Iteration 39575: loss = 2.3314228e-10,5.03774e-09\n",
      "Iteration 39580: loss = 2.3312172e-10,5.0375752e-09\n",
      "Iteration 39585: loss = 2.3310118e-10,5.0374105e-09\n",
      "Iteration 39590: loss = 2.3308042e-10,5.0372475e-09\n",
      "Iteration 39595: loss = 2.3305968e-10,5.037084e-09\n",
      "Iteration 39600: loss = 2.3303828e-10,5.036921e-09\n",
      "Iteration 39605: loss = 2.3301658e-10,5.036761e-09\n",
      "Iteration 39610: loss = 2.3299487e-10,5.0366e-09\n",
      "Iteration 39615: loss = 2.329725e-10,5.0364415e-09\n",
      "Iteration 39620: loss = 2.3295124e-10,5.03628e-09\n",
      "Iteration 39625: loss = 2.3293786e-10,5.036095e-09\n",
      "Iteration 39630: loss = 2.3293198e-10,5.03589e-09\n",
      "Iteration 39635: loss = 2.3293012e-10,5.035674e-09\n",
      "Iteration 39640: loss = 2.3292956e-10,5.0354547e-09\n",
      "Iteration 39645: loss = 2.3292937e-10,5.0352353e-09\n",
      "Iteration 39650: loss = 2.3292854e-10,5.0350155e-09\n",
      "Iteration 39655: loss = 2.3292654e-10,5.0348e-09\n",
      "Iteration 39660: loss = 2.3292346e-10,5.0345874e-09\n",
      "Iteration 39665: loss = 2.3291888e-10,5.0343796e-09\n",
      "Iteration 39670: loss = 2.3291283e-10,5.034175e-09\n",
      "Iteration 39675: loss = 2.3290578e-10,5.033973e-09\n",
      "Iteration 39680: loss = 2.3289734e-10,5.033775e-09\n",
      "Iteration 39685: loss = 2.328874e-10,5.0335816e-09\n",
      "Iteration 39690: loss = 2.3287627e-10,5.0333915e-09\n",
      "Iteration 39695: loss = 2.3286403e-10,5.033203e-09\n",
      "Iteration 39700: loss = 2.3285104e-10,5.03302e-09\n",
      "Iteration 39705: loss = 2.328372e-10,5.032837e-09\n",
      "Iteration 39710: loss = 2.3282222e-10,5.0326556e-09\n",
      "Iteration 39715: loss = 2.328072e-10,5.0324767e-09\n",
      "Iteration 39720: loss = 2.3279124e-10,5.0323e-09\n",
      "Iteration 39725: loss = 2.327743e-10,5.0321245e-09\n",
      "Iteration 39730: loss = 2.3275638e-10,5.0319526e-09\n",
      "Iteration 39735: loss = 2.3273795e-10,5.031784e-09\n",
      "Iteration 39740: loss = 2.3271936e-10,5.031615e-09\n",
      "Iteration 39745: loss = 2.3270035e-10,5.031447e-09\n",
      "Iteration 39750: loss = 2.3267983e-10,5.031281e-09\n",
      "Iteration 39755: loss = 2.3266057e-10,5.0311155e-09\n",
      "Iteration 39760: loss = 2.3264002e-10,5.030951e-09\n",
      "Iteration 39765: loss = 2.3094958e-10,5.0307802e-09\n",
      "Iteration 39770: loss = 2.3263295e-10,5.030527e-09\n",
      "Iteration 39775: loss = 2.3263497e-10,5.030301e-09\n",
      "Iteration 39780: loss = 2.3096003e-10,5.0300875e-09\n",
      "Iteration 39785: loss = 2.3264102e-10,5.0298423e-09\n",
      "Iteration 39790: loss = 2.3096429e-10,5.0296336e-09\n",
      "Iteration 39795: loss = 2.3264574e-10,5.029386e-09\n",
      "Iteration 39800: loss = 2.3096969e-10,5.0291766e-09\n",
      "Iteration 39805: loss = 2.3264951e-10,5.0289337e-09\n",
      "Iteration 39810: loss = 2.3097545e-10,5.0287188e-09\n",
      "Iteration 39815: loss = 2.3265201e-10,5.028485e-09\n",
      "Iteration 39820: loss = 2.3265602e-10,5.0282503e-09\n",
      "Iteration 39825: loss = 2.3098096e-10,5.0280393e-09\n",
      "Iteration 39830: loss = 2.3098529e-10,5.027806e-09\n",
      "Iteration 39835: loss = 2.30988e-10,5.0275766e-09\n",
      "Iteration 39840: loss = 2.30991e-10,5.027347e-09\n",
      "Iteration 39845: loss = 2.309931e-10,5.027121e-09\n",
      "Iteration 39850: loss = 2.309919e-10,5.0269033e-09\n",
      "Iteration 39855: loss = 2.3267199e-10,5.0266595e-09\n",
      "Iteration 39860: loss = 2.3100077e-10,5.0264366e-09\n",
      "Iteration 39865: loss = 2.3100348e-10,5.0262057e-09\n",
      "Iteration 39870: loss = 2.3100155e-10,5.0259907e-09\n",
      "Iteration 39875: loss = 2.3100671e-10,5.025756e-09\n",
      "Iteration 39880: loss = 2.3100914e-10,5.025528e-09\n",
      "Iteration 39885: loss = 2.3101142e-10,5.0253e-09\n",
      "Iteration 39890: loss = 2.3101494e-10,5.0250684e-09\n",
      "Iteration 39895: loss = 2.3101217e-10,5.024855e-09\n",
      "Iteration 39900: loss = 2.3101836e-10,5.0246176e-09\n",
      "Iteration 39905: loss = 2.3102391e-10,5.0243814e-09\n",
      "Iteration 39910: loss = 2.3102718e-10,5.02415e-09\n",
      "Iteration 39915: loss = 2.310259e-10,5.023932e-09\n",
      "Iteration 39920: loss = 2.3102786e-10,5.0237063e-09\n",
      "Iteration 39925: loss = 2.310321e-10,5.0234727e-09\n",
      "Iteration 39930: loss = 2.310367e-10,5.0232387e-09\n",
      "Iteration 39935: loss = 2.310368e-10,5.0230176e-09\n",
      "Iteration 39940: loss = 2.310327e-10,5.022807e-09\n",
      "Iteration 39945: loss = 2.3103064e-10,5.022592e-09\n",
      "Iteration 39950: loss = 2.310299e-10,5.0223727e-09\n",
      "Iteration 39955: loss = 2.3102752e-10,5.0221574e-09\n",
      "Iteration 39960: loss = 2.3102496e-10,5.0219437e-09\n",
      "Iteration 39965: loss = 2.310211e-10,5.021733e-09\n",
      "Iteration 39970: loss = 2.3101597e-10,5.0215254e-09\n",
      "Iteration 39975: loss = 2.3100967e-10,5.0213202e-09\n",
      "Iteration 39980: loss = 2.3100288e-10,5.021118e-09\n",
      "Iteration 39985: loss = 2.309939e-10,5.020922e-09\n",
      "Iteration 39990: loss = 2.3098373e-10,5.0207283e-09\n",
      "Iteration 39995: loss = 2.3097242e-10,5.0205378e-09\n",
      "Iteration 40000: loss = 2.3096014e-10,5.0203504e-09\n",
      "Iteration 40005: loss = 2.3094748e-10,5.020165e-09\n",
      "Iteration 40010: loss = 2.309336e-10,5.0199804e-09\n",
      "Iteration 40015: loss = 2.309245e-10,5.0197855e-09\n",
      "Iteration 40020: loss = 2.3092302e-10,5.019566e-09\n",
      "Iteration 40025: loss = 2.3092661e-10,5.019336e-09\n",
      "Iteration 40030: loss = 2.3093176e-10,5.0191e-09\n",
      "Iteration 40035: loss = 2.309372e-10,5.0188635e-09\n",
      "Iteration 40040: loss = 2.3094182e-10,5.0186304e-09\n",
      "Iteration 40045: loss = 2.30945e-10,5.0183995e-09\n",
      "Iteration 40050: loss = 2.3094748e-10,5.0181725e-09\n",
      "Iteration 40055: loss = 2.3094761e-10,5.0179487e-09\n",
      "Iteration 40060: loss = 2.3094658e-10,5.0177307e-09\n",
      "Iteration 40065: loss = 2.3094371e-10,5.017518e-09\n",
      "Iteration 40070: loss = 2.3093882e-10,5.017309e-09\n",
      "Iteration 40075: loss = 2.3093305e-10,5.017104e-09\n",
      "Iteration 40080: loss = 2.3092579e-10,5.0169016e-09\n",
      "Iteration 40085: loss = 2.3091673e-10,5.016705e-09\n",
      "Iteration 40090: loss = 2.3090663e-10,5.0165108e-09\n",
      "Iteration 40095: loss = 2.308954e-10,5.0163202e-09\n",
      "Iteration 40100: loss = 2.3088371e-10,5.016132e-09\n",
      "Iteration 40105: loss = 2.3087059e-10,5.015946e-09\n",
      "Iteration 40110: loss = 2.3085707e-10,5.015762e-09\n",
      "Iteration 40115: loss = 2.3084236e-10,5.015581e-09\n",
      "Iteration 40120: loss = 2.3082675e-10,5.0154036e-09\n",
      "Iteration 40125: loss = 2.3080994e-10,5.0152265e-09\n",
      "Iteration 40130: loss = 2.3079257e-10,5.0150537e-09\n",
      "Iteration 40135: loss = 2.307753e-10,5.0148796e-09\n",
      "Iteration 40140: loss = 2.307571e-10,5.0147086e-09\n",
      "Iteration 40145: loss = 2.307387e-10,5.0145377e-09\n",
      "Iteration 40150: loss = 2.3071996e-10,5.0143685e-09\n",
      "Iteration 40155: loss = 2.3070092e-10,5.014199e-09\n",
      "Iteration 40160: loss = 2.3068107e-10,5.0140327e-09\n",
      "Iteration 40165: loss = 2.3066138e-10,5.013865e-09\n",
      "Iteration 40170: loss = 2.306411e-10,5.0136983e-09\n",
      "Iteration 40175: loss = 2.3062087e-10,5.013532e-09\n",
      "Iteration 40180: loss = 2.3060036e-10,5.0133684e-09\n",
      "Iteration 40185: loss = 2.3057907e-10,5.013205e-09\n",
      "Iteration 40190: loss = 2.3055792e-10,5.013042e-09\n",
      "Iteration 40195: loss = 2.3053648e-10,5.01288e-09\n",
      "Iteration 40200: loss = 2.3051495e-10,5.012717e-09\n",
      "Iteration 40205: loss = 2.304935e-10,5.012555e-09\n",
      "Iteration 40210: loss = 2.3047247e-10,5.0123896e-09\n",
      "Iteration 40215: loss = 2.3046187e-10,5.012199e-09\n",
      "Iteration 40220: loss = 2.3045706e-10,5.0119895e-09\n",
      "Iteration 40225: loss = 2.3045622e-10,5.011771e-09\n",
      "Iteration 40230: loss = 2.3045595e-10,5.0115485e-09\n",
      "Iteration 40235: loss = 2.3045606e-10,5.0113265e-09\n",
      "Iteration 40240: loss = 2.3045622e-10,5.011106e-09\n",
      "Iteration 40245: loss = 2.304545e-10,5.0108877e-09\n",
      "Iteration 40250: loss = 2.3045131e-10,5.0106754e-09\n",
      "Iteration 40255: loss = 2.3044684e-10,5.010465e-09\n",
      "Iteration 40260: loss = 2.3044122e-10,5.0102584e-09\n",
      "Iteration 40265: loss = 2.3043445e-10,5.010056e-09\n",
      "Iteration 40270: loss = 2.3042597e-10,5.0098565e-09\n",
      "Iteration 40275: loss = 2.3041696e-10,5.0096616e-09\n",
      "Iteration 40280: loss = 2.304054e-10,5.009469e-09\n",
      "Iteration 40285: loss = 2.3039447e-10,5.009279e-09\n",
      "Iteration 40290: loss = 2.3038138e-10,5.009092e-09\n",
      "Iteration 40295: loss = 2.3036807e-10,5.008908e-09\n",
      "Iteration 40300: loss = 2.3035424e-10,5.0087254e-09\n",
      "Iteration 40305: loss = 2.3033941e-10,5.008544e-09\n",
      "Iteration 40310: loss = 2.3032372e-10,5.0083635e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40315: loss = 2.3030772e-10,5.0081868e-09\n",
      "Iteration 40320: loss = 2.3028988e-10,5.0080136e-09\n",
      "Iteration 40325: loss = 2.3027215e-10,5.0078413e-09\n",
      "Iteration 40330: loss = 2.3025447e-10,5.007669e-09\n",
      "Iteration 40335: loss = 2.3023512e-10,5.0075e-09\n",
      "Iteration 40340: loss = 2.3021567e-10,5.007333e-09\n",
      "Iteration 40345: loss = 2.3019638e-10,5.0071636e-09\n",
      "Iteration 40350: loss = 2.301766e-10,5.0069966e-09\n",
      "Iteration 40355: loss = 2.3015632e-10,5.0068296e-09\n",
      "Iteration 40360: loss = 2.301359e-10,5.0066644e-09\n",
      "Iteration 40365: loss = 2.301156e-10,5.0064997e-09\n",
      "Iteration 40370: loss = 2.3009468e-10,5.0063345e-09\n",
      "Iteration 40375: loss = 2.300739e-10,5.0061697e-09\n",
      "Iteration 40380: loss = 2.3005302e-10,5.0060063e-09\n",
      "Iteration 40385: loss = 2.3003188e-10,5.0058437e-09\n",
      "Iteration 40390: loss = 2.300115e-10,5.0056777e-09\n",
      "Iteration 40395: loss = 2.3000134e-10,5.0054836e-09\n",
      "Iteration 40400: loss = 2.2999724e-10,5.0052718e-09\n",
      "Iteration 40405: loss = 2.2999692e-10,5.005051e-09\n",
      "Iteration 40410: loss = 2.2999751e-10,5.0048277e-09\n",
      "Iteration 40415: loss = 2.2999784e-10,5.004603e-09\n",
      "Iteration 40420: loss = 2.2999795e-10,5.004381e-09\n",
      "Iteration 40425: loss = 2.2999652e-10,5.0041624e-09\n",
      "Iteration 40430: loss = 2.2999486e-10,5.0039457e-09\n",
      "Iteration 40435: loss = 2.299907e-10,5.0037343e-09\n",
      "Iteration 40440: loss = 2.2998524e-10,5.0035274e-09\n",
      "Iteration 40445: loss = 2.2997844e-10,5.0033235e-09\n",
      "Iteration 40450: loss = 2.2997078e-10,5.0031224e-09\n",
      "Iteration 40455: loss = 2.2996192e-10,5.002925e-09\n",
      "Iteration 40460: loss = 2.2995167e-10,5.002731e-09\n",
      "Iteration 40465: loss = 2.2994012e-10,5.002541e-09\n",
      "Iteration 40470: loss = 2.2992774e-10,5.002353e-09\n",
      "Iteration 40475: loss = 2.2825997e-10,5.0021445e-09\n",
      "Iteration 40480: loss = 2.2994079e-10,5.0018727e-09\n",
      "Iteration 40485: loss = 2.2994366e-10,5.0016418e-09\n",
      "Iteration 40490: loss = 2.2993696e-10,5.001437e-09\n",
      "Iteration 40495: loss = 2.2993712e-10,5.001215e-09\n",
      "Iteration 40500: loss = 2.2993808e-10,5.0009903e-09\n",
      "Iteration 40505: loss = 2.299413e-10,5.0007594e-09\n",
      "Iteration 40510: loss = 2.2994262e-10,5.0005324e-09\n",
      "Iteration 40515: loss = 2.2995002e-10,5.00029e-09\n",
      "Iteration 40520: loss = 2.2994734e-10,5.0000746e-09\n",
      "Iteration 40525: loss = 2.2995601e-10,4.9998286e-09\n",
      "Iteration 40530: loss = 2.2995873e-10,4.9995967e-09\n",
      "Iteration 40535: loss = 2.2829105e-10,4.999388e-09\n",
      "Iteration 40540: loss = 2.2996277e-10,4.9991415e-09\n",
      "Iteration 40545: loss = 2.2829726e-10,4.9989266e-09\n",
      "Iteration 40550: loss = 2.2996828e-10,4.9986815e-09\n",
      "Iteration 40555: loss = 2.2830095e-10,4.99847e-09\n",
      "Iteration 40560: loss = 2.2996943e-10,4.998231e-09\n",
      "Iteration 40565: loss = 2.2997208e-10,4.998002e-09\n",
      "Iteration 40570: loss = 2.2997439e-10,4.9977724e-09\n",
      "Iteration 40575: loss = 2.2997641e-10,4.997545e-09\n",
      "Iteration 40580: loss = 2.2997944e-10,4.997313e-09\n",
      "Iteration 40585: loss = 2.2998387e-10,4.9970796e-09\n",
      "Iteration 40590: loss = 2.2831871e-10,4.996862e-09\n",
      "Iteration 40595: loss = 2.2999386e-10,4.9966054e-09\n",
      "Iteration 40600: loss = 2.2832392e-10,4.996403e-09\n",
      "Iteration 40605: loss = 2.299939e-10,4.9961595e-09\n",
      "Iteration 40610: loss = 2.2999798e-10,4.9959263e-09\n",
      "Iteration 40615: loss = 2.2833135e-10,4.995714e-09\n",
      "Iteration 40620: loss = 2.3000118e-10,4.9954716e-09\n",
      "Iteration 40625: loss = 2.3000533e-10,4.9952376e-09\n",
      "Iteration 40630: loss = 2.2834008e-10,4.9950213e-09\n",
      "Iteration 40635: loss = 2.3001007e-10,4.9947784e-09\n",
      "Iteration 40640: loss = 2.3001313e-10,4.9945466e-09\n",
      "Iteration 40645: loss = 2.2834923e-10,4.9943276e-09\n",
      "Iteration 40650: loss = 2.2835124e-10,4.994099e-09\n",
      "Iteration 40655: loss = 2.3002066e-10,4.9938578e-09\n",
      "Iteration 40660: loss = 2.3002288e-10,4.993629e-09\n",
      "Iteration 40665: loss = 2.30025e-10,4.9934012e-09\n",
      "Iteration 40670: loss = 2.2836477e-10,4.9931703e-09\n",
      "Iteration 40675: loss = 2.2836366e-10,4.992951e-09\n",
      "Iteration 40680: loss = 2.2836448e-10,4.9927245e-09\n",
      "Iteration 40685: loss = 2.283711e-10,4.992485e-09\n",
      "Iteration 40690: loss = 2.3003971e-10,4.9922457e-09\n",
      "Iteration 40695: loss = 2.3004149e-10,4.9920175e-09\n",
      "Iteration 40700: loss = 2.2838043e-10,4.9917888e-09\n",
      "Iteration 40705: loss = 2.283808e-10,4.991565e-09\n",
      "Iteration 40710: loss = 2.3005174e-10,4.99132e-09\n",
      "Iteration 40715: loss = 2.2838115e-10,4.9911177e-09\n",
      "Iteration 40720: loss = 2.2838786e-10,4.990876e-09\n",
      "Iteration 40725: loss = 2.2839221e-10,4.990641e-09\n",
      "Iteration 40730: loss = 2.2839051e-10,4.990423e-09\n",
      "Iteration 40735: loss = 2.300669e-10,4.990163e-09\n",
      "Iteration 40740: loss = 2.283961e-10,4.9899613e-09\n",
      "Iteration 40745: loss = 2.2839862e-10,4.989731e-09\n",
      "Iteration 40750: loss = 2.3007268e-10,4.9894773e-09\n",
      "Iteration 40755: loss = 2.2840707e-10,4.989262e-09\n",
      "Iteration 40760: loss = 2.3007925e-10,4.9890123e-09\n",
      "Iteration 40765: loss = 2.2840994e-10,4.9888076e-09\n",
      "Iteration 40770: loss = 2.2841433e-10,4.9885727e-09\n",
      "Iteration 40775: loss = 2.3008769e-10,4.9883195e-09\n",
      "Iteration 40780: loss = 2.2841727e-10,4.9881175e-09\n",
      "Iteration 40785: loss = 2.2841833e-10,4.987892e-09\n",
      "Iteration 40790: loss = 2.2842565e-10,4.987647e-09\n",
      "Iteration 40795: loss = 2.3009776e-10,4.9873976e-09\n",
      "Iteration 40800: loss = 2.2843401e-10,4.987178e-09\n",
      "Iteration 40805: loss = 2.2842883e-10,4.986969e-09\n",
      "Iteration 40810: loss = 2.2841419e-10,4.9867865e-09\n",
      "Iteration 40815: loss = 2.3006853e-10,4.9865854e-09\n",
      "Iteration 40820: loss = 2.2838305e-10,4.9864255e-09\n",
      "Iteration 40825: loss = 2.2837258e-10,4.986231e-09\n",
      "Iteration 40830: loss = 2.300316e-10,4.986018e-09\n",
      "Iteration 40835: loss = 2.2834686e-10,4.985855e-09\n",
      "Iteration 40840: loss = 2.283334e-10,4.9856688e-09\n",
      "Iteration 40845: loss = 2.2832718e-10,4.9854614e-09\n",
      "Iteration 40850: loss = 2.2831681e-10,4.985267e-09\n",
      "Iteration 40855: loss = 2.2830197e-10,4.9850852e-09\n",
      "Iteration 40860: loss = 2.2828735e-10,4.9849014e-09\n",
      "Iteration 40865: loss = 2.2827923e-10,4.9847015e-09\n",
      "Iteration 40870: loss = 2.2826385e-10,4.98452e-09\n",
      "Iteration 40875: loss = 2.2825258e-10,4.9843285e-09\n",
      "Iteration 40880: loss = 2.2824026e-10,4.9841393e-09\n",
      "Iteration 40885: loss = 2.2822479e-10,4.983958e-09\n",
      "Iteration 40890: loss = 2.2821507e-10,4.983761e-09\n",
      "Iteration 40895: loss = 2.2820439e-10,4.9835673e-09\n",
      "Iteration 40900: loss = 2.2818701e-10,4.9833906e-09\n",
      "Iteration 40905: loss = 2.2817474e-10,4.983202e-09\n",
      "Iteration 40910: loss = 2.281653e-10,4.983004e-09\n",
      "Iteration 40915: loss = 2.2981966e-10,4.9828035e-09\n",
      "Iteration 40920: loss = 2.2813651e-10,4.9826356e-09\n",
      "Iteration 40925: loss = 2.281212e-10,4.982455e-09\n",
      "Iteration 40930: loss = 2.2810855e-10,4.982266e-09\n",
      "Iteration 40935: loss = 2.2809665e-10,4.9820743e-09\n",
      "Iteration 40940: loss = 2.2809349e-10,4.9818603e-09\n",
      "Iteration 40945: loss = 2.2808028e-10,4.9816724e-09\n",
      "Iteration 40950: loss = 2.2973608e-10,4.9814677e-09\n",
      "Iteration 40955: loss = 2.2805353e-10,4.981299e-09\n",
      "Iteration 40960: loss = 2.2804576e-10,4.981097e-09\n",
      "Iteration 40965: loss = 2.2802905e-10,4.9809192e-09\n",
      "Iteration 40970: loss = 2.2801887e-10,4.980724e-09\n",
      "Iteration 40975: loss = 2.2800901e-10,4.980527e-09\n",
      "Iteration 40980: loss = 2.2799285e-10,4.9803486e-09\n",
      "Iteration 40985: loss = 2.2965013e-10,4.9801394e-09\n",
      "Iteration 40990: loss = 2.2796626e-10,4.9799738e-09\n",
      "Iteration 40995: loss = 2.2795604e-10,4.979779e-09\n",
      "Iteration 41000: loss = 2.29613e-10,4.9795705e-09\n",
      "Iteration 41005: loss = 2.2792941e-10,4.9794053e-09\n",
      "Iteration 41010: loss = 2.2791953e-10,4.979207e-09\n",
      "Iteration 41015: loss = 2.2957646e-10,4.9790003e-09\n",
      "Iteration 41020: loss = 2.2789147e-10,4.978838e-09\n",
      "Iteration 41025: loss = 2.2788017e-10,4.9786446e-09\n",
      "Iteration 41030: loss = 2.278754e-10,4.978433e-09\n",
      "Iteration 41035: loss = 2.2786406e-10,4.978241e-09\n",
      "Iteration 41040: loss = 2.2785052e-10,4.978056e-09\n",
      "Iteration 41045: loss = 2.2783647e-10,4.9778706e-09\n",
      "Iteration 41050: loss = 2.2782816e-10,4.977668e-09\n",
      "Iteration 41055: loss = 2.2781366e-10,4.977485e-09\n",
      "Iteration 41060: loss = 2.2780265e-10,4.9772906e-09\n",
      "Iteration 41065: loss = 2.2779241e-10,4.977096e-09\n",
      "Iteration 41070: loss = 2.2777598e-10,4.9769158e-09\n",
      "Iteration 41075: loss = 2.2776593e-10,4.9767217e-09\n",
      "Iteration 41080: loss = 2.2775777e-10,4.9765196e-09\n",
      "Iteration 41085: loss = 2.27741e-10,4.976343e-09\n",
      "Iteration 41090: loss = 2.2772635e-10,4.976158e-09\n",
      "Iteration 41095: loss = 2.2771564e-10,4.975963e-09\n",
      "Iteration 41100: loss = 2.2770653e-10,4.9757642e-09\n",
      "Iteration 41105: loss = 2.2769775e-10,4.975564e-09\n",
      "Iteration 41110: loss = 2.2768336e-10,4.9753797e-09\n",
      "Iteration 41115: loss = 2.2766487e-10,4.975208e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41120: loss = 2.2764907e-10,4.975027e-09\n",
      "Iteration 41125: loss = 2.276377e-10,4.9748348e-09\n",
      "Iteration 41130: loss = 2.276369e-10,4.9746123e-09\n",
      "Iteration 41135: loss = 2.2762624e-10,4.9744173e-09\n",
      "Iteration 41140: loss = 2.2760684e-10,4.974247e-09\n",
      "Iteration 41145: loss = 2.276017e-10,4.9740363e-09\n",
      "Iteration 41150: loss = 2.2759039e-10,4.973843e-09\n",
      "Iteration 41155: loss = 2.2757844e-10,4.9736526e-09\n",
      "Iteration 41160: loss = 2.2756852e-10,4.9734554e-09\n",
      "Iteration 41165: loss = 2.2755513e-10,4.9732676e-09\n",
      "Iteration 41170: loss = 2.292101e-10,4.973066e-09\n",
      "Iteration 41175: loss = 2.2919722e-10,4.9728763e-09\n",
      "Iteration 41180: loss = 2.2751845e-10,4.9726965e-09\n",
      "Iteration 41185: loss = 2.291749e-10,4.97249e-09\n",
      "Iteration 41190: loss = 2.2748965e-10,4.9723283e-09\n",
      "Iteration 41195: loss = 2.2747675e-10,4.9721387e-09\n",
      "Iteration 41200: loss = 2.2747226e-10,4.971928e-09\n",
      "Iteration 41205: loss = 2.2746251e-10,4.9717297e-09\n",
      "Iteration 41210: loss = 2.2744313e-10,4.971559e-09\n",
      "Iteration 41215: loss = 2.2743292e-10,4.9713615e-09\n",
      "Iteration 41220: loss = 2.2742841e-10,4.9711506e-09\n",
      "Iteration 41225: loss = 2.2741614e-10,4.9709605e-09\n",
      "Iteration 41230: loss = 2.2740436e-10,4.970768e-09\n",
      "Iteration 41235: loss = 2.2738816e-10,4.970588e-09\n",
      "Iteration 41240: loss = 2.2737813e-10,4.9703908e-09\n",
      "Iteration 41245: loss = 2.2903492e-10,4.970184e-09\n",
      "Iteration 41250: loss = 2.2735545e-10,4.9700053e-09\n",
      "Iteration 41255: loss = 2.2734431e-10,4.969812e-09\n",
      "Iteration 41260: loss = 2.2733583e-10,4.9696096e-09\n",
      "Iteration 41265: loss = 2.2731912e-10,4.969431e-09\n",
      "Iteration 41270: loss = 2.2730572e-10,4.9692455e-09\n",
      "Iteration 41275: loss = 2.2896547e-10,4.969026e-09\n",
      "Iteration 41280: loss = 2.2729231e-10,4.9688302e-09\n",
      "Iteration 41285: loss = 2.2728062e-10,4.968638e-09\n",
      "Iteration 41290: loss = 2.2726986e-10,4.9684448e-09\n",
      "Iteration 41295: loss = 2.2725899e-10,4.9682494e-09\n",
      "Iteration 41300: loss = 2.2724844e-10,4.9680553e-09\n",
      "Iteration 41305: loss = 2.2723717e-10,4.96786e-09\n",
      "Iteration 41310: loss = 2.272255e-10,4.967668e-09\n",
      "Iteration 41315: loss = 2.2721369e-10,4.9674758e-09\n",
      "Iteration 41320: loss = 2.2720037e-10,4.9672875e-09\n",
      "Iteration 41325: loss = 2.271897e-10,4.9670925e-09\n",
      "Iteration 41330: loss = 2.2719338e-10,4.966857e-09\n",
      "Iteration 41335: loss = 2.2719383e-10,4.9666307e-09\n",
      "Iteration 41340: loss = 2.271971e-10,4.966396e-09\n",
      "Iteration 41345: loss = 2.2720026e-10,4.9661617e-09\n",
      "Iteration 41350: loss = 2.2720144e-10,4.9659334e-09\n",
      "Iteration 41355: loss = 2.272041e-10,4.9657016e-09\n",
      "Iteration 41360: loss = 2.2720675e-10,4.96547e-09\n",
      "Iteration 41365: loss = 2.2720788e-10,4.9652407e-09\n",
      "Iteration 41370: loss = 2.2720877e-10,4.9650124e-09\n",
      "Iteration 41375: loss = 2.272078e-10,4.964789e-09\n",
      "Iteration 41380: loss = 2.2720588e-10,4.96457e-09\n",
      "Iteration 41385: loss = 2.2720191e-10,4.964356e-09\n",
      "Iteration 41390: loss = 2.2719637e-10,4.964146e-09\n",
      "Iteration 41395: loss = 2.2719022e-10,4.9639377e-09\n",
      "Iteration 41400: loss = 2.2718322e-10,4.9637334e-09\n",
      "Iteration 41405: loss = 2.2717418e-10,4.9635323e-09\n",
      "Iteration 41410: loss = 2.2716391e-10,4.9633355e-09\n",
      "Iteration 41415: loss = 2.2715248e-10,4.963142e-09\n",
      "Iteration 41420: loss = 2.2714075e-10,4.96295e-09\n",
      "Iteration 41425: loss = 2.2712775e-10,4.9627595e-09\n",
      "Iteration 41430: loss = 2.2711431e-10,4.9625717e-09\n",
      "Iteration 41435: loss = 2.2710007e-10,4.962386e-09\n",
      "Iteration 41440: loss = 2.2708552e-10,4.9622004e-09\n",
      "Iteration 41445: loss = 2.2707013e-10,4.962018e-09\n",
      "Iteration 41450: loss = 2.2705443e-10,4.9618354e-09\n",
      "Iteration 41455: loss = 2.2703817e-10,4.961655e-09\n",
      "Iteration 41460: loss = 2.2702107e-10,4.9614775e-09\n",
      "Iteration 41465: loss = 2.2700271e-10,4.961302e-09\n",
      "Iteration 41470: loss = 2.2698482e-10,4.9611275e-09\n",
      "Iteration 41475: loss = 2.2696578e-10,4.9609548e-09\n",
      "Iteration 41480: loss = 2.2694667e-10,4.960783e-09\n",
      "Iteration 41485: loss = 2.2692749e-10,4.9606106e-09\n",
      "Iteration 41490: loss = 2.2690805e-10,4.9604387e-09\n",
      "Iteration 41495: loss = 2.2688829e-10,4.9602678e-09\n",
      "Iteration 41500: loss = 2.2686802e-10,4.9600986e-09\n",
      "Iteration 41505: loss = 2.2684733e-10,4.9599294e-09\n",
      "Iteration 41510: loss = 2.2682696e-10,4.9597615e-09\n",
      "Iteration 41515: loss = 2.268067e-10,4.9595914e-09\n",
      "Iteration 41520: loss = 2.2678644e-10,4.959423e-09\n",
      "Iteration 41525: loss = 2.267662e-10,4.9592543e-09\n",
      "Iteration 41530: loss = 2.2674472e-10,4.9590874e-09\n",
      "Iteration 41535: loss = 2.2672698e-10,4.9589115e-09\n",
      "Iteration 41540: loss = 2.2671871e-10,4.9587077e-09\n",
      "Iteration 41545: loss = 2.2671677e-10,4.9584874e-09\n",
      "Iteration 41550: loss = 2.2671687e-10,4.9582605e-09\n",
      "Iteration 41555: loss = 2.2671785e-10,4.958032e-09\n",
      "Iteration 41560: loss = 2.2671857e-10,4.9578044e-09\n",
      "Iteration 41565: loss = 2.2671932e-10,4.9575783e-09\n",
      "Iteration 41570: loss = 2.2671827e-10,4.957354e-09\n",
      "Iteration 41575: loss = 2.267168e-10,4.9571316e-09\n",
      "Iteration 41580: loss = 2.2671388e-10,4.956914e-09\n",
      "Iteration 41585: loss = 2.2670983e-10,4.956699e-09\n",
      "Iteration 41590: loss = 2.2670406e-10,4.9564894e-09\n",
      "Iteration 41595: loss = 2.2669665e-10,4.9562847e-09\n",
      "Iteration 41600: loss = 2.2668856e-10,4.95608e-09\n",
      "Iteration 41605: loss = 2.2667933e-10,4.9558806e-09\n",
      "Iteration 41610: loss = 2.266688e-10,4.955682e-09\n",
      "Iteration 41615: loss = 2.2665786e-10,4.9554876e-09\n",
      "Iteration 41620: loss = 2.2664552e-10,4.955295e-09\n",
      "Iteration 41625: loss = 2.2663248e-10,4.9551048e-09\n",
      "Iteration 41630: loss = 2.2661811e-10,4.9549183e-09\n",
      "Iteration 41635: loss = 2.2660342e-10,4.9547335e-09\n",
      "Iteration 41640: loss = 2.2658797e-10,4.95455e-09\n",
      "Iteration 41645: loss = 2.2657176e-10,4.9543685e-09\n",
      "Iteration 41650: loss = 2.2655565e-10,4.954188e-09\n",
      "Iteration 41655: loss = 2.2653907e-10,4.9540074e-09\n",
      "Iteration 41660: loss = 2.2652145e-10,4.9538293e-09\n",
      "Iteration 41665: loss = 2.265041e-10,4.9536526e-09\n",
      "Iteration 41670: loss = 2.2648616e-10,4.9534754e-09\n",
      "Iteration 41675: loss = 2.2646814e-10,4.9532995e-09\n",
      "Iteration 41680: loss = 2.2644941e-10,4.953125e-09\n",
      "Iteration 41685: loss = 2.2643093e-10,4.952952e-09\n",
      "Iteration 41690: loss = 2.2641168e-10,4.952777e-09\n",
      "Iteration 41695: loss = 2.2639296e-10,4.952603e-09\n",
      "Iteration 41700: loss = 2.2637296e-10,4.9524314e-09\n",
      "Iteration 41705: loss = 2.26353e-10,4.952261e-09\n",
      "Iteration 41710: loss = 2.2633323e-10,4.9520894e-09\n",
      "Iteration 41715: loss = 2.263131e-10,4.951919e-09\n",
      "Iteration 41720: loss = 2.262928e-10,4.9517492e-09\n",
      "Iteration 41725: loss = 2.2628148e-10,4.951554e-09\n",
      "Iteration 41730: loss = 2.2628238e-10,4.9513247e-09\n",
      "Iteration 41735: loss = 2.2628098e-10,4.9511013e-09\n",
      "Iteration 41740: loss = 2.2628183e-10,4.950873e-09\n",
      "Iteration 41745: loss = 2.2628295e-10,4.9506435e-09\n",
      "Iteration 41750: loss = 2.2628381e-10,4.950414e-09\n",
      "Iteration 41755: loss = 2.2628399e-10,4.950186e-09\n",
      "Iteration 41760: loss = 2.2628259e-10,4.949962e-09\n",
      "Iteration 41765: loss = 2.2627981e-10,4.9497437e-09\n",
      "Iteration 41770: loss = 2.2627551e-10,4.9495297e-09\n",
      "Iteration 41775: loss = 2.2627028e-10,4.949317e-09\n",
      "Iteration 41780: loss = 2.2626379e-10,4.949108e-09\n",
      "Iteration 41785: loss = 2.2625635e-10,4.948902e-09\n",
      "Iteration 41790: loss = 2.2624769e-10,4.9487e-09\n",
      "Iteration 41795: loss = 2.2623763e-10,4.9485007e-09\n",
      "Iteration 41800: loss = 2.2622622e-10,4.948305e-09\n",
      "Iteration 41805: loss = 2.2621433e-10,4.9481104e-09\n",
      "Iteration 41810: loss = 2.262016e-10,4.947919e-09\n",
      "Iteration 41815: loss = 2.2619683e-10,4.9477054e-09\n",
      "Iteration 41820: loss = 2.2620252e-10,4.947463e-09\n",
      "Iteration 41825: loss = 2.2454838e-10,4.947238e-09\n",
      "Iteration 41830: loss = 2.2621366e-10,4.946976e-09\n",
      "Iteration 41835: loss = 2.2621192e-10,4.9467546e-09\n",
      "Iteration 41840: loss = 2.2621027e-10,4.946533e-09\n",
      "Iteration 41845: loss = 2.2621405e-10,4.9462936e-09\n",
      "Iteration 41850: loss = 2.2621893e-10,4.9460533e-09\n",
      "Iteration 41855: loss = 2.262205e-10,4.9458215e-09\n",
      "Iteration 41860: loss = 2.2622247e-10,4.945588e-09\n",
      "Iteration 41865: loss = 2.245682e-10,4.9453646e-09\n",
      "Iteration 41870: loss = 2.262287e-10,4.9451168e-09\n",
      "Iteration 41875: loss = 2.2622865e-10,4.94489e-09\n",
      "Iteration 41880: loss = 2.2457597e-10,4.9446602e-09\n",
      "Iteration 41885: loss = 2.2623503e-10,4.944416e-09\n",
      "Iteration 41890: loss = 2.2623753e-10,4.9441837e-09\n",
      "Iteration 41895: loss = 2.2624021e-10,4.9439475e-09\n",
      "Iteration 41900: loss = 2.2458595e-10,4.9437223e-09\n",
      "Iteration 41905: loss = 2.2625023e-10,4.943465e-09\n",
      "Iteration 41910: loss = 2.2459207e-10,4.9432507e-09\n",
      "Iteration 41915: loss = 2.2625275e-10,4.9430016e-09\n",
      "Iteration 41920: loss = 2.2625468e-10,4.9427684e-09\n",
      "Iteration 41925: loss = 2.246003e-10,4.9425433e-09\n",
      "Iteration 41930: loss = 2.2626019e-10,4.9422977e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41935: loss = 2.2626025e-10,4.9420703e-09\n",
      "Iteration 41940: loss = 2.2626495e-10,4.941829e-09\n",
      "Iteration 41945: loss = 2.2461216e-10,4.9416e-09\n",
      "Iteration 41950: loss = 2.2627182e-10,4.9413544e-09\n",
      "Iteration 41955: loss = 2.2461259e-10,4.9411435e-09\n",
      "Iteration 41960: loss = 2.2627693e-10,4.9408855e-09\n",
      "Iteration 41965: loss = 2.2462116e-10,4.9406643e-09\n",
      "Iteration 41970: loss = 2.2462543e-10,4.940424e-09\n",
      "Iteration 41975: loss = 2.2628187e-10,4.940188e-09\n",
      "Iteration 41980: loss = 2.246292e-10,4.9399587e-09\n",
      "Iteration 41985: loss = 2.2463224e-10,4.939722e-09\n",
      "Iteration 41990: loss = 2.2629322e-10,4.939472e-09\n",
      "Iteration 41995: loss = 2.2463385e-10,4.939261e-09\n",
      "Iteration 42000: loss = 2.2629602e-10,4.9390074e-09\n",
      "Iteration 42005: loss = 2.2630099e-10,4.9387676e-09\n",
      "Iteration 42010: loss = 2.2464573e-10,4.938544e-09\n",
      "Iteration 42015: loss = 2.2464743e-10,4.9383115e-09\n",
      "Iteration 42020: loss = 2.2465334e-10,4.938067e-09\n",
      "Iteration 42025: loss = 2.2465403e-10,4.937837e-09\n",
      "Iteration 42030: loss = 2.2465556e-10,4.9376037e-09\n",
      "Iteration 42035: loss = 2.2631574e-10,4.937358e-09\n",
      "Iteration 42040: loss = 2.246563e-10,4.9371467e-09\n",
      "Iteration 42045: loss = 2.263233e-10,4.9368793e-09\n",
      "Iteration 42050: loss = 2.2466784e-10,4.936658e-09\n",
      "Iteration 42055: loss = 2.2466973e-10,4.9364233e-09\n",
      "Iteration 42060: loss = 2.2632995e-10,4.936177e-09\n",
      "Iteration 42065: loss = 2.2467721e-10,4.935947e-09\n",
      "Iteration 42070: loss = 2.2467657e-10,4.9357203e-09\n",
      "Iteration 42075: loss = 2.2633999e-10,4.9354636e-09\n",
      "Iteration 42080: loss = 2.2467822e-10,4.9352584e-09\n",
      "Iteration 42085: loss = 2.2468284e-10,4.935017e-09\n",
      "Iteration 42090: loss = 2.2469147e-10,4.934765e-09\n",
      "Iteration 42095: loss = 2.2469415e-10,4.9345275e-09\n",
      "Iteration 42100: loss = 2.263521e-10,4.934287e-09\n",
      "Iteration 42105: loss = 2.2469922e-10,4.934057e-09\n",
      "Iteration 42110: loss = 2.2470316e-10,4.9338182e-09\n",
      "Iteration 42115: loss = 2.2470399e-10,4.9335878e-09\n",
      "Iteration 42120: loss = 2.2636316e-10,4.9333417e-09\n",
      "Iteration 42125: loss = 2.2470593e-10,4.9331255e-09\n",
      "Iteration 42130: loss = 2.2470036e-10,4.9329127e-09\n",
      "Iteration 42135: loss = 2.2469297e-10,4.9327027e-09\n",
      "Iteration 42140: loss = 2.2468498e-10,4.9324966e-09\n",
      "Iteration 42145: loss = 2.2467579e-10,4.9322946e-09\n",
      "Iteration 42150: loss = 2.2466591e-10,4.932094e-09\n",
      "Iteration 42155: loss = 2.2465464e-10,4.931895e-09\n",
      "Iteration 42160: loss = 2.2464262e-10,4.9317e-09\n",
      "Iteration 42165: loss = 2.2462904e-10,4.9315085e-09\n",
      "Iteration 42170: loss = 2.2461542e-10,4.9313185e-09\n",
      "Iteration 42175: loss = 2.2460117e-10,4.9311284e-09\n",
      "Iteration 42180: loss = 2.2458553e-10,4.9309445e-09\n",
      "Iteration 42185: loss = 2.2456971e-10,4.930758e-09\n",
      "Iteration 42190: loss = 2.2455406e-10,4.930574e-09\n",
      "Iteration 42195: loss = 2.2453704e-10,4.930392e-09\n",
      "Iteration 42200: loss = 2.2452011e-10,4.93021e-09\n",
      "Iteration 42205: loss = 2.2450615e-10,4.930021e-09\n",
      "Iteration 42210: loss = 2.2449802e-10,4.929812e-09\n",
      "Iteration 42215: loss = 2.244857e-10,4.929618e-09\n",
      "Iteration 42220: loss = 2.2447044e-10,4.929432e-09\n",
      "Iteration 42225: loss = 2.2446062e-10,4.9292304e-09\n",
      "Iteration 42230: loss = 2.2444717e-10,4.9290385e-09\n",
      "Iteration 42235: loss = 2.2443698e-10,4.928837e-09\n",
      "Iteration 42240: loss = 2.2442416e-10,4.928645e-09\n",
      "Iteration 42245: loss = 2.2441161e-10,4.928449e-09\n",
      "Iteration 42250: loss = 2.2440111e-10,4.9282503e-09\n",
      "Iteration 42255: loss = 2.243885e-10,4.9280553e-09\n",
      "Iteration 42260: loss = 2.2437639e-10,4.927859e-09\n",
      "Iteration 42265: loss = 2.2436436e-10,4.927664e-09\n",
      "Iteration 42270: loss = 2.2435599e-10,4.9274584e-09\n",
      "Iteration 42275: loss = 2.2435194e-10,4.927241e-09\n",
      "Iteration 42280: loss = 2.2434882e-10,4.9270206e-09\n",
      "Iteration 42285: loss = 2.2434593e-10,4.926799e-09\n",
      "Iteration 42290: loss = 2.2434264e-10,4.9265783e-09\n",
      "Iteration 42295: loss = 2.2433859e-10,4.9263598e-09\n",
      "Iteration 42300: loss = 2.2433401e-10,4.926144e-09\n",
      "Iteration 42305: loss = 2.2432824e-10,4.92593e-09\n",
      "Iteration 42310: loss = 2.2432135e-10,4.9257203e-09\n",
      "Iteration 42315: loss = 2.2431353e-10,4.9255116e-09\n",
      "Iteration 42320: loss = 2.24304e-10,4.9253086e-09\n",
      "Iteration 42325: loss = 2.242937e-10,4.9251083e-09\n",
      "Iteration 42330: loss = 2.2428258e-10,4.92491e-09\n",
      "Iteration 42335: loss = 2.2427081e-10,4.924713e-09\n",
      "Iteration 42340: loss = 2.2425801e-10,4.924519e-09\n",
      "Iteration 42345: loss = 2.2424491e-10,4.9243263e-09\n",
      "Iteration 42350: loss = 2.2423069e-10,4.924136e-09\n",
      "Iteration 42355: loss = 2.2421626e-10,4.923946e-09\n",
      "Iteration 42360: loss = 2.2420132e-10,4.9237587e-09\n",
      "Iteration 42365: loss = 2.2418557e-10,4.923572e-09\n",
      "Iteration 42370: loss = 2.2416934e-10,4.923387e-09\n",
      "Iteration 42375: loss = 2.2415246e-10,4.923205e-09\n",
      "Iteration 42380: loss = 2.2413481e-10,4.9230247e-09\n",
      "Iteration 42385: loss = 2.2411652e-10,4.922846e-09\n",
      "Iteration 42390: loss = 2.2409857e-10,4.922666e-09\n",
      "Iteration 42395: loss = 2.2408025e-10,4.9224873e-09\n",
      "Iteration 42400: loss = 2.2406094e-10,4.9223106e-09\n",
      "Iteration 42405: loss = 2.2404188e-10,4.9221347e-09\n",
      "Iteration 42410: loss = 2.240259e-10,4.921949e-09\n",
      "Iteration 42415: loss = 2.2401823e-10,4.9217403e-09\n",
      "Iteration 42420: loss = 2.24006e-10,4.921544e-09\n",
      "Iteration 42425: loss = 2.2399362e-10,4.921349e-09\n",
      "Iteration 42430: loss = 2.2398329e-10,4.921148e-09\n",
      "Iteration 42435: loss = 2.2396907e-10,4.9209574e-09\n",
      "Iteration 42440: loss = 2.2396011e-10,4.920751e-09\n",
      "Iteration 42445: loss = 2.2394638e-10,4.92056e-09\n",
      "Iteration 42450: loss = 2.2393698e-10,4.9203557e-09\n",
      "Iteration 42455: loss = 2.2392489e-10,4.9201594e-09\n",
      "Iteration 42460: loss = 2.2391444e-10,4.9199587e-09\n",
      "Iteration 42465: loss = 2.239038e-10,4.9197575e-09\n",
      "Iteration 42470: loss = 2.2390094e-10,4.9195354e-09\n",
      "Iteration 42475: loss = 2.2389895e-10,4.9193094e-09\n",
      "Iteration 42480: loss = 2.2389683e-10,4.919086e-09\n",
      "Iteration 42485: loss = 2.2389378e-10,4.9188627e-09\n",
      "Iteration 42490: loss = 2.2389014e-10,4.9186433e-09\n",
      "Iteration 42495: loss = 2.2388585e-10,4.9184257e-09\n",
      "Iteration 42500: loss = 2.2387942e-10,4.9182125e-09\n",
      "Iteration 42505: loss = 2.2387192e-10,4.918003e-09\n",
      "Iteration 42510: loss = 2.2386387e-10,4.917796e-09\n",
      "Iteration 42515: loss = 2.2385445e-10,4.9175917e-09\n",
      "Iteration 42520: loss = 2.2384405e-10,4.9173883e-09\n",
      "Iteration 42525: loss = 2.2383267e-10,4.9171898e-09\n",
      "Iteration 42530: loss = 2.238217e-10,4.9169913e-09\n",
      "Iteration 42535: loss = 2.2380918e-10,4.9167954e-09\n",
      "Iteration 42540: loss = 2.2379548e-10,4.916603e-09\n",
      "Iteration 42545: loss = 2.2378051e-10,4.9164126e-09\n",
      "Iteration 42550: loss = 2.2376588e-10,4.9162243e-09\n",
      "Iteration 42555: loss = 2.2375025e-10,4.9160356e-09\n",
      "Iteration 42560: loss = 2.237339e-10,4.915851e-09\n",
      "Iteration 42565: loss = 2.2371738e-10,4.915667e-09\n",
      "Iteration 42570: loss = 2.2370071e-10,4.9154827e-09\n",
      "Iteration 42575: loss = 2.2368363e-10,4.9153e-09\n",
      "Iteration 42580: loss = 2.2366585e-10,4.915119e-09\n",
      "Iteration 42585: loss = 2.2364789e-10,4.9149382e-09\n",
      "Iteration 42590: loss = 2.2362966e-10,4.9147575e-09\n",
      "Iteration 42595: loss = 2.2361142e-10,4.914577e-09\n",
      "Iteration 42600: loss = 2.2359538e-10,4.9143907e-09\n",
      "Iteration 42605: loss = 2.2359369e-10,4.9141646e-09\n",
      "Iteration 42610: loss = 2.2359777e-10,4.9139235e-09\n",
      "Iteration 42615: loss = 2.2360075e-10,4.913683e-09\n",
      "Iteration 42620: loss = 2.2360465e-10,4.913441e-09\n",
      "Iteration 42625: loss = 2.2195946e-10,4.9132147e-09\n",
      "Iteration 42630: loss = 2.2361264e-10,4.912957e-09\n",
      "Iteration 42635: loss = 2.2196446e-10,4.9127378e-09\n",
      "Iteration 42640: loss = 2.2361223e-10,4.912496e-09\n",
      "Iteration 42645: loss = 2.2197044e-10,4.912258e-09\n",
      "Iteration 42650: loss = 2.2197257e-10,4.912022e-09\n",
      "Iteration 42655: loss = 2.2197554e-10,4.9117816e-09\n",
      "Iteration 42660: loss = 2.2362313e-10,4.9115405e-09\n",
      "Iteration 42665: loss = 2.2362913e-10,4.9112927e-09\n",
      "Iteration 42670: loss = 2.2198192e-10,4.91107e-09\n",
      "Iteration 42675: loss = 2.2363428e-10,4.910815e-09\n",
      "Iteration 42680: loss = 2.2198937e-10,4.910586e-09\n",
      "Iteration 42685: loss = 2.2199226e-10,4.9103477e-09\n",
      "Iteration 42690: loss = 2.2364192e-10,4.910099e-09\n",
      "Iteration 42695: loss = 2.2199603e-10,4.9098734e-09\n",
      "Iteration 42700: loss = 2.2364693e-10,4.909622e-09\n",
      "Iteration 42705: loss = 2.2199964e-10,4.9094e-09\n",
      "Iteration 42710: loss = 2.2365167e-10,4.909145e-09\n",
      "Iteration 42715: loss = 2.2200451e-10,4.9089235e-09\n",
      "Iteration 42720: loss = 2.2366146e-10,4.9086553e-09\n",
      "Iteration 42725: loss = 2.2201219e-10,4.908439e-09\n",
      "Iteration 42730: loss = 2.2366346e-10,4.908186e-09\n",
      "Iteration 42735: loss = 2.2201556e-10,4.907966e-09\n",
      "Iteration 42740: loss = 2.2366915e-10,4.907706e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42745: loss = 2.2202214e-10,4.907484e-09\n",
      "Iteration 42750: loss = 2.2367423e-10,4.907228e-09\n",
      "Iteration 42755: loss = 2.2202463e-10,4.9070135e-09\n",
      "Iteration 42760: loss = 2.2367913e-10,4.9067506e-09\n",
      "Iteration 42765: loss = 2.2203439e-10,4.906522e-09\n",
      "Iteration 42770: loss = 2.2203793e-10,4.9062803e-09\n",
      "Iteration 42775: loss = 2.220406e-10,4.906041e-09\n",
      "Iteration 42780: loss = 2.2368928e-10,4.905795e-09\n",
      "Iteration 42785: loss = 2.2204528e-10,4.905563e-09\n",
      "Iteration 42790: loss = 2.220505e-10,4.905317e-09\n",
      "Iteration 42795: loss = 2.2369767e-10,4.9050746e-09\n",
      "Iteration 42800: loss = 2.2205104e-10,4.904851e-09\n",
      "Iteration 42805: loss = 2.2370317e-10,4.904595e-09\n",
      "Iteration 42810: loss = 2.220578e-10,4.904367e-09\n",
      "Iteration 42815: loss = 2.2370934e-10,4.9041127e-09\n",
      "Iteration 42820: loss = 2.220622e-10,4.9038906e-09\n",
      "Iteration 42825: loss = 2.220678e-10,4.9036437e-09\n",
      "Iteration 42830: loss = 2.2207215e-10,4.903399e-09\n",
      "Iteration 42835: loss = 2.2206954e-10,4.903173e-09\n",
      "Iteration 42840: loss = 2.2207475e-10,4.902926e-09\n",
      "Iteration 42845: loss = 2.2208159e-10,4.9026743e-09\n",
      "Iteration 42850: loss = 2.2208368e-10,4.902436e-09\n",
      "Iteration 42855: loss = 2.2208406e-10,4.902204e-09\n",
      "Iteration 42860: loss = 2.2208395e-10,4.9019713e-09\n",
      "Iteration 42865: loss = 2.2208757e-10,4.9017284e-09\n",
      "Iteration 42870: loss = 2.2209246e-10,4.901482e-09\n",
      "Iteration 42875: loss = 2.2209717e-10,4.9012363e-09\n",
      "Iteration 42880: loss = 2.2210127e-10,4.900993e-09\n",
      "Iteration 42885: loss = 2.2210368e-10,4.9007536e-09\n",
      "Iteration 42890: loss = 2.2210483e-10,4.9005178e-09\n",
      "Iteration 42895: loss = 2.2210413e-10,4.9002864e-09\n",
      "Iteration 42900: loss = 2.2210195e-10,4.90006e-09\n",
      "Iteration 42905: loss = 2.2209902e-10,4.8998356e-09\n",
      "Iteration 42910: loss = 2.2209466e-10,4.8996145e-09\n",
      "Iteration 42915: loss = 2.2208811e-10,4.899401e-09\n",
      "Iteration 42920: loss = 2.220809e-10,4.899188e-09\n",
      "Iteration 42925: loss = 2.2207187e-10,4.8989794e-09\n",
      "Iteration 42930: loss = 2.2206272e-10,4.898773e-09\n",
      "Iteration 42935: loss = 2.2205239e-10,4.8985687e-09\n",
      "Iteration 42940: loss = 2.220414e-10,4.8983657e-09\n",
      "Iteration 42945: loss = 2.220294e-10,4.8981663e-09\n",
      "Iteration 42950: loss = 2.220168e-10,4.8979687e-09\n",
      "Iteration 42955: loss = 2.2200237e-10,4.897775e-09\n",
      "Iteration 42960: loss = 2.2198786e-10,4.897583e-09\n",
      "Iteration 42965: loss = 2.2197306e-10,4.8973914e-09\n",
      "Iteration 42970: loss = 2.2196488e-10,4.8971796e-09\n",
      "Iteration 42975: loss = 2.2196522e-10,4.896947e-09\n",
      "Iteration 42980: loss = 2.2196918e-10,4.8967026e-09\n",
      "Iteration 42985: loss = 2.2197484e-10,4.896454e-09\n",
      "Iteration 42990: loss = 2.2198066e-10,4.8962048e-09\n",
      "Iteration 42995: loss = 2.2198554e-10,4.895958e-09\n",
      "Iteration 43000: loss = 2.2198958e-10,4.8957136e-09\n",
      "Iteration 43005: loss = 2.219923e-10,4.895473e-09\n",
      "Iteration 43010: loss = 2.219938e-10,4.8952358e-09\n",
      "Iteration 43015: loss = 2.2199305e-10,4.895005e-09\n",
      "Iteration 43020: loss = 2.2199108e-10,4.8947775e-09\n",
      "Iteration 43025: loss = 2.2198714e-10,4.894553e-09\n",
      "Iteration 43030: loss = 2.2198227e-10,4.894333e-09\n",
      "Iteration 43035: loss = 2.2197681e-10,4.8941162e-09\n",
      "Iteration 43040: loss = 2.2196922e-10,4.893904e-09\n",
      "Iteration 43045: loss = 2.2196021e-10,4.893695e-09\n",
      "Iteration 43050: loss = 2.2195061e-10,4.893489e-09\n",
      "Iteration 43055: loss = 2.2193995e-10,4.8932844e-09\n",
      "Iteration 43060: loss = 2.2192857e-10,4.893082e-09\n",
      "Iteration 43065: loss = 2.2191633e-10,4.892882e-09\n",
      "Iteration 43070: loss = 2.2190359e-10,4.8926845e-09\n",
      "Iteration 43075: loss = 2.2189016e-10,4.8924886e-09\n",
      "Iteration 43080: loss = 2.2187589e-10,4.8922946e-09\n",
      "Iteration 43085: loss = 2.2186163e-10,4.8921e-09\n",
      "Iteration 43090: loss = 2.2184625e-10,4.89191e-09\n",
      "Iteration 43095: loss = 2.2183e-10,4.891721e-09\n",
      "Iteration 43100: loss = 2.2181333e-10,4.891534e-09\n",
      "Iteration 43105: loss = 2.2179608e-10,4.8913478e-09\n",
      "Iteration 43110: loss = 2.2177858e-10,4.8911626e-09\n",
      "Iteration 43115: loss = 2.217605e-10,4.890979e-09\n",
      "Iteration 43120: loss = 2.217429e-10,4.8907953e-09\n",
      "Iteration 43125: loss = 2.2172453e-10,4.8906115e-09\n",
      "Iteration 43130: loss = 2.2170606e-10,4.89043e-09\n",
      "Iteration 43135: loss = 2.2168685e-10,4.890249e-09\n",
      "Iteration 43140: loss = 2.216678e-10,4.8900684e-09\n",
      "Iteration 43145: loss = 2.2164844e-10,4.8898876e-09\n",
      "Iteration 43150: loss = 2.2162934e-10,4.8897077e-09\n",
      "Iteration 43155: loss = 2.2161005e-10,4.889528e-09\n",
      "Iteration 43160: loss = 2.2159019e-10,4.8893485e-09\n",
      "Iteration 43165: loss = 2.2157015e-10,4.889171e-09\n",
      "Iteration 43170: loss = 2.2154989e-10,4.8889928e-09\n",
      "Iteration 43175: loss = 2.2152986e-10,4.8888147e-09\n",
      "Iteration 43180: loss = 2.2151757e-10,4.888615e-09\n",
      "Iteration 43185: loss = 2.2151313e-10,4.8883932e-09\n",
      "Iteration 43190: loss = 2.2151297e-10,4.888159e-09\n",
      "Iteration 43195: loss = 2.2151547e-10,4.8879167e-09\n",
      "Iteration 43200: loss = 2.2151841e-10,4.8876734e-09\n",
      "Iteration 43205: loss = 2.2152134e-10,4.8874322e-09\n",
      "Iteration 43210: loss = 2.2152219e-10,4.8871946e-09\n",
      "Iteration 43215: loss = 2.2152184e-10,4.8869606e-09\n",
      "Iteration 43220: loss = 2.2152068e-10,4.8867306e-09\n",
      "Iteration 43225: loss = 2.2151779e-10,4.886504e-09\n",
      "Iteration 43230: loss = 2.2151388e-10,4.8862803e-09\n",
      "Iteration 43235: loss = 2.2150909e-10,4.8860587e-09\n",
      "Iteration 43240: loss = 2.2150211e-10,4.8858437e-09\n",
      "Iteration 43245: loss = 2.2149416e-10,4.8856315e-09\n",
      "Iteration 43250: loss = 2.2148512e-10,4.885421e-09\n",
      "Iteration 43255: loss = 2.2147562e-10,4.885214e-09\n",
      "Iteration 43260: loss = 2.2146457e-10,4.8850093e-09\n",
      "Iteration 43265: loss = 2.214537e-10,4.8848054e-09\n",
      "Iteration 43270: loss = 2.2144143e-10,4.8846056e-09\n",
      "Iteration 43275: loss = 2.2142906e-10,4.8844053e-09\n",
      "Iteration 43280: loss = 2.2141496e-10,4.8842095e-09\n",
      "Iteration 43285: loss = 2.214001e-10,4.8840154e-09\n",
      "Iteration 43290: loss = 2.2138473e-10,4.8838236e-09\n",
      "Iteration 43295: loss = 2.2136877e-10,4.883634e-09\n",
      "Iteration 43300: loss = 2.2135238e-10,4.8834448e-09\n",
      "Iteration 43305: loss = 2.2133569e-10,4.8832556e-09\n",
      "Iteration 43310: loss = 2.2131925e-10,4.8830673e-09\n",
      "Iteration 43315: loss = 2.2130163e-10,4.8828817e-09\n",
      "Iteration 43320: loss = 2.21284e-10,4.8826956e-09\n",
      "Iteration 43325: loss = 2.2126591e-10,4.8825104e-09\n",
      "Iteration 43330: loss = 2.2124798e-10,4.882326e-09\n",
      "Iteration 43335: loss = 2.2122976e-10,4.8821414e-09\n",
      "Iteration 43340: loss = 2.2121067e-10,4.881959e-09\n",
      "Iteration 43345: loss = 2.21192e-10,4.881777e-09\n",
      "Iteration 43350: loss = 2.2117269e-10,4.8815956e-09\n",
      "Iteration 43355: loss = 2.211535e-10,4.8814126e-09\n",
      "Iteration 43360: loss = 2.2113451e-10,4.8812314e-09\n",
      "Iteration 43365: loss = 2.2111478e-10,4.8810502e-09\n",
      "Iteration 43370: loss = 2.2109466e-10,4.8808717e-09\n",
      "Iteration 43375: loss = 2.2107517e-10,4.880692e-09\n",
      "Iteration 43380: loss = 2.2106277e-10,4.8804902e-09\n",
      "Iteration 43385: loss = 2.2105827e-10,4.880267e-09\n",
      "Iteration 43390: loss = 2.210588e-10,4.880031e-09\n",
      "Iteration 43395: loss = 2.2106052e-10,4.879791e-09\n",
      "Iteration 43400: loss = 2.2106235e-10,4.8795497e-09\n",
      "Iteration 43405: loss = 2.2106426e-10,4.879309e-09\n",
      "Iteration 43410: loss = 2.2106535e-10,4.87907e-09\n",
      "Iteration 43415: loss = 2.210655e-10,4.878835e-09\n",
      "Iteration 43420: loss = 2.2106421e-10,4.878602e-09\n",
      "Iteration 43425: loss = 2.210619e-10,4.8783724e-09\n",
      "Iteration 43430: loss = 2.2105867e-10,4.8781468e-09\n",
      "Iteration 43435: loss = 2.2105305e-10,4.8779265e-09\n",
      "Iteration 43440: loss = 2.2104647e-10,4.87771e-09\n",
      "Iteration 43445: loss = 2.2104157e-10,4.877487e-09\n",
      "Iteration 43450: loss = 2.1940945e-10,4.8772444e-09\n",
      "Iteration 43455: loss = 2.2105172e-10,4.876987e-09\n",
      "Iteration 43460: loss = 2.2105064e-10,4.876753e-09\n",
      "Iteration 43465: loss = 2.2105388e-10,4.876508e-09\n",
      "Iteration 43470: loss = 2.2105619e-10,4.876266e-09\n",
      "Iteration 43475: loss = 2.1942186e-10,4.87603e-09\n",
      "Iteration 43480: loss = 2.2106939e-10,4.8757554e-09\n",
      "Iteration 43485: loss = 2.2107005e-10,4.8755178e-09\n",
      "Iteration 43490: loss = 2.1943143e-10,4.8752944e-09\n",
      "Iteration 43495: loss = 2.2107106e-10,4.875043e-09\n",
      "Iteration 43500: loss = 2.2107617e-10,4.8747926e-09\n",
      "Iteration 43505: loss = 2.2107788e-10,4.874552e-09\n",
      "Iteration 43510: loss = 2.2107934e-10,4.8743116e-09\n",
      "Iteration 43515: loss = 2.2108015e-10,4.8740723e-09\n",
      "Iteration 43520: loss = 2.2108394e-10,4.8738267e-09\n",
      "Iteration 43525: loss = 2.1945089e-10,4.873586e-09\n",
      "Iteration 43530: loss = 2.2108972e-10,4.8733373e-09\n",
      "Iteration 43535: loss = 2.2109592e-10,4.8730833e-09\n",
      "Iteration 43540: loss = 2.1945877e-10,4.872855e-09\n",
      "Iteration 43545: loss = 2.2110362e-10,4.8725877e-09\n",
      "Iteration 43550: loss = 2.1946317e-10,4.8723705e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43555: loss = 2.2110995e-10,4.872098e-09\n",
      "Iteration 43560: loss = 2.2110962e-10,4.8718616e-09\n",
      "Iteration 43565: loss = 2.1947166e-10,4.871637e-09\n",
      "Iteration 43570: loss = 2.2111363e-10,4.871377e-09\n",
      "Iteration 43575: loss = 2.211144e-10,4.871138e-09\n",
      "Iteration 43580: loss = 2.2112169e-10,4.8708815e-09\n",
      "Iteration 43585: loss = 2.194835e-10,4.870656e-09\n",
      "Iteration 43590: loss = 2.1948833e-10,4.870405e-09\n",
      "Iteration 43595: loss = 2.1949047e-10,4.870163e-09\n",
      "Iteration 43600: loss = 2.2113233e-10,4.869905e-09\n",
      "Iteration 43605: loss = 2.2113307e-10,4.8696647e-09\n",
      "Iteration 43610: loss = 2.2113474e-10,4.8694235e-09\n",
      "Iteration 43615: loss = 2.195027e-10,4.8691806e-09\n",
      "Iteration 43620: loss = 2.2114144e-10,4.86893e-09\n",
      "Iteration 43625: loss = 2.2114406e-10,4.868686e-09\n",
      "Iteration 43630: loss = 2.1951004e-10,4.868447e-09\n",
      "Iteration 43635: loss = 2.2115053e-10,4.8681943e-09\n",
      "Iteration 43640: loss = 2.2115516e-10,4.867944e-09\n",
      "Iteration 43645: loss = 2.1951574e-10,4.8677196e-09\n",
      "Iteration 43650: loss = 2.2116008e-10,4.8674558e-09\n",
      "Iteration 43655: loss = 2.211612e-10,4.8672155e-09\n",
      "Iteration 43660: loss = 2.2116438e-10,4.8669686e-09\n",
      "Iteration 43665: loss = 2.2116746e-10,4.866723e-09\n",
      "Iteration 43670: loss = 2.211703e-10,4.8664783e-09\n",
      "Iteration 43675: loss = 2.1953765e-10,4.866236e-09\n",
      "Iteration 43680: loss = 2.1953615e-10,4.866003e-09\n",
      "Iteration 43685: loss = 2.2117995e-10,4.8657394e-09\n",
      "Iteration 43690: loss = 2.1954327e-10,4.8655093e-09\n",
      "Iteration 43695: loss = 2.2117491e-10,4.8652793e-09\n",
      "Iteration 43700: loss = 2.1951839e-10,4.865104e-09\n",
      "Iteration 43705: loss = 2.2114977e-10,4.864874e-09\n",
      "Iteration 43710: loss = 2.1949707e-10,4.864688e-09\n",
      "Iteration 43715: loss = 2.1948744e-10,4.8644773e-09\n",
      "Iteration 43720: loss = 2.2111298e-10,4.864263e-09\n",
      "Iteration 43725: loss = 2.1946196e-10,4.864072e-09\n",
      "Iteration 43730: loss = 2.1945341e-10,4.8638578e-09\n",
      "Iteration 43735: loss = 2.2107653e-10,4.8636526e-09\n",
      "Iteration 43740: loss = 2.2106462e-10,4.8634483e-09\n",
      "Iteration 43745: loss = 2.2105452e-10,4.8632383e-09\n",
      "Iteration 43750: loss = 2.1940127e-10,4.8630526e-09\n",
      "Iteration 43755: loss = 2.210318e-10,4.8628257e-09\n",
      "Iteration 43760: loss = 2.193759e-10,4.86265e-09\n",
      "Iteration 43765: loss = 2.2100875e-10,4.862413e-09\n",
      "Iteration 43770: loss = 2.1935298e-10,4.8622364e-09\n",
      "Iteration 43775: loss = 2.2098438e-10,4.8620055e-09\n",
      "Iteration 43780: loss = 2.1932983e-10,4.861825e-09\n",
      "Iteration 43785: loss = 2.1932135e-10,4.8616107e-09\n",
      "Iteration 43790: loss = 2.209506e-10,4.8613873e-09\n",
      "Iteration 43795: loss = 2.1929701e-10,4.8612003e-09\n",
      "Iteration 43800: loss = 2.1928799e-10,4.860989e-09\n",
      "Iteration 43805: loss = 2.2091275e-10,4.8607776e-09\n",
      "Iteration 43810: loss = 2.1926046e-10,4.8605893e-09\n",
      "Iteration 43815: loss = 2.2089024e-10,4.8603637e-09\n",
      "Iteration 43820: loss = 2.1924002e-10,4.8601705e-09\n",
      "Iteration 43825: loss = 2.1923069e-10,4.8599573e-09\n",
      "Iteration 43830: loss = 2.2085543e-10,4.8597464e-09\n",
      "Iteration 43835: loss = 2.208438e-10,4.85954e-09\n",
      "Iteration 43840: loss = 2.1919488e-10,4.859343e-09\n",
      "Iteration 43845: loss = 2.1918267e-10,4.859138e-09\n",
      "Iteration 43850: loss = 2.1916861e-10,4.858939e-09\n",
      "Iteration 43855: loss = 2.2079817e-10,4.8587134e-09\n",
      "Iteration 43860: loss = 2.1914641e-10,4.858524e-09\n",
      "Iteration 43865: loss = 2.2077527e-10,4.8583004e-09\n",
      "Iteration 43870: loss = 2.1912916e-10,4.8580944e-09\n",
      "Iteration 43875: loss = 2.2075346e-10,4.8578848e-09\n",
      "Iteration 43880: loss = 2.1910206e-10,4.857694e-09\n",
      "Iteration 43885: loss = 2.2073199e-10,4.857466e-09\n",
      "Iteration 43890: loss = 2.1908332e-10,4.8572684e-09\n",
      "Iteration 43895: loss = 2.1907152e-10,4.857062e-09\n",
      "Iteration 43900: loss = 2.1906005e-10,4.856855e-09\n",
      "Iteration 43905: loss = 2.2068454e-10,4.8566435e-09\n",
      "Iteration 43910: loss = 2.1903727e-10,4.8564415e-09\n",
      "Iteration 43915: loss = 2.1902462e-10,4.856237e-09\n",
      "Iteration 43920: loss = 2.206509e-10,4.856021e-09\n",
      "Iteration 43925: loss = 2.206393e-10,4.8558153e-09\n",
      "Iteration 43930: loss = 2.1899238e-10,4.8556115e-09\n",
      "Iteration 43935: loss = 2.1897988e-10,4.855407e-09\n",
      "Iteration 43940: loss = 2.1896784e-10,4.8552016e-09\n",
      "Iteration 43945: loss = 2.2059482e-10,4.854983e-09\n",
      "Iteration 43950: loss = 2.1894747e-10,4.8547806e-09\n",
      "Iteration 43955: loss = 2.1893425e-10,4.854578e-09\n",
      "Iteration 43960: loss = 2.189235e-10,4.8543685e-09\n",
      "Iteration 43965: loss = 2.2055159e-10,4.8541478e-09\n",
      "Iteration 43970: loss = 2.1889963e-10,4.853958e-09\n",
      "Iteration 43975: loss = 2.1889052e-10,4.8537436e-09\n",
      "Iteration 43980: loss = 2.1887854e-10,4.853538e-09\n",
      "Iteration 43985: loss = 2.1886905e-10,4.8533253e-09\n",
      "Iteration 43990: loss = 2.2049275e-10,4.8531157e-09\n",
      "Iteration 43995: loss = 2.188471e-10,4.8529083e-09\n",
      "Iteration 44000: loss = 2.2047075e-10,4.8526982e-09\n",
      "Iteration 44005: loss = 2.1882196e-10,4.852499e-09\n",
      "Iteration 44010: loss = 2.188134e-10,4.852284e-09\n",
      "Iteration 44015: loss = 2.1880224e-10,4.8520747e-09\n",
      "Iteration 44020: loss = 2.1879061e-10,4.8518674e-09\n",
      "Iteration 44025: loss = 2.2041613e-10,4.8516537e-09\n",
      "Iteration 44030: loss = 2.1876605e-10,4.8514575e-09\n",
      "Iteration 44035: loss = 2.1875556e-10,4.8512474e-09\n",
      "Iteration 44040: loss = 2.2038356e-10,4.8510245e-09\n",
      "Iteration 44045: loss = 2.1872783e-10,4.850846e-09\n",
      "Iteration 44050: loss = 2.2036044e-10,4.850611e-09\n",
      "Iteration 44055: loss = 2.1870777e-10,4.8504223e-09\n",
      "Iteration 44060: loss = 2.1870299e-10,4.850195e-09\n",
      "Iteration 44065: loss = 2.1868896e-10,4.8499946e-09\n",
      "Iteration 44070: loss = 2.2031553e-10,4.8497757e-09\n",
      "Iteration 44075: loss = 2.2030638e-10,4.849563e-09\n",
      "Iteration 44080: loss = 2.1865339e-10,4.849374e-09\n",
      "Iteration 44085: loss = 2.2028439e-10,4.8491438e-09\n",
      "Iteration 44090: loss = 2.186367e-10,4.8489417e-09\n",
      "Iteration 44095: loss = 2.2026055e-10,4.8487303e-09\n",
      "Iteration 44100: loss = 2.202505e-10,4.848518e-09\n",
      "Iteration 44105: loss = 2.1860073e-10,4.8483217e-09\n",
      "Iteration 44110: loss = 2.202281e-10,4.8481006e-09\n",
      "Iteration 44115: loss = 2.1857921e-10,4.8479016e-09\n",
      "Iteration 44120: loss = 2.1856734e-10,4.8476947e-09\n",
      "Iteration 44125: loss = 2.2019392e-10,4.8474758e-09\n",
      "Iteration 44130: loss = 2.1854792e-10,4.8472684e-09\n",
      "Iteration 44135: loss = 2.1853665e-10,4.8470596e-09\n",
      "Iteration 44140: loss = 2.1852603e-10,4.846849e-09\n",
      "Iteration 44145: loss = 2.1851475e-10,4.8466395e-09\n",
      "Iteration 44150: loss = 2.2014023e-10,4.8464246e-09\n",
      "Iteration 44155: loss = 2.1849302e-10,4.84622e-09\n",
      "Iteration 44160: loss = 2.2011777e-10,4.846006e-09\n",
      "Iteration 44165: loss = 2.2010638e-10,4.845798e-09\n",
      "Iteration 44170: loss = 2.2009554e-10,4.8455866e-09\n",
      "Iteration 44175: loss = 2.200856e-10,4.8453734e-09\n",
      "Iteration 44180: loss = 2.1844616e-10,4.8451474e-09\n",
      "Iteration 44185: loss = 2.1845235e-10,4.8448894e-09\n",
      "Iteration 44190: loss = 2.1845571e-10,4.8446394e-09\n",
      "Iteration 44195: loss = 2.1845475e-10,4.844401e-09\n",
      "Iteration 44200: loss = 2.200967e-10,4.8441393e-09\n",
      "Iteration 44205: loss = 2.1846257e-10,4.843897e-09\n",
      "Iteration 44210: loss = 2.1846656e-10,4.8436464e-09\n",
      "Iteration 44215: loss = 2.2010554e-10,4.843391e-09\n",
      "Iteration 44220: loss = 2.1846382e-10,4.8431708e-09\n",
      "Iteration 44225: loss = 2.1846813e-10,4.842918e-09\n",
      "Iteration 44230: loss = 2.20115e-10,4.8426414e-09\n",
      "Iteration 44235: loss = 2.1847545e-10,4.8424154e-09\n",
      "Iteration 44240: loss = 2.1847872e-10,4.842165e-09\n",
      "Iteration 44245: loss = 2.1848605e-10,4.8419038e-09\n",
      "Iteration 44250: loss = 2.1848828e-10,4.841656e-09\n",
      "Iteration 44255: loss = 2.1849063e-10,4.8414086e-09\n",
      "Iteration 44260: loss = 2.1849104e-10,4.841165e-09\n",
      "Iteration 44265: loss = 2.1849646e-10,4.840908e-09\n",
      "Iteration 44270: loss = 2.1849823e-10,4.840662e-09\n",
      "Iteration 44275: loss = 2.1849729e-10,4.8404245e-09\n",
      "Iteration 44280: loss = 2.1849926e-10,4.8401776e-09\n",
      "Iteration 44285: loss = 2.1850359e-10,4.839925e-09\n",
      "Iteration 44290: loss = 2.1850795e-10,4.839671e-09\n",
      "Iteration 44295: loss = 2.1851128e-10,4.839419e-09\n",
      "Iteration 44300: loss = 2.1851394e-10,4.839171e-09\n",
      "Iteration 44305: loss = 2.1851415e-10,4.8389284e-09\n",
      "Iteration 44310: loss = 2.1851365e-10,4.8386886e-09\n",
      "Iteration 44315: loss = 2.1851153e-10,4.8384523e-09\n",
      "Iteration 44320: loss = 2.185087e-10,4.8382187e-09\n",
      "Iteration 44325: loss = 2.1850399e-10,4.8379887e-09\n",
      "Iteration 44330: loss = 2.1849829e-10,4.8377644e-09\n",
      "Iteration 44335: loss = 2.1849088e-10,4.837543e-09\n",
      "Iteration 44340: loss = 2.1848283e-10,4.8373248e-09\n",
      "Iteration 44345: loss = 2.1847364e-10,4.837108e-09\n",
      "Iteration 44350: loss = 2.1846354e-10,4.836894e-09\n",
      "Iteration 44355: loss = 2.1845255e-10,4.8366826e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44360: loss = 2.1844103e-10,4.8364726e-09\n",
      "Iteration 44365: loss = 2.1842926e-10,4.836265e-09\n",
      "Iteration 44370: loss = 2.1841601e-10,4.836058e-09\n",
      "Iteration 44375: loss = 2.1840225e-10,4.8358553e-09\n",
      "Iteration 44380: loss = 2.1838718e-10,4.8356537e-09\n",
      "Iteration 44385: loss = 2.1837222e-10,4.8354547e-09\n",
      "Iteration 44390: loss = 2.1835667e-10,4.8352566e-09\n",
      "Iteration 44395: loss = 2.1834069e-10,4.8350595e-09\n",
      "Iteration 44400: loss = 2.1832484e-10,4.8348627e-09\n",
      "Iteration 44405: loss = 2.1830947e-10,4.834663e-09\n",
      "Iteration 44410: loss = 2.1830371e-10,4.8344364e-09\n",
      "Iteration 44415: loss = 2.1830472e-10,4.834192e-09\n",
      "Iteration 44420: loss = 2.1830804e-10,4.8339395e-09\n",
      "Iteration 44425: loss = 2.1831272e-10,4.8336846e-09\n",
      "Iteration 44430: loss = 2.183167e-10,4.8334305e-09\n",
      "Iteration 44435: loss = 2.1832004e-10,4.833178e-09\n",
      "Iteration 44440: loss = 2.1832279e-10,4.8329274e-09\n",
      "Iteration 44445: loss = 2.1832426e-10,4.8326814e-09\n",
      "Iteration 44450: loss = 2.1832419e-10,4.8324384e-09\n",
      "Iteration 44455: loss = 2.1832286e-10,4.8322e-09\n",
      "Iteration 44460: loss = 2.1831943e-10,4.831966e-09\n",
      "Iteration 44465: loss = 2.1831524e-10,4.8317355e-09\n",
      "Iteration 44470: loss = 2.1830958e-10,4.8315094e-09\n",
      "Iteration 44475: loss = 2.1830271e-10,4.8312856e-09\n",
      "Iteration 44480: loss = 2.1829548e-10,4.8310627e-09\n",
      "Iteration 44485: loss = 2.1828679e-10,4.8308437e-09\n",
      "Iteration 44490: loss = 2.1827735e-10,4.830629e-09\n",
      "Iteration 44495: loss = 2.1826603e-10,4.8304174e-09\n",
      "Iteration 44500: loss = 2.1825391e-10,4.8302065e-09\n",
      "Iteration 44505: loss = 2.1824159e-10,4.8299995e-09\n",
      "Iteration 44510: loss = 2.1822882e-10,4.8297926e-09\n",
      "Iteration 44515: loss = 2.1821515e-10,4.8295883e-09\n",
      "Iteration 44520: loss = 2.1820085e-10,4.829385e-09\n",
      "Iteration 44525: loss = 2.1818601e-10,4.829183e-09\n",
      "Iteration 44530: loss = 2.1817069e-10,4.8289825e-09\n",
      "Iteration 44535: loss = 2.181552e-10,4.8287823e-09\n",
      "Iteration 44540: loss = 2.1813916e-10,4.828584e-09\n",
      "Iteration 44545: loss = 2.1812258e-10,4.828388e-09\n",
      "Iteration 44550: loss = 2.1810592e-10,4.828191e-09\n",
      "Iteration 44555: loss = 2.1808948e-10,4.8279944e-09\n",
      "Iteration 44560: loss = 2.1807191e-10,4.827799e-09\n",
      "Iteration 44565: loss = 2.1805452e-10,4.8276054e-09\n",
      "Iteration 44570: loss = 2.1803619e-10,4.827412e-09\n",
      "Iteration 44575: loss = 2.1801853e-10,4.8272186e-09\n",
      "Iteration 44580: loss = 2.1800034e-10,4.827026e-09\n",
      "Iteration 44585: loss = 2.1798231e-10,4.826833e-09\n",
      "Iteration 44590: loss = 2.1796331e-10,4.826642e-09\n",
      "Iteration 44595: loss = 2.1794416e-10,4.826452e-09\n",
      "Iteration 44600: loss = 2.1792523e-10,4.826262e-09\n",
      "Iteration 44605: loss = 2.1790586e-10,4.826073e-09\n",
      "Iteration 44610: loss = 2.1788697e-10,4.8258824e-09\n",
      "Iteration 44615: loss = 2.1786774e-10,4.8256927e-09\n",
      "Iteration 44620: loss = 2.17853e-10,4.8254902e-09\n",
      "Iteration 44625: loss = 2.1784706e-10,4.8252624e-09\n",
      "Iteration 44630: loss = 2.1784717e-10,4.825019e-09\n",
      "Iteration 44635: loss = 2.1784878e-10,4.82477e-09\n",
      "Iteration 44640: loss = 2.1785133e-10,4.824518e-09\n",
      "Iteration 44645: loss = 2.1785362e-10,4.8242668e-09\n",
      "Iteration 44650: loss = 2.1785536e-10,4.8240194e-09\n",
      "Iteration 44655: loss = 2.1785564e-10,4.823774e-09\n",
      "Iteration 44660: loss = 2.1785533e-10,4.823531e-09\n",
      "Iteration 44665: loss = 2.1785335e-10,4.823293e-09\n",
      "Iteration 44670: loss = 2.1784924e-10,4.8230597e-09\n",
      "Iteration 44675: loss = 2.1784428e-10,4.8228297e-09\n",
      "Iteration 44680: loss = 2.178387e-10,4.822602e-09\n",
      "Iteration 44685: loss = 2.1783146e-10,4.822377e-09\n",
      "Iteration 44690: loss = 2.1782365e-10,4.8221542e-09\n",
      "Iteration 44695: loss = 2.178147e-10,4.821935e-09\n",
      "Iteration 44700: loss = 2.1780433e-10,4.82172e-09\n",
      "Iteration 44705: loss = 2.1779319e-10,4.8215067e-09\n",
      "Iteration 44710: loss = 2.177813e-10,4.8212963e-09\n",
      "Iteration 44715: loss = 2.177686e-10,4.8210875e-09\n",
      "Iteration 44720: loss = 2.1775526e-10,4.82088e-09\n",
      "Iteration 44725: loss = 2.1774128e-10,4.8206745e-09\n",
      "Iteration 44730: loss = 2.1772734e-10,4.8204694e-09\n",
      "Iteration 44735: loss = 2.1771222e-10,4.820267e-09\n",
      "Iteration 44740: loss = 2.1769693e-10,4.8200657e-09\n",
      "Iteration 44745: loss = 2.1768161e-10,4.8198636e-09\n",
      "Iteration 44750: loss = 2.1766554e-10,4.8196642e-09\n",
      "Iteration 44755: loss = 2.1764894e-10,4.8194666e-09\n",
      "Iteration 44760: loss = 2.1763225e-10,4.8192685e-09\n",
      "Iteration 44765: loss = 2.1761531e-10,4.8190714e-09\n",
      "Iteration 44770: loss = 2.1759801e-10,4.818875e-09\n",
      "Iteration 44775: loss = 2.1758016e-10,4.81868e-09\n",
      "Iteration 44780: loss = 2.1756236e-10,4.818485e-09\n",
      "Iteration 44785: loss = 2.1754427e-10,4.8182907e-09\n",
      "Iteration 44790: loss = 2.1752637e-10,4.8180957e-09\n",
      "Iteration 44795: loss = 2.1750823e-10,4.8179016e-09\n",
      "Iteration 44800: loss = 2.1748907e-10,4.81771e-09\n",
      "Iteration 44805: loss = 2.1747015e-10,4.817519e-09\n",
      "Iteration 44810: loss = 2.1745057e-10,4.817328e-09\n",
      "Iteration 44815: loss = 2.1581333e-10,4.817119e-09\n",
      "Iteration 44820: loss = 2.1745157e-10,4.8168354e-09\n",
      "Iteration 44825: loss = 2.174519e-10,4.816588e-09\n",
      "Iteration 44830: loss = 2.1582448e-10,4.816351e-09\n",
      "Iteration 44835: loss = 2.1745883e-10,4.8160786e-09\n",
      "Iteration 44840: loss = 2.1582847e-10,4.8158477e-09\n",
      "Iteration 44845: loss = 2.1746194e-10,4.815579e-09\n",
      "Iteration 44850: loss = 2.1583453e-10,4.8153406e-09\n",
      "Iteration 44855: loss = 2.1746799e-10,4.815071e-09\n",
      "Iteration 44860: loss = 2.1583912e-10,4.8148365e-09\n",
      "Iteration 44865: loss = 2.1747097e-10,4.8145705e-09\n",
      "Iteration 44870: loss = 2.1584677e-10,4.8143236e-09\n",
      "Iteration 44875: loss = 2.1584767e-10,4.8140767e-09\n",
      "Iteration 44880: loss = 2.1584985e-10,4.8138227e-09\n",
      "Iteration 44885: loss = 2.1748085e-10,4.8135598e-09\n",
      "Iteration 44890: loss = 2.1585563e-10,4.8133155e-09\n",
      "Iteration 44895: loss = 2.1585993e-10,4.8130575e-09\n",
      "Iteration 44900: loss = 2.1586198e-10,4.8128057e-09\n",
      "Iteration 44905: loss = 2.1749141e-10,4.8125464e-09\n",
      "Iteration 44910: loss = 2.158674e-10,4.8122977e-09\n",
      "Iteration 44915: loss = 2.1587014e-10,4.812044e-09\n",
      "Iteration 44920: loss = 2.1587247e-10,4.8117923e-09\n",
      "Iteration 44925: loss = 2.158748e-10,4.81154e-09\n",
      "Iteration 44930: loss = 2.158778e-10,4.8112856e-09\n",
      "Iteration 44935: loss = 2.1588285e-10,4.8110245e-09\n",
      "Iteration 44940: loss = 2.1588266e-10,4.810778e-09\n",
      "Iteration 44945: loss = 2.1588831e-10,4.8105173e-09\n",
      "Iteration 44950: loss = 2.1588736e-10,4.810274e-09\n",
      "Iteration 44955: loss = 2.1589065e-10,4.810018e-09\n",
      "Iteration 44960: loss = 2.1589441e-10,4.8097593e-09\n",
      "Iteration 44965: loss = 2.1589246e-10,4.809519e-09\n",
      "Iteration 44970: loss = 2.1589823e-10,4.809256e-09\n",
      "Iteration 44975: loss = 2.1753033e-10,4.8089888e-09\n",
      "Iteration 44980: loss = 2.1589915e-10,4.8087614e-09\n",
      "Iteration 44985: loss = 2.1590037e-10,4.808511e-09\n",
      "Iteration 44990: loss = 2.1590589e-10,4.8082494e-09\n",
      "Iteration 44995: loss = 2.1591225e-10,4.8079842e-09\n",
      "Iteration 45000: loss = 2.1591905e-10,4.8077187e-09\n",
      "Iteration 45005: loss = 2.1591924e-10,4.807471e-09\n",
      "Iteration 45010: loss = 2.159175e-10,4.8072284e-09\n",
      "Iteration 45015: loss = 2.1591723e-10,4.8069833e-09\n",
      "Iteration 45020: loss = 2.1591716e-10,4.806737e-09\n",
      "Iteration 45025: loss = 2.1591696e-10,4.8064908e-09\n",
      "Iteration 45030: loss = 2.1591613e-10,4.806245e-09\n",
      "Iteration 45035: loss = 2.1591469e-10,4.806003e-09\n",
      "Iteration 45040: loss = 2.1591161e-10,4.8057642e-09\n",
      "Iteration 45045: loss = 2.1590911e-10,4.8055235e-09\n",
      "Iteration 45050: loss = 2.1591533e-10,4.805261e-09\n",
      "Iteration 45055: loss = 2.1591713e-10,4.805008e-09\n",
      "Iteration 45060: loss = 2.159201e-10,4.804754e-09\n",
      "Iteration 45065: loss = 2.1592256e-10,4.804499e-09\n",
      "Iteration 45070: loss = 2.1592618e-10,4.8042406e-09\n",
      "Iteration 45075: loss = 2.1592839e-10,4.8039865e-09\n",
      "Iteration 45080: loss = 2.1593131e-10,4.803732e-09\n",
      "Iteration 45085: loss = 2.1593365e-10,4.8034794e-09\n",
      "Iteration 45090: loss = 2.1593721e-10,4.8032205e-09\n",
      "Iteration 45095: loss = 2.1593911e-10,4.8029682e-09\n",
      "Iteration 45100: loss = 2.1594126e-10,4.8027156e-09\n",
      "Iteration 45105: loss = 2.1594349e-10,4.802461e-09\n",
      "Iteration 45110: loss = 2.1594547e-10,4.802207e-09\n",
      "Iteration 45115: loss = 2.1594658e-10,4.8019566e-09\n",
      "Iteration 45120: loss = 2.1594655e-10,4.801709e-09\n",
      "Iteration 45125: loss = 2.1594489e-10,4.8014663e-09\n",
      "Iteration 45130: loss = 2.159422e-10,4.8012274e-09\n",
      "Iteration 45135: loss = 2.1593825e-10,4.8009907e-09\n",
      "Iteration 45140: loss = 2.1593276e-10,4.800757e-09\n",
      "Iteration 45145: loss = 2.1592693e-10,4.8005266e-09\n",
      "Iteration 45150: loss = 2.1591973e-10,4.800298e-09\n",
      "Iteration 45155: loss = 2.1591103e-10,4.8000754e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45160: loss = 2.1590107e-10,4.7998547e-09\n",
      "Iteration 45165: loss = 2.158903e-10,4.7996376e-09\n",
      "Iteration 45170: loss = 2.1587938e-10,4.7994195e-09\n",
      "Iteration 45175: loss = 2.1586703e-10,4.7992064e-09\n",
      "Iteration 45180: loss = 2.158546e-10,4.798995e-09\n",
      "Iteration 45185: loss = 2.1584128e-10,4.7987827e-09\n",
      "Iteration 45190: loss = 2.1582737e-10,4.7985735e-09\n",
      "Iteration 45195: loss = 2.1581277e-10,4.7983657e-09\n",
      "Iteration 45200: loss = 2.1579827e-10,4.7981583e-09\n",
      "Iteration 45205: loss = 2.1578271e-10,4.797953e-09\n",
      "Iteration 45210: loss = 2.157672e-10,4.7977484e-09\n",
      "Iteration 45215: loss = 2.1575121e-10,4.7975446e-09\n",
      "Iteration 45220: loss = 2.1573476e-10,4.797343e-09\n",
      "Iteration 45225: loss = 2.1571826e-10,4.7971422e-09\n",
      "Iteration 45230: loss = 2.1570123e-10,4.7969406e-09\n",
      "Iteration 45235: loss = 2.1568408e-10,4.7967403e-09\n",
      "Iteration 45240: loss = 2.1566611e-10,4.796541e-09\n",
      "Iteration 45245: loss = 2.1564815e-10,4.7963438e-09\n",
      "Iteration 45250: loss = 2.1562997e-10,4.796146e-09\n",
      "Iteration 45255: loss = 2.156114e-10,4.795948e-09\n",
      "Iteration 45260: loss = 2.1559317e-10,4.795752e-09\n",
      "Iteration 45265: loss = 2.1557493e-10,4.7955546e-09\n",
      "Iteration 45270: loss = 2.1555595e-10,4.7953583e-09\n",
      "Iteration 45275: loss = 2.1553688e-10,4.7951634e-09\n",
      "Iteration 45280: loss = 2.1551777e-10,4.794969e-09\n",
      "Iteration 45285: loss = 2.1549838e-10,4.7947744e-09\n",
      "Iteration 45290: loss = 2.1547907e-10,4.79458e-09\n",
      "Iteration 45295: loss = 2.1546702e-10,4.794366e-09\n",
      "Iteration 45300: loss = 2.154625e-10,4.794128e-09\n",
      "Iteration 45305: loss = 2.1546281e-10,4.793879e-09\n",
      "Iteration 45310: loss = 2.1546492e-10,4.7936233e-09\n",
      "Iteration 45315: loss = 2.1546746e-10,4.7933666e-09\n",
      "Iteration 45320: loss = 2.154702e-10,4.7931104e-09\n",
      "Iteration 45325: loss = 2.154716e-10,4.7928577e-09\n",
      "Iteration 45330: loss = 2.1547207e-10,4.792607e-09\n",
      "Iteration 45335: loss = 2.1547153e-10,4.79236e-09\n",
      "Iteration 45340: loss = 2.154686e-10,4.7921187e-09\n",
      "Iteration 45345: loss = 2.1546519e-10,4.7918793e-09\n",
      "Iteration 45350: loss = 2.1545978e-10,4.7916435e-09\n",
      "Iteration 45355: loss = 2.1545381e-10,4.7914117e-09\n",
      "Iteration 45360: loss = 2.1544687e-10,4.7911817e-09\n",
      "Iteration 45365: loss = 2.1543906e-10,4.7909547e-09\n",
      "Iteration 45370: loss = 2.1542979e-10,4.7907305e-09\n",
      "Iteration 45375: loss = 2.154192e-10,4.7905107e-09\n",
      "Iteration 45380: loss = 2.1540791e-10,4.7902935e-09\n",
      "Iteration 45385: loss = 2.1539598e-10,4.7900772e-09\n",
      "Iteration 45390: loss = 2.1538343e-10,4.7898636e-09\n",
      "Iteration 45395: loss = 2.153706e-10,4.7896505e-09\n",
      "Iteration 45400: loss = 2.1535657e-10,4.7894386e-09\n",
      "Iteration 45405: loss = 2.1534226e-10,4.7892295e-09\n",
      "Iteration 45410: loss = 2.1532788e-10,4.7890203e-09\n",
      "Iteration 45415: loss = 2.1531239e-10,4.7888133e-09\n",
      "Iteration 45420: loss = 2.1529717e-10,4.788607e-09\n",
      "Iteration 45425: loss = 2.1528153e-10,4.788401e-09\n",
      "Iteration 45430: loss = 2.1526513e-10,4.7881974e-09\n",
      "Iteration 45435: loss = 2.152485e-10,4.787995e-09\n",
      "Iteration 45440: loss = 2.1523101e-10,4.787792e-09\n",
      "Iteration 45445: loss = 2.1521457e-10,4.7875894e-09\n",
      "Iteration 45450: loss = 2.1519696e-10,4.7873883e-09\n",
      "Iteration 45455: loss = 2.1517894e-10,4.7871893e-09\n",
      "Iteration 45460: loss = 2.1516076e-10,4.7869904e-09\n",
      "Iteration 45465: loss = 2.1514286e-10,4.786791e-09\n",
      "Iteration 45470: loss = 2.1512443e-10,4.786591e-09\n",
      "Iteration 45475: loss = 2.1510642e-10,4.7863917e-09\n",
      "Iteration 45480: loss = 2.1508734e-10,4.7861946e-09\n",
      "Iteration 45485: loss = 2.1506823e-10,4.7859987e-09\n",
      "Iteration 45490: loss = 2.1504902e-10,4.785803e-09\n",
      "Iteration 45495: loss = 2.1502995e-10,4.7856066e-09\n",
      "Iteration 45500: loss = 2.1501369e-10,4.7854014e-09\n",
      "Iteration 45505: loss = 2.1500689e-10,4.78517e-09\n",
      "Iteration 45510: loss = 2.1500585e-10,4.7849213e-09\n",
      "Iteration 45515: loss = 2.1500791e-10,4.784666e-09\n",
      "Iteration 45520: loss = 2.1501012e-10,4.7844084e-09\n",
      "Iteration 45525: loss = 2.1501245e-10,4.784151e-09\n",
      "Iteration 45530: loss = 2.1501484e-10,4.7838946e-09\n",
      "Iteration 45535: loss = 2.1501512e-10,4.783641e-09\n",
      "Iteration 45540: loss = 2.1501541e-10,4.783391e-09\n",
      "Iteration 45545: loss = 2.1501385e-10,4.783144e-09\n",
      "Iteration 45550: loss = 2.1501101e-10,4.782902e-09\n",
      "Iteration 45555: loss = 2.1500618e-10,4.782664e-09\n",
      "Iteration 45560: loss = 2.1500074e-10,4.7824282e-09\n",
      "Iteration 45565: loss = 2.1499415e-10,4.7821955e-09\n",
      "Iteration 45570: loss = 2.1498649e-10,4.781966e-09\n",
      "Iteration 45575: loss = 2.1497797e-10,4.7817386e-09\n",
      "Iteration 45580: loss = 2.1496856e-10,4.7815143e-09\n",
      "Iteration 45585: loss = 2.1495739e-10,4.7812945e-09\n",
      "Iteration 45590: loss = 2.1494588e-10,4.781076e-09\n",
      "Iteration 45595: loss = 2.1493336e-10,4.78086e-09\n",
      "Iteration 45600: loss = 2.149207e-10,4.7806448e-09\n",
      "Iteration 45605: loss = 2.1490715e-10,4.7804307e-09\n",
      "Iteration 45610: loss = 2.1490894e-10,4.7801745e-09\n",
      "Iteration 45615: loss = 2.1492254e-10,4.7798845e-09\n",
      "Iteration 45620: loss = 2.1492258e-10,4.7796322e-09\n",
      "Iteration 45625: loss = 2.1491482e-10,4.779404e-09\n",
      "Iteration 45630: loss = 2.1492412e-10,4.7791247e-09\n",
      "Iteration 45635: loss = 2.1493195e-10,4.7788515e-09\n",
      "Iteration 45640: loss = 2.1492684e-10,4.7786153e-09\n",
      "Iteration 45645: loss = 2.1492896e-10,4.7783577e-09\n",
      "Iteration 45650: loss = 2.1493785e-10,4.77808e-09\n",
      "Iteration 45655: loss = 2.1331563e-10,4.7778475e-09\n",
      "Iteration 45660: loss = 2.1493556e-10,4.7775823e-09\n",
      "Iteration 45665: loss = 2.1332126e-10,4.777328e-09\n",
      "Iteration 45670: loss = 2.1494206e-10,4.7770605e-09\n",
      "Iteration 45675: loss = 2.1494555e-10,4.776799e-09\n",
      "Iteration 45680: loss = 2.1494691e-10,4.7765427e-09\n",
      "Iteration 45685: loss = 2.1494939e-10,4.7762834e-09\n",
      "Iteration 45690: loss = 2.1495271e-10,4.7760222e-09\n",
      "Iteration 45695: loss = 2.1495634e-10,4.77576e-09\n",
      "Iteration 45700: loss = 2.1334008e-10,4.7755098e-09\n",
      "Iteration 45705: loss = 2.1496521e-10,4.77523e-09\n",
      "Iteration 45710: loss = 2.1334523e-10,4.774992e-09\n",
      "Iteration 45715: loss = 2.1496782e-10,4.774719e-09\n",
      "Iteration 45720: loss = 2.1496899e-10,4.7744617e-09\n",
      "Iteration 45725: loss = 2.1335528e-10,4.7742055e-09\n",
      "Iteration 45730: loss = 2.1497498e-10,4.7739404e-09\n",
      "Iteration 45735: loss = 2.1497865e-10,4.7736792e-09\n",
      "Iteration 45740: loss = 2.1498069e-10,4.7734208e-09\n",
      "Iteration 45745: loss = 2.1498625e-10,4.773152e-09\n",
      "Iteration 45750: loss = 2.1337009e-10,4.772903e-09\n",
      "Iteration 45755: loss = 2.1337378e-10,4.7726396e-09\n",
      "Iteration 45760: loss = 2.1499376e-10,4.772373e-09\n",
      "Iteration 45765: loss = 2.1337764e-10,4.7721223e-09\n",
      "Iteration 45770: loss = 2.1500175e-10,4.771845e-09\n",
      "Iteration 45775: loss = 2.1338135e-10,4.771608e-09\n",
      "Iteration 45780: loss = 2.1500533e-10,4.771329e-09\n",
      "Iteration 45785: loss = 2.1500857e-10,4.771068e-09\n",
      "Iteration 45790: loss = 2.133942e-10,4.7708117e-09\n",
      "Iteration 45795: loss = 2.1339398e-10,4.770561e-09\n",
      "Iteration 45800: loss = 2.150209e-10,4.7702744e-09\n",
      "Iteration 45805: loss = 2.134005e-10,4.770035e-09\n",
      "Iteration 45810: loss = 2.1502455e-10,4.769757e-09\n",
      "Iteration 45815: loss = 2.1502698e-10,4.769497e-09\n",
      "Iteration 45820: loss = 2.1340878e-10,4.7692534e-09\n",
      "Iteration 45825: loss = 2.1503137e-10,4.76898e-09\n",
      "Iteration 45830: loss = 2.1341699e-10,4.7687245e-09\n",
      "Iteration 45835: loss = 2.1342178e-10,4.768456e-09\n",
      "Iteration 45840: loss = 2.1342317e-10,4.7681983e-09\n",
      "Iteration 45845: loss = 2.1342578e-10,4.767939e-09\n",
      "Iteration 45850: loss = 2.1342894e-10,4.767676e-09\n",
      "Iteration 45855: loss = 2.1505064e-10,4.767405e-09\n",
      "Iteration 45860: loss = 2.1343603e-10,4.7671493e-09\n",
      "Iteration 45865: loss = 2.1343523e-10,4.766898e-09\n",
      "Iteration 45870: loss = 2.1505897e-10,4.7666204e-09\n",
      "Iteration 45875: loss = 2.134446e-10,4.766365e-09\n",
      "Iteration 45880: loss = 2.1344676e-10,4.7661053e-09\n",
      "Iteration 45885: loss = 2.1345192e-10,4.7658366e-09\n",
      "Iteration 45890: loss = 2.134532e-10,4.765579e-09\n",
      "Iteration 45895: loss = 2.1345604e-10,4.765318e-09\n",
      "Iteration 45900: loss = 2.134558e-10,4.765065e-09\n",
      "Iteration 45905: loss = 2.1508105e-10,4.7647823e-09\n",
      "Iteration 45910: loss = 2.1346336e-10,4.7645363e-09\n",
      "Iteration 45915: loss = 2.134703e-10,4.764264e-09\n",
      "Iteration 45920: loss = 2.134707e-10,4.764008e-09\n",
      "Iteration 45925: loss = 2.134747e-10,4.763742e-09\n",
      "Iteration 45930: loss = 2.1347908e-10,4.763476e-09\n",
      "Iteration 45935: loss = 2.1347879e-10,4.7632236e-09\n",
      "Iteration 45940: loss = 2.1348277e-10,4.762958e-09\n",
      "Iteration 45945: loss = 2.134869e-10,4.7626925e-09\n",
      "Iteration 45950: loss = 2.1348968e-10,4.7624305e-09\n",
      "Iteration 45955: loss = 2.1349411e-10,4.762164e-09\n",
      "Iteration 45960: loss = 2.13494e-10,4.76191e-09\n",
      "Iteration 45965: loss = 2.1349796e-10,4.761643e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45970: loss = 2.1350437e-10,4.761372e-09\n",
      "Iteration 45975: loss = 2.1350181e-10,4.7611257e-09\n",
      "Iteration 45980: loss = 2.135007e-10,4.760874e-09\n",
      "Iteration 45985: loss = 2.1350248e-10,4.7606123e-09\n",
      "Iteration 45990: loss = 2.135056e-10,4.7603503e-09\n",
      "Iteration 45995: loss = 2.135081e-10,4.760089e-09\n",
      "Iteration 46000: loss = 2.1351e-10,4.75983e-09\n",
      "Iteration 46005: loss = 2.1351065e-10,4.7595727e-09\n",
      "Iteration 46010: loss = 2.1350921e-10,4.7593227e-09\n",
      "Iteration 46015: loss = 2.1350686e-10,4.759074e-09\n",
      "Iteration 46020: loss = 2.1350353e-10,4.758828e-09\n",
      "Iteration 46025: loss = 2.134991e-10,4.758586e-09\n",
      "Iteration 46030: loss = 2.1349321e-10,4.758347e-09\n",
      "Iteration 46035: loss = 2.1348646e-10,4.7581126e-09\n",
      "Iteration 46040: loss = 2.1347822e-10,4.7578808e-09\n",
      "Iteration 46045: loss = 2.1346876e-10,4.757652e-09\n",
      "Iteration 46050: loss = 2.1345888e-10,4.757425e-09\n",
      "Iteration 46055: loss = 2.1344766e-10,4.7572013e-09\n",
      "Iteration 46060: loss = 2.1343616e-10,4.756977e-09\n",
      "Iteration 46065: loss = 2.1342368e-10,4.7567568e-09\n",
      "Iteration 46070: loss = 2.1341107e-10,4.7565383e-09\n",
      "Iteration 46075: loss = 2.1339741e-10,4.7563207e-09\n",
      "Iteration 46080: loss = 2.1338381e-10,4.756105e-09\n",
      "Iteration 46085: loss = 2.1336947e-10,4.7558886e-09\n",
      "Iteration 46090: loss = 2.1335446e-10,4.755676e-09\n",
      "Iteration 46095: loss = 2.1333896e-10,4.755464e-09\n",
      "Iteration 46100: loss = 2.1332346e-10,4.7552513e-09\n",
      "Iteration 46105: loss = 2.1330736e-10,4.7550417e-09\n",
      "Iteration 46110: loss = 2.1329082e-10,4.7548325e-09\n",
      "Iteration 46115: loss = 2.1327384e-10,4.7546242e-09\n",
      "Iteration 46120: loss = 2.1325643e-10,4.754418e-09\n",
      "Iteration 46125: loss = 2.1323854e-10,4.7542126e-09\n",
      "Iteration 46130: loss = 2.1322004e-10,4.75401e-09\n",
      "Iteration 46135: loss = 2.1320307e-10,4.753801e-09\n",
      "Iteration 46140: loss = 2.131964e-10,4.7535655e-09\n",
      "Iteration 46145: loss = 2.131953e-10,4.7533115e-09\n",
      "Iteration 46150: loss = 2.1319853e-10,4.7530477e-09\n",
      "Iteration 46155: loss = 2.1320247e-10,4.7527795e-09\n",
      "Iteration 46160: loss = 2.1320597e-10,4.752514e-09\n",
      "Iteration 46165: loss = 2.1320963e-10,4.7522484e-09\n",
      "Iteration 46170: loss = 2.1321235e-10,4.7519846e-09\n",
      "Iteration 46175: loss = 2.1321252e-10,4.751728e-09\n",
      "Iteration 46180: loss = 2.132119e-10,4.7514743e-09\n",
      "Iteration 46185: loss = 2.1320994e-10,4.7512234e-09\n",
      "Iteration 46190: loss = 2.132064e-10,4.7509774e-09\n",
      "Iteration 46195: loss = 2.1320225e-10,4.750732e-09\n",
      "Iteration 46200: loss = 2.1319607e-10,4.7504933e-09\n",
      "Iteration 46205: loss = 2.1318909e-10,4.750258e-09\n",
      "Iteration 46210: loss = 2.131813e-10,4.750024e-09\n",
      "Iteration 46215: loss = 2.1317127e-10,4.7497943e-09\n",
      "Iteration 46220: loss = 2.1316164e-10,4.749566e-09\n",
      "Iteration 46225: loss = 2.1315093e-10,4.7493387e-09\n",
      "Iteration 46230: loss = 2.1313969e-10,4.749114e-09\n",
      "Iteration 46235: loss = 2.1312714e-10,4.7488937e-09\n",
      "Iteration 46240: loss = 2.1311414e-10,4.748673e-09\n",
      "Iteration 46245: loss = 2.1310087e-10,4.748454e-09\n",
      "Iteration 46250: loss = 2.1308737e-10,4.7482356e-09\n",
      "Iteration 46255: loss = 2.1307349e-10,4.7480175e-09\n",
      "Iteration 46260: loss = 2.1305828e-10,4.747803e-09\n",
      "Iteration 46265: loss = 2.130419e-10,4.7475934e-09\n",
      "Iteration 46270: loss = 2.1302525e-10,4.7473834e-09\n",
      "Iteration 46275: loss = 2.1300829e-10,4.747174e-09\n",
      "Iteration 46280: loss = 2.1299136e-10,4.7469646e-09\n",
      "Iteration 46285: loss = 2.1297454e-10,4.746756e-09\n",
      "Iteration 46290: loss = 2.1295699e-10,4.7465476e-09\n",
      "Iteration 46295: loss = 2.129399e-10,4.7463393e-09\n",
      "Iteration 46300: loss = 2.1292239e-10,4.7461315e-09\n",
      "Iteration 46305: loss = 2.1290396e-10,4.7459268e-09\n",
      "Iteration 46310: loss = 2.128847e-10,4.7457234e-09\n",
      "Iteration 46315: loss = 2.1286584e-10,4.7455195e-09\n",
      "Iteration 46320: loss = 2.1284668e-10,4.745317e-09\n",
      "Iteration 46325: loss = 2.1282738e-10,4.7451145e-09\n",
      "Iteration 46330: loss = 2.1280809e-10,4.744911e-09\n",
      "Iteration 46335: loss = 2.1278879e-10,4.7447086e-09\n",
      "Iteration 46340: loss = 2.1276965e-10,4.7445043e-09\n",
      "Iteration 46345: loss = 2.1275241e-10,4.7442947e-09\n",
      "Iteration 46350: loss = 2.127445e-10,4.7440603e-09\n",
      "Iteration 46355: loss = 2.1274306e-10,4.743807e-09\n",
      "Iteration 46360: loss = 2.1274499e-10,4.7435442e-09\n",
      "Iteration 46365: loss = 2.1274854e-10,4.743277e-09\n",
      "Iteration 46370: loss = 2.1275225e-10,4.743008e-09\n",
      "Iteration 46375: loss = 2.1275465e-10,4.742744e-09\n",
      "Iteration 46380: loss = 2.1275619e-10,4.742481e-09\n",
      "Iteration 46385: loss = 2.1275681e-10,4.7422217e-09\n",
      "Iteration 46390: loss = 2.1275542e-10,4.741968e-09\n",
      "Iteration 46395: loss = 2.1275283e-10,4.7417172e-09\n",
      "Iteration 46400: loss = 2.1274925e-10,4.7414703e-09\n",
      "Iteration 46405: loss = 2.1274406e-10,4.7412274e-09\n",
      "Iteration 46410: loss = 2.1273842e-10,4.7409845e-09\n",
      "Iteration 46415: loss = 2.1273155e-10,4.7407474e-09\n",
      "Iteration 46420: loss = 2.1272234e-10,4.740515e-09\n",
      "Iteration 46425: loss = 2.1271256e-10,4.7402837e-09\n",
      "Iteration 46430: loss = 2.1270274e-10,4.7400532e-09\n",
      "Iteration 46435: loss = 2.126912e-10,4.7398276e-09\n",
      "Iteration 46440: loss = 2.126792e-10,4.739604e-09\n",
      "Iteration 46445: loss = 2.1266659e-10,4.73938e-09\n",
      "Iteration 46450: loss = 2.1265421e-10,4.739158e-09\n",
      "Iteration 46455: loss = 2.12641e-10,4.7389364e-09\n",
      "Iteration 46460: loss = 2.126264e-10,4.7387188e-09\n",
      "Iteration 46465: loss = 2.1261158e-10,4.738502e-09\n",
      "Iteration 46470: loss = 2.1259655e-10,4.738286e-09\n",
      "Iteration 46475: loss = 2.1258138e-10,4.7380704e-09\n",
      "Iteration 46480: loss = 2.1256565e-10,4.737855e-09\n",
      "Iteration 46485: loss = 2.1255014e-10,4.7376405e-09\n",
      "Iteration 46490: loss = 2.1253414e-10,4.737428e-09\n",
      "Iteration 46495: loss = 2.1251657e-10,4.737217e-09\n",
      "Iteration 46500: loss = 2.1249935e-10,4.737008e-09\n",
      "Iteration 46505: loss = 2.1248185e-10,4.7367985e-09\n",
      "Iteration 46510: loss = 2.108605e-10,4.7365836e-09\n",
      "Iteration 46515: loss = 2.1247915e-10,4.7362874e-09\n",
      "Iteration 46520: loss = 2.124833e-10,4.736018e-09\n",
      "Iteration 46525: loss = 2.108702e-10,4.735779e-09\n",
      "Iteration 46530: loss = 2.1248607e-10,4.735493e-09\n",
      "Iteration 46535: loss = 2.1248575e-10,4.735234e-09\n",
      "Iteration 46540: loss = 2.124924e-10,4.7349555e-09\n",
      "Iteration 46545: loss = 2.124943e-10,4.7346913e-09\n",
      "Iteration 46550: loss = 2.1249376e-10,4.7344333e-09\n",
      "Iteration 46555: loss = 2.1249891e-10,4.7341593e-09\n",
      "Iteration 46560: loss = 2.1089315e-10,4.7339004e-09\n",
      "Iteration 46565: loss = 2.1089519e-10,4.7336357e-09\n",
      "Iteration 46570: loss = 2.1251001e-10,4.7333515e-09\n",
      "Iteration 46575: loss = 2.1090074e-10,4.7331e-09\n",
      "Iteration 46580: loss = 2.1250528e-10,4.7328466e-09\n",
      "Iteration 46585: loss = 2.1088083e-10,4.7326383e-09\n",
      "Iteration 46590: loss = 2.1087038e-10,4.732409e-09\n",
      "Iteration 46595: loss = 2.1246598e-10,4.7321778e-09\n",
      "Iteration 46600: loss = 2.1245529e-10,4.7319486e-09\n",
      "Iteration 46605: loss = 2.1244652e-10,4.7317137e-09\n",
      "Iteration 46610: loss = 2.1243375e-10,4.73149e-09\n",
      "Iteration 46615: loss = 2.1242676e-10,4.73125e-09\n",
      "Iteration 46620: loss = 2.108051e-10,4.7310347e-09\n",
      "Iteration 46625: loss = 2.107955e-10,4.730803e-09\n",
      "Iteration 46630: loss = 2.123931e-10,4.7305653e-09\n",
      "Iteration 46635: loss = 2.1238089e-10,4.7303397e-09\n",
      "Iteration 46640: loss = 2.1236969e-10,4.730112e-09\n",
      "Iteration 46645: loss = 2.1236087e-10,4.729878e-09\n",
      "Iteration 46650: loss = 2.1234829e-10,4.729652e-09\n",
      "Iteration 46655: loss = 2.123393e-10,4.7294173e-09\n",
      "Iteration 46660: loss = 2.12328e-10,4.729189e-09\n",
      "Iteration 46665: loss = 2.1231868e-10,4.728955e-09\n",
      "Iteration 46670: loss = 2.1069925e-10,4.728733e-09\n",
      "Iteration 46675: loss = 2.1230344e-10,4.728478e-09\n",
      "Iteration 46680: loss = 2.1229123e-10,4.728251e-09\n",
      "Iteration 46685: loss = 2.1066714e-10,4.728044e-09\n",
      "Iteration 46690: loss = 2.1226719e-10,4.7278e-09\n",
      "Iteration 46695: loss = 2.1225473e-10,4.7275734e-09\n",
      "Iteration 46700: loss = 2.1224217e-10,4.727349e-09\n",
      "Iteration 46705: loss = 2.1223445e-10,4.7271103e-09\n",
      "Iteration 46710: loss = 2.106121e-10,4.7268967e-09\n",
      "Iteration 46715: loss = 2.1221287e-10,4.7266506e-09\n",
      "Iteration 46720: loss = 2.1059271e-10,4.72643e-09\n",
      "Iteration 46725: loss = 2.1218827e-10,4.726197e-09\n",
      "Iteration 46730: loss = 2.1217837e-10,4.7259663e-09\n",
      "Iteration 46735: loss = 2.1055972e-10,4.7257407e-09\n",
      "Iteration 46740: loss = 2.1216164e-10,4.725492e-09\n",
      "Iteration 46745: loss = 2.1214569e-10,4.725275e-09\n",
      "Iteration 46750: loss = 2.1213674e-10,4.7250412e-09\n",
      "Iteration 46755: loss = 2.12125e-10,4.7248125e-09\n",
      "Iteration 46760: loss = 2.1211444e-10,4.7245816e-09\n",
      "Iteration 46765: loss = 2.1210354e-10,4.7243507e-09\n",
      "Iteration 46770: loss = 2.120968e-10,4.724108e-09\n",
      "Iteration 46775: loss = 2.104762e-10,4.72389e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46780: loss = 2.1207358e-10,4.723652e-09\n",
      "Iteration 46785: loss = 2.1206421e-10,4.723417e-09\n",
      "Iteration 46790: loss = 2.1044437e-10,4.723196e-09\n",
      "Iteration 46795: loss = 2.1204134e-10,4.72296e-09\n",
      "Iteration 46800: loss = 2.120309e-10,4.722728e-09\n",
      "Iteration 46805: loss = 2.1201961e-10,4.722498e-09\n",
      "Iteration 46810: loss = 2.1040193e-10,4.722271e-09\n",
      "Iteration 46815: loss = 2.1200136e-10,4.722028e-09\n",
      "Iteration 46820: loss = 2.1038109e-10,4.7218065e-09\n",
      "Iteration 46825: loss = 2.119811e-10,4.721561e-09\n",
      "Iteration 46830: loss = 2.1035908e-10,4.721345e-09\n",
      "Iteration 46835: loss = 2.1195735e-10,4.7211044e-09\n",
      "Iteration 46840: loss = 2.1194764e-10,4.72087e-09\n",
      "Iteration 46845: loss = 2.1193725e-10,4.720639e-09\n",
      "Iteration 46850: loss = 2.1031893e-10,4.720413e-09\n",
      "Iteration 46855: loss = 2.1191705e-10,4.7201714e-09\n",
      "Iteration 46860: loss = 2.1190621e-10,4.7199413e-09\n",
      "Iteration 46865: loss = 2.1189667e-10,4.7197064e-09\n",
      "Iteration 46870: loss = 2.1188527e-10,4.7194773e-09\n",
      "Iteration 46875: loss = 2.102673e-10,4.719249e-09\n",
      "Iteration 46880: loss = 2.1186464e-10,4.7190105e-09\n",
      "Iteration 46885: loss = 2.1185319e-10,4.7187814e-09\n",
      "Iteration 46890: loss = 2.1184306e-10,4.718548e-09\n",
      "Iteration 46895: loss = 2.1183198e-10,4.7183173e-09\n",
      "Iteration 46900: loss = 2.1182169e-10,4.718084e-09\n",
      "Iteration 46905: loss = 2.1181312e-10,4.717846e-09\n",
      "Iteration 46910: loss = 2.1180141e-10,4.717617e-09\n",
      "Iteration 46915: loss = 2.1179165e-10,4.717383e-09\n",
      "Iteration 46920: loss = 2.1178011e-10,4.717153e-09\n",
      "Iteration 46925: loss = 2.117712e-10,4.716915e-09\n",
      "Iteration 46930: loss = 2.1015194e-10,4.71669e-09\n",
      "Iteration 46935: loss = 2.1175295e-10,4.716442e-09\n",
      "Iteration 46940: loss = 2.101323e-10,4.7162225e-09\n",
      "Iteration 46945: loss = 2.1173159e-10,4.7159774e-09\n",
      "Iteration 46950: loss = 2.1011119e-10,4.7157562e-09\n",
      "Iteration 46955: loss = 2.117101e-10,4.7155124e-09\n",
      "Iteration 46960: loss = 2.1169722e-10,4.715287e-09\n",
      "Iteration 46965: loss = 2.1008022e-10,4.715055e-09\n",
      "Iteration 46970: loss = 2.1167894e-10,4.7148117e-09\n",
      "Iteration 46975: loss = 2.1005882e-10,4.714591e-09\n",
      "Iteration 46980: loss = 2.1165601e-10,4.7143525e-09\n",
      "Iteration 46985: loss = 2.1164796e-10,4.7141113e-09\n",
      "Iteration 46990: loss = 2.1002812e-10,4.713889e-09\n",
      "Iteration 46995: loss = 2.1162472e-10,4.7136517e-09\n",
      "Iteration 47000: loss = 2.1162017e-10,4.713402e-09\n",
      "Iteration 47005: loss = 2.1160841e-10,4.7131716e-09\n",
      "Iteration 47010: loss = 2.115943e-10,4.7129465e-09\n",
      "Iteration 47015: loss = 2.1158596e-10,4.7127076e-09\n",
      "Iteration 47020: loss = 2.1157327e-10,4.712481e-09\n",
      "Iteration 47025: loss = 2.0995618e-10,4.71225e-09\n",
      "Iteration 47030: loss = 2.1155448e-10,4.7120077e-09\n",
      "Iteration 47035: loss = 2.1154484e-10,4.711772e-09\n",
      "Iteration 47040: loss = 2.115339e-10,4.7115396e-09\n",
      "Iteration 47045: loss = 2.11526e-10,4.711298e-09\n",
      "Iteration 47050: loss = 2.0990487e-10,4.7110778e-09\n",
      "Iteration 47055: loss = 2.1150377e-10,4.7108344e-09\n",
      "Iteration 47060: loss = 2.0988443e-10,4.7106092e-09\n",
      "Iteration 47065: loss = 2.114829e-10,4.710366e-09\n",
      "Iteration 47070: loss = 2.0986414e-10,4.7101394e-09\n",
      "Iteration 47075: loss = 2.1146418e-10,4.709891e-09\n",
      "Iteration 47080: loss = 2.1145162e-10,4.709663e-09\n",
      "Iteration 47085: loss = 2.0984064e-10,4.709414e-09\n",
      "Iteration 47090: loss = 2.1145818e-10,4.709117e-09\n",
      "Iteration 47095: loss = 2.0984649e-10,4.7088693e-09\n",
      "Iteration 47100: loss = 2.1146158e-10,4.7085793e-09\n",
      "Iteration 47105: loss = 2.0985462e-10,4.70832e-09\n",
      "Iteration 47110: loss = 2.1146422e-10,4.7080437e-09\n",
      "Iteration 47115: loss = 2.1146827e-10,4.7077684e-09\n",
      "Iteration 47120: loss = 2.0986234e-10,4.7075033e-09\n",
      "Iteration 47125: loss = 2.0986653e-10,4.7072284e-09\n",
      "Iteration 47130: loss = 2.0986836e-10,4.706958e-09\n",
      "Iteration 47135: loss = 2.098718e-10,4.706684e-09\n",
      "Iteration 47140: loss = 2.114819e-10,4.7064077e-09\n",
      "Iteration 47145: loss = 2.1148416e-10,4.706137e-09\n",
      "Iteration 47150: loss = 2.0988083e-10,4.705866e-09\n",
      "Iteration 47155: loss = 2.098831e-10,4.7055932e-09\n",
      "Iteration 47160: loss = 2.0988607e-10,4.7053206e-09\n",
      "Iteration 47165: loss = 2.0988644e-10,4.7050555e-09\n",
      "Iteration 47170: loss = 2.0989127e-10,4.704778e-09\n",
      "Iteration 47175: loss = 2.0989409e-10,4.704505e-09\n",
      "Iteration 47180: loss = 2.0989877e-10,4.7042255e-09\n",
      "Iteration 47185: loss = 2.0989793e-10,4.703965e-09\n",
      "Iteration 47190: loss = 2.1151132e-10,4.7036774e-09\n",
      "Iteration 47195: loss = 2.0990286e-10,4.703421e-09\n",
      "Iteration 47200: loss = 2.0990858e-10,4.703139e-09\n",
      "Iteration 47205: loss = 2.0991009e-10,4.702871e-09\n",
      "Iteration 47210: loss = 2.0991024e-10,4.702605e-09\n",
      "Iteration 47215: loss = 2.0991751e-10,4.702319e-09\n",
      "Iteration 47220: loss = 2.0991815e-10,4.702053e-09\n",
      "Iteration 47225: loss = 2.0991593e-10,4.7017945e-09\n",
      "Iteration 47230: loss = 2.099194e-10,4.701519e-09\n",
      "Iteration 47235: loss = 2.0992656e-10,4.701233e-09\n",
      "Iteration 47240: loss = 2.0993444e-10,4.7009463e-09\n",
      "Iteration 47245: loss = 2.0993417e-10,4.7006816e-09\n",
      "Iteration 47250: loss = 2.0993458e-10,4.7004143e-09\n",
      "Iteration 47255: loss = 2.0993629e-10,4.7001434e-09\n",
      "Iteration 47260: loss = 2.0993797e-10,4.699875e-09\n",
      "Iteration 47265: loss = 2.0993947e-10,4.6996043e-09\n",
      "Iteration 47270: loss = 2.0994029e-10,4.6993356e-09\n",
      "Iteration 47275: loss = 2.0994007e-10,4.699072e-09\n",
      "Iteration 47280: loss = 2.0993902e-10,4.69881e-09\n",
      "Iteration 47285: loss = 2.0993707e-10,4.69855e-09\n",
      "Iteration 47290: loss = 2.0993257e-10,4.6982964e-09\n",
      "Iteration 47295: loss = 2.099268e-10,4.698046e-09\n",
      "Iteration 47300: loss = 2.0992065e-10,4.697798e-09\n",
      "Iteration 47305: loss = 2.0991271e-10,4.697555e-09\n",
      "Iteration 47310: loss = 2.0990425e-10,4.6973128e-09\n",
      "Iteration 47315: loss = 2.0989561e-10,4.697071e-09\n",
      "Iteration 47320: loss = 2.0988526e-10,4.696835e-09\n",
      "Iteration 47325: loss = 2.0987391e-10,4.696599e-09\n",
      "Iteration 47330: loss = 2.098628e-10,4.696366e-09\n",
      "Iteration 47335: loss = 2.0985035e-10,4.696134e-09\n",
      "Iteration 47340: loss = 2.0983691e-10,4.6959063e-09\n",
      "Iteration 47345: loss = 2.0982242e-10,4.695682e-09\n",
      "Iteration 47350: loss = 2.0980773e-10,4.695458e-09\n",
      "Iteration 47355: loss = 2.0979252e-10,4.6952335e-09\n",
      "Iteration 47360: loss = 2.0977764e-10,4.69501e-09\n",
      "Iteration 47365: loss = 2.0976237e-10,4.694787e-09\n",
      "Iteration 47370: loss = 2.0974698e-10,4.6945647e-09\n",
      "Iteration 47375: loss = 2.0972986e-10,4.6943454e-09\n",
      "Iteration 47380: loss = 2.0971273e-10,4.694128e-09\n",
      "Iteration 47385: loss = 2.096953e-10,4.69391e-09\n",
      "Iteration 47390: loss = 2.0967787e-10,4.6936934e-09\n",
      "Iteration 47395: loss = 2.0966016e-10,4.693477e-09\n",
      "Iteration 47400: loss = 2.0964251e-10,4.69326e-09\n",
      "Iteration 47405: loss = 2.0962478e-10,4.6930437e-09\n",
      "Iteration 47410: loss = 2.0960696e-10,4.6928266e-09\n",
      "Iteration 47415: loss = 2.095892e-10,4.6926116e-09\n",
      "Iteration 47420: loss = 2.0957137e-10,4.692395e-09\n",
      "Iteration 47425: loss = 2.0955267e-10,4.692181e-09\n",
      "Iteration 47430: loss = 2.0953322e-10,4.6919686e-09\n",
      "Iteration 47435: loss = 2.0951352e-10,4.6917576e-09\n",
      "Iteration 47440: loss = 2.0949421e-10,4.6915463e-09\n",
      "Iteration 47445: loss = 2.0947423e-10,4.6913353e-09\n",
      "Iteration 47450: loss = 2.0946267e-10,4.691101e-09\n",
      "Iteration 47455: loss = 2.0945873e-10,4.690844e-09\n",
      "Iteration 47460: loss = 2.0945962e-10,4.690575e-09\n",
      "Iteration 47465: loss = 2.0946245e-10,4.6902997e-09\n",
      "Iteration 47470: loss = 2.0946578e-10,4.6900235e-09\n",
      "Iteration 47475: loss = 2.0946873e-10,4.6897473e-09\n",
      "Iteration 47480: loss = 2.0947122e-10,4.689473e-09\n",
      "Iteration 47485: loss = 2.094714e-10,4.6892055e-09\n",
      "Iteration 47490: loss = 2.0947055e-10,4.688939e-09\n",
      "Iteration 47495: loss = 2.0946844e-10,4.6886774e-09\n",
      "Iteration 47500: loss = 2.09465e-10,4.6884194e-09\n",
      "Iteration 47505: loss = 2.0946099e-10,4.6881627e-09\n",
      "Iteration 47510: loss = 2.0945558e-10,4.687912e-09\n",
      "Iteration 47515: loss = 2.0944856e-10,4.687664e-09\n",
      "Iteration 47520: loss = 2.094404e-10,4.6874185e-09\n",
      "Iteration 47525: loss = 2.0943071e-10,4.6871778e-09\n",
      "Iteration 47530: loss = 2.0942081e-10,4.6869384e-09\n",
      "Iteration 47535: loss = 2.094102e-10,4.6867e-09\n",
      "Iteration 47540: loss = 2.0939996e-10,4.6864623e-09\n",
      "Iteration 47545: loss = 2.0938719e-10,4.686229e-09\n",
      "Iteration 47550: loss = 2.0937457e-10,4.6859974e-09\n",
      "Iteration 47555: loss = 2.0936146e-10,4.6857656e-09\n",
      "Iteration 47560: loss = 2.0934827e-10,4.685535e-09\n",
      "Iteration 47565: loss = 2.0933466e-10,4.6853055e-09\n",
      "Iteration 47570: loss = 2.0931917e-10,4.685081e-09\n",
      "Iteration 47575: loss = 2.0930309e-10,4.6848587e-09\n",
      "Iteration 47580: loss = 2.0928659e-10,4.684637e-09\n",
      "Iteration 47585: loss = 2.0927012e-10,4.6844155e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47590: loss = 2.0925334e-10,4.6841953e-09\n",
      "Iteration 47595: loss = 2.0923628e-10,4.6839745e-09\n",
      "Iteration 47600: loss = 2.0921927e-10,4.683754e-09\n",
      "Iteration 47605: loss = 2.0920234e-10,4.6835353e-09\n",
      "Iteration 47610: loss = 2.0918452e-10,4.683317e-09\n",
      "Iteration 47615: loss = 2.0916567e-10,4.6831015e-09\n",
      "Iteration 47620: loss = 2.0914692e-10,4.682885e-09\n",
      "Iteration 47625: loss = 2.0912821e-10,4.68267e-09\n",
      "Iteration 47630: loss = 2.0910963e-10,4.6824553e-09\n",
      "Iteration 47635: loss = 2.0909052e-10,4.6822404e-09\n",
      "Iteration 47640: loss = 2.0907172e-10,4.6820254e-09\n",
      "Iteration 47645: loss = 2.090527e-10,4.6818105e-09\n",
      "Iteration 47650: loss = 2.0903407e-10,4.6815956e-09\n",
      "Iteration 47655: loss = 2.0901554e-10,4.681378e-09\n",
      "Iteration 47660: loss = 2.0900193e-10,4.681148e-09\n",
      "Iteration 47665: loss = 2.0899694e-10,4.680892e-09\n",
      "Iteration 47670: loss = 2.0899708e-10,4.680623e-09\n",
      "Iteration 47675: loss = 2.0899928e-10,4.6803468e-09\n",
      "Iteration 47680: loss = 2.0900343e-10,4.680067e-09\n",
      "Iteration 47685: loss = 2.0900619e-10,4.67979e-09\n",
      "Iteration 47690: loss = 2.0900764e-10,4.6795154e-09\n",
      "Iteration 47695: loss = 2.0900885e-10,4.679242e-09\n",
      "Iteration 47700: loss = 2.090091e-10,4.6789723e-09\n",
      "Iteration 47705: loss = 2.0900691e-10,4.678709e-09\n",
      "Iteration 47710: loss = 2.09004e-10,4.6784474e-09\n",
      "Iteration 47715: loss = 2.0899922e-10,4.678192e-09\n",
      "Iteration 47720: loss = 2.0899404e-10,4.6779367e-09\n",
      "Iteration 47725: loss = 2.0898784e-10,4.677684e-09\n",
      "Iteration 47730: loss = 2.0897993e-10,4.677436e-09\n",
      "Iteration 47735: loss = 2.0897128e-10,4.677192e-09\n",
      "Iteration 47740: loss = 2.0896174e-10,4.676949e-09\n",
      "Iteration 47745: loss = 2.089507e-10,4.676711e-09\n",
      "Iteration 47750: loss = 2.0893885e-10,4.6764743e-09\n",
      "Iteration 47755: loss = 2.089268e-10,4.6762376e-09\n",
      "Iteration 47760: loss = 2.0891468e-10,4.6760027e-09\n",
      "Iteration 47765: loss = 2.089023e-10,4.6757687e-09\n",
      "Iteration 47770: loss = 2.0889603e-10,4.6755146e-09\n",
      "Iteration 47775: loss = 2.0890123e-10,4.6752313e-09\n",
      "Iteration 47780: loss = 2.0890241e-10,4.6749573e-09\n",
      "Iteration 47785: loss = 2.0890673e-10,4.6746735e-09\n",
      "Iteration 47790: loss = 2.0731022e-10,4.674405e-09\n",
      "Iteration 47795: loss = 2.0731289e-10,4.674126e-09\n",
      "Iteration 47800: loss = 2.0731639e-10,4.6738458e-09\n",
      "Iteration 47805: loss = 2.0891645e-10,4.673564e-09\n",
      "Iteration 47810: loss = 2.0892399e-10,4.6732715e-09\n",
      "Iteration 47815: loss = 2.0892278e-10,4.6730055e-09\n",
      "Iteration 47820: loss = 2.0892664e-10,4.672724e-09\n",
      "Iteration 47825: loss = 2.0892943e-10,4.6724447e-09\n",
      "Iteration 47830: loss = 2.0893266e-10,4.6721658e-09\n",
      "Iteration 47835: loss = 2.0893483e-10,4.6718878e-09\n",
      "Iteration 47840: loss = 2.0893687e-10,4.6716115e-09\n",
      "Iteration 47845: loss = 2.0894357e-10,4.6713216e-09\n",
      "Iteration 47850: loss = 2.0734353e-10,4.6710618e-09\n",
      "Iteration 47855: loss = 2.0895036e-10,4.6707607e-09\n",
      "Iteration 47860: loss = 2.089491e-10,4.6704933e-09\n",
      "Iteration 47865: loss = 2.0895212e-10,4.6702135e-09\n",
      "Iteration 47870: loss = 2.0895626e-10,4.669932e-09\n",
      "Iteration 47875: loss = 2.0896007e-10,4.6696496e-09\n",
      "Iteration 47880: loss = 2.0896067e-10,4.669375e-09\n",
      "Iteration 47885: loss = 2.0896569e-10,4.6690904e-09\n",
      "Iteration 47890: loss = 2.0896801e-10,4.668813e-09\n",
      "Iteration 47895: loss = 2.0897216e-10,4.6685287e-09\n",
      "Iteration 47900: loss = 2.073759e-10,4.668258e-09\n",
      "Iteration 47905: loss = 2.0897763e-10,4.6679713e-09\n",
      "Iteration 47910: loss = 2.0738179e-10,4.6676987e-09\n",
      "Iteration 47915: loss = 2.0898332e-10,4.6674122e-09\n",
      "Iteration 47920: loss = 2.0898593e-10,4.6671325e-09\n",
      "Iteration 47925: loss = 2.0738856e-10,4.6668647e-09\n",
      "Iteration 47930: loss = 2.0899492e-10,4.666565e-09\n",
      "Iteration 47935: loss = 2.0899397e-10,4.666294e-09\n",
      "Iteration 47940: loss = 2.089976e-10,4.6660142e-09\n",
      "Iteration 47945: loss = 2.0900294e-10,4.665727e-09\n",
      "Iteration 47950: loss = 2.0900374e-10,4.665452e-09\n",
      "Iteration 47955: loss = 2.0740965e-10,4.665173e-09\n",
      "Iteration 47960: loss = 2.0740927e-10,4.6649027e-09\n",
      "Iteration 47965: loss = 2.0741392e-10,4.6646185e-09\n",
      "Iteration 47970: loss = 2.090164e-10,4.6643285e-09\n",
      "Iteration 47975: loss = 2.0901886e-10,4.6640483e-09\n",
      "Iteration 47980: loss = 2.0902392e-10,4.6637627e-09\n",
      "Iteration 47985: loss = 2.0742404e-10,4.6635007e-09\n",
      "Iteration 47990: loss = 2.0903113e-10,4.663198e-09\n",
      "Iteration 47995: loss = 2.0742663e-10,4.6629482e-09\n",
      "Iteration 48000: loss = 2.0903523e-10,4.6626423e-09\n",
      "Iteration 48005: loss = 2.0743708e-10,4.662376e-09\n",
      "Iteration 48010: loss = 2.090413e-10,4.6620787e-09\n",
      "Iteration 48015: loss = 2.0744313e-10,4.661813e-09\n",
      "Iteration 48020: loss = 2.0904833e-10,4.6615147e-09\n",
      "Iteration 48025: loss = 2.074495e-10,4.6612496e-09\n",
      "Iteration 48030: loss = 2.0905339e-10,4.6609556e-09\n",
      "Iteration 48035: loss = 2.074514e-10,4.660699e-09\n",
      "Iteration 48040: loss = 2.0906059e-10,4.6603894e-09\n",
      "Iteration 48045: loss = 2.074587e-10,4.6601327e-09\n",
      "Iteration 48050: loss = 2.0746399e-10,4.6598445e-09\n",
      "Iteration 48055: loss = 2.0906947e-10,4.659546e-09\n",
      "Iteration 48060: loss = 2.0746505e-10,4.6592956e-09\n",
      "Iteration 48065: loss = 2.0747333e-10,4.659e-09\n",
      "Iteration 48070: loss = 2.0907893e-10,4.658701e-09\n",
      "Iteration 48075: loss = 2.0747243e-10,4.6584576e-09\n",
      "Iteration 48080: loss = 2.0747744e-10,4.6581703e-09\n",
      "Iteration 48085: loss = 2.0749046e-10,4.657859e-09\n",
      "Iteration 48090: loss = 2.074903e-10,4.657588e-09\n",
      "Iteration 48095: loss = 2.0749252e-10,4.6573074e-09\n",
      "Iteration 48100: loss = 2.0909861e-10,4.6570054e-09\n",
      "Iteration 48105: loss = 2.0749417e-10,4.6567568e-09\n",
      "Iteration 48110: loss = 2.0749381e-10,4.656483e-09\n",
      "Iteration 48115: loss = 2.0750031e-10,4.6561923e-09\n",
      "Iteration 48120: loss = 2.0750968e-10,4.655892e-09\n",
      "Iteration 48125: loss = 2.0751428e-10,4.655606e-09\n",
      "Iteration 48130: loss = 2.0751478e-10,4.6553312e-09\n",
      "Iteration 48135: loss = 2.075184e-10,4.655047e-09\n",
      "Iteration 48140: loss = 2.0752329e-10,4.6547592e-09\n",
      "Iteration 48145: loss = 2.0752843e-10,4.6544715e-09\n",
      "Iteration 48150: loss = 2.0753216e-10,4.6541873e-09\n",
      "Iteration 48155: loss = 2.0753471e-10,4.6539057e-09\n",
      "Iteration 48160: loss = 2.0753682e-10,4.653626e-09\n",
      "Iteration 48165: loss = 2.0753689e-10,4.6533524e-09\n",
      "Iteration 48170: loss = 2.0753567e-10,4.6530824e-09\n",
      "Iteration 48175: loss = 2.0753249e-10,4.6528172e-09\n",
      "Iteration 48180: loss = 2.0752862e-10,4.6525543e-09\n",
      "Iteration 48185: loss = 2.0752373e-10,4.652294e-09\n",
      "Iteration 48190: loss = 2.0751711e-10,4.6520388e-09\n",
      "Iteration 48195: loss = 2.075098e-10,4.6517843e-09\n",
      "Iteration 48200: loss = 2.0750164e-10,4.6515343e-09\n",
      "Iteration 48205: loss = 2.0749118e-10,4.6512887e-09\n",
      "Iteration 48210: loss = 2.0748046e-10,4.6510453e-09\n",
      "Iteration 48215: loss = 2.0746944e-10,4.6508024e-09\n",
      "Iteration 48220: loss = 2.0745816e-10,4.6505604e-09\n",
      "Iteration 48225: loss = 2.0744517e-10,4.6503232e-09\n",
      "Iteration 48230: loss = 2.074317e-10,4.6500865e-09\n",
      "Iteration 48235: loss = 2.0741812e-10,4.6498503e-09\n",
      "Iteration 48240: loss = 2.074043e-10,4.649615e-09\n",
      "Iteration 48245: loss = 2.0738994e-10,4.6493804e-09\n",
      "Iteration 48250: loss = 2.0737549e-10,4.6491455e-09\n",
      "Iteration 48255: loss = 2.0735959e-10,4.648916e-09\n",
      "Iteration 48260: loss = 2.073436e-10,4.648687e-09\n",
      "Iteration 48265: loss = 2.0732742e-10,4.648459e-09\n",
      "Iteration 48270: loss = 2.0731049e-10,4.6482307e-09\n",
      "Iteration 48275: loss = 2.0729414e-10,4.648002e-09\n",
      "Iteration 48280: loss = 2.0727732e-10,4.647774e-09\n",
      "Iteration 48285: loss = 2.072606e-10,4.647547e-09\n",
      "Iteration 48290: loss = 2.0724356e-10,4.64732e-09\n",
      "Iteration 48295: loss = 2.072259e-10,4.6470947e-09\n",
      "Iteration 48300: loss = 2.0721226e-10,4.6468576e-09\n",
      "Iteration 48305: loss = 2.0720818e-10,4.6465947e-09\n",
      "Iteration 48310: loss = 2.0720854e-10,4.6463184e-09\n",
      "Iteration 48315: loss = 2.072113e-10,4.6460356e-09\n",
      "Iteration 48320: loss = 2.0721468e-10,4.6457513e-09\n",
      "Iteration 48325: loss = 2.072177e-10,4.645467e-09\n",
      "Iteration 48330: loss = 2.0722019e-10,4.645184e-09\n",
      "Iteration 48335: loss = 2.0722164e-10,4.644905e-09\n",
      "Iteration 48340: loss = 2.0722196e-10,4.64463e-09\n",
      "Iteration 48345: loss = 2.0721992e-10,4.6443587e-09\n",
      "Iteration 48350: loss = 2.0721673e-10,4.6440927e-09\n",
      "Iteration 48355: loss = 2.0721308e-10,4.6438275e-09\n",
      "Iteration 48360: loss = 2.0720801e-10,4.6435664e-09\n",
      "Iteration 48365: loss = 2.0720155e-10,4.643308e-09\n",
      "Iteration 48370: loss = 2.0719439e-10,4.643052e-09\n",
      "Iteration 48375: loss = 2.0718584e-10,4.6428017e-09\n",
      "Iteration 48380: loss = 2.0717565e-10,4.642555e-09\n",
      "Iteration 48385: loss = 2.0716491e-10,4.6423088e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48390: loss = 2.0715407e-10,4.642063e-09\n",
      "Iteration 48395: loss = 2.0714232e-10,4.6418216e-09\n",
      "Iteration 48400: loss = 2.0712937e-10,4.6415813e-09\n",
      "Iteration 48405: loss = 2.071161e-10,4.6413433e-09\n",
      "Iteration 48410: loss = 2.0710296e-10,4.641105e-09\n",
      "Iteration 48415: loss = 2.0708936e-10,4.640867e-09\n",
      "Iteration 48420: loss = 2.0707525e-10,4.6406314e-09\n",
      "Iteration 48425: loss = 2.0705948e-10,4.640398e-09\n",
      "Iteration 48430: loss = 2.070436e-10,4.6401665e-09\n",
      "Iteration 48435: loss = 2.070275e-10,4.639936e-09\n",
      "Iteration 48440: loss = 2.0701164e-10,4.6397046e-09\n",
      "Iteration 48445: loss = 2.069954e-10,4.6394746e-09\n",
      "Iteration 48450: loss = 2.0697932e-10,4.6392437e-09\n",
      "Iteration 48455: loss = 2.0696234e-10,4.6390145e-09\n",
      "Iteration 48460: loss = 2.0694568e-10,4.6387862e-09\n",
      "Iteration 48465: loss = 2.069272e-10,4.6385606e-09\n",
      "Iteration 48470: loss = 2.069092e-10,4.6383355e-09\n",
      "Iteration 48475: loss = 2.068906e-10,4.6381103e-09\n",
      "Iteration 48480: loss = 2.0687234e-10,4.6378847e-09\n",
      "Iteration 48485: loss = 2.0685369e-10,4.637661e-09\n",
      "Iteration 48490: loss = 2.0683542e-10,4.6374367e-09\n",
      "Iteration 48495: loss = 2.0681688e-10,4.637212e-09\n",
      "Iteration 48500: loss = 2.0679813e-10,4.636988e-09\n",
      "Iteration 48505: loss = 2.0677958e-10,4.636764e-09\n",
      "Iteration 48510: loss = 2.0676223e-10,4.6365356e-09\n",
      "Iteration 48515: loss = 2.06754e-10,4.636282e-09\n",
      "Iteration 48520: loss = 2.0675182e-10,4.6360094e-09\n",
      "Iteration 48525: loss = 2.0675277e-10,4.63573e-09\n",
      "Iteration 48530: loss = 2.0675577e-10,4.6354454e-09\n",
      "Iteration 48535: loss = 2.067585e-10,4.63516e-09\n",
      "Iteration 48540: loss = 2.0676033e-10,4.6348787e-09\n",
      "Iteration 48545: loss = 2.067609e-10,4.634598e-09\n",
      "Iteration 48550: loss = 2.0676133e-10,4.6343196e-09\n",
      "Iteration 48555: loss = 2.067595e-10,4.6340465e-09\n",
      "Iteration 48560: loss = 2.067571e-10,4.633777e-09\n",
      "Iteration 48565: loss = 2.0675273e-10,4.6335114e-09\n",
      "Iteration 48570: loss = 2.0674706e-10,4.63325e-09\n",
      "Iteration 48575: loss = 2.0674097e-10,4.6329887e-09\n",
      "Iteration 48580: loss = 2.0673378e-10,4.632732e-09\n",
      "Iteration 48585: loss = 2.067252e-10,4.6324793e-09\n",
      "Iteration 48590: loss = 2.0671602e-10,4.6322253e-09\n",
      "Iteration 48595: loss = 2.067067e-10,4.631975e-09\n",
      "Iteration 48600: loss = 2.0669501e-10,4.6317306e-09\n",
      "Iteration 48605: loss = 2.0668274e-10,4.6314876e-09\n",
      "Iteration 48610: loss = 2.0667018e-10,4.631245e-09\n",
      "Iteration 48615: loss = 2.0665708e-10,4.6310036e-09\n",
      "Iteration 48620: loss = 2.0664366e-10,4.630764e-09\n",
      "Iteration 48625: loss = 2.0662883e-10,4.6305284e-09\n",
      "Iteration 48630: loss = 2.0661355e-10,4.6302926e-09\n",
      "Iteration 48635: loss = 2.0659859e-10,4.6300577e-09\n",
      "Iteration 48640: loss = 2.0658314e-10,4.6298227e-09\n",
      "Iteration 48645: loss = 2.0656772e-10,4.6295883e-09\n",
      "Iteration 48650: loss = 2.0655165e-10,4.6293542e-09\n",
      "Iteration 48655: loss = 2.0653557e-10,4.629122e-09\n",
      "Iteration 48660: loss = 2.0651841e-10,4.628893e-09\n",
      "Iteration 48665: loss = 2.0491597e-10,4.6286583e-09\n",
      "Iteration 48670: loss = 2.0651704e-10,4.628339e-09\n",
      "Iteration 48675: loss = 2.0652162e-10,4.6280477e-09\n",
      "Iteration 48680: loss = 2.0492825e-10,4.627788e-09\n",
      "Iteration 48685: loss = 2.0652464e-10,4.6274815e-09\n",
      "Iteration 48690: loss = 2.0652446e-10,4.627202e-09\n",
      "Iteration 48695: loss = 2.0652624e-10,4.626919e-09\n",
      "Iteration 48700: loss = 2.0653075e-10,4.626627e-09\n",
      "Iteration 48705: loss = 2.0653353e-10,4.62634e-09\n",
      "Iteration 48710: loss = 2.0653784e-10,4.6260493e-09\n",
      "Iteration 48715: loss = 2.0653941e-10,4.6257655e-09\n",
      "Iteration 48720: loss = 2.0495394e-10,4.625482e-09\n",
      "Iteration 48725: loss = 2.0495693e-10,4.625195e-09\n",
      "Iteration 48730: loss = 2.0495998e-10,4.624907e-09\n",
      "Iteration 48735: loss = 2.0655244e-10,4.6246127e-09\n",
      "Iteration 48740: loss = 2.0496617e-10,4.6243316e-09\n",
      "Iteration 48745: loss = 2.0656012e-10,4.624032e-09\n",
      "Iteration 48750: loss = 2.049717e-10,4.6237574e-09\n",
      "Iteration 48755: loss = 2.049758e-10,4.623465e-09\n",
      "Iteration 48760: loss = 2.0497808e-10,4.623179e-09\n",
      "Iteration 48765: loss = 2.0498146e-10,4.6228896e-09\n",
      "Iteration 48770: loss = 2.0657424e-10,4.622594e-09\n",
      "Iteration 48775: loss = 2.065763e-10,4.622308e-09\n",
      "Iteration 48780: loss = 2.0499019e-10,4.6220263e-09\n",
      "Iteration 48785: loss = 2.065824e-10,4.6217306e-09\n",
      "Iteration 48790: loss = 2.0658626e-10,4.62144e-09\n",
      "Iteration 48795: loss = 2.0499906e-10,4.6211612e-09\n",
      "Iteration 48800: loss = 2.0500346e-10,4.6208695e-09\n",
      "Iteration 48805: loss = 2.0659545e-10,4.620575e-09\n",
      "Iteration 48810: loss = 2.050078e-10,4.6202966e-09\n",
      "Iteration 48815: loss = 2.0501252e-10,4.620003e-09\n",
      "Iteration 48820: loss = 2.0660397e-10,4.619711e-09\n",
      "Iteration 48825: loss = 2.0660797e-10,4.6194186e-09\n",
      "Iteration 48830: loss = 2.0501806e-10,4.6191473e-09\n",
      "Iteration 48835: loss = 2.0661488e-10,4.6188386e-09\n",
      "Iteration 48840: loss = 2.0502622e-10,4.6185624e-09\n",
      "Iteration 48845: loss = 2.0662083e-10,4.6182604e-09\n",
      "Iteration 48850: loss = 2.0502995e-10,4.6179927e-09\n",
      "Iteration 48855: loss = 2.066265e-10,4.6176845e-09\n",
      "Iteration 48860: loss = 2.0504155e-10,4.6174e-09\n",
      "Iteration 48865: loss = 2.0663275e-10,4.617106e-09\n",
      "Iteration 48870: loss = 2.0504697e-10,4.616822e-09\n",
      "Iteration 48875: loss = 2.0504276e-10,4.616554e-09\n",
      "Iteration 48880: loss = 2.050401e-10,4.6162807e-09\n",
      "Iteration 48885: loss = 2.0503758e-10,4.6160076e-09\n",
      "Iteration 48890: loss = 2.0503471e-10,4.615735e-09\n",
      "Iteration 48895: loss = 2.0502983e-10,4.615467e-09\n",
      "Iteration 48900: loss = 2.0502472e-10,4.6152007e-09\n",
      "Iteration 48905: loss = 2.0501918e-10,4.6149347e-09\n",
      "Iteration 48910: loss = 2.0501274e-10,4.6146726e-09\n",
      "Iteration 48915: loss = 2.0500442e-10,4.6144146e-09\n",
      "Iteration 48920: loss = 2.049955e-10,4.6141593e-09\n",
      "Iteration 48925: loss = 2.0498629e-10,4.613905e-09\n",
      "Iteration 48930: loss = 2.0497459e-10,4.6136552e-09\n",
      "Iteration 48935: loss = 2.0496316e-10,4.6134074e-09\n",
      "Iteration 48940: loss = 2.0495121e-10,4.6131605e-09\n",
      "Iteration 48945: loss = 2.0493907e-10,4.6129136e-09\n",
      "Iteration 48950: loss = 2.0492608e-10,4.6126685e-09\n",
      "Iteration 48955: loss = 2.0491198e-10,4.612427e-09\n",
      "Iteration 48960: loss = 2.0489772e-10,4.6121866e-09\n",
      "Iteration 48965: loss = 2.0488311e-10,4.6119464e-09\n",
      "Iteration 48970: loss = 2.0487127e-10,4.6116986e-09\n",
      "Iteration 48975: loss = 2.0486812e-10,4.611425e-09\n",
      "Iteration 48980: loss = 2.0487058e-10,4.611137e-09\n",
      "Iteration 48985: loss = 2.0487552e-10,4.610841e-09\n",
      "Iteration 48990: loss = 2.048806e-10,4.610545e-09\n",
      "Iteration 48995: loss = 2.0488562e-10,4.610249e-09\n",
      "Iteration 49000: loss = 2.0489044e-10,4.6099533e-09\n",
      "Iteration 49005: loss = 2.0489367e-10,4.609661e-09\n",
      "Iteration 49010: loss = 2.0489532e-10,4.609375e-09\n",
      "Iteration 49015: loss = 2.0489592e-10,4.6090918e-09\n",
      "Iteration 49020: loss = 2.0489431e-10,4.6088147e-09\n",
      "Iteration 49025: loss = 2.048921e-10,4.6085384e-09\n",
      "Iteration 49030: loss = 2.0488816e-10,4.608267e-09\n",
      "Iteration 49035: loss = 2.048831e-10,4.6079993e-09\n",
      "Iteration 49040: loss = 2.0487684e-10,4.607735e-09\n",
      "Iteration 49045: loss = 2.0486919e-10,4.6074753e-09\n",
      "Iteration 49050: loss = 2.0486036e-10,4.6072177e-09\n",
      "Iteration 49055: loss = 2.0485103e-10,4.606962e-09\n",
      "Iteration 49060: loss = 2.0484087e-10,4.606708e-09\n",
      "Iteration 49065: loss = 2.0482993e-10,4.6064565e-09\n",
      "Iteration 49070: loss = 2.0481837e-10,4.6062074e-09\n",
      "Iteration 49075: loss = 2.0480612e-10,4.6059587e-09\n",
      "Iteration 49080: loss = 2.0479413e-10,4.6057105e-09\n",
      "Iteration 49085: loss = 2.047797e-10,4.6054667e-09\n",
      "Iteration 49090: loss = 2.0476489e-10,4.6052273e-09\n",
      "Iteration 49095: loss = 2.0474987e-10,4.6049875e-09\n",
      "Iteration 49100: loss = 2.0473435e-10,4.6047486e-09\n",
      "Iteration 49105: loss = 2.0471862e-10,4.6045097e-09\n",
      "Iteration 49110: loss = 2.047028e-10,4.6042725e-09\n",
      "Iteration 49115: loss = 2.0468664e-10,4.604035e-09\n",
      "Iteration 49120: loss = 2.0466895e-10,4.603802e-09\n",
      "Iteration 49125: loss = 2.0465164e-10,4.603568e-09\n",
      "Iteration 49130: loss = 2.0463407e-10,4.603335e-09\n",
      "Iteration 49135: loss = 2.0461598e-10,4.6031023e-09\n",
      "Iteration 49140: loss = 2.0459838e-10,4.602871e-09\n",
      "Iteration 49145: loss = 2.0458026e-10,4.6026387e-09\n",
      "Iteration 49150: loss = 2.0456216e-10,4.6024073e-09\n",
      "Iteration 49155: loss = 2.0454381e-10,4.602175e-09\n",
      "Iteration 49160: loss = 2.0452555e-10,4.6019437e-09\n",
      "Iteration 49165: loss = 2.0450718e-10,4.6017123e-09\n",
      "Iteration 49170: loss = 2.0448924e-10,4.601481e-09\n",
      "Iteration 49175: loss = 2.0446972e-10,4.6012523e-09\n",
      "Iteration 49180: loss = 2.0444996e-10,4.6010253e-09\n",
      "Iteration 49185: loss = 2.0443003e-10,4.6007975e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49190: loss = 2.0441393e-10,4.6005613e-09\n",
      "Iteration 49195: loss = 2.0440717e-10,4.6002953e-09\n",
      "Iteration 49200: loss = 2.0440627e-10,4.600014e-09\n",
      "Iteration 49205: loss = 2.0440834e-10,4.599725e-09\n",
      "Iteration 49210: loss = 2.044107e-10,4.5994324e-09\n",
      "Iteration 49215: loss = 2.0441339e-10,4.5991415e-09\n",
      "Iteration 49220: loss = 2.0441582e-10,4.598851e-09\n",
      "Iteration 49225: loss = 2.0441743e-10,4.5985624e-09\n",
      "Iteration 49230: loss = 2.0441716e-10,4.598279e-09\n",
      "Iteration 49235: loss = 2.0441566e-10,4.597999e-09\n",
      "Iteration 49240: loss = 2.0441226e-10,4.5977244e-09\n",
      "Iteration 49245: loss = 2.0440838e-10,4.5974518e-09\n",
      "Iteration 49250: loss = 2.044035e-10,4.5971813e-09\n",
      "Iteration 49255: loss = 2.0439672e-10,4.5969153e-09\n",
      "Iteration 49260: loss = 2.0438941e-10,4.5966515e-09\n",
      "Iteration 49265: loss = 2.0438122e-10,4.59639e-09\n",
      "Iteration 49270: loss = 2.0437112e-10,4.596136e-09\n",
      "Iteration 49275: loss = 2.0436018e-10,4.595881e-09\n",
      "Iteration 49280: loss = 2.043493e-10,4.595628e-09\n",
      "Iteration 49285: loss = 2.0433778e-10,4.5953756e-09\n",
      "Iteration 49290: loss = 2.0432472e-10,4.5951283e-09\n",
      "Iteration 49295: loss = 2.0431128e-10,4.5948814e-09\n",
      "Iteration 49300: loss = 2.0429752e-10,4.5946353e-09\n",
      "Iteration 49305: loss = 2.0428392e-10,4.5943898e-09\n",
      "Iteration 49310: loss = 2.0426978e-10,4.594144e-09\n",
      "Iteration 49315: loss = 2.0425485e-10,4.5939013e-09\n",
      "Iteration 49320: loss = 2.0423885e-10,4.5936623e-09\n",
      "Iteration 49325: loss = 2.0422301e-10,4.5934225e-09\n",
      "Iteration 49330: loss = 2.0420653e-10,4.5931836e-09\n",
      "Iteration 49335: loss = 2.0419028e-10,4.5929447e-09\n",
      "Iteration 49340: loss = 2.041739e-10,4.592707e-09\n",
      "Iteration 49345: loss = 2.0415712e-10,4.592468e-09\n",
      "Iteration 49350: loss = 2.0414055e-10,4.5922315e-09\n",
      "Iteration 49355: loss = 2.0412282e-10,4.591995e-09\n",
      "Iteration 49360: loss = 2.0410458e-10,4.5917616e-09\n",
      "Iteration 49365: loss = 2.0408629e-10,4.591528e-09\n",
      "Iteration 49370: loss = 2.040672e-10,4.5912953e-09\n",
      "Iteration 49375: loss = 2.04049e-10,4.5910635e-09\n",
      "Iteration 49380: loss = 2.0402996e-10,4.590831e-09\n",
      "Iteration 49385: loss = 2.040268e-10,4.5905546e-09\n",
      "Iteration 49390: loss = 2.0403508e-10,4.590245e-09\n",
      "Iteration 49395: loss = 2.0245315e-10,4.589971e-09\n",
      "Iteration 49400: loss = 2.0403983e-10,4.58966e-09\n",
      "Iteration 49405: loss = 2.0403917e-10,4.5893755e-09\n",
      "Iteration 49410: loss = 2.040439e-10,4.5890767e-09\n",
      "Iteration 49415: loss = 2.040471e-10,4.588781e-09\n",
      "Iteration 49420: loss = 2.0405073e-10,4.5884843e-09\n",
      "Iteration 49425: loss = 2.040529e-10,4.588192e-09\n",
      "Iteration 49430: loss = 2.040555e-10,4.587898e-09\n",
      "Iteration 49435: loss = 2.040617e-10,4.5875943e-09\n",
      "Iteration 49440: loss = 2.0248304e-10,4.5873114e-09\n",
      "Iteration 49445: loss = 2.0406793e-10,4.587003e-09\n",
      "Iteration 49450: loss = 2.0248891e-10,4.586722e-09\n",
      "Iteration 49455: loss = 2.0249201e-10,4.586426e-09\n",
      "Iteration 49460: loss = 2.0407499e-10,4.586124e-09\n",
      "Iteration 49465: loss = 2.0249709e-10,4.5858393e-09\n",
      "Iteration 49470: loss = 2.0408374e-10,4.585527e-09\n",
      "Iteration 49475: loss = 2.0408047e-10,4.5852486e-09\n",
      "Iteration 49480: loss = 2.0406998e-10,4.584992e-09\n",
      "Iteration 49485: loss = 2.0248131e-10,4.5847375e-09\n",
      "Iteration 49490: loss = 2.0246997e-10,4.584483e-09\n",
      "Iteration 49495: loss = 2.0403962e-10,4.584219e-09\n",
      "Iteration 49500: loss = 2.0403117e-10,4.5839554e-09\n",
      "Iteration 49505: loss = 2.0402062e-10,4.583699e-09\n",
      "Iteration 49510: loss = 2.040179e-10,4.583418e-09\n",
      "Iteration 49515: loss = 2.0400524e-10,4.5831676e-09\n",
      "Iteration 49520: loss = 2.0240849e-10,4.582936e-09\n",
      "Iteration 49525: loss = 2.0398432e-10,4.5826534e-09\n",
      "Iteration 49530: loss = 2.0397312e-10,4.5823985e-09\n",
      "Iteration 49535: loss = 2.023791e-10,4.5821578e-09\n",
      "Iteration 49540: loss = 2.0395485e-10,4.5818758e-09\n",
      "Iteration 49545: loss = 2.0394055e-10,4.5816284e-09\n",
      "Iteration 49550: loss = 2.0392715e-10,4.5813797e-09\n",
      "Iteration 49555: loss = 2.0392049e-10,4.5811106e-09\n",
      "Iteration 49560: loss = 2.0232778e-10,4.580867e-09\n",
      "Iteration 49565: loss = 2.03903e-10,4.5805866e-09\n",
      "Iteration 49570: loss = 2.0388702e-10,4.580344e-09\n",
      "Iteration 49575: loss = 2.0388079e-10,4.5800745e-09\n",
      "Iteration 49580: loss = 2.0386759e-10,4.5798236e-09\n",
      "Iteration 49585: loss = 2.0385688e-10,4.579567e-09\n",
      "Iteration 49590: loss = 2.0384544e-10,4.579311e-09\n",
      "Iteration 49595: loss = 2.0384043e-10,4.5790376e-09\n",
      "Iteration 49600: loss = 2.0382702e-10,4.5787876e-09\n",
      "Iteration 49605: loss = 2.0381848e-10,4.5785242e-09\n",
      "Iteration 49610: loss = 2.0381553e-10,4.5782445e-09\n",
      "Iteration 49615: loss = 2.0379655e-10,4.578011e-09\n",
      "Iteration 49620: loss = 2.0378498e-10,4.5777555e-09\n",
      "Iteration 49625: loss = 2.0377428e-10,4.5774984e-09\n",
      "Iteration 49630: loss = 2.0218616e-10,4.5772404e-09\n",
      "Iteration 49635: loss = 2.0217515e-10,4.5769837e-09\n",
      "Iteration 49640: loss = 2.0374863e-10,4.576706e-09\n",
      "Iteration 49645: loss = 2.0215464e-10,4.5764645e-09\n",
      "Iteration 49650: loss = 2.037236e-10,4.5762008e-09\n",
      "Iteration 49655: loss = 2.0371609e-10,4.5759334e-09\n",
      "Iteration 49660: loss = 2.021258e-10,4.5756825e-09\n",
      "Iteration 49665: loss = 2.0369384e-10,4.5754196e-09\n",
      "Iteration 49670: loss = 2.0369006e-10,4.5751425e-09\n",
      "Iteration 49675: loss = 2.0367934e-10,4.5748845e-09\n",
      "Iteration 49680: loss = 2.0208263e-10,4.57465e-09\n",
      "Iteration 49685: loss = 2.0366342e-10,4.574351e-09\n",
      "Iteration 49690: loss = 2.0364577e-10,4.5741135e-09\n",
      "Iteration 49695: loss = 2.0363622e-10,4.5738515e-09\n",
      "Iteration 49700: loss = 2.020434e-10,4.573607e-09\n",
      "Iteration 49705: loss = 2.0361822e-10,4.5733253e-09\n",
      "Iteration 49710: loss = 2.0202484e-10,4.5730824e-09\n",
      "Iteration 49715: loss = 2.0201563e-10,4.5728195e-09\n",
      "Iteration 49720: loss = 2.035845e-10,4.5725534e-09\n",
      "Iteration 49725: loss = 2.0357353e-10,4.572297e-09\n",
      "Iteration 49730: loss = 2.0357123e-10,4.572014e-09\n",
      "Iteration 49735: loss = 2.0356257e-10,4.571749e-09\n",
      "Iteration 49740: loss = 2.035443e-10,4.571511e-09\n",
      "Iteration 49745: loss = 2.0353798e-10,4.571241e-09\n",
      "Iteration 49750: loss = 2.035257e-10,4.5709854e-09\n",
      "Iteration 49755: loss = 2.0351813e-10,4.5707185e-09\n",
      "Iteration 49760: loss = 2.0350895e-10,4.570455e-09\n",
      "Iteration 49765: loss = 2.0191471e-10,4.5702127e-09\n",
      "Iteration 49770: loss = 2.0348716e-10,4.5699364e-09\n",
      "Iteration 49775: loss = 2.0189495e-10,4.5696895e-09\n",
      "Iteration 49780: loss = 2.0346815e-10,4.5694115e-09\n",
      "Iteration 49785: loss = 2.0187531e-10,4.569166e-09\n",
      "Iteration 49790: loss = 2.0344747e-10,4.56889e-09\n",
      "Iteration 49795: loss = 2.0343639e-10,4.568633e-09\n",
      "Iteration 49800: loss = 2.034275e-10,4.568368e-09\n",
      "Iteration 49805: loss = 2.0341463e-10,4.5681148e-09\n",
      "Iteration 49810: loss = 2.0340367e-10,4.5678554e-09\n",
      "Iteration 49815: loss = 2.0339712e-10,4.5675836e-09\n",
      "Iteration 49820: loss = 2.033842e-10,4.5673314e-09\n",
      "Iteration 49825: loss = 2.0337704e-10,4.567061e-09\n",
      "Iteration 49830: loss = 2.0178557e-10,4.566811e-09\n",
      "Iteration 49835: loss = 2.0336122e-10,4.566526e-09\n",
      "Iteration 49840: loss = 2.0176559e-10,4.566287e-09\n",
      "Iteration 49845: loss = 2.0175643e-10,4.5660222e-09\n",
      "Iteration 49850: loss = 2.0332509e-10,4.5657575e-09\n",
      "Iteration 49855: loss = 2.0331643e-10,4.565491e-09\n",
      "Iteration 49860: loss = 2.0330647e-10,4.565229e-09\n",
      "Iteration 49865: loss = 2.0329677e-10,4.5649657e-09\n",
      "Iteration 49870: loss = 2.0328661e-10,4.5647037e-09\n",
      "Iteration 49875: loss = 2.0327513e-10,4.564447e-09\n",
      "Iteration 49880: loss = 2.0326552e-10,4.5641837e-09\n",
      "Iteration 49885: loss = 2.0325654e-10,4.563918e-09\n",
      "Iteration 49890: loss = 2.0324768e-10,4.563652e-09\n",
      "Iteration 49895: loss = 2.016557e-10,4.563403e-09\n",
      "Iteration 49900: loss = 2.0323188e-10,4.5631166e-09\n",
      "Iteration 49905: loss = 2.0163586e-10,4.5628776e-09\n",
      "Iteration 49910: loss = 2.0320824e-10,4.562601e-09\n",
      "Iteration 49915: loss = 2.0319896e-10,4.562336e-09\n",
      "Iteration 49920: loss = 2.0318856e-10,4.5620743e-09\n",
      "Iteration 49925: loss = 2.0318104e-10,4.5618047e-09\n",
      "Iteration 49930: loss = 2.0158629e-10,4.5615627e-09\n",
      "Iteration 49935: loss = 2.0316275e-10,4.5612745e-09\n",
      "Iteration 49940: loss = 2.0314878e-10,4.5610227e-09\n",
      "Iteration 49945: loss = 2.0314099e-10,4.560754e-09\n",
      "Iteration 49950: loss = 2.0313091e-10,4.5604907e-09\n",
      "Iteration 49955: loss = 2.0153816e-10,4.5602437e-09\n",
      "Iteration 49960: loss = 2.0311426e-10,4.559955e-09\n",
      "Iteration 49965: loss = 2.0151912e-10,4.5597144e-09\n",
      "Iteration 49970: loss = 2.0309164e-10,4.5594373e-09\n",
      "Iteration 49975: loss = 2.0149958e-10,4.5591864e-09\n",
      "Iteration 49980: loss = 2.0307235e-10,4.558908e-09\n",
      "Iteration 49985: loss = 2.014812e-10,4.5586557e-09\n",
      "Iteration 49990: loss = 2.0148333e-10,4.5583572e-09\n",
      "Iteration 49995: loss = 2.0306724e-10,4.5580464e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50000: loss = 2.0307057e-10,4.557744e-09\n",
      "Iteration 50005: loss = 2.0307232e-10,4.5574473e-09\n",
      "Iteration 50010: loss = 2.0307656e-10,4.5571436e-09\n",
      "Iteration 50015: loss = 2.0149617e-10,4.5568607e-09\n",
      "Iteration 50020: loss = 2.0308792e-10,4.556526e-09\n",
      "Iteration 50025: loss = 2.0150336e-10,4.5562536e-09\n",
      "Iteration 50030: loss = 2.0308823e-10,4.5559396e-09\n",
      "Iteration 50035: loss = 2.030896e-10,4.5556434e-09\n",
      "Iteration 50040: loss = 2.0309476e-10,4.555336e-09\n",
      "Iteration 50045: loss = 2.0309653e-10,4.5550395e-09\n",
      "Iteration 50050: loss = 2.030998e-10,4.554736e-09\n",
      "Iteration 50055: loss = 2.0310499e-10,4.5544297e-09\n",
      "Iteration 50060: loss = 2.0152431e-10,4.554147e-09\n",
      "Iteration 50065: loss = 2.031086e-10,4.553834e-09\n",
      "Iteration 50070: loss = 2.0311264e-10,4.553529e-09\n",
      "Iteration 50075: loss = 2.0153233e-10,4.5532444e-09\n",
      "Iteration 50080: loss = 2.031213e-10,4.552918e-09\n",
      "Iteration 50085: loss = 2.0154116e-10,4.552633e-09\n",
      "Iteration 50090: loss = 2.0154513e-10,4.5523283e-09\n",
      "Iteration 50095: loss = 2.0312768e-10,4.5520214e-09\n",
      "Iteration 50100: loss = 2.0313103e-10,4.551718e-09\n",
      "Iteration 50105: loss = 2.0155573e-10,4.551419e-09\n",
      "Iteration 50110: loss = 2.0155712e-10,4.551122e-09\n",
      "Iteration 50115: loss = 2.0156175e-10,4.550815e-09\n",
      "Iteration 50120: loss = 2.0156432e-10,4.5505155e-09\n",
      "Iteration 50125: loss = 2.0314782e-10,4.5502038e-09\n",
      "Iteration 50130: loss = 2.015661e-10,4.5499227e-09\n",
      "Iteration 50135: loss = 2.0315742e-10,4.5495896e-09\n",
      "Iteration 50140: loss = 2.0157126e-10,4.5493205e-09\n",
      "Iteration 50145: loss = 2.0157997e-10,4.549002e-09\n",
      "Iteration 50150: loss = 2.0158224e-10,4.5487027e-09\n",
      "Iteration 50155: loss = 2.0316683e-10,4.5483883e-09\n",
      "Iteration 50160: loss = 2.0159056e-10,4.5480912e-09\n",
      "Iteration 50165: loss = 2.0159426e-10,4.547788e-09\n",
      "Iteration 50170: loss = 2.0317582e-10,4.547482e-09\n",
      "Iteration 50175: loss = 2.0159714e-10,4.547191e-09\n",
      "Iteration 50180: loss = 2.0160319e-10,4.54688e-09\n",
      "Iteration 50185: loss = 2.0160644e-10,4.546578e-09\n",
      "Iteration 50190: loss = 2.0318895e-10,4.5462674e-09\n",
      "Iteration 50195: loss = 2.0319209e-10,4.5459654e-09\n",
      "Iteration 50200: loss = 2.0161728e-10,4.5456643e-09\n",
      "Iteration 50205: loss = 2.0162055e-10,4.54536e-09\n",
      "Iteration 50210: loss = 2.0162201e-10,4.545062e-09\n",
      "Iteration 50215: loss = 2.0162523e-10,4.5447575e-09\n",
      "Iteration 50220: loss = 2.0320857e-10,4.544447e-09\n",
      "Iteration 50225: loss = 2.0163178e-10,4.54415e-09\n",
      "Iteration 50230: loss = 2.0163478e-10,4.5438475e-09\n",
      "Iteration 50235: loss = 2.0163864e-10,4.5435424e-09\n",
      "Iteration 50240: loss = 2.016417e-10,4.5432382e-09\n",
      "Iteration 50245: loss = 2.0164437e-10,4.5429362e-09\n",
      "Iteration 50250: loss = 2.0322795e-10,4.5426236e-09\n",
      "Iteration 50255: loss = 2.0164824e-10,4.5423363e-09\n",
      "Iteration 50260: loss = 2.0165127e-10,4.5420334e-09\n",
      "Iteration 50265: loss = 2.0165654e-10,4.5417234e-09\n",
      "Iteration 50270: loss = 2.0166213e-10,4.5414117e-09\n",
      "Iteration 50275: loss = 2.0166235e-10,4.5411155e-09\n",
      "Iteration 50280: loss = 2.0165768e-10,4.5408353e-09\n",
      "Iteration 50285: loss = 2.0165686e-10,4.540542e-09\n",
      "Iteration 50290: loss = 2.0165693e-10,4.5402464e-09\n",
      "Iteration 50295: loss = 2.0165725e-10,4.53995e-09\n",
      "Iteration 50300: loss = 2.0165725e-10,4.539655e-09\n",
      "Iteration 50305: loss = 2.0165646e-10,4.539362e-09\n",
      "Iteration 50310: loss = 2.0165321e-10,4.539075e-09\n",
      "Iteration 50315: loss = 2.0164882e-10,4.538792e-09\n",
      "Iteration 50320: loss = 2.0164315e-10,4.538513e-09\n",
      "Iteration 50325: loss = 2.0163633e-10,4.538238e-09\n",
      "Iteration 50330: loss = 2.0162894e-10,4.537964e-09\n",
      "Iteration 50335: loss = 2.0162079e-10,4.537692e-09\n",
      "Iteration 50340: loss = 2.016109e-10,4.537424e-09\n",
      "Iteration 50345: loss = 2.0160085e-10,4.537157e-09\n",
      "Iteration 50350: loss = 2.0159018e-10,4.536891e-09\n",
      "Iteration 50355: loss = 2.015792e-10,4.5366275e-09\n",
      "Iteration 50360: loss = 2.0156682e-10,4.5363673e-09\n",
      "Iteration 50365: loss = 2.0155393e-10,4.536108e-09\n",
      "Iteration 50370: loss = 2.0154071e-10,4.535849e-09\n",
      "Iteration 50375: loss = 2.0152736e-10,4.5355915e-09\n",
      "Iteration 50380: loss = 2.0151375e-10,4.5353343e-09\n",
      "Iteration 50385: loss = 2.0149828e-10,4.535081e-09\n",
      "Iteration 50390: loss = 2.01483e-10,4.5348294e-09\n",
      "Iteration 50395: loss = 2.0146751e-10,4.534577e-09\n",
      "Iteration 50400: loss = 2.0145162e-10,4.534326e-09\n",
      "Iteration 50405: loss = 2.0143588e-10,4.5340744e-09\n",
      "Iteration 50410: loss = 2.0141942e-10,4.533824e-09\n",
      "Iteration 50415: loss = 2.0140338e-10,4.5335744e-09\n",
      "Iteration 50420: loss = 2.0138556e-10,4.5333275e-09\n",
      "Iteration 50425: loss = 2.0136783e-10,4.533082e-09\n",
      "Iteration 50430: loss = 2.0134978e-10,4.5328363e-09\n",
      "Iteration 50435: loss = 2.0133206e-10,4.5325903e-09\n",
      "Iteration 50440: loss = 2.013139e-10,4.5323456e-09\n",
      "Iteration 50445: loss = 2.0129588e-10,4.5321005e-09\n",
      "Iteration 50450: loss = 2.0127762e-10,4.531856e-09\n",
      "Iteration 50455: loss = 2.0125927e-10,4.531611e-09\n",
      "Iteration 50460: loss = 2.0124734e-10,4.5313486e-09\n",
      "Iteration 50465: loss = 2.012432e-10,4.5310626e-09\n",
      "Iteration 50470: loss = 2.0124301e-10,4.530767e-09\n",
      "Iteration 50475: loss = 2.0124451e-10,4.530466e-09\n",
      "Iteration 50480: loss = 2.0124667e-10,4.530163e-09\n",
      "Iteration 50485: loss = 2.0124837e-10,4.5298614e-09\n",
      "Iteration 50490: loss = 2.0124889e-10,4.529564e-09\n",
      "Iteration 50495: loss = 2.0124848e-10,4.5292667e-09\n",
      "Iteration 50500: loss = 2.0124753e-10,4.528973e-09\n",
      "Iteration 50505: loss = 2.0124469e-10,4.528683e-09\n",
      "Iteration 50510: loss = 2.0124123e-10,4.5283954e-09\n",
      "Iteration 50515: loss = 2.0123636e-10,4.528111e-09\n",
      "Iteration 50520: loss = 2.0123074e-10,4.52783e-09\n",
      "Iteration 50525: loss = 2.0122459e-10,4.527551e-09\n",
      "Iteration 50530: loss = 2.0121639e-10,4.5272754e-09\n",
      "Iteration 50535: loss = 2.0120805e-10,4.5270023e-09\n",
      "Iteration 50540: loss = 2.0119877e-10,4.526731e-09\n",
      "Iteration 50545: loss = 2.0118858e-10,4.526461e-09\n",
      "Iteration 50550: loss = 2.0117742e-10,4.5261954e-09\n",
      "Iteration 50555: loss = 2.0116563e-10,4.525932e-09\n",
      "Iteration 50560: loss = 2.011517e-10,4.5256727e-09\n",
      "Iteration 50565: loss = 2.0113818e-10,4.5254147e-09\n",
      "Iteration 50570: loss = 2.0112316e-10,4.5251602e-09\n",
      "Iteration 50575: loss = 2.0110713e-10,4.524906e-09\n",
      "Iteration 50580: loss = 2.0109142e-10,4.5246544e-09\n",
      "Iteration 50585: loss = 2.0107534e-10,4.5244026e-09\n",
      "Iteration 50590: loss = 2.0105924e-10,4.524151e-09\n",
      "Iteration 50595: loss = 2.0104272e-10,4.5238986e-09\n",
      "Iteration 50600: loss = 2.0102653e-10,4.5236477e-09\n",
      "Iteration 50605: loss = 2.0100982e-10,4.5233968e-09\n",
      "Iteration 50610: loss = 2.0099156e-10,4.52315e-09\n",
      "Iteration 50615: loss = 2.0097352e-10,4.522904e-09\n",
      "Iteration 50620: loss = 2.0095547e-10,4.522658e-09\n",
      "Iteration 50625: loss = 2.0093689e-10,4.522412e-09\n",
      "Iteration 50630: loss = 2.0091857e-10,4.522166e-09\n",
      "Iteration 50635: loss = 2.0090003e-10,4.52192e-09\n",
      "Iteration 50640: loss = 2.0088152e-10,4.5216746e-09\n",
      "Iteration 50645: loss = 2.0086306e-10,4.521429e-09\n",
      "Iteration 50650: loss = 2.0084458e-10,4.5211834e-09\n",
      "Iteration 50655: loss = 2.0082581e-10,4.520939e-09\n",
      "Iteration 50660: loss = 2.0080711e-10,4.520694e-09\n",
      "Iteration 50665: loss = 2.0079001e-10,4.5204445e-09\n",
      "Iteration 50670: loss = 2.0078154e-10,4.5201682e-09\n",
      "Iteration 50675: loss = 2.0078043e-10,4.5198725e-09\n",
      "Iteration 50680: loss = 2.0078168e-10,4.5195705e-09\n",
      "Iteration 50685: loss = 2.0078354e-10,4.5192645e-09\n",
      "Iteration 50690: loss = 2.0078601e-10,4.5189594e-09\n",
      "Iteration 50695: loss = 2.0078728e-10,4.5186557e-09\n",
      "Iteration 50700: loss = 2.0078834e-10,4.5183532e-09\n",
      "Iteration 50705: loss = 2.0078866e-10,4.518053e-09\n",
      "Iteration 50710: loss = 2.007874e-10,4.5177577e-09\n",
      "Iteration 50715: loss = 2.0078529e-10,4.517463e-09\n",
      "Iteration 50720: loss = 2.0078171e-10,4.5171733e-09\n",
      "Iteration 50725: loss = 2.0077706e-10,4.5168873e-09\n",
      "Iteration 50730: loss = 2.0077212e-10,4.5166013e-09\n",
      "Iteration 50735: loss = 2.007637e-10,4.5163255e-09\n",
      "Iteration 50740: loss = 2.0075475e-10,4.516052e-09\n",
      "Iteration 50745: loss = 2.0074518e-10,4.5157798e-09\n",
      "Iteration 50750: loss = 2.0073415e-10,4.5155115e-09\n",
      "Iteration 50755: loss = 2.007227e-10,4.515245e-09\n",
      "Iteration 50760: loss = 2.0071066e-10,4.5149795e-09\n",
      "Iteration 50765: loss = 2.0069839e-10,4.5147144e-09\n",
      "Iteration 50770: loss = 2.006856e-10,4.5144515e-09\n",
      "Iteration 50775: loss = 2.0067122e-10,4.5141904e-09\n",
      "Iteration 50780: loss = 1.9908913e-10,4.513932e-09\n",
      "Iteration 50785: loss = 2.0066919e-10,4.5135957e-09\n",
      "Iteration 50790: loss = 2.0067888e-10,4.5132684e-09\n",
      "Iteration 50795: loss = 2.0067605e-10,4.5129758e-09\n",
      "Iteration 50800: loss = 1.9910056e-10,4.5126973e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50805: loss = 2.0067804e-10,4.5123687e-09\n",
      "Iteration 50810: loss = 2.0067804e-10,4.512069e-09\n",
      "Iteration 50815: loss = 2.0068229e-10,4.511757e-09\n",
      "Iteration 50820: loss = 2.006851e-10,4.511447e-09\n",
      "Iteration 50825: loss = 2.0068769e-10,4.511139e-09\n",
      "Iteration 50830: loss = 2.0069156e-10,4.5108273e-09\n",
      "Iteration 50835: loss = 2.0069535e-10,4.5105164e-09\n",
      "Iteration 50840: loss = 1.9912781e-10,4.510213e-09\n",
      "Iteration 50845: loss = 2.0070424e-10,4.509889e-09\n",
      "Iteration 50850: loss = 1.9913254e-10,4.5095976e-09\n",
      "Iteration 50855: loss = 2.0071178e-10,4.509264e-09\n",
      "Iteration 50860: loss = 1.9914087e-10,4.508972e-09\n",
      "Iteration 50865: loss = 1.9914524e-10,4.5086566e-09\n",
      "Iteration 50870: loss = 2.0071726e-10,4.508345e-09\n",
      "Iteration 50875: loss = 2.0072137e-10,4.5080313e-09\n",
      "Iteration 50880: loss = 2.0072503e-10,4.50772e-09\n",
      "Iteration 50885: loss = 1.9915619e-10,4.50742e-09\n",
      "Iteration 50890: loss = 2.0073306e-10,4.507094e-09\n",
      "Iteration 50895: loss = 2.007337e-10,4.5067905e-09\n",
      "Iteration 50900: loss = 2.0073776e-10,4.5064774e-09\n",
      "Iteration 50905: loss = 1.9916989e-10,4.506176e-09\n",
      "Iteration 50910: loss = 2.0074574e-10,4.505851e-09\n",
      "Iteration 50915: loss = 1.9917236e-10,4.505565e-09\n",
      "Iteration 50920: loss = 2.0075101e-10,4.5052326e-09\n",
      "Iteration 50925: loss = 2.007541e-10,4.504921e-09\n",
      "Iteration 50930: loss = 1.9918778e-10,4.5046153e-09\n",
      "Iteration 50935: loss = 2.0076084e-10,4.5042983e-09\n",
      "Iteration 50940: loss = 2.0076478e-10,4.5039856e-09\n",
      "Iteration 50945: loss = 1.9919404e-10,4.5036916e-09\n",
      "Iteration 50950: loss = 2.0077258e-10,4.50336e-09\n",
      "Iteration 50955: loss = 1.992e-10,4.50307e-09\n",
      "Iteration 50960: loss = 1.9920361e-10,4.5027577e-09\n",
      "Iteration 50965: loss = 2.0078261e-10,4.502424e-09\n",
      "Iteration 50970: loss = 1.9920675e-10,4.502145e-09\n",
      "Iteration 50975: loss = 1.9921381e-10,4.501821e-09\n",
      "Iteration 50980: loss = 1.992211e-10,4.5014987e-09\n",
      "Iteration 50985: loss = 1.9922297e-10,4.5011905e-09\n",
      "Iteration 50990: loss = 1.9922426e-10,4.5008846e-09\n",
      "Iteration 50995: loss = 2.0080139e-10,4.5005555e-09\n",
      "Iteration 51000: loss = 1.992333e-10,4.500254e-09\n",
      "Iteration 51005: loss = 1.992378e-10,4.499938e-09\n",
      "Iteration 51010: loss = 1.9924117e-10,4.4996264e-09\n",
      "Iteration 51015: loss = 2.0081399e-10,4.4993103e-09\n",
      "Iteration 51020: loss = 1.9924717e-10,4.4990034e-09\n",
      "Iteration 51025: loss = 2.0082021e-10,4.4986868e-09\n",
      "Iteration 51030: loss = 1.9924568e-10,4.4984025e-09\n",
      "Iteration 51035: loss = 1.9924756e-10,4.498094e-09\n",
      "Iteration 51040: loss = 1.9925302e-10,4.4977755e-09\n",
      "Iteration 51045: loss = 1.9926065e-10,4.4974504e-09\n",
      "Iteration 51050: loss = 1.992684e-10,4.4971262e-09\n",
      "Iteration 51055: loss = 1.9926889e-10,4.496821e-09\n",
      "Iteration 51060: loss = 1.9926989e-10,4.496516e-09\n",
      "Iteration 51065: loss = 1.9927197e-10,4.496206e-09\n",
      "Iteration 51070: loss = 1.9927442e-10,4.4958957e-09\n",
      "Iteration 51075: loss = 1.9927633e-10,4.495588e-09\n",
      "Iteration 51080: loss = 1.9927637e-10,4.495283e-09\n",
      "Iteration 51085: loss = 1.9927254e-10,4.4949915e-09\n",
      "Iteration 51090: loss = 1.9925822e-10,4.4947277e-09\n",
      "Iteration 51095: loss = 1.9924422e-10,4.4944644e-09\n",
      "Iteration 51100: loss = 1.9923385e-10,4.494191e-09\n",
      "Iteration 51105: loss = 1.9922534e-10,4.4939106e-09\n",
      "Iteration 51110: loss = 1.9921709e-10,4.4936312e-09\n",
      "Iteration 51115: loss = 1.9920625e-10,4.493359e-09\n",
      "Iteration 51120: loss = 1.9919544e-10,4.493085e-09\n",
      "Iteration 51125: loss = 1.9918546e-10,4.49281e-09\n",
      "Iteration 51130: loss = 1.9917508e-10,4.4925352e-09\n",
      "Iteration 51135: loss = 1.9916387e-10,4.4922635e-09\n",
      "Iteration 51140: loss = 1.9915342e-10,4.4919894e-09\n",
      "Iteration 51145: loss = 1.9914259e-10,4.4917168e-09\n",
      "Iteration 51150: loss = 1.9913156e-10,4.491443e-09\n",
      "Iteration 51155: loss = 1.9911937e-10,4.4911745e-09\n",
      "Iteration 51160: loss = 1.9910638e-10,4.4909063e-09\n",
      "Iteration 51165: loss = 1.9909353e-10,4.4906394e-09\n",
      "Iteration 51170: loss = 1.9908024e-10,4.4903734e-09\n",
      "Iteration 51175: loss = 1.9906682e-10,4.4901065e-09\n",
      "Iteration 51180: loss = 1.9905326e-10,4.489842e-09\n",
      "Iteration 51185: loss = 1.990378e-10,4.489581e-09\n",
      "Iteration 51190: loss = 1.9902219e-10,4.4893205e-09\n",
      "Iteration 51195: loss = 1.990066e-10,4.489061e-09\n",
      "Iteration 51200: loss = 1.9899078e-10,4.4888013e-09\n",
      "Iteration 51205: loss = 1.9897588e-10,4.4885393e-09\n",
      "Iteration 51210: loss = 1.9897073e-10,4.488249e-09\n",
      "Iteration 51215: loss = 1.989706e-10,4.4879447e-09\n",
      "Iteration 51220: loss = 1.9897423e-10,4.487629e-09\n",
      "Iteration 51225: loss = 1.9897962e-10,4.4873087e-09\n",
      "Iteration 51230: loss = 1.9897667e-10,4.487012e-09\n",
      "Iteration 51235: loss = 1.9896489e-10,4.4867403e-09\n",
      "Iteration 51240: loss = 1.9894796e-10,4.486483e-09\n",
      "Iteration 51245: loss = 1.989345e-10,4.486217e-09\n",
      "Iteration 51250: loss = 1.9892805e-10,4.4859307e-09\n",
      "Iteration 51255: loss = 1.9891644e-10,4.4856567e-09\n",
      "Iteration 51260: loss = 1.989063e-10,4.485381e-09\n",
      "Iteration 51265: loss = 1.9889694e-10,4.485103e-09\n",
      "Iteration 51270: loss = 1.9888464e-10,4.4848307e-09\n",
      "Iteration 51275: loss = 1.9887679e-10,4.484549e-09\n",
      "Iteration 51280: loss = 1.988653e-10,4.484276e-09\n",
      "Iteration 51285: loss = 1.9885586e-10,4.4839963e-09\n",
      "Iteration 51290: loss = 1.9884543e-10,4.483721e-09\n",
      "Iteration 51295: loss = 1.9883366e-10,4.4834483e-09\n",
      "Iteration 51300: loss = 1.9882367e-10,4.4831707e-09\n",
      "Iteration 51305: loss = 1.9881476e-10,4.4828896e-09\n",
      "Iteration 51310: loss = 1.9880371e-10,4.482615e-09\n",
      "Iteration 51315: loss = 1.987933e-10,4.48234e-09\n",
      "Iteration 51320: loss = 1.9878399e-10,4.4820605e-09\n",
      "Iteration 51325: loss = 1.987746e-10,4.4817803e-09\n",
      "Iteration 51330: loss = 1.9876258e-10,4.4815094e-09\n",
      "Iteration 51335: loss = 1.98751e-10,4.481235e-09\n",
      "Iteration 51340: loss = 1.9873934e-10,4.4809623e-09\n",
      "Iteration 51345: loss = 1.9872788e-10,4.4806883e-09\n",
      "Iteration 51350: loss = 1.9871592e-10,4.480416e-09\n",
      "Iteration 51355: loss = 1.987041e-10,4.4801434e-09\n",
      "Iteration 51360: loss = 1.9869167e-10,4.4798716e-09\n",
      "Iteration 51365: loss = 1.9867917e-10,4.479601e-09\n",
      "Iteration 51370: loss = 1.9866554e-10,4.4793333e-09\n",
      "Iteration 51375: loss = 1.9865082e-10,4.4790682e-09\n",
      "Iteration 51380: loss = 1.9863637e-10,4.4788027e-09\n",
      "Iteration 51385: loss = 1.986219e-10,4.4785375e-09\n",
      "Iteration 51390: loss = 1.986069e-10,4.4782733e-09\n",
      "Iteration 51395: loss = 1.9859174e-10,4.478009e-09\n",
      "Iteration 51400: loss = 1.9857542e-10,4.477749e-09\n",
      "Iteration 51405: loss = 1.9855882e-10,4.477489e-09\n",
      "Iteration 51410: loss = 1.9854167e-10,4.4772315e-09\n",
      "Iteration 51415: loss = 1.9852449e-10,4.476974e-09\n",
      "Iteration 51420: loss = 1.9851139e-10,4.476703e-09\n",
      "Iteration 51425: loss = 1.9850728e-10,4.4764077e-09\n",
      "Iteration 51430: loss = 1.9850738e-10,4.4761e-09\n",
      "Iteration 51435: loss = 1.9850989e-10,4.4757855e-09\n",
      "Iteration 51440: loss = 1.9851291e-10,4.4754698e-09\n",
      "Iteration 51445: loss = 1.9851566e-10,4.475155e-09\n",
      "Iteration 51450: loss = 1.9851726e-10,4.474843e-09\n",
      "Iteration 51455: loss = 1.985168e-10,4.474538e-09\n",
      "Iteration 51460: loss = 1.9850528e-10,4.4742623e-09\n",
      "Iteration 51465: loss = 1.9848956e-10,4.4739985e-09\n",
      "Iteration 51470: loss = 1.9847861e-10,4.473724e-09\n",
      "Iteration 51475: loss = 1.984709e-10,4.473438e-09\n",
      "Iteration 51480: loss = 1.9846286e-10,4.4731534e-09\n",
      "Iteration 51485: loss = 1.9845268e-10,4.472874e-09\n",
      "Iteration 51490: loss = 1.9844322e-10,4.472593e-09\n",
      "Iteration 51495: loss = 1.9843079e-10,4.472322e-09\n",
      "Iteration 51500: loss = 1.9842057e-10,4.472043e-09\n",
      "Iteration 51505: loss = 1.9841064e-10,4.4717643e-09\n",
      "Iteration 51510: loss = 1.9840078e-10,4.4714845e-09\n",
      "Iteration 51515: loss = 1.9839114e-10,4.4712047e-09\n",
      "Iteration 51520: loss = 1.9838108e-10,4.470925e-09\n",
      "Iteration 51525: loss = 1.9836988e-10,4.4706487e-09\n",
      "Iteration 51530: loss = 1.9835805e-10,4.4703747e-09\n",
      "Iteration 51535: loss = 1.9834578e-10,4.470101e-09\n",
      "Iteration 51540: loss = 1.9833346e-10,4.4698294e-09\n",
      "Iteration 51545: loss = 1.9832062e-10,4.469558e-09\n",
      "Iteration 51550: loss = 1.9830705e-10,4.469288e-09\n",
      "Iteration 51555: loss = 1.9829237e-10,4.4690216e-09\n",
      "Iteration 51560: loss = 1.9827735e-10,4.4687547e-09\n",
      "Iteration 51565: loss = 1.982845e-10,4.468426e-09\n",
      "Iteration 51570: loss = 1.9829194e-10,4.468097e-09\n",
      "Iteration 51575: loss = 1.9672218e-10,4.46782e-09\n",
      "Iteration 51580: loss = 1.9829703e-10,4.4674637e-09\n",
      "Iteration 51585: loss = 1.9829792e-10,4.4671524e-09\n",
      "Iteration 51590: loss = 1.967342e-10,4.4668584e-09\n",
      "Iteration 51595: loss = 1.9830715e-10,4.466508e-09\n",
      "Iteration 51600: loss = 1.9673947e-10,4.4662256e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51605: loss = 1.9831174e-10,4.4658766e-09\n",
      "Iteration 51610: loss = 1.9674949e-10,4.465578e-09\n",
      "Iteration 51615: loss = 1.9831482e-10,4.465249e-09\n",
      "Iteration 51620: loss = 1.9675545e-10,4.4649426e-09\n",
      "Iteration 51625: loss = 1.9832268e-10,4.4646082e-09\n",
      "Iteration 51630: loss = 1.9676026e-10,4.4643107e-09\n",
      "Iteration 51635: loss = 1.9832876e-10,4.4639723e-09\n",
      "Iteration 51640: loss = 1.9676565e-10,4.4636757e-09\n",
      "Iteration 51645: loss = 1.9677387e-10,4.4633426e-09\n",
      "Iteration 51650: loss = 1.9677497e-10,4.463029e-09\n",
      "Iteration 51655: loss = 1.9678005e-10,4.462705e-09\n",
      "Iteration 51660: loss = 1.9678188e-10,4.46239e-09\n",
      "Iteration 51665: loss = 1.983474e-10,4.4620596e-09\n",
      "Iteration 51670: loss = 1.967882e-10,4.4617514e-09\n",
      "Iteration 51675: loss = 1.9679382e-10,4.461426e-09\n",
      "Iteration 51680: loss = 1.9679279e-10,4.461119e-09\n",
      "Iteration 51685: loss = 1.9836222e-10,4.4607784e-09\n",
      "Iteration 51690: loss = 1.9679834e-10,4.460483e-09\n",
      "Iteration 51695: loss = 1.9680496e-10,4.460154e-09\n",
      "Iteration 51700: loss = 1.9680756e-10,4.459837e-09\n",
      "Iteration 51705: loss = 1.968144e-10,4.4595057e-09\n",
      "Iteration 51710: loss = 1.96813e-10,4.4592015e-09\n",
      "Iteration 51715: loss = 1.9838081e-10,4.4588635e-09\n",
      "Iteration 51720: loss = 1.9838442e-10,4.458543e-09\n",
      "Iteration 51725: loss = 1.9682454e-10,4.458237e-09\n",
      "Iteration 51730: loss = 1.9683048e-10,4.4579096e-09\n",
      "Iteration 51735: loss = 1.9839382e-10,4.457584e-09\n",
      "Iteration 51740: loss = 1.9683317e-10,4.45728e-09\n",
      "Iteration 51745: loss = 1.9840081e-10,4.456944e-09\n",
      "Iteration 51750: loss = 1.9684175e-10,4.456635e-09\n",
      "Iteration 51755: loss = 1.9684628e-10,4.4563104e-09\n",
      "Iteration 51760: loss = 1.9684702e-10,4.455998e-09\n",
      "Iteration 51765: loss = 1.9841545e-10,4.4556576e-09\n",
      "Iteration 51770: loss = 1.968542e-10,4.4553543e-09\n",
      "Iteration 51775: loss = 1.9842104e-10,4.455019e-09\n",
      "Iteration 51780: loss = 1.9842529e-10,4.454696e-09\n",
      "Iteration 51785: loss = 1.9686737e-10,4.454384e-09\n",
      "Iteration 51790: loss = 1.968715e-10,4.4540616e-09\n",
      "Iteration 51795: loss = 1.9687461e-10,4.453741e-09\n",
      "Iteration 51800: loss = 1.9687753e-10,4.453422e-09\n",
      "Iteration 51805: loss = 1.9688053e-10,4.4531014e-09\n",
      "Iteration 51810: loss = 1.9688425e-10,4.4527804e-09\n",
      "Iteration 51815: loss = 1.968849e-10,4.452467e-09\n",
      "Iteration 51820: loss = 1.9688934e-10,4.452142e-09\n",
      "Iteration 51825: loss = 1.9845221e-10,4.451818e-09\n",
      "Iteration 51830: loss = 1.9687979e-10,4.4515476e-09\n",
      "Iteration 51835: loss = 1.9686891e-10,4.4512656e-09\n",
      "Iteration 51840: loss = 1.9685953e-10,4.4509805e-09\n",
      "Iteration 51845: loss = 1.9684382e-10,4.4507136e-09\n",
      "Iteration 51850: loss = 1.96827e-10,4.45045e-09\n",
      "Iteration 51855: loss = 1.96819e-10,4.4501607e-09\n",
      "Iteration 51860: loss = 1.9681666e-10,4.449855e-09\n",
      "Iteration 51865: loss = 1.9680363e-10,4.4495807e-09\n",
      "Iteration 51870: loss = 1.9678681e-10,4.4493156e-09\n",
      "Iteration 51875: loss = 1.9677804e-10,4.44903e-09\n",
      "Iteration 51880: loss = 1.9677378e-10,4.4487294e-09\n",
      "Iteration 51885: loss = 1.9676581e-10,4.4484403e-09\n",
      "Iteration 51890: loss = 1.9675454e-10,4.44816e-09\n",
      "Iteration 51895: loss = 1.9674536e-10,4.447873e-09\n",
      "Iteration 51900: loss = 1.9673026e-10,4.447604e-09\n",
      "Iteration 51905: loss = 1.9671993e-10,4.4473216e-09\n",
      "Iteration 51910: loss = 1.9671344e-10,4.4470276e-09\n",
      "Iteration 51915: loss = 1.9670221e-10,4.4467465e-09\n",
      "Iteration 51920: loss = 1.966912e-10,4.4464663e-09\n",
      "Iteration 51925: loss = 1.9668457e-10,4.4461714e-09\n",
      "Iteration 51930: loss = 1.982335e-10,4.4458868e-09\n",
      "Iteration 51935: loss = 1.966638e-10,4.4456065e-09\n",
      "Iteration 51940: loss = 1.9665401e-10,4.445321e-09\n",
      "Iteration 51945: loss = 1.9664219e-10,4.4450426e-09\n",
      "Iteration 51950: loss = 1.9663342e-10,4.444754e-09\n",
      "Iteration 51955: loss = 1.9662176e-10,4.444474e-09\n",
      "Iteration 51960: loss = 1.9661289e-10,4.444187e-09\n",
      "Iteration 51965: loss = 1.966003e-10,4.4439092e-09\n",
      "Iteration 51970: loss = 1.9815148e-10,4.4436175e-09\n",
      "Iteration 51975: loss = 1.981416e-10,4.4433324e-09\n",
      "Iteration 51980: loss = 1.9813075e-10,4.4430504e-09\n",
      "Iteration 51985: loss = 1.9655917e-10,4.442774e-09\n",
      "Iteration 51990: loss = 1.9655134e-10,4.4424833e-09\n",
      "Iteration 51995: loss = 1.9653816e-10,4.442207e-09\n",
      "Iteration 52000: loss = 1.9653114e-10,4.441914e-09\n",
      "Iteration 52005: loss = 1.9651886e-10,4.4416346e-09\n",
      "Iteration 52010: loss = 1.965053e-10,4.441361e-09\n",
      "Iteration 52015: loss = 1.9650086e-10,4.4410604e-09\n",
      "Iteration 52020: loss = 1.9648412e-10,4.4407935e-09\n",
      "Iteration 52025: loss = 1.9647195e-10,4.4405155e-09\n",
      "Iteration 52030: loss = 1.9646705e-10,4.440215e-09\n",
      "Iteration 52035: loss = 1.9645992e-10,4.439921e-09\n",
      "Iteration 52040: loss = 1.9644598e-10,4.439647e-09\n",
      "Iteration 52045: loss = 1.9643631e-10,4.4393613e-09\n",
      "Iteration 52050: loss = 1.979895e-10,4.4390625e-09\n",
      "Iteration 52055: loss = 1.9641856e-10,4.438783e-09\n",
      "Iteration 52060: loss = 1.9640685e-10,4.4385042e-09\n",
      "Iteration 52065: loss = 1.9639429e-10,4.438225e-09\n",
      "Iteration 52070: loss = 1.9638702e-10,4.437931e-09\n",
      "Iteration 52075: loss = 1.9637736e-10,4.437645e-09\n",
      "Iteration 52080: loss = 1.9636592e-10,4.4373634e-09\n",
      "Iteration 52085: loss = 1.9635787e-10,4.437072e-09\n",
      "Iteration 52090: loss = 1.9634668e-10,4.4367896e-09\n",
      "Iteration 52095: loss = 1.963335e-10,4.4365125e-09\n",
      "Iteration 52100: loss = 1.9632389e-10,4.4362256e-09\n",
      "Iteration 52105: loss = 1.9631684e-10,4.435932e-09\n",
      "Iteration 52110: loss = 1.9786806e-10,4.435637e-09\n",
      "Iteration 52115: loss = 1.962922e-10,4.435373e-09\n",
      "Iteration 52120: loss = 1.9627928e-10,4.4350945e-09\n",
      "Iteration 52125: loss = 1.9626918e-10,4.434809e-09\n",
      "Iteration 52130: loss = 1.9625951e-10,4.4345216e-09\n",
      "Iteration 52135: loss = 1.9624974e-10,4.4342348e-09\n",
      "Iteration 52140: loss = 1.9624008e-10,4.433947e-09\n",
      "Iteration 52145: loss = 1.9623803e-10,4.4336383e-09\n",
      "Iteration 52150: loss = 1.9622674e-10,4.4333563e-09\n",
      "Iteration 52155: loss = 1.9777907e-10,4.4330584e-09\n",
      "Iteration 52160: loss = 1.962027e-10,4.432794e-09\n",
      "Iteration 52165: loss = 1.96196e-10,4.4324984e-09\n",
      "Iteration 52170: loss = 1.9618969e-10,4.432201e-09\n",
      "Iteration 52175: loss = 1.961782e-10,4.4319184e-09\n",
      "Iteration 52180: loss = 1.9616923e-10,4.4316284e-09\n",
      "Iteration 52185: loss = 1.9615713e-10,4.4313477e-09\n",
      "Iteration 52190: loss = 1.9614833e-10,4.431057e-09\n",
      "Iteration 52195: loss = 1.9613423e-10,4.430781e-09\n",
      "Iteration 52200: loss = 1.9612006e-10,4.430507e-09\n",
      "Iteration 52205: loss = 1.9611539e-10,4.430206e-09\n",
      "Iteration 52210: loss = 1.9610912e-10,4.4299058e-09\n",
      "Iteration 52215: loss = 1.9609799e-10,4.4296224e-09\n",
      "Iteration 52220: loss = 1.9608715e-10,4.429337e-09\n",
      "Iteration 52225: loss = 1.9607803e-10,4.4290465e-09\n",
      "Iteration 52230: loss = 1.9606818e-10,4.428759e-09\n",
      "Iteration 52235: loss = 1.9605882e-10,4.4284696e-09\n",
      "Iteration 52240: loss = 1.960484e-10,4.428183e-09\n",
      "Iteration 52245: loss = 1.9603785e-10,4.4278967e-09\n",
      "Iteration 52250: loss = 1.9602599e-10,4.427615e-09\n",
      "Iteration 52255: loss = 1.960196e-10,4.4273167e-09\n",
      "Iteration 52260: loss = 1.9600656e-10,4.4270383e-09\n",
      "Iteration 52265: loss = 1.9599654e-10,4.426751e-09\n",
      "Iteration 52270: loss = 1.9599011e-10,4.426452e-09\n",
      "Iteration 52275: loss = 1.9597757e-10,4.4261714e-09\n",
      "Iteration 52280: loss = 1.9596506e-10,4.4258908e-09\n",
      "Iteration 52285: loss = 1.9595647e-10,4.4255986e-09\n",
      "Iteration 52290: loss = 1.9595033e-10,4.4252997e-09\n",
      "Iteration 52295: loss = 1.9593693e-10,4.4250217e-09\n",
      "Iteration 52300: loss = 1.9592183e-10,4.424747e-09\n",
      "Iteration 52305: loss = 1.9591127e-10,4.4244612e-09\n",
      "Iteration 52310: loss = 1.9590253e-10,4.424169e-09\n",
      "Iteration 52315: loss = 1.9435115e-10,4.423859e-09\n",
      "Iteration 52320: loss = 1.9589406e-10,4.423559e-09\n",
      "Iteration 52325: loss = 1.9588814e-10,4.4232586e-09\n",
      "Iteration 52330: loss = 1.9588388e-10,4.4229536e-09\n",
      "Iteration 52335: loss = 1.9587647e-10,4.4226565e-09\n",
      "Iteration 52340: loss = 1.9586771e-10,4.4223665e-09\n",
      "Iteration 52345: loss = 1.9585633e-10,4.42208e-09\n",
      "Iteration 52350: loss = 1.9584497e-10,4.4217963e-09\n",
      "Iteration 52355: loss = 1.9582984e-10,4.4215214e-09\n",
      "Iteration 52360: loss = 1.9581468e-10,4.421248e-09\n",
      "Iteration 52365: loss = 1.9580644e-10,4.420954e-09\n",
      "Iteration 52370: loss = 1.9579693e-10,4.420664e-09\n",
      "Iteration 52375: loss = 1.9423584e-10,4.420381e-09\n",
      "Iteration 52380: loss = 1.9578766e-10,4.4200545e-09\n",
      "Iteration 52385: loss = 1.957916e-10,4.4197255e-09\n",
      "Iteration 52390: loss = 1.9579649e-10,4.4193933e-09\n",
      "Iteration 52395: loss = 1.9580014e-10,4.4190647e-09\n",
      "Iteration 52400: loss = 1.958038e-10,4.4187365e-09\n",
      "Iteration 52405: loss = 1.9580616e-10,4.4184114e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52410: loss = 1.9580693e-10,4.4180912e-09\n",
      "Iteration 52415: loss = 1.9580704e-10,4.4177715e-09\n",
      "Iteration 52420: loss = 1.9580519e-10,4.4174584e-09\n",
      "Iteration 52425: loss = 1.9580194e-10,4.41715e-09\n",
      "Iteration 52430: loss = 1.9579662e-10,4.416848e-09\n",
      "Iteration 52435: loss = 1.9578988e-10,4.416549e-09\n",
      "Iteration 52440: loss = 1.9578282e-10,4.4162505e-09\n",
      "Iteration 52445: loss = 1.9577394e-10,4.415957e-09\n",
      "Iteration 52450: loss = 1.957648e-10,4.4156647e-09\n",
      "Iteration 52455: loss = 1.9575525e-10,4.4153734e-09\n",
      "Iteration 52460: loss = 1.9574464e-10,4.415086e-09\n",
      "Iteration 52465: loss = 1.9573299e-10,4.414801e-09\n",
      "Iteration 52470: loss = 1.9572072e-10,4.4145168e-09\n",
      "Iteration 52475: loss = 1.9570846e-10,4.414234e-09\n",
      "Iteration 52480: loss = 1.956954e-10,4.413952e-09\n",
      "Iteration 52485: loss = 1.956812e-10,4.413673e-09\n",
      "Iteration 52490: loss = 1.9566698e-10,4.413396e-09\n",
      "Iteration 52495: loss = 1.9565184e-10,4.413119e-09\n",
      "Iteration 52500: loss = 1.9563696e-10,4.4128434e-09\n",
      "Iteration 52505: loss = 1.9562174e-10,4.4125676e-09\n",
      "Iteration 52510: loss = 1.9560618e-10,4.4122914e-09\n",
      "Iteration 52515: loss = 1.9558981e-10,4.41202e-09\n",
      "Iteration 52520: loss = 1.9557306e-10,4.411749e-09\n",
      "Iteration 52525: loss = 1.9555597e-10,4.411479e-09\n",
      "Iteration 52530: loss = 1.955381e-10,4.4112096e-09\n",
      "Iteration 52535: loss = 1.9552082e-10,4.410939e-09\n",
      "Iteration 52540: loss = 1.9550327e-10,4.41067e-09\n",
      "Iteration 52545: loss = 1.9548545e-10,4.4104014e-09\n",
      "Iteration 52550: loss = 1.9546774e-10,4.4101323e-09\n",
      "Iteration 52555: loss = 1.9545006e-10,4.409864e-09\n",
      "Iteration 52560: loss = 1.9543202e-10,4.409596e-09\n",
      "Iteration 52565: loss = 1.9541276e-10,4.409331e-09\n",
      "Iteration 52570: loss = 1.9540479e-10,4.409035e-09\n",
      "Iteration 52575: loss = 1.9539786e-10,4.4087343e-09\n",
      "Iteration 52580: loss = 1.9538456e-10,4.408453e-09\n",
      "Iteration 52585: loss = 1.9537366e-10,4.4081636e-09\n",
      "Iteration 52590: loss = 1.95365e-10,4.4078687e-09\n",
      "Iteration 52595: loss = 1.9535472e-10,4.4075765e-09\n",
      "Iteration 52600: loss = 1.9534495e-10,4.4072848e-09\n",
      "Iteration 52605: loss = 1.9533396e-10,4.406996e-09\n",
      "Iteration 52610: loss = 1.9532855e-10,4.4066915e-09\n",
      "Iteration 52615: loss = 1.9532546e-10,4.40638e-09\n",
      "Iteration 52620: loss = 1.9532277e-10,4.4060675e-09\n",
      "Iteration 52625: loss = 1.9531989e-10,4.405755e-09\n",
      "Iteration 52630: loss = 1.9531737e-10,4.4054422e-09\n",
      "Iteration 52635: loss = 1.9531386e-10,4.405132e-09\n",
      "Iteration 52640: loss = 1.9530831e-10,4.404826e-09\n",
      "Iteration 52645: loss = 1.9530288e-10,4.4045203e-09\n",
      "Iteration 52650: loss = 1.9529701e-10,4.404218e-09\n",
      "Iteration 52655: loss = 1.95289e-10,4.4039186e-09\n",
      "Iteration 52660: loss = 1.9528094e-10,4.403621e-09\n",
      "Iteration 52665: loss = 1.9527252e-10,4.4033244e-09\n",
      "Iteration 52670: loss = 1.9526246e-10,4.4030326e-09\n",
      "Iteration 52675: loss = 1.9525141e-10,4.402742e-09\n",
      "Iteration 52680: loss = 1.9524064e-10,4.4024526e-09\n",
      "Iteration 52685: loss = 1.9522939e-10,4.402164e-09\n",
      "Iteration 52690: loss = 1.9521507e-10,4.4018833e-09\n",
      "Iteration 52695: loss = 1.9520034e-10,4.4016044e-09\n",
      "Iteration 52700: loss = 1.9518531e-10,4.4013264e-09\n",
      "Iteration 52705: loss = 1.9516998e-10,4.4010493e-09\n",
      "Iteration 52710: loss = 1.9515407e-10,4.400774e-09\n",
      "Iteration 52715: loss = 1.9513825e-10,4.4004977e-09\n",
      "Iteration 52720: loss = 1.9512103e-10,4.4002264e-09\n",
      "Iteration 52725: loss = 1.9510377e-10,4.399955e-09\n",
      "Iteration 52730: loss = 1.950965e-10,4.3996535e-09\n",
      "Iteration 52735: loss = 1.950927e-10,4.399343e-09\n",
      "Iteration 52740: loss = 1.9508316e-10,4.399049e-09\n",
      "Iteration 52745: loss = 1.9506913e-10,4.3987662e-09\n",
      "Iteration 52750: loss = 1.9505393e-10,4.398488e-09\n",
      "Iteration 52755: loss = 1.9349483e-10,4.398195e-09\n",
      "Iteration 52760: loss = 1.9503783e-10,4.397891e-09\n",
      "Iteration 52765: loss = 1.9502557e-10,4.3976045e-09\n",
      "Iteration 52770: loss = 1.9501595e-10,4.39731e-09\n",
      "Iteration 52775: loss = 1.9500783e-10,4.397011e-09\n",
      "Iteration 52780: loss = 1.9344691e-10,4.396722e-09\n",
      "Iteration 52785: loss = 1.9498743e-10,4.396424e-09\n",
      "Iteration 52790: loss = 1.9497949e-10,4.3961257e-09\n",
      "Iteration 52795: loss = 1.9341816e-10,4.3958384e-09\n",
      "Iteration 52800: loss = 1.9496343e-10,4.395526e-09\n",
      "Iteration 52805: loss = 1.949505e-10,4.3952406e-09\n",
      "Iteration 52810: loss = 1.9494262e-10,4.3949417e-09\n",
      "Iteration 52815: loss = 1.9494033e-10,4.394625e-09\n",
      "Iteration 52820: loss = 1.949259e-10,4.3943436e-09\n",
      "Iteration 52825: loss = 1.9336631e-10,4.3940505e-09\n",
      "Iteration 52830: loss = 1.9492215e-10,4.3937085e-09\n",
      "Iteration 52835: loss = 1.9492531e-10,4.393376e-09\n",
      "Iteration 52840: loss = 1.9492848e-10,4.393044e-09\n",
      "Iteration 52845: loss = 1.9338169e-10,4.3927137e-09\n",
      "Iteration 52850: loss = 1.9338435e-10,4.3923833e-09\n",
      "Iteration 52855: loss = 1.9494008e-10,4.3920405e-09\n",
      "Iteration 52860: loss = 1.9338882e-10,4.3917243e-09\n",
      "Iteration 52865: loss = 1.9494495e-10,4.3913797e-09\n",
      "Iteration 52870: loss = 1.9494838e-10,4.3910466e-09\n",
      "Iteration 52875: loss = 1.9340209e-10,4.3907153e-09\n",
      "Iteration 52880: loss = 1.9340467e-10,4.390385e-09\n",
      "Iteration 52885: loss = 1.9340823e-10,4.3900505e-09\n",
      "Iteration 52890: loss = 1.934119e-10,4.3897157e-09\n",
      "Iteration 52895: loss = 1.9341478e-10,4.389384e-09\n",
      "Iteration 52900: loss = 1.9341863e-10,4.3890496e-09\n",
      "Iteration 52905: loss = 1.9342229e-10,4.3887143e-09\n",
      "Iteration 52910: loss = 1.9342554e-10,4.388382e-09\n",
      "Iteration 52915: loss = 1.9342888e-10,4.3880477e-09\n",
      "Iteration 52920: loss = 1.934321e-10,4.387715e-09\n",
      "Iteration 52925: loss = 1.9343406e-10,4.3873856e-09\n",
      "Iteration 52930: loss = 1.9343845e-10,4.387049e-09\n",
      "Iteration 52935: loss = 1.9499304e-10,4.3867083e-09\n",
      "Iteration 52940: loss = 1.934438e-10,4.386384e-09\n",
      "Iteration 52945: loss = 1.9344791e-10,4.386048e-09\n",
      "Iteration 52950: loss = 1.9345144e-10,4.3857136e-09\n",
      "Iteration 52955: loss = 1.9345596e-10,4.3853774e-09\n",
      "Iteration 52960: loss = 1.9345832e-10,4.3850443e-09\n",
      "Iteration 52965: loss = 1.9346207e-10,4.384709e-09\n",
      "Iteration 52970: loss = 1.9346551e-10,4.384374e-09\n",
      "Iteration 52975: loss = 1.9346846e-10,4.3840416e-09\n",
      "Iteration 52980: loss = 1.9347372e-10,4.383702e-09\n",
      "Iteration 52985: loss = 1.9347325e-10,4.383379e-09\n",
      "Iteration 52990: loss = 1.9347778e-10,4.383041e-09\n",
      "Iteration 52995: loss = 1.9348323e-10,4.3827004e-09\n",
      "Iteration 53000: loss = 1.9348291e-10,4.3823762e-09\n",
      "Iteration 53005: loss = 1.9349068e-10,4.38203e-09\n",
      "Iteration 53010: loss = 1.9349417e-10,4.3816946e-09\n",
      "Iteration 53015: loss = 1.9349311e-10,4.381373e-09\n",
      "Iteration 53020: loss = 1.9349751e-10,4.3810346e-09\n",
      "Iteration 53025: loss = 1.935039e-10,4.3806914e-09\n",
      "Iteration 53030: loss = 1.935062e-10,4.38036e-09\n",
      "Iteration 53035: loss = 1.9350847e-10,4.380027e-09\n",
      "Iteration 53040: loss = 1.9351237e-10,4.3796904e-09\n",
      "Iteration 53045: loss = 1.9351616e-10,4.3793538e-09\n",
      "Iteration 53050: loss = 1.9352021e-10,4.379017e-09\n",
      "Iteration 53055: loss = 1.9352374e-10,4.3786814e-09\n",
      "Iteration 53060: loss = 1.9352575e-10,4.3783484e-09\n",
      "Iteration 53065: loss = 1.9352725e-10,4.3780197e-09\n",
      "Iteration 53070: loss = 1.9352608e-10,4.377698e-09\n",
      "Iteration 53075: loss = 1.935226e-10,4.377382e-09\n",
      "Iteration 53080: loss = 1.9351896e-10,4.3770667e-09\n",
      "Iteration 53085: loss = 1.935137e-10,4.376756e-09\n",
      "Iteration 53090: loss = 1.9350749e-10,4.3764476e-09\n",
      "Iteration 53095: loss = 1.9350095e-10,4.376141e-09\n",
      "Iteration 53100: loss = 1.9350177e-10,4.3758117e-09\n",
      "Iteration 53105: loss = 1.9350822e-10,4.3754675e-09\n",
      "Iteration 53110: loss = 1.9351713e-10,4.375117e-09\n",
      "Iteration 53115: loss = 1.9351888e-10,4.374785e-09\n",
      "Iteration 53120: loss = 1.9351827e-10,4.37446e-09\n",
      "Iteration 53125: loss = 1.9352152e-10,4.374125e-09\n",
      "Iteration 53130: loss = 1.9352643e-10,4.3737844e-09\n",
      "Iteration 53135: loss = 1.9353125e-10,4.3734443e-09\n",
      "Iteration 53140: loss = 1.9353584e-10,4.3731045e-09\n",
      "Iteration 53145: loss = 1.9353923e-10,4.3727675e-09\n",
      "Iteration 53150: loss = 1.935419e-10,4.3724353e-09\n",
      "Iteration 53155: loss = 1.9354297e-10,4.3721045e-09\n",
      "Iteration 53160: loss = 1.9354275e-10,4.3717785e-09\n",
      "Iteration 53165: loss = 1.9354174e-10,4.371454e-09\n",
      "Iteration 53170: loss = 1.9353781e-10,4.371138e-09\n",
      "Iteration 53175: loss = 1.9353258e-10,4.370826e-09\n",
      "Iteration 53180: loss = 1.9352593e-10,4.3705186e-09\n",
      "Iteration 53185: loss = 1.9351859e-10,4.3702144e-09\n",
      "Iteration 53190: loss = 1.9351025e-10,4.3699098e-09\n",
      "Iteration 53195: loss = 1.9350149e-10,4.3696096e-09\n",
      "Iteration 53200: loss = 1.9349122e-10,4.369311e-09\n",
      "Iteration 53205: loss = 1.9348094e-10,4.369015e-09\n",
      "Iteration 53210: loss = 1.9346962e-10,4.3687183e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53215: loss = 1.9345751e-10,4.368426e-09\n",
      "Iteration 53220: loss = 1.9344454e-10,4.3681356e-09\n",
      "Iteration 53225: loss = 1.9343131e-10,4.367846e-09\n",
      "Iteration 53230: loss = 1.9341774e-10,4.367558e-09\n",
      "Iteration 53235: loss = 1.9340411e-10,4.367269e-09\n",
      "Iteration 53240: loss = 1.9338946e-10,4.366984e-09\n",
      "Iteration 53245: loss = 1.933737e-10,4.3667003e-09\n",
      "Iteration 53250: loss = 1.9335822e-10,4.366418e-09\n",
      "Iteration 53255: loss = 1.9334227e-10,4.3661355e-09\n",
      "Iteration 53260: loss = 1.9332623e-10,4.3658543e-09\n",
      "Iteration 53265: loss = 1.9330972e-10,4.365572e-09\n",
      "Iteration 53270: loss = 1.9329328e-10,4.3652912e-09\n",
      "Iteration 53275: loss = 1.9327666e-10,4.3650124e-09\n",
      "Iteration 53280: loss = 1.9325853e-10,4.364735e-09\n",
      "Iteration 53285: loss = 1.9324047e-10,4.364459e-09\n",
      "Iteration 53290: loss = 1.9322192e-10,4.3641832e-09\n",
      "Iteration 53295: loss = 1.9320351e-10,4.363907e-09\n",
      "Iteration 53300: loss = 1.931853e-10,4.3636312e-09\n",
      "Iteration 53305: loss = 1.9316694e-10,4.363356e-09\n",
      "Iteration 53310: loss = 1.931485e-10,4.3630806e-09\n",
      "Iteration 53315: loss = 1.9312962e-10,4.362806e-09\n",
      "Iteration 53320: loss = 1.9311101e-10,4.362531e-09\n",
      "Iteration 53325: loss = 1.9309228e-10,4.3622572e-09\n",
      "Iteration 53330: loss = 1.9307352e-10,4.361982e-09\n",
      "Iteration 53335: loss = 1.9305413e-10,4.3617083e-09\n",
      "Iteration 53340: loss = 1.9304296e-10,4.3614112e-09\n",
      "Iteration 53345: loss = 1.9303865e-10,4.3610946e-09\n",
      "Iteration 53350: loss = 1.930376e-10,4.3607704e-09\n",
      "Iteration 53355: loss = 1.9303743e-10,4.360441e-09\n",
      "Iteration 53360: loss = 1.9303902e-10,4.360108e-09\n",
      "Iteration 53365: loss = 1.9303863e-10,4.35978e-09\n",
      "Iteration 53370: loss = 1.9303821e-10,4.3594532e-09\n",
      "Iteration 53375: loss = 1.9303653e-10,4.359128e-09\n",
      "Iteration 53380: loss = 1.9303394e-10,4.3588066e-09\n",
      "Iteration 53385: loss = 1.9303044e-10,4.3584882e-09\n",
      "Iteration 53390: loss = 1.9302604e-10,4.358171e-09\n",
      "Iteration 53395: loss = 1.9301995e-10,4.357859e-09\n",
      "Iteration 53400: loss = 1.9301312e-10,4.3575494e-09\n",
      "Iteration 53405: loss = 1.9300574e-10,4.357241e-09\n",
      "Iteration 53410: loss = 1.9299691e-10,4.3569366e-09\n",
      "Iteration 53415: loss = 1.929878e-10,4.356634e-09\n",
      "Iteration 53420: loss = 1.9297756e-10,4.3563317e-09\n",
      "Iteration 53425: loss = 1.9296664e-10,4.3560338e-09\n",
      "Iteration 53430: loss = 1.9295514e-10,4.355737e-09\n",
      "Iteration 53435: loss = 1.9294279e-10,4.3554422e-09\n",
      "Iteration 53440: loss = 1.9292971e-10,4.355151e-09\n",
      "Iteration 53445: loss = 1.9291511e-10,4.354863e-09\n",
      "Iteration 53450: loss = 1.9289918e-10,4.3545785e-09\n",
      "Iteration 53455: loss = 1.9288322e-10,4.354295e-09\n",
      "Iteration 53460: loss = 1.9286678e-10,4.3540123e-09\n",
      "Iteration 53465: loss = 1.9285033e-10,4.35373e-09\n",
      "Iteration 53470: loss = 1.928337e-10,4.353448e-09\n",
      "Iteration 53475: loss = 1.9281719e-10,4.3531663e-09\n",
      "Iteration 53480: loss = 1.9280023e-10,4.3528843e-09\n",
      "Iteration 53485: loss = 1.9278212e-10,4.352606e-09\n",
      "Iteration 53490: loss = 1.9276364e-10,4.352329e-09\n",
      "Iteration 53495: loss = 1.9274525e-10,4.352052e-09\n",
      "Iteration 53500: loss = 1.9272672e-10,4.351775e-09\n",
      "Iteration 53505: loss = 1.9270813e-10,4.351498e-09\n",
      "Iteration 53510: loss = 1.926894e-10,4.351221e-09\n",
      "Iteration 53515: loss = 1.9267077e-10,4.3509445e-09\n",
      "Iteration 53520: loss = 1.92652e-10,4.3506687e-09\n",
      "Iteration 53525: loss = 1.9263298e-10,4.3503925e-09\n",
      "Iteration 53530: loss = 1.9261413e-10,4.3501163e-09\n",
      "Iteration 53535: loss = 1.9259501e-10,4.349841e-09\n",
      "Iteration 53540: loss = 1.9257738e-10,4.3495607e-09\n",
      "Iteration 53545: loss = 1.9256934e-10,4.349253e-09\n",
      "Iteration 53550: loss = 1.9256703e-10,4.3489274e-09\n",
      "Iteration 53555: loss = 1.9256734e-10,4.3485953e-09\n",
      "Iteration 53560: loss = 1.9256959e-10,4.3482586e-09\n",
      "Iteration 53565: loss = 1.9257179e-10,4.3479202e-09\n",
      "Iteration 53570: loss = 1.925724e-10,4.3475867e-09\n",
      "Iteration 53575: loss = 1.9257286e-10,4.347255e-09\n",
      "Iteration 53580: loss = 1.9257741e-10,4.34691e-09\n",
      "Iteration 53585: loss = 1.9258428e-10,4.3465573e-09\n",
      "Iteration 53590: loss = 1.9258554e-10,4.3462225e-09\n",
      "Iteration 53595: loss = 1.9258621e-10,4.345889e-09\n",
      "Iteration 53600: loss = 1.9259148e-10,4.3455417e-09\n",
      "Iteration 53605: loss = 1.9259282e-10,4.3452064e-09\n",
      "Iteration 53610: loss = 1.9259694e-10,4.344863e-09\n",
      "Iteration 53615: loss = 1.9260109e-10,4.3445185e-09\n",
      "Iteration 53620: loss = 1.9260295e-10,4.3441797e-09\n",
      "Iteration 53625: loss = 1.92606e-10,4.34384e-09\n",
      "Iteration 53630: loss = 1.9261615e-10,4.343477e-09\n",
      "Iteration 53635: loss = 1.9261905e-10,4.3431365e-09\n",
      "Iteration 53640: loss = 1.9107495e-10,4.342818e-09\n",
      "Iteration 53645: loss = 1.9263023e-10,4.342441e-09\n",
      "Iteration 53650: loss = 1.9263097e-10,4.3421062e-09\n",
      "Iteration 53655: loss = 1.9262748e-10,4.341784e-09\n",
      "Iteration 53660: loss = 1.9263109e-10,4.341441e-09\n",
      "Iteration 53665: loss = 1.9263736e-10,4.34109e-09\n",
      "Iteration 53670: loss = 1.926418e-10,4.340745e-09\n",
      "Iteration 53675: loss = 1.9264267e-10,4.3404094e-09\n",
      "Iteration 53680: loss = 1.9264688e-10,4.3400648e-09\n",
      "Iteration 53685: loss = 1.9110853e-10,4.339729e-09\n",
      "Iteration 53690: loss = 1.9265295e-10,4.339381e-09\n",
      "Iteration 53695: loss = 1.9265828e-10,4.3390327e-09\n",
      "Iteration 53700: loss = 1.9266151e-10,4.33869e-09\n",
      "Iteration 53705: loss = 1.9266355e-10,4.338351e-09\n",
      "Iteration 53710: loss = 1.926679e-10,4.338006e-09\n",
      "Iteration 53715: loss = 1.9112933e-10,4.3376702e-09\n",
      "Iteration 53720: loss = 1.9267722e-10,4.337312e-09\n",
      "Iteration 53725: loss = 1.91134e-10,4.33699e-09\n",
      "Iteration 53730: loss = 1.911407e-10,4.336637e-09\n",
      "Iteration 53735: loss = 1.9114489e-10,4.3362913e-09\n",
      "Iteration 53740: loss = 1.9268948e-10,4.335944e-09\n",
      "Iteration 53745: loss = 1.9115182e-10,4.3356043e-09\n",
      "Iteration 53750: loss = 1.9115515e-10,4.335262e-09\n",
      "Iteration 53755: loss = 1.9270059e-10,4.334911e-09\n",
      "Iteration 53760: loss = 1.9116196e-10,4.334575e-09\n",
      "Iteration 53765: loss = 1.9116612e-10,4.334229e-09\n",
      "Iteration 53770: loss = 1.9116943e-10,4.3338866e-09\n",
      "Iteration 53775: loss = 1.9117152e-10,4.333546e-09\n",
      "Iteration 53780: loss = 1.927186e-10,4.333189e-09\n",
      "Iteration 53785: loss = 1.9117924e-10,4.3328545e-09\n",
      "Iteration 53790: loss = 1.9118511e-10,4.332505e-09\n",
      "Iteration 53795: loss = 1.927289e-10,4.3321586e-09\n",
      "Iteration 53800: loss = 1.9119245e-10,4.331816e-09\n",
      "Iteration 53805: loss = 1.9119045e-10,4.331487e-09\n",
      "Iteration 53810: loss = 1.9119589e-10,4.3311377e-09\n",
      "Iteration 53815: loss = 1.9120298e-10,4.330783e-09\n",
      "Iteration 53820: loss = 1.9120329e-10,4.330447e-09\n",
      "Iteration 53825: loss = 1.9275076e-10,4.330091e-09\n",
      "Iteration 53830: loss = 1.9120551e-10,4.3297717e-09\n",
      "Iteration 53835: loss = 1.9121112e-10,4.329421e-09\n",
      "Iteration 53840: loss = 1.9276263e-10,4.3290527e-09\n",
      "Iteration 53845: loss = 1.9276564e-10,4.328709e-09\n",
      "Iteration 53850: loss = 1.9122326e-10,4.328382e-09\n",
      "Iteration 53855: loss = 1.9122785e-10,4.3280344e-09\n",
      "Iteration 53860: loss = 1.9123547e-10,4.3276787e-09\n",
      "Iteration 53865: loss = 1.9123685e-10,4.3273394e-09\n",
      "Iteration 53870: loss = 1.9123854e-10,4.3269996e-09\n",
      "Iteration 53875: loss = 1.912422e-10,4.326654e-09\n",
      "Iteration 53880: loss = 1.9124752e-10,4.3263038e-09\n",
      "Iteration 53885: loss = 1.9125239e-10,4.3259547e-09\n",
      "Iteration 53890: loss = 1.9125601e-10,4.3256096e-09\n",
      "Iteration 53895: loss = 1.9125879e-10,4.3252655e-09\n",
      "Iteration 53900: loss = 1.9126084e-10,4.3249258e-09\n",
      "Iteration 53905: loss = 1.9126149e-10,4.3245887e-09\n",
      "Iteration 53910: loss = 1.9126105e-10,4.324255e-09\n",
      "Iteration 53915: loss = 1.9125847e-10,4.3239257e-09\n",
      "Iteration 53920: loss = 1.9125379e-10,4.3236033e-09\n",
      "Iteration 53925: loss = 1.9124735e-10,4.3232857e-09\n",
      "Iteration 53930: loss = 1.9124047e-10,4.322971e-09\n",
      "Iteration 53935: loss = 1.912327e-10,4.322658e-09\n",
      "Iteration 53940: loss = 1.912233e-10,4.3223483e-09\n",
      "Iteration 53945: loss = 1.9121392e-10,4.3220396e-09\n",
      "Iteration 53950: loss = 1.91204e-10,4.321733e-09\n",
      "Iteration 53955: loss = 1.9119263e-10,4.32143e-09\n",
      "Iteration 53960: loss = 1.9118056e-10,4.321128e-09\n",
      "Iteration 53965: loss = 1.9116829e-10,4.3208273e-09\n",
      "Iteration 53970: loss = 1.91156e-10,4.320527e-09\n",
      "Iteration 53975: loss = 1.9114248e-10,4.3202286e-09\n",
      "Iteration 53980: loss = 1.9112807e-10,4.319934e-09\n",
      "Iteration 53985: loss = 1.9111317e-10,4.319641e-09\n",
      "Iteration 53990: loss = 1.910983e-10,4.319347e-09\n",
      "Iteration 53995: loss = 1.9108327e-10,4.319054e-09\n",
      "Iteration 54000: loss = 1.910676e-10,4.3187613e-09\n",
      "Iteration 54005: loss = 1.9105147e-10,4.318471e-09\n",
      "Iteration 54010: loss = 1.9103463e-10,4.318184e-09\n",
      "Iteration 54015: loss = 1.9101716e-10,4.317897e-09\n",
      "Iteration 54020: loss = 1.9099959e-10,4.3176103e-09\n",
      "Iteration 54025: loss = 1.9098212e-10,4.3173243e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54030: loss = 1.9096469e-10,4.317037e-09\n",
      "Iteration 54035: loss = 1.9094683e-10,4.316751e-09\n",
      "Iteration 54040: loss = 1.9092915e-10,4.316465e-09\n",
      "Iteration 54045: loss = 1.9091102e-10,4.3161794e-09\n",
      "Iteration 54050: loss = 1.908929e-10,4.3158948e-09\n",
      "Iteration 54055: loss = 1.9087339e-10,4.3156136e-09\n",
      "Iteration 54060: loss = 1.9085389e-10,4.3153316e-09\n",
      "Iteration 54065: loss = 1.9083442e-10,4.3150505e-09\n",
      "Iteration 54070: loss = 1.9081464e-10,4.3147703e-09\n",
      "Iteration 54075: loss = 1.9079947e-10,4.3144768e-09\n",
      "Iteration 54080: loss = 1.9079348e-10,4.314156e-09\n",
      "Iteration 54085: loss = 1.9079198e-10,4.313824e-09\n",
      "Iteration 54090: loss = 1.9079167e-10,4.313487e-09\n",
      "Iteration 54095: loss = 1.9079167e-10,4.313149e-09\n",
      "Iteration 54100: loss = 1.9079238e-10,4.312811e-09\n",
      "Iteration 54105: loss = 1.9079209e-10,4.3124744e-09\n",
      "Iteration 54110: loss = 1.9079156e-10,4.312138e-09\n",
      "Iteration 54115: loss = 1.907889e-10,4.311808e-09\n",
      "Iteration 54120: loss = 1.9078573e-10,4.3114783e-09\n",
      "Iteration 54125: loss = 1.9078207e-10,4.3111528e-09\n",
      "Iteration 54130: loss = 1.9077634e-10,4.3108304e-09\n",
      "Iteration 54135: loss = 1.9077066e-10,4.3105093e-09\n",
      "Iteration 54140: loss = 1.9076322e-10,4.310192e-09\n",
      "Iteration 54145: loss = 1.9075536e-10,4.309877e-09\n",
      "Iteration 54150: loss = 1.9074649e-10,4.309564e-09\n",
      "Iteration 54155: loss = 1.9073713e-10,4.309254e-09\n",
      "Iteration 54160: loss = 1.907261e-10,4.3089456e-09\n",
      "Iteration 54165: loss = 1.9071494e-10,4.3086397e-09\n",
      "Iteration 54170: loss = 1.907035e-10,4.308335e-09\n",
      "Iteration 54175: loss = 1.9068924e-10,4.3080366e-09\n",
      "Iteration 54180: loss = 1.9067424e-10,4.3077417e-09\n",
      "Iteration 54185: loss = 1.9065886e-10,4.3074473e-09\n",
      "Iteration 54190: loss = 1.906435e-10,4.3071524e-09\n",
      "Iteration 54195: loss = 1.906279e-10,4.3068593e-09\n",
      "Iteration 54200: loss = 1.9061212e-10,4.3065658e-09\n",
      "Iteration 54205: loss = 1.9059564e-10,4.3062744e-09\n",
      "Iteration 54210: loss = 1.9057801e-10,4.3059867e-09\n",
      "Iteration 54215: loss = 1.9056068e-10,4.3056985e-09\n",
      "Iteration 54220: loss = 1.9054303e-10,4.305411e-09\n",
      "Iteration 54225: loss = 1.9052515e-10,4.3051234e-09\n",
      "Iteration 54230: loss = 1.9050699e-10,4.3048365e-09\n",
      "Iteration 54235: loss = 1.9048911e-10,4.304549e-09\n",
      "Iteration 54240: loss = 1.9047082e-10,4.3042627e-09\n",
      "Iteration 54245: loss = 1.9045239e-10,4.303976e-09\n",
      "Iteration 54250: loss = 1.9043435e-10,4.3036894e-09\n",
      "Iteration 54255: loss = 1.9041596e-10,4.303404e-09\n",
      "Iteration 54260: loss = 1.9039605e-10,4.303121e-09\n",
      "Iteration 54265: loss = 1.903761e-10,4.302839e-09\n",
      "Iteration 54270: loss = 1.903563e-10,4.3025565e-09\n",
      "Iteration 54275: loss = 1.9033651e-10,4.302275e-09\n",
      "Iteration 54280: loss = 1.9032419e-10,4.3019703e-09\n",
      "Iteration 54285: loss = 1.9031994e-10,4.3016435e-09\n",
      "Iteration 54290: loss = 1.9031952e-10,4.3013046e-09\n",
      "Iteration 54295: loss = 1.903207e-10,4.3009614e-09\n",
      "Iteration 54300: loss = 1.9032177e-10,4.3006176e-09\n",
      "Iteration 54305: loss = 1.9032294e-10,4.3002744e-09\n",
      "Iteration 54310: loss = 1.9032372e-10,4.299932e-09\n",
      "Iteration 54315: loss = 1.9032365e-10,4.2995927e-09\n",
      "Iteration 54320: loss = 1.9032188e-10,4.299257e-09\n",
      "Iteration 54325: loss = 1.9031987e-10,4.2989226e-09\n",
      "Iteration 54330: loss = 1.9031582e-10,4.298593e-09\n",
      "Iteration 54335: loss = 1.9031138e-10,4.298266e-09\n",
      "Iteration 54340: loss = 1.9030577e-10,4.297941e-09\n",
      "Iteration 54345: loss = 1.9029739e-10,4.2976245e-09\n",
      "Iteration 54350: loss = 1.902885e-10,4.297311e-09\n",
      "Iteration 54355: loss = 1.9027886e-10,4.296998e-09\n",
      "Iteration 54360: loss = 1.9028536e-10,4.296638e-09\n",
      "Iteration 54365: loss = 1.9029306e-10,4.296276e-09\n",
      "Iteration 54370: loss = 1.9029134e-10,4.2959396e-09\n",
      "Iteration 54375: loss = 1.9029595e-10,4.295586e-09\n",
      "Iteration 54380: loss = 1.9030483e-10,4.2952184e-09\n",
      "Iteration 54385: loss = 1.9030172e-10,4.2948853e-09\n",
      "Iteration 54390: loss = 1.9030506e-10,4.294536e-09\n",
      "Iteration 54395: loss = 1.90311e-10,4.2941783e-09\n",
      "Iteration 54400: loss = 1.903117e-10,4.2938333e-09\n",
      "Iteration 54405: loss = 1.903188e-10,4.2934722e-09\n",
      "Iteration 54410: loss = 1.9032069e-10,4.2931263e-09\n",
      "Iteration 54415: loss = 1.903223e-10,4.2927804e-09\n",
      "Iteration 54420: loss = 1.9032825e-10,4.2924224e-09\n",
      "Iteration 54425: loss = 1.8879807e-10,4.29208e-09\n",
      "Iteration 54430: loss = 1.903343e-10,4.291721e-09\n",
      "Iteration 54435: loss = 1.9034012e-10,4.2913624e-09\n",
      "Iteration 54440: loss = 1.8880895e-10,4.2910235e-09\n",
      "Iteration 54445: loss = 1.9034498e-10,4.290665e-09\n",
      "Iteration 54450: loss = 1.9034979e-10,4.2903108e-09\n",
      "Iteration 54455: loss = 1.8881989e-10,4.2899666e-09\n",
      "Iteration 54460: loss = 1.8882416e-10,4.289613e-09\n",
      "Iteration 54465: loss = 1.9035933e-10,4.289257e-09\n",
      "Iteration 54470: loss = 1.903638e-10,4.2889017e-09\n",
      "Iteration 54475: loss = 1.8883528e-10,4.288555e-09\n",
      "Iteration 54480: loss = 1.8883838e-10,4.2882045e-09\n",
      "Iteration 54485: loss = 1.9037492e-10,4.287844e-09\n",
      "Iteration 54490: loss = 1.8884662e-10,4.2874957e-09\n",
      "Iteration 54495: loss = 1.8885032e-10,4.287143e-09\n",
      "Iteration 54500: loss = 1.8885354e-10,4.2867923e-09\n",
      "Iteration 54505: loss = 1.8885754e-10,4.2864383e-09\n",
      "Iteration 54510: loss = 1.9039272e-10,4.2860813e-09\n",
      "Iteration 54515: loss = 1.8886509e-10,4.2857313e-09\n",
      "Iteration 54520: loss = 1.8886696e-10,4.285384e-09\n",
      "Iteration 54525: loss = 1.9040597e-10,4.2850155e-09\n",
      "Iteration 54530: loss = 1.8887471e-10,4.284676e-09\n",
      "Iteration 54535: loss = 1.9041312e-10,4.2843085e-09\n",
      "Iteration 54540: loss = 1.8887995e-10,4.283976e-09\n",
      "Iteration 54545: loss = 1.9042092e-10,4.283602e-09\n",
      "Iteration 54550: loss = 1.8888786e-10,4.2832666e-09\n",
      "Iteration 54555: loss = 1.8889514e-10,4.2829034e-09\n",
      "Iteration 54560: loss = 1.8889786e-10,4.2825525e-09\n",
      "Iteration 54565: loss = 1.9043399e-10,4.2821915e-09\n",
      "Iteration 54570: loss = 1.8890312e-10,4.2818513e-09\n",
      "Iteration 54575: loss = 1.9044233e-10,4.281482e-09\n",
      "Iteration 54580: loss = 1.8891072e-10,4.2811426e-09\n",
      "Iteration 54585: loss = 1.8891662e-10,4.280782e-09\n",
      "Iteration 54590: loss = 1.8892259e-10,4.280423e-09\n",
      "Iteration 54595: loss = 1.8891273e-10,4.280107e-09\n",
      "Iteration 54600: loss = 1.8890474e-10,4.2797863e-09\n",
      "Iteration 54605: loss = 1.8889333e-10,4.279476e-09\n",
      "Iteration 54610: loss = 1.9041606e-10,4.279154e-09\n",
      "Iteration 54615: loss = 1.8887196e-10,4.2788497e-09\n",
      "Iteration 54620: loss = 1.9039542e-10,4.2785246e-09\n",
      "Iteration 54625: loss = 1.88853e-10,4.2782173e-09\n",
      "Iteration 54630: loss = 1.8884455e-10,4.277897e-09\n",
      "Iteration 54635: loss = 1.888335e-10,4.277584e-09\n",
      "Iteration 54640: loss = 1.8882289e-10,4.2772714e-09\n",
      "Iteration 54645: loss = 1.8881251e-10,4.2769566e-09\n",
      "Iteration 54650: loss = 1.8879913e-10,4.276653e-09\n",
      "Iteration 54655: loss = 1.9032413e-10,4.2763215e-09\n",
      "Iteration 54660: loss = 1.8878164e-10,4.276013e-09\n",
      "Iteration 54665: loss = 1.8877257e-10,4.2756967e-09\n",
      "Iteration 54670: loss = 1.8875991e-10,4.275388e-09\n",
      "Iteration 54675: loss = 1.887468e-10,4.2750807e-09\n",
      "Iteration 54680: loss = 1.8874101e-10,4.274753e-09\n",
      "Iteration 54685: loss = 1.8872974e-10,4.2744417e-09\n",
      "Iteration 54690: loss = 1.8871658e-10,4.274135e-09\n",
      "Iteration 54695: loss = 1.8870949e-10,4.2738098e-09\n",
      "Iteration 54700: loss = 1.8870254e-10,4.2734856e-09\n",
      "Iteration 54705: loss = 1.902236e-10,4.2731667e-09\n",
      "Iteration 54710: loss = 1.8867974e-10,4.272862e-09\n",
      "Iteration 54715: loss = 1.8867073e-10,4.272543e-09\n",
      "Iteration 54720: loss = 1.886614e-10,4.272224e-09\n",
      "Iteration 54725: loss = 1.8865039e-10,4.2719117e-09\n",
      "Iteration 54730: loss = 1.8864182e-10,4.271591e-09\n",
      "Iteration 54735: loss = 1.8863015e-10,4.2712793e-09\n",
      "Iteration 54740: loss = 1.8861813e-10,4.2709685e-09\n",
      "Iteration 54745: loss = 1.886089e-10,4.27065e-09\n",
      "Iteration 54750: loss = 1.8860082e-10,4.270328e-09\n",
      "Iteration 54755: loss = 1.8859214e-10,4.2700083e-09\n",
      "Iteration 54760: loss = 1.8857706e-10,4.2697064e-09\n",
      "Iteration 54765: loss = 1.8856479e-10,4.2693955e-09\n",
      "Iteration 54770: loss = 1.8855352e-10,4.269082e-09\n",
      "Iteration 54775: loss = 1.8854883e-10,4.268751e-09\n",
      "Iteration 54780: loss = 1.9007627e-10,4.268413e-09\n",
      "Iteration 54785: loss = 1.8853079e-10,4.268112e-09\n",
      "Iteration 54790: loss = 1.8852371e-10,4.2677875e-09\n",
      "Iteration 54795: loss = 1.8851291e-10,4.267471e-09\n",
      "Iteration 54800: loss = 1.8850121e-10,4.26716e-09\n",
      "Iteration 54805: loss = 1.8849124e-10,4.266842e-09\n",
      "Iteration 54810: loss = 1.9001438e-10,4.2665165e-09\n",
      "Iteration 54815: loss = 1.8847367e-10,4.266202e-09\n",
      "Iteration 54820: loss = 1.8846302e-10,4.2658854e-09\n",
      "Iteration 54825: loss = 1.8845243e-10,4.2655697e-09\n",
      "Iteration 54830: loss = 1.8844247e-10,4.2652517e-09\n",
      "Iteration 54835: loss = 1.8843303e-10,4.2649337e-09\n",
      "Iteration 54840: loss = 1.8842307e-10,4.264615e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54845: loss = 1.8841385e-10,4.2642965e-09\n",
      "Iteration 54850: loss = 1.8840295e-10,4.2639816e-09\n",
      "Iteration 54855: loss = 1.8839315e-10,4.2636623e-09\n",
      "Iteration 54860: loss = 1.883847e-10,4.26334e-09\n",
      "Iteration 54865: loss = 1.8837344e-10,4.2630264e-09\n",
      "Iteration 54870: loss = 1.8836459e-10,4.2627044e-09\n",
      "Iteration 54875: loss = 1.8835344e-10,4.26239e-09\n",
      "Iteration 54880: loss = 1.8834334e-10,4.2620725e-09\n",
      "Iteration 54885: loss = 1.8833302e-10,4.2617554e-09\n",
      "Iteration 54890: loss = 1.8832481e-10,4.2614325e-09\n",
      "Iteration 54895: loss = 1.8831282e-10,4.26112e-09\n",
      "Iteration 54900: loss = 1.883052e-10,4.260795e-09\n",
      "Iteration 54905: loss = 1.8829331e-10,4.2604817e-09\n",
      "Iteration 54910: loss = 1.8828195e-10,4.2601664e-09\n",
      "Iteration 54915: loss = 1.8827372e-10,4.2598423e-09\n",
      "Iteration 54920: loss = 1.897969e-10,4.259516e-09\n",
      "Iteration 54925: loss = 1.882509e-10,4.2592143e-09\n",
      "Iteration 54930: loss = 1.8824027e-10,4.2588972e-09\n",
      "Iteration 54935: loss = 1.882305e-10,4.258579e-09\n",
      "Iteration 54940: loss = 1.882217e-10,4.258255e-09\n",
      "Iteration 54945: loss = 1.8821339e-10,4.2579322e-09\n",
      "Iteration 54950: loss = 1.8820351e-10,4.257612e-09\n",
      "Iteration 54955: loss = 1.8819397e-10,4.2572923e-09\n",
      "Iteration 54960: loss = 1.8818426e-10,4.256973e-09\n",
      "Iteration 54965: loss = 1.8817409e-10,4.256654e-09\n",
      "Iteration 54970: loss = 1.8816311e-10,4.2563375e-09\n",
      "Iteration 54975: loss = 1.8815127e-10,4.256024e-09\n",
      "Iteration 54980: loss = 1.881388e-10,4.2557113e-09\n",
      "Iteration 54985: loss = 1.8812607e-10,4.2554e-09\n",
      "Iteration 54990: loss = 1.881131e-10,4.2550896e-09\n",
      "Iteration 54995: loss = 1.8809886e-10,4.254782e-09\n",
      "Iteration 55000: loss = 1.8808659e-10,4.254468e-09\n",
      "Iteration 55005: loss = 1.8808326e-10,4.25413e-09\n",
      "Iteration 55010: loss = 1.8808505e-10,4.2537764e-09\n",
      "Iteration 55015: loss = 1.8961276e-10,4.253435e-09\n",
      "Iteration 55020: loss = 1.8806406e-10,4.2531405e-09\n",
      "Iteration 55025: loss = 1.8805496e-10,4.252818e-09\n",
      "Iteration 55030: loss = 1.8958213e-10,4.2524775e-09\n",
      "Iteration 55035: loss = 1.8803563e-10,4.252176e-09\n",
      "Iteration 55040: loss = 1.8802586e-10,4.2518558e-09\n",
      "Iteration 55045: loss = 1.8802147e-10,4.2515187e-09\n",
      "Iteration 55050: loss = 1.880064e-10,4.251214e-09\n",
      "Iteration 55055: loss = 1.87994e-10,4.250901e-09\n",
      "Iteration 55060: loss = 1.8798618e-10,4.250574e-09\n",
      "Iteration 55065: loss = 1.8798242e-10,4.2502357e-09\n",
      "Iteration 55070: loss = 1.8797192e-10,4.249917e-09\n",
      "Iteration 55075: loss = 1.8796692e-10,4.2495816e-09\n",
      "Iteration 55080: loss = 1.8796732e-10,4.249231e-09\n",
      "Iteration 55085: loss = 1.879663e-10,4.248886e-09\n",
      "Iteration 55090: loss = 1.8796281e-10,4.248546e-09\n",
      "Iteration 55095: loss = 1.8795872e-10,4.248209e-09\n",
      "Iteration 55100: loss = 1.8795272e-10,4.247876e-09\n",
      "Iteration 55105: loss = 1.879446e-10,4.2475494e-09\n",
      "Iteration 55110: loss = 1.8793571e-10,4.2472266e-09\n",
      "Iteration 55115: loss = 1.879251e-10,4.2469073e-09\n",
      "Iteration 55120: loss = 1.879143e-10,4.2465884e-09\n",
      "Iteration 55125: loss = 1.8790307e-10,4.2462713e-09\n",
      "Iteration 55130: loss = 1.8789025e-10,4.2459587e-09\n",
      "Iteration 55135: loss = 1.8787687e-10,4.2456474e-09\n",
      "Iteration 55140: loss = 1.8786334e-10,4.2453365e-09\n",
      "Iteration 55145: loss = 1.8784983e-10,4.2450257e-09\n",
      "Iteration 55150: loss = 1.8783568e-10,4.244717e-09\n",
      "Iteration 55155: loss = 1.8781998e-10,4.2444115e-09\n",
      "Iteration 55160: loss = 1.8780456e-10,4.244107e-09\n",
      "Iteration 55165: loss = 1.8626785e-10,4.243802e-09\n",
      "Iteration 55170: loss = 1.8778125e-10,4.2434722e-09\n",
      "Iteration 55175: loss = 1.877712e-10,4.2431525e-09\n",
      "Iteration 55180: loss = 1.8776453e-10,4.24282e-09\n",
      "Iteration 55185: loss = 1.8775209e-10,4.2425055e-09\n",
      "Iteration 55190: loss = 1.8774217e-10,4.242184e-09\n",
      "Iteration 55195: loss = 1.8773127e-10,4.241865e-09\n",
      "Iteration 55200: loss = 1.8772857e-10,4.241522e-09\n",
      "Iteration 55205: loss = 1.877226e-10,4.241188e-09\n",
      "Iteration 55210: loss = 1.8770842e-10,4.2408783e-09\n",
      "Iteration 55215: loss = 1.8617007e-10,4.2405786e-09\n",
      "Iteration 55220: loss = 1.8768596e-10,4.2402415e-09\n",
      "Iteration 55225: loss = 1.8767381e-10,4.2399257e-09\n",
      "Iteration 55230: loss = 1.8766567e-10,4.2395985e-09\n",
      "Iteration 55235: loss = 1.8765756e-10,4.2392703e-09\n",
      "Iteration 55240: loss = 1.8764544e-10,4.238954e-09\n",
      "Iteration 55245: loss = 1.8763818e-10,4.238625e-09\n",
      "Iteration 55250: loss = 1.87633e-10,4.238288e-09\n",
      "Iteration 55255: loss = 1.8762807e-10,4.2379513e-09\n",
      "Iteration 55260: loss = 1.8762299e-10,4.2376143e-09\n",
      "Iteration 55265: loss = 1.8761757e-10,4.237278e-09\n",
      "Iteration 55270: loss = 1.8761072e-10,4.236947e-09\n",
      "Iteration 55275: loss = 1.8760361e-10,4.236616e-09\n",
      "Iteration 55280: loss = 1.8759609e-10,4.236286e-09\n",
      "Iteration 55285: loss = 1.87587e-10,4.2359605e-09\n",
      "Iteration 55290: loss = 1.8757719e-10,4.2356367e-09\n",
      "Iteration 55295: loss = 1.87567e-10,4.2353134e-09\n",
      "Iteration 55300: loss = 1.8755648e-10,4.234992e-09\n",
      "Iteration 55305: loss = 1.8754442e-10,4.234674e-09\n",
      "Iteration 55310: loss = 1.8753199e-10,4.234358e-09\n",
      "Iteration 55315: loss = 1.8751949e-10,4.2340424e-09\n",
      "Iteration 55320: loss = 1.875065e-10,4.2337276e-09\n",
      "Iteration 55325: loss = 1.874914e-10,4.233419e-09\n",
      "Iteration 55330: loss = 1.8747544e-10,4.233114e-09\n",
      "Iteration 55335: loss = 1.8746288e-10,4.232798e-09\n",
      "Iteration 55340: loss = 1.8745389e-10,4.2324717e-09\n",
      "Iteration 55345: loss = 1.8744538e-10,4.232143e-09\n",
      "Iteration 55350: loss = 1.8743734e-10,4.2318145e-09\n",
      "Iteration 55355: loss = 1.8742481e-10,4.231498e-09\n",
      "Iteration 55360: loss = 1.8741474e-10,4.231174e-09\n",
      "Iteration 55365: loss = 1.8740558e-10,4.2308477e-09\n",
      "Iteration 55370: loss = 1.8739749e-10,4.230519e-09\n",
      "Iteration 55375: loss = 1.8738587e-10,4.230199e-09\n",
      "Iteration 55380: loss = 1.87376e-10,4.2298747e-09\n",
      "Iteration 55385: loss = 1.8736741e-10,4.2295456e-09\n",
      "Iteration 55390: loss = 1.8735646e-10,4.2292254e-09\n",
      "Iteration 55395: loss = 1.8734946e-10,4.2288915e-09\n",
      "Iteration 55400: loss = 1.8733927e-10,4.228569e-09\n",
      "Iteration 55405: loss = 1.8732808e-10,4.228248e-09\n",
      "Iteration 55410: loss = 1.873187e-10,4.2279216e-09\n",
      "Iteration 55415: loss = 1.8731024e-10,4.2275925e-09\n",
      "Iteration 55420: loss = 1.8730001e-10,4.227268e-09\n",
      "Iteration 55425: loss = 1.8729047e-10,4.2269424e-09\n",
      "Iteration 55430: loss = 1.8728062e-10,4.2266173e-09\n",
      "Iteration 55435: loss = 1.8727099e-10,4.2262926e-09\n",
      "Iteration 55440: loss = 1.8726178e-10,4.2259662e-09\n",
      "Iteration 55445: loss = 1.8725162e-10,4.225641e-09\n",
      "Iteration 55450: loss = 1.8572123e-10,4.225316e-09\n",
      "Iteration 55455: loss = 1.8571146e-10,4.22499e-09\n",
      "Iteration 55460: loss = 1.872243e-10,4.224659e-09\n",
      "Iteration 55465: loss = 1.8721354e-10,4.224336e-09\n",
      "Iteration 55470: loss = 1.8720418e-10,4.22401e-09\n",
      "Iteration 55475: loss = 1.8719759e-10,4.2236743e-09\n",
      "Iteration 55480: loss = 1.8718525e-10,4.2233546e-09\n",
      "Iteration 55485: loss = 1.8717898e-10,4.223019e-09\n",
      "Iteration 55490: loss = 1.8716856e-10,4.2226955e-09\n",
      "Iteration 55495: loss = 1.8563556e-10,4.2223762e-09\n",
      "Iteration 55500: loss = 1.8715164e-10,4.222035e-09\n",
      "Iteration 55505: loss = 1.8714214e-10,4.2217083e-09\n",
      "Iteration 55510: loss = 1.8712905e-10,4.221392e-09\n",
      "Iteration 55515: loss = 1.8712039e-10,4.2210626e-09\n",
      "Iteration 55520: loss = 1.8711016e-10,4.220738e-09\n",
      "Iteration 55525: loss = 1.8710052e-10,4.2204116e-09\n",
      "Iteration 55530: loss = 1.870908e-10,4.2200847e-09\n",
      "Iteration 55535: loss = 1.8708285e-10,4.2197534e-09\n",
      "Iteration 55540: loss = 1.8708035e-10,4.2194053e-09\n",
      "Iteration 55545: loss = 1.870694e-10,4.219081e-09\n",
      "Iteration 55550: loss = 1.8705341e-10,4.2187733e-09\n",
      "Iteration 55555: loss = 1.8704382e-10,4.2184456e-09\n",
      "Iteration 55560: loss = 1.8703554e-10,4.2181143e-09\n",
      "Iteration 55565: loss = 1.8550161e-10,4.2177986e-09\n",
      "Iteration 55570: loss = 1.8549984e-10,4.2174473e-09\n",
      "Iteration 55575: loss = 1.8550485e-10,4.2170774e-09\n",
      "Iteration 55580: loss = 1.8703138e-10,4.2167048e-09\n",
      "Iteration 55585: loss = 1.8703428e-10,4.216341e-09\n",
      "Iteration 55590: loss = 1.8551621e-10,4.2159787e-09\n",
      "Iteration 55595: loss = 1.8551917e-10,4.2156136e-09\n",
      "Iteration 55600: loss = 1.8552339e-10,4.215245e-09\n",
      "Iteration 55605: loss = 1.8552676e-10,4.2148804e-09\n",
      "Iteration 55610: loss = 1.8705182e-10,4.2145114e-09\n",
      "Iteration 55615: loss = 1.870578e-10,4.2141375e-09\n",
      "Iteration 55620: loss = 1.8705992e-10,4.2137764e-09\n",
      "Iteration 55625: loss = 1.855414e-10,4.213414e-09\n",
      "Iteration 55630: loss = 1.8706846e-10,4.2130393e-09\n",
      "Iteration 55635: loss = 1.8707168e-10,4.2126738e-09\n",
      "Iteration 55640: loss = 1.8707463e-10,4.2123087e-09\n",
      "Iteration 55645: loss = 1.87079e-10,4.21194e-09\n",
      "Iteration 55650: loss = 1.8555933e-10,4.2115804e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55655: loss = 1.8556477e-10,4.2112083e-09\n",
      "Iteration 55660: loss = 1.8556799e-10,4.2108432e-09\n",
      "Iteration 55665: loss = 1.85572e-10,4.210475e-09\n",
      "Iteration 55670: loss = 1.8557643e-10,4.210106e-09\n",
      "Iteration 55675: loss = 1.8557962e-10,4.2097392e-09\n",
      "Iteration 55680: loss = 1.8710626e-10,4.2093644e-09\n",
      "Iteration 55685: loss = 1.8558366e-10,4.2090145e-09\n",
      "Iteration 55690: loss = 1.855911e-10,4.2086357e-09\n",
      "Iteration 55695: loss = 1.8559378e-10,4.2082715e-09\n",
      "Iteration 55700: loss = 1.8559909e-10,4.207899e-09\n",
      "Iteration 55705: loss = 1.8559893e-10,4.2075428e-09\n",
      "Iteration 55710: loss = 1.8560377e-10,4.207172e-09\n",
      "Iteration 55715: loss = 1.8560962e-10,4.2067985e-09\n",
      "Iteration 55720: loss = 1.8713577e-10,4.2064237e-09\n",
      "Iteration 55725: loss = 1.8561497e-10,4.206068e-09\n",
      "Iteration 55730: loss = 1.8714424e-10,4.205685e-09\n",
      "Iteration 55735: loss = 1.8562342e-10,4.205328e-09\n",
      "Iteration 55740: loss = 1.8715118e-10,4.2049497e-09\n",
      "Iteration 55745: loss = 1.8562855e-10,4.204599e-09\n",
      "Iteration 55750: loss = 1.8563533e-10,4.2042214e-09\n",
      "Iteration 55755: loss = 1.8564127e-10,4.203847e-09\n",
      "Iteration 55760: loss = 1.8564394e-10,4.203481e-09\n",
      "Iteration 55765: loss = 1.8564723e-10,4.203115e-09\n",
      "Iteration 55770: loss = 1.8717479e-10,4.2027373e-09\n",
      "Iteration 55775: loss = 1.8565466e-10,4.202378e-09\n",
      "Iteration 55780: loss = 1.8565825e-10,4.2020094e-09\n",
      "Iteration 55785: loss = 1.8566311e-10,4.2016377e-09\n",
      "Iteration 55790: loss = 1.856682e-10,4.2012647e-09\n",
      "Iteration 55795: loss = 1.8566874e-10,4.2009054e-09\n",
      "Iteration 55800: loss = 1.8565949e-10,4.2005746e-09\n",
      "Iteration 55805: loss = 1.8564456e-10,4.20026e-09\n",
      "Iteration 55810: loss = 1.8562656e-10,4.1999524e-09\n",
      "Iteration 55815: loss = 1.8561597e-10,4.199626e-09\n",
      "Iteration 55820: loss = 1.856098e-10,4.1992854e-09\n",
      "Iteration 55825: loss = 1.8559936e-10,4.1989576e-09\n",
      "Iteration 55830: loss = 1.855887e-10,4.1986303e-09\n",
      "Iteration 55835: loss = 1.8557977e-10,4.1982973e-09\n",
      "Iteration 55840: loss = 1.85569e-10,4.197971e-09\n",
      "Iteration 55845: loss = 1.8555922e-10,4.1976396e-09\n",
      "Iteration 55850: loss = 1.8554945e-10,4.197311e-09\n",
      "Iteration 55855: loss = 1.8553965e-10,4.1969805e-09\n",
      "Iteration 55860: loss = 1.8552924e-10,4.1966524e-09\n",
      "Iteration 55865: loss = 1.855197e-10,4.1963206e-09\n",
      "Iteration 55870: loss = 1.8551104e-10,4.1959867e-09\n",
      "Iteration 55875: loss = 1.8549873e-10,4.195663e-09\n",
      "Iteration 55880: loss = 1.8548824e-10,4.1953347e-09\n",
      "Iteration 55885: loss = 1.8547817e-10,4.195006e-09\n",
      "Iteration 55890: loss = 1.8546814e-10,4.1946753e-09\n",
      "Iteration 55895: loss = 1.8545794e-10,4.194346e-09\n",
      "Iteration 55900: loss = 1.8544706e-10,4.1940176e-09\n",
      "Iteration 55905: loss = 1.8543526e-10,4.1936934e-09\n",
      "Iteration 55910: loss = 1.8542334e-10,4.193369e-09\n",
      "Iteration 55915: loss = 1.8541037e-10,4.1930464e-09\n",
      "Iteration 55920: loss = 1.8539785e-10,4.1927235e-09\n",
      "Iteration 55925: loss = 1.8538432e-10,4.192403e-09\n",
      "Iteration 55930: loss = 1.8537309e-10,4.1920765e-09\n",
      "Iteration 55935: loss = 1.8537037e-10,4.1917243e-09\n",
      "Iteration 55940: loss = 1.8537318e-10,4.1913575e-09\n",
      "Iteration 55945: loss = 1.8537032e-10,4.1910035e-09\n",
      "Iteration 55950: loss = 1.8536157e-10,4.190671e-09\n",
      "Iteration 55955: loss = 1.85347e-10,4.190352e-09\n",
      "Iteration 55960: loss = 1.8533199e-10,4.1900363e-09\n",
      "Iteration 55965: loss = 1.8532431e-10,4.1896993e-09\n",
      "Iteration 55970: loss = 1.8531447e-10,4.189366e-09\n",
      "Iteration 55975: loss = 1.8530437e-10,4.189036e-09\n",
      "Iteration 55980: loss = 1.8529507e-10,4.1887023e-09\n",
      "Iteration 55985: loss = 1.8528411e-10,4.1883745e-09\n",
      "Iteration 55990: loss = 1.8527517e-10,4.18804e-09\n",
      "Iteration 55995: loss = 1.8526432e-10,4.1877097e-09\n",
      "Iteration 56000: loss = 1.8525519e-10,4.1873767e-09\n",
      "Iteration 56005: loss = 1.8524582e-10,4.1870436e-09\n",
      "Iteration 56010: loss = 1.8523572e-10,4.186713e-09\n",
      "Iteration 56015: loss = 1.8522645e-10,4.1863775e-09\n",
      "Iteration 56020: loss = 1.8521458e-10,4.18605e-09\n",
      "Iteration 56025: loss = 1.8520667e-10,4.185713e-09\n",
      "Iteration 56030: loss = 1.8519532e-10,4.185385e-09\n",
      "Iteration 56035: loss = 1.8518437e-10,4.185056e-09\n",
      "Iteration 56040: loss = 1.8517564e-10,4.1847206e-09\n",
      "Iteration 56045: loss = 1.8516637e-10,4.184385e-09\n",
      "Iteration 56050: loss = 1.8515793e-10,4.184048e-09\n",
      "Iteration 56055: loss = 1.8514834e-10,4.1837156e-09\n",
      "Iteration 56060: loss = 1.8513586e-10,4.1833896e-09\n",
      "Iteration 56065: loss = 1.8512453e-10,4.183062e-09\n",
      "Iteration 56070: loss = 1.8511309e-10,4.1827333e-09\n",
      "Iteration 56075: loss = 1.8510184e-10,4.182404e-09\n",
      "Iteration 56080: loss = 1.850905e-10,4.182076e-09\n",
      "Iteration 56085: loss = 1.8507845e-10,4.181748e-09\n",
      "Iteration 56090: loss = 1.8506661e-10,4.1814205e-09\n",
      "Iteration 56095: loss = 1.8505392e-10,4.1810946e-09\n",
      "Iteration 56100: loss = 1.8504037e-10,4.1807726e-09\n",
      "Iteration 56105: loss = 1.8502683e-10,4.1804507e-09\n",
      "Iteration 56110: loss = 1.8501072e-10,4.1801353e-09\n",
      "Iteration 56115: loss = 1.8499491e-10,4.1798205e-09\n",
      "Iteration 56120: loss = 1.849786e-10,4.179506e-09\n",
      "Iteration 56125: loss = 1.8496182e-10,4.1791925e-09\n",
      "Iteration 56130: loss = 1.8494412e-10,4.178882e-09\n",
      "Iteration 56135: loss = 1.8492637e-10,4.178572e-09\n",
      "Iteration 56140: loss = 1.8490832e-10,4.1782626e-09\n",
      "Iteration 56145: loss = 1.8489799e-10,4.1779304e-09\n",
      "Iteration 56150: loss = 1.848952e-10,4.1775765e-09\n",
      "Iteration 56155: loss = 1.8489593e-10,4.177211e-09\n",
      "Iteration 56160: loss = 1.848978e-10,4.1768424e-09\n",
      "Iteration 56165: loss = 1.8490022e-10,4.1764725e-09\n",
      "Iteration 56170: loss = 1.8490248e-10,4.176104e-09\n",
      "Iteration 56175: loss = 1.8490398e-10,4.1757366e-09\n",
      "Iteration 56180: loss = 1.8490387e-10,4.175374e-09\n",
      "Iteration 56185: loss = 1.849e-10,4.1750225e-09\n",
      "Iteration 56190: loss = 1.8488595e-10,4.174701e-09\n",
      "Iteration 56195: loss = 1.8487228e-10,4.1743773e-09\n",
      "Iteration 56200: loss = 1.8486207e-10,4.174044e-09\n",
      "Iteration 56205: loss = 1.8485281e-10,4.1737076e-09\n",
      "Iteration 56210: loss = 1.8484538e-10,4.173367e-09\n",
      "Iteration 56215: loss = 1.8483752e-10,4.1730264e-09\n",
      "Iteration 56220: loss = 1.8482883e-10,4.1726893e-09\n",
      "Iteration 56225: loss = 1.8481856e-10,4.1723562e-09\n",
      "Iteration 56230: loss = 1.8480832e-10,4.172022e-09\n",
      "Iteration 56235: loss = 1.8479838e-10,4.171688e-09\n",
      "Iteration 56240: loss = 1.8478773e-10,4.1713553e-09\n",
      "Iteration 56245: loss = 1.8477592e-10,4.171026e-09\n",
      "Iteration 56250: loss = 1.8476375e-10,4.170698e-09\n",
      "Iteration 56255: loss = 1.847516e-10,4.17037e-09\n",
      "Iteration 56260: loss = 1.8473889e-10,4.1700434e-09\n",
      "Iteration 56265: loss = 1.8321435e-10,4.169719e-09\n",
      "Iteration 56270: loss = 1.8473699e-10,4.169321e-09\n",
      "Iteration 56275: loss = 1.8322294e-10,4.1689647e-09\n",
      "Iteration 56280: loss = 1.8474096e-10,4.168581e-09\n",
      "Iteration 56285: loss = 1.8322761e-10,4.1682235e-09\n",
      "Iteration 56290: loss = 1.8475148e-10,4.1678225e-09\n",
      "Iteration 56295: loss = 1.8323638e-10,4.1674695e-09\n",
      "Iteration 56300: loss = 1.8475778e-10,4.1670756e-09\n",
      "Iteration 56305: loss = 1.8475867e-10,4.166708e-09\n",
      "Iteration 56310: loss = 1.832456e-10,4.16635e-09\n",
      "Iteration 56315: loss = 1.8476788e-10,4.165954e-09\n",
      "Iteration 56320: loss = 1.8477064e-10,4.1655808e-09\n",
      "Iteration 56325: loss = 1.8326156e-10,4.16521e-09\n",
      "Iteration 56330: loss = 1.8326268e-10,4.164843e-09\n",
      "Iteration 56335: loss = 1.8478262e-10,4.1644515e-09\n",
      "Iteration 56340: loss = 1.8327011e-10,4.1640917e-09\n",
      "Iteration 56345: loss = 1.8327616e-10,4.163709e-09\n",
      "Iteration 56350: loss = 1.8479289e-10,4.163328e-09\n",
      "Iteration 56355: loss = 1.8328054e-10,4.162966e-09\n",
      "Iteration 56360: loss = 1.8480086e-10,4.1625743e-09\n",
      "Iteration 56365: loss = 1.8329051e-10,4.1622075e-09\n",
      "Iteration 56370: loss = 1.8481033e-10,4.1618167e-09\n",
      "Iteration 56375: loss = 1.8329831e-10,4.161455e-09\n",
      "Iteration 56380: loss = 1.8481626e-10,4.1610697e-09\n",
      "Iteration 56385: loss = 1.8330142e-10,4.160715e-09\n",
      "Iteration 56390: loss = 1.848286e-10,4.1603037e-09\n",
      "Iteration 56395: loss = 1.833117e-10,4.159955e-09\n",
      "Iteration 56400: loss = 1.8483282e-10,4.15956e-09\n",
      "Iteration 56405: loss = 1.8332359e-10,4.159189e-09\n",
      "Iteration 56410: loss = 1.8484131e-10,4.158804e-09\n",
      "Iteration 56415: loss = 1.8333236e-10,4.1584336e-09\n",
      "Iteration 56420: loss = 1.8333417e-10,4.1580623e-09\n",
      "Iteration 56425: loss = 1.8485279e-10,4.1576738e-09\n",
      "Iteration 56430: loss = 1.8334252e-10,4.1573065e-09\n",
      "Iteration 56435: loss = 1.8334502e-10,4.1569335e-09\n",
      "Iteration 56440: loss = 1.8335133e-10,4.156549e-09\n",
      "Iteration 56445: loss = 1.8335573e-10,4.1561696e-09\n",
      "Iteration 56450: loss = 1.8487195e-10,4.1557886e-09\n",
      "Iteration 56455: loss = 1.8336464e-10,4.155412e-09\n",
      "Iteration 56460: loss = 1.8336795e-10,4.155036e-09\n",
      "Iteration 56465: loss = 1.8337139e-10,4.1546597e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56470: loss = 1.8337591e-10,4.15428e-09\n",
      "Iteration 56475: loss = 1.833801e-10,4.153903e-09\n",
      "Iteration 56480: loss = 1.8338288e-10,4.1535286e-09\n",
      "Iteration 56485: loss = 1.8338797e-10,4.1531476e-09\n",
      "Iteration 56490: loss = 1.8339064e-10,4.1527723e-09\n",
      "Iteration 56495: loss = 1.8339434e-10,4.152395e-09\n",
      "Iteration 56500: loss = 1.849124e-10,4.152008e-09\n",
      "Iteration 56505: loss = 1.8339996e-10,4.1516457e-09\n",
      "Iteration 56510: loss = 1.8340297e-10,4.151271e-09\n",
      "Iteration 56515: loss = 1.834079e-10,4.150889e-09\n",
      "Iteration 56520: loss = 1.8341355e-10,4.1505075e-09\n",
      "Iteration 56525: loss = 1.8341818e-10,4.150126e-09\n",
      "Iteration 56530: loss = 1.8342271e-10,4.149747e-09\n",
      "Iteration 56535: loss = 1.8342534e-10,4.1493715e-09\n",
      "Iteration 56540: loss = 1.8342754e-10,4.1489985e-09\n",
      "Iteration 56545: loss = 1.8342755e-10,4.1486303e-09\n",
      "Iteration 56550: loss = 1.8342705e-10,4.148265e-09\n",
      "Iteration 56555: loss = 1.8342387e-10,4.1479065e-09\n",
      "Iteration 56560: loss = 1.8341945e-10,4.1475525e-09\n",
      "Iteration 56565: loss = 1.834134e-10,4.147203e-09\n",
      "Iteration 56570: loss = 1.8340647e-10,4.1468557e-09\n",
      "Iteration 56575: loss = 1.8339891e-10,4.1465107e-09\n",
      "Iteration 56580: loss = 1.833898e-10,4.146168e-09\n",
      "Iteration 56585: loss = 1.8338013e-10,4.1458295e-09\n",
      "Iteration 56590: loss = 1.8336999e-10,4.14549e-09\n",
      "Iteration 56595: loss = 1.8335904e-10,4.145154e-09\n",
      "Iteration 56600: loss = 1.8334712e-10,4.1448214e-09\n",
      "Iteration 56605: loss = 1.8333457e-10,4.144489e-09\n",
      "Iteration 56610: loss = 1.8332195e-10,4.144158e-09\n",
      "Iteration 56615: loss = 1.833087e-10,4.1438275e-09\n",
      "Iteration 56620: loss = 1.832941e-10,4.1435015e-09\n",
      "Iteration 56625: loss = 1.8327928e-10,4.143176e-09\n",
      "Iteration 56630: loss = 1.8326428e-10,4.1428505e-09\n",
      "Iteration 56635: loss = 1.8324887e-10,4.1425268e-09\n",
      "Iteration 56640: loss = 1.832338e-10,4.142202e-09\n",
      "Iteration 56645: loss = 1.8321777e-10,4.14188e-09\n",
      "Iteration 56650: loss = 1.8320072e-10,4.141561e-09\n",
      "Iteration 56655: loss = 1.8318314e-10,4.1412425e-09\n",
      "Iteration 56660: loss = 1.8316561e-10,4.140925e-09\n",
      "Iteration 56665: loss = 1.8315617e-10,4.140583e-09\n",
      "Iteration 56670: loss = 1.8315348e-10,4.140223e-09\n",
      "Iteration 56675: loss = 1.8315406e-10,4.139853e-09\n",
      "Iteration 56680: loss = 1.8315516e-10,4.13948e-09\n",
      "Iteration 56685: loss = 1.8315682e-10,4.139107e-09\n",
      "Iteration 56690: loss = 1.8315811e-10,4.138734e-09\n",
      "Iteration 56695: loss = 1.8315875e-10,4.1383634e-09\n",
      "Iteration 56700: loss = 1.8315754e-10,4.1379975e-09\n",
      "Iteration 56705: loss = 1.8315616e-10,4.137633e-09\n",
      "Iteration 56710: loss = 1.831532e-10,4.1372723e-09\n",
      "Iteration 56715: loss = 1.8314933e-10,4.136914e-09\n",
      "Iteration 56720: loss = 1.8314482e-10,4.1365587e-09\n",
      "Iteration 56725: loss = 1.831381e-10,4.1362074e-09\n",
      "Iteration 56730: loss = 1.8313139e-10,4.1358574e-09\n",
      "Iteration 56735: loss = 1.8312231e-10,4.1355137e-09\n",
      "Iteration 56740: loss = 1.8311219e-10,4.1351744e-09\n",
      "Iteration 56745: loss = 1.8310094e-10,4.1348374e-09\n",
      "Iteration 56750: loss = 1.8308953e-10,4.1345003e-09\n",
      "Iteration 56755: loss = 1.8307693e-10,4.1341672e-09\n",
      "Iteration 56760: loss = 1.8306356e-10,4.1338364e-09\n",
      "Iteration 56765: loss = 1.8305045e-10,4.1335055e-09\n",
      "Iteration 56770: loss = 1.830367e-10,4.1331756e-09\n",
      "Iteration 56775: loss = 1.8302244e-10,4.132847e-09\n",
      "Iteration 56780: loss = 1.8300694e-10,4.132522e-09\n",
      "Iteration 56785: loss = 1.8299108e-10,4.132197e-09\n",
      "Iteration 56790: loss = 1.8297508e-10,4.1318726e-09\n",
      "Iteration 56795: loss = 1.8295908e-10,4.13155e-09\n",
      "Iteration 56800: loss = 1.8294288e-10,4.1312274e-09\n",
      "Iteration 56805: loss = 1.8292644e-10,4.1309045e-09\n",
      "Iteration 56810: loss = 1.8290951e-10,4.1305834e-09\n",
      "Iteration 56815: loss = 1.8289138e-10,4.130265e-09\n",
      "Iteration 56820: loss = 1.8287295e-10,4.1299466e-09\n",
      "Iteration 56825: loss = 1.8285483e-10,4.12963e-09\n",
      "Iteration 56830: loss = 1.8283625e-10,4.129312e-09\n",
      "Iteration 56835: loss = 1.8281791e-10,4.128995e-09\n",
      "Iteration 56840: loss = 1.8279916e-10,4.128678e-09\n",
      "Iteration 56845: loss = 1.8278073e-10,4.1283617e-09\n",
      "Iteration 56850: loss = 1.8276186e-10,4.128045e-09\n",
      "Iteration 56855: loss = 1.8274321e-10,4.127729e-09\n",
      "Iteration 56860: loss = 1.8272432e-10,4.127412e-09\n",
      "Iteration 56865: loss = 1.8270437e-10,4.127099e-09\n",
      "Iteration 56870: loss = 1.826872e-10,4.1267767e-09\n",
      "Iteration 56875: loss = 1.8267947e-10,4.1264285e-09\n",
      "Iteration 56880: loss = 1.8267642e-10,4.1260653e-09\n",
      "Iteration 56885: loss = 1.8267637e-10,4.125694e-09\n",
      "Iteration 56890: loss = 1.8267805e-10,4.125318e-09\n",
      "Iteration 56895: loss = 1.8267922e-10,4.124943e-09\n",
      "Iteration 56900: loss = 1.8267925e-10,4.1245705e-09\n",
      "Iteration 56905: loss = 1.8267876e-10,4.1242005e-09\n",
      "Iteration 56910: loss = 1.8267741e-10,4.1238315e-09\n",
      "Iteration 56915: loss = 1.8267349e-10,4.1234713e-09\n",
      "Iteration 56920: loss = 1.8266877e-10,4.123114e-09\n",
      "Iteration 56925: loss = 1.8266254e-10,4.12276e-09\n",
      "Iteration 56930: loss = 1.8265534e-10,4.1224095e-09\n",
      "Iteration 56935: loss = 1.8264794e-10,4.122059e-09\n",
      "Iteration 56940: loss = 1.8263883e-10,4.121713e-09\n",
      "Iteration 56945: loss = 1.8262898e-10,4.12137e-09\n",
      "Iteration 56950: loss = 1.8261874e-10,4.121028e-09\n",
      "Iteration 56955: loss = 1.8260769e-10,4.1206873e-09\n",
      "Iteration 56960: loss = 1.8259577e-10,4.12035e-09\n",
      "Iteration 56965: loss = 1.8258312e-10,4.1200137e-09\n",
      "Iteration 56970: loss = 1.8257064e-10,4.119678e-09\n",
      "Iteration 56975: loss = 1.8255765e-10,4.1193426e-09\n",
      "Iteration 56980: loss = 1.825431e-10,4.119013e-09\n",
      "Iteration 56985: loss = 1.8252781e-10,4.118683e-09\n",
      "Iteration 56990: loss = 1.8251284e-10,4.1183537e-09\n",
      "Iteration 56995: loss = 1.8249768e-10,4.1180255e-09\n",
      "Iteration 57000: loss = 1.8099106e-10,4.117666e-09\n",
      "Iteration 57005: loss = 1.8251234e-10,4.1172368e-09\n",
      "Iteration 57010: loss = 1.8251622e-10,4.116852e-09\n",
      "Iteration 57015: loss = 1.8100794e-10,4.1164987e-09\n",
      "Iteration 57020: loss = 1.82517e-10,4.1161035e-09\n",
      "Iteration 57025: loss = 1.8101586e-10,4.115729e-09\n",
      "Iteration 57030: loss = 1.8101992e-10,4.1153445e-09\n",
      "Iteration 57035: loss = 1.82526e-10,4.114957e-09\n",
      "Iteration 57040: loss = 1.8253073e-10,4.1145705e-09\n",
      "Iteration 57045: loss = 1.8253564e-10,4.1141837e-09\n",
      "Iteration 57050: loss = 1.825382e-10,4.1138026e-09\n",
      "Iteration 57055: loss = 1.8254274e-10,4.1134163e-09\n",
      "Iteration 57060: loss = 1.8254658e-10,4.1130304e-09\n",
      "Iteration 57065: loss = 1.825507e-10,4.112645e-09\n",
      "Iteration 57070: loss = 1.8105151e-10,4.112264e-09\n",
      "Iteration 57075: loss = 1.8105512e-10,4.11188e-09\n",
      "Iteration 57080: loss = 1.8256235e-10,4.1114894e-09\n",
      "Iteration 57085: loss = 1.8256802e-10,4.1111e-09\n",
      "Iteration 57090: loss = 1.825722e-10,4.1107135e-09\n",
      "Iteration 57095: loss = 1.8257475e-10,4.1103325e-09\n",
      "Iteration 57100: loss = 1.8257919e-10,4.1099444e-09\n",
      "Iteration 57105: loss = 1.8258361e-10,4.109558e-09\n",
      "Iteration 57110: loss = 1.8108338e-10,4.1091788e-09\n",
      "Iteration 57115: loss = 1.8108842e-10,4.108792e-09\n",
      "Iteration 57120: loss = 1.8259634e-10,4.1083994e-09\n",
      "Iteration 57125: loss = 1.810947e-10,4.1080246e-09\n",
      "Iteration 57130: loss = 1.8110086e-10,4.1076316e-09\n",
      "Iteration 57135: loss = 1.8110513e-10,4.107244e-09\n",
      "Iteration 57140: loss = 1.811095e-10,4.106858e-09\n",
      "Iteration 57145: loss = 1.8111317e-10,4.106472e-09\n",
      "Iteration 57150: loss = 1.826206e-10,4.1060813e-09\n",
      "Iteration 57155: loss = 1.8111973e-10,4.1057047e-09\n",
      "Iteration 57160: loss = 1.8112604e-10,4.1053103e-09\n",
      "Iteration 57165: loss = 1.8113e-10,4.104925e-09\n",
      "Iteration 57170: loss = 1.8113461e-10,4.1045376e-09\n",
      "Iteration 57175: loss = 1.8113709e-10,4.104155e-09\n",
      "Iteration 57180: loss = 1.8114184e-10,4.1037653e-09\n",
      "Iteration 57185: loss = 1.8114392e-10,4.1033843e-09\n",
      "Iteration 57190: loss = 1.826543e-10,4.1029846e-09\n",
      "Iteration 57195: loss = 1.8115141e-10,4.1026116e-09\n",
      "Iteration 57200: loss = 1.811594e-10,4.102215e-09\n",
      "Iteration 57205: loss = 1.8116368e-10,4.101826e-09\n",
      "Iteration 57210: loss = 1.8116453e-10,4.101449e-09\n",
      "Iteration 57215: loss = 1.8117012e-10,4.101057e-09\n",
      "Iteration 57220: loss = 1.8117595e-10,4.100665e-09\n",
      "Iteration 57225: loss = 1.811782e-10,4.1002823e-09\n",
      "Iteration 57230: loss = 1.8118322e-10,4.099892e-09\n",
      "Iteration 57235: loss = 1.811859e-10,4.0995083e-09\n",
      "Iteration 57240: loss = 1.8118483e-10,4.0991366e-09\n",
      "Iteration 57245: loss = 1.8118716e-10,4.098754e-09\n",
      "Iteration 57250: loss = 1.8269146e-10,4.098371e-09\n",
      "Iteration 57255: loss = 1.811704e-10,4.0980503e-09\n",
      "Iteration 57260: loss = 1.8115513e-10,4.0977195e-09\n",
      "Iteration 57265: loss = 1.8114744e-10,4.097365e-09\n",
      "Iteration 57270: loss = 1.8114243e-10,4.097003e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57275: loss = 1.8264108e-10,4.096637e-09\n",
      "Iteration 57280: loss = 1.8112101e-10,4.0963144e-09\n",
      "Iteration 57285: loss = 1.8110786e-10,4.0959747e-09\n",
      "Iteration 57290: loss = 1.8109969e-10,4.0956216e-09\n",
      "Iteration 57295: loss = 1.8109282e-10,4.0952655e-09\n",
      "Iteration 57300: loss = 1.8258918e-10,4.0949035e-09\n",
      "Iteration 57305: loss = 1.8107306e-10,4.094569e-09\n",
      "Iteration 57310: loss = 1.8106316e-10,4.0942223e-09\n",
      "Iteration 57315: loss = 1.8105402e-10,4.093872e-09\n",
      "Iteration 57320: loss = 1.8104608e-10,4.0935175e-09\n",
      "Iteration 57325: loss = 1.8103423e-10,4.0931756e-09\n",
      "Iteration 57330: loss = 1.8101777e-10,4.0928456e-09\n",
      "Iteration 57335: loss = 1.8100511e-10,4.092505e-09\n",
      "Iteration 57340: loss = 1.8099398e-10,4.092161e-09\n",
      "Iteration 57345: loss = 1.8098362e-10,4.091814e-09\n",
      "Iteration 57350: loss = 1.8097313e-10,4.0914667e-09\n",
      "Iteration 57355: loss = 1.8096125e-10,4.0911234e-09\n",
      "Iteration 57360: loss = 1.8095227e-10,4.0907726e-09\n",
      "Iteration 57365: loss = 1.8095166e-10,4.090397e-09\n",
      "Iteration 57370: loss = 1.8245072e-10,4.090027e-09\n",
      "Iteration 57375: loss = 1.809327e-10,4.089696e-09\n",
      "Iteration 57380: loss = 1.8092718e-10,4.089336e-09\n",
      "Iteration 57385: loss = 1.8091424e-10,4.0889954e-09\n",
      "Iteration 57390: loss = 1.8090575e-10,4.0886423e-09\n",
      "Iteration 57395: loss = 1.8089363e-10,4.088299e-09\n",
      "Iteration 57400: loss = 1.808819e-10,4.0879558e-09\n",
      "Iteration 57405: loss = 1.8237901e-10,4.087591e-09\n",
      "Iteration 57410: loss = 1.8086593e-10,4.0872474e-09\n",
      "Iteration 57415: loss = 1.8085478e-10,4.0869015e-09\n",
      "Iteration 57420: loss = 1.8084671e-10,4.0865458e-09\n",
      "Iteration 57425: loss = 1.808368e-10,4.0861976e-09\n",
      "Iteration 57430: loss = 1.8082603e-10,4.08585e-09\n",
      "Iteration 57435: loss = 1.8081697e-10,4.0854986e-09\n",
      "Iteration 57440: loss = 1.8080647e-10,4.0851504e-09\n",
      "Iteration 57445: loss = 1.8079678e-10,4.0848014e-09\n",
      "Iteration 57450: loss = 1.8078623e-10,4.0844528e-09\n",
      "Iteration 57455: loss = 1.8077445e-10,4.084108e-09\n",
      "Iteration 57460: loss = 1.8076605e-10,4.0837547e-09\n",
      "Iteration 57465: loss = 1.8075748e-10,4.0834003e-09\n",
      "Iteration 57470: loss = 1.8074477e-10,4.083059e-09\n",
      "Iteration 57475: loss = 1.8073452e-10,4.0827106e-09\n",
      "Iteration 57480: loss = 1.8072578e-10,4.0823576e-09\n",
      "Iteration 57485: loss = 1.8071732e-10,4.0820027e-09\n",
      "Iteration 57490: loss = 1.8070763e-10,4.0816532e-09\n",
      "Iteration 57495: loss = 1.8069263e-10,4.0813175e-09\n",
      "Iteration 57500: loss = 1.8067998e-10,4.080976e-09\n",
      "Iteration 57505: loss = 1.8066872e-10,4.0806296e-09\n",
      "Iteration 57510: loss = 1.8065745e-10,4.080282e-09\n",
      "Iteration 57515: loss = 1.8064662e-10,4.0799346e-09\n",
      "Iteration 57520: loss = 1.8063535e-10,4.0795882e-09\n",
      "Iteration 57525: loss = 1.8062436e-10,4.079241e-09\n",
      "Iteration 57530: loss = 1.8061129e-10,4.078899e-09\n",
      "Iteration 57535: loss = 1.8059837e-10,4.0785566e-09\n",
      "Iteration 57540: loss = 1.8058514e-10,4.078216e-09\n",
      "Iteration 57545: loss = 1.8057193e-10,4.077876e-09\n",
      "Iteration 57550: loss = 1.8055775e-10,4.0775365e-09\n",
      "Iteration 57555: loss = 1.8054302e-10,4.0772004e-09\n",
      "Iteration 57560: loss = 1.8052737e-10,4.0768655e-09\n",
      "Iteration 57565: loss = 1.8051209e-10,4.0765307e-09\n",
      "Iteration 57570: loss = 1.8049624e-10,4.0761963e-09\n",
      "Iteration 57575: loss = 1.8048137e-10,4.0758596e-09\n",
      "Iteration 57580: loss = 1.8047493e-10,4.075498e-09\n",
      "Iteration 57585: loss = 1.804738e-10,4.0751225e-09\n",
      "Iteration 57590: loss = 1.8047625e-10,4.0747343e-09\n",
      "Iteration 57595: loss = 1.8048031e-10,4.074343e-09\n",
      "Iteration 57600: loss = 1.804842e-10,4.0739505e-09\n",
      "Iteration 57605: loss = 1.8198633e-10,4.0735686e-09\n",
      "Iteration 57610: loss = 1.8046276e-10,4.0732524e-09\n",
      "Iteration 57615: loss = 1.8044838e-10,4.072914e-09\n",
      "Iteration 57620: loss = 1.8043954e-10,4.072558e-09\n",
      "Iteration 57625: loss = 1.8043389e-10,4.0721932e-09\n",
      "Iteration 57630: loss = 1.8042927e-10,4.071827e-09\n",
      "Iteration 57635: loss = 1.8192597e-10,4.07146e-09\n",
      "Iteration 57640: loss = 1.8041095e-10,4.071118e-09\n",
      "Iteration 57645: loss = 1.8040203e-10,4.0707637e-09\n",
      "Iteration 57650: loss = 1.8039416e-10,4.0704053e-09\n",
      "Iteration 57655: loss = 1.8038071e-10,4.070063e-09\n",
      "Iteration 57660: loss = 1.8036651e-10,4.069724e-09\n",
      "Iteration 57665: loss = 1.8035524e-10,4.0693746e-09\n",
      "Iteration 57670: loss = 1.8034545e-10,4.0690225e-09\n",
      "Iteration 57675: loss = 1.803357e-10,4.06867e-09\n",
      "Iteration 57680: loss = 1.8032591e-10,4.0683164e-09\n",
      "Iteration 57685: loss = 1.8031591e-10,4.067964e-09\n",
      "Iteration 57690: loss = 1.8030555e-10,4.0676134e-09\n",
      "Iteration 57695: loss = 1.80295e-10,4.0672608e-09\n",
      "Iteration 57700: loss = 1.8028322e-10,4.0669144e-09\n",
      "Iteration 57705: loss = 1.7878027e-10,4.0665644e-09\n",
      "Iteration 57710: loss = 1.8178693e-10,4.066144e-09\n",
      "Iteration 57715: loss = 1.8026863e-10,4.0658112e-09\n",
      "Iteration 57720: loss = 1.802632e-10,4.0654453e-09\n",
      "Iteration 57725: loss = 1.8025638e-10,4.0650834e-09\n",
      "Iteration 57730: loss = 1.8024486e-10,4.0647334e-09\n",
      "Iteration 57735: loss = 1.8022937e-10,4.0643973e-09\n",
      "Iteration 57740: loss = 1.8022513e-10,4.064028e-09\n",
      "Iteration 57745: loss = 1.8021797e-10,4.0636667e-09\n",
      "Iteration 57750: loss = 1.8020445e-10,4.0633235e-09\n",
      "Iteration 57755: loss = 1.7869678e-10,4.0629864e-09\n",
      "Iteration 57760: loss = 1.8018532e-10,4.062615e-09\n",
      "Iteration 57765: loss = 1.8017365e-10,4.0622665e-09\n",
      "Iteration 57770: loss = 1.801621e-10,4.061918e-09\n",
      "Iteration 57775: loss = 1.8015568e-10,4.0615538e-09\n",
      "Iteration 57780: loss = 1.7864893e-10,4.0612145e-09\n",
      "Iteration 57785: loss = 1.8013652e-10,4.060844e-09\n",
      "Iteration 57790: loss = 1.8012393e-10,4.060498e-09\n",
      "Iteration 57795: loss = 1.8011427e-10,4.0601433e-09\n",
      "Iteration 57800: loss = 1.8010382e-10,4.0597916e-09\n",
      "Iteration 57805: loss = 1.8009465e-10,4.0594346e-09\n",
      "Iteration 57810: loss = 1.8008427e-10,4.0590824e-09\n",
      "Iteration 57815: loss = 1.8007777e-10,4.058719e-09\n",
      "Iteration 57820: loss = 1.800643e-10,4.0583745e-09\n",
      "Iteration 57825: loss = 1.8005579e-10,4.058016e-09\n",
      "Iteration 57830: loss = 1.7855178e-10,4.0576666e-09\n",
      "Iteration 57835: loss = 1.8004183e-10,4.05729e-09\n",
      "Iteration 57840: loss = 1.8003592e-10,4.0569246e-09\n",
      "Iteration 57845: loss = 1.8002734e-10,4.0565653e-09\n",
      "Iteration 57850: loss = 1.8001729e-10,4.056212e-09\n",
      "Iteration 57855: loss = 1.8000638e-10,4.0558605e-09\n",
      "Iteration 57860: loss = 1.7999435e-10,4.0555115e-09\n",
      "Iteration 57865: loss = 1.799812e-10,4.055166e-09\n",
      "Iteration 57870: loss = 1.7996764e-10,4.0548214e-09\n",
      "Iteration 57875: loss = 1.7996073e-10,4.0544577e-09\n",
      "Iteration 57880: loss = 1.7995055e-10,4.0541033e-09\n",
      "Iteration 57885: loss = 1.7844566e-10,4.0537564e-09\n",
      "Iteration 57890: loss = 1.7993189e-10,4.05339e-09\n",
      "Iteration 57895: loss = 1.799203e-10,4.05304e-09\n",
      "Iteration 57900: loss = 1.7990996e-10,4.0526853e-09\n",
      "Iteration 57905: loss = 1.7989958e-10,4.0523314e-09\n",
      "Iteration 57910: loss = 1.7989116e-10,4.0519716e-09\n",
      "Iteration 57915: loss = 1.7838747e-10,4.0516213e-09\n",
      "Iteration 57920: loss = 1.7987255e-10,4.0512576e-09\n",
      "Iteration 57925: loss = 1.7986057e-10,4.050908e-09\n",
      "Iteration 57930: loss = 1.783581e-10,4.0505537e-09\n",
      "Iteration 57935: loss = 1.7984268e-10,4.0501917e-09\n",
      "Iteration 57940: loss = 1.7983386e-10,4.0498334e-09\n",
      "Iteration 57945: loss = 1.798286e-10,4.049463e-09\n",
      "Iteration 57950: loss = 1.7981407e-10,4.04912e-09\n",
      "Iteration 57955: loss = 1.7980441e-10,4.048765e-09\n",
      "Iteration 57960: loss = 1.7979407e-10,4.0484105e-09\n",
      "Iteration 57965: loss = 1.7829072e-10,4.048057e-09\n",
      "Iteration 57970: loss = 1.7977436e-10,4.047698e-09\n",
      "Iteration 57975: loss = 1.7827118e-10,4.047344e-09\n",
      "Iteration 57980: loss = 1.7826107e-10,4.046988e-09\n",
      "Iteration 57985: loss = 1.797497e-10,4.0466133e-09\n",
      "Iteration 57990: loss = 1.7973571e-10,4.046269e-09\n",
      "Iteration 57995: loss = 1.7972927e-10,4.045902e-09\n",
      "Iteration 58000: loss = 1.7972096e-10,4.0455417e-09\n",
      "Iteration 58005: loss = 1.7970712e-10,4.0451966e-09\n",
      "Iteration 58010: loss = 1.796966e-10,4.044841e-09\n",
      "Iteration 58015: loss = 1.7968753e-10,4.044482e-09\n",
      "Iteration 58020: loss = 1.7818437e-10,4.0441286e-09\n",
      "Iteration 58025: loss = 1.7967004e-10,4.043762e-09\n",
      "Iteration 58030: loss = 1.7965894e-10,4.043409e-09\n",
      "Iteration 58035: loss = 1.7815462e-10,4.0430574e-09\n",
      "Iteration 58040: loss = 1.7964398e-10,4.0426804e-09\n",
      "Iteration 58045: loss = 1.7962927e-10,4.042337e-09\n",
      "Iteration 58050: loss = 1.7962191e-10,4.0419725e-09\n",
      "Iteration 58055: loss = 1.7961503e-10,4.041607e-09\n",
      "Iteration 58060: loss = 1.7810675e-10,4.0412687e-09\n",
      "Iteration 58065: loss = 1.7959213e-10,4.040901e-09\n",
      "Iteration 58070: loss = 1.7958258e-10,4.040543e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58075: loss = 1.7957334e-10,4.0401824e-09\n",
      "Iteration 58080: loss = 1.7956249e-10,4.039828e-09\n",
      "Iteration 58085: loss = 1.780599e-10,4.0394723e-09\n",
      "Iteration 58090: loss = 1.7954481e-10,4.0391077e-09\n",
      "Iteration 58095: loss = 1.795345e-10,4.038751e-09\n",
      "Iteration 58100: loss = 1.7803058e-10,4.0383985e-09\n",
      "Iteration 58105: loss = 1.795208e-10,4.038017e-09\n",
      "Iteration 58110: loss = 1.7951035e-10,4.0376604e-09\n",
      "Iteration 58115: loss = 1.780011e-10,4.037325e-09\n",
      "Iteration 58120: loss = 1.7948704e-10,4.036955e-09\n",
      "Iteration 58125: loss = 1.7798325e-10,4.036602e-09\n",
      "Iteration 58130: loss = 1.7947117e-10,4.0362282e-09\n",
      "Iteration 58135: loss = 1.7945635e-10,4.035884e-09\n",
      "Iteration 58140: loss = 1.7945144e-10,4.03551e-09\n",
      "Iteration 58145: loss = 1.7944451e-10,4.0351447e-09\n",
      "Iteration 58150: loss = 1.7793428e-10,4.03481e-09\n",
      "Iteration 58155: loss = 1.7942343e-10,4.034431e-09\n",
      "Iteration 58160: loss = 1.7941205e-10,4.034078e-09\n",
      "Iteration 58165: loss = 1.7940033e-10,4.033723e-09\n",
      "Iteration 58170: loss = 1.7789652e-10,4.0333705e-09\n",
      "Iteration 58175: loss = 1.7938144e-10,4.0330037e-09\n",
      "Iteration 58180: loss = 1.7937118e-10,4.032646e-09\n",
      "Iteration 58185: loss = 1.7937563e-10,4.0322456e-09\n",
      "Iteration 58190: loss = 1.7788543e-10,4.031853e-09\n",
      "Iteration 58195: loss = 1.7938229e-10,4.03145e-09\n",
      "Iteration 58200: loss = 1.7938862e-10,4.031043e-09\n",
      "Iteration 58205: loss = 1.7789664e-10,4.0306554e-09\n",
      "Iteration 58210: loss = 1.7939467e-10,4.0302495e-09\n",
      "Iteration 58215: loss = 1.793991e-10,4.0298485e-09\n",
      "Iteration 58220: loss = 1.7790931e-10,4.0294537e-09\n",
      "Iteration 58225: loss = 1.7791445e-10,4.02905e-09\n",
      "Iteration 58230: loss = 1.7791767e-10,4.0286525e-09\n",
      "Iteration 58235: loss = 1.7792277e-10,4.0282497e-09\n",
      "Iteration 58240: loss = 1.7792735e-10,4.027848e-09\n",
      "Iteration 58245: loss = 1.7792963e-10,4.0274517e-09\n",
      "Iteration 58250: loss = 1.7793521e-10,4.027047e-09\n",
      "Iteration 58255: loss = 1.7793815e-10,4.02665e-09\n",
      "Iteration 58260: loss = 1.7794168e-10,4.0262496e-09\n",
      "Iteration 58265: loss = 1.7794438e-10,4.0258534e-09\n",
      "Iteration 58270: loss = 1.7795083e-10,4.025446e-09\n",
      "Iteration 58275: loss = 1.7795469e-10,4.0250456e-09\n",
      "Iteration 58280: loss = 1.7795514e-10,4.0246553e-09\n",
      "Iteration 58285: loss = 1.7795738e-10,4.02426e-09\n",
      "Iteration 58290: loss = 1.779597e-10,4.0238644e-09\n",
      "Iteration 58295: loss = 1.7796198e-10,4.0234696e-09\n",
      "Iteration 58300: loss = 1.779637e-10,4.0230743e-09\n",
      "Iteration 58305: loss = 1.7796493e-10,4.0226817e-09\n",
      "Iteration 58310: loss = 1.779646e-10,4.0222936e-09\n",
      "Iteration 58315: loss = 1.7796382e-10,4.0219064e-09\n",
      "Iteration 58320: loss = 1.7796069e-10,4.021526e-09\n",
      "Iteration 58325: loss = 1.7795761e-10,4.0211456e-09\n",
      "Iteration 58330: loss = 1.7795292e-10,4.02077e-09\n",
      "Iteration 58335: loss = 1.7794699e-10,4.020396e-09\n",
      "Iteration 58340: loss = 1.7794015e-10,4.0200274e-09\n",
      "Iteration 58345: loss = 1.7793063e-10,4.0196655e-09\n",
      "Iteration 58350: loss = 1.7792072e-10,4.0193053e-09\n",
      "Iteration 58355: loss = 1.779102e-10,4.0189447e-09\n",
      "Iteration 58360: loss = 1.7789852e-10,4.01859e-09\n",
      "Iteration 58365: loss = 1.7788603e-10,4.018236e-09\n",
      "Iteration 58370: loss = 1.7787376e-10,4.017883e-09\n",
      "Iteration 58375: loss = 1.7786063e-10,4.017531e-09\n",
      "Iteration 58380: loss = 1.7784695e-10,4.017181e-09\n",
      "Iteration 58385: loss = 1.778321e-10,4.016834e-09\n",
      "Iteration 58390: loss = 1.7781714e-10,4.016487e-09\n",
      "Iteration 58395: loss = 1.7780204e-10,4.0161403e-09\n",
      "Iteration 58400: loss = 1.7778694e-10,4.015795e-09\n",
      "Iteration 58405: loss = 1.7777159e-10,4.0154493e-09\n",
      "Iteration 58410: loss = 1.7775433e-10,4.0151087e-09\n",
      "Iteration 58415: loss = 1.7773998e-10,4.014759e-09\n",
      "Iteration 58420: loss = 1.7773459e-10,4.0143853e-09\n",
      "Iteration 58425: loss = 1.7773348e-10,4.013998e-09\n",
      "Iteration 58430: loss = 1.7773545e-10,4.0136006e-09\n",
      "Iteration 58435: loss = 1.7773825e-10,4.013201e-09\n",
      "Iteration 58440: loss = 1.7774067e-10,4.0128016e-09\n",
      "Iteration 58445: loss = 1.7774306e-10,4.012404e-09\n",
      "Iteration 58450: loss = 1.7774343e-10,4.0120116e-09\n",
      "Iteration 58455: loss = 1.7774243e-10,4.0116257e-09\n",
      "Iteration 58460: loss = 1.7774027e-10,4.0112393e-09\n",
      "Iteration 58465: loss = 1.7773645e-10,4.0108596e-09\n",
      "Iteration 58470: loss = 1.77732e-10,4.010481e-09\n",
      "Iteration 58475: loss = 1.777264e-10,4.0101065e-09\n",
      "Iteration 58480: loss = 1.7771955e-10,4.009734e-09\n",
      "Iteration 58485: loss = 1.7771241e-10,4.0093635e-09\n",
      "Iteration 58490: loss = 1.7770352e-10,4.0089976e-09\n",
      "Iteration 58495: loss = 1.7769448e-10,4.0086325e-09\n",
      "Iteration 58500: loss = 1.7768455e-10,4.0082693e-09\n",
      "Iteration 58505: loss = 1.7767353e-10,4.007909e-09\n",
      "Iteration 58510: loss = 1.7766011e-10,4.007557e-09\n",
      "Iteration 58515: loss = 1.7764658e-10,4.007205e-09\n",
      "Iteration 58520: loss = 1.7763273e-10,4.0068526e-09\n",
      "Iteration 58525: loss = 1.776189e-10,4.0065027e-09\n",
      "Iteration 58530: loss = 1.7760303e-10,4.006156e-09\n",
      "Iteration 58535: loss = 1.7758744e-10,4.0058104e-09\n",
      "Iteration 58540: loss = 1.7757155e-10,4.0054653e-09\n",
      "Iteration 58545: loss = 1.7755535e-10,4.0051202e-09\n",
      "Iteration 58550: loss = 1.7753903e-10,4.004776e-09\n",
      "Iteration 58555: loss = 1.7752223e-10,4.004432e-09\n",
      "Iteration 58560: loss = 1.7750491e-10,4.0040904e-09\n",
      "Iteration 58565: loss = 1.7748697e-10,4.00375e-09\n",
      "Iteration 58570: loss = 1.7746904e-10,4.003411e-09\n",
      "Iteration 58575: loss = 1.7745051e-10,4.003072e-09\n",
      "Iteration 58580: loss = 1.7743267e-10,4.0027324e-09\n",
      "Iteration 58585: loss = 1.7741392e-10,4.0023935e-09\n",
      "Iteration 58590: loss = 1.7739533e-10,4.002055e-09\n",
      "Iteration 58595: loss = 1.7737671e-10,4.0017163e-09\n",
      "Iteration 58600: loss = 1.7735781e-10,4.0013792e-09\n",
      "Iteration 58605: loss = 1.7733932e-10,4.001041e-09\n",
      "Iteration 58610: loss = 1.7732005e-10,4.000704e-09\n",
      "Iteration 58615: loss = 1.7729979e-10,4.00037e-09\n",
      "Iteration 58620: loss = 1.7727951e-10,4.000036e-09\n",
      "Iteration 58625: loss = 1.7726125e-10,3.999697e-09\n",
      "Iteration 58630: loss = 1.7725155e-10,3.9993315e-09\n",
      "Iteration 58635: loss = 1.7724798e-10,3.998948e-09\n",
      "Iteration 58640: loss = 1.7724712e-10,3.998557e-09\n",
      "Iteration 58645: loss = 1.772482e-10,3.998161e-09\n",
      "Iteration 58650: loss = 1.7724894e-10,3.997764e-09\n",
      "Iteration 58655: loss = 1.772485e-10,3.9973713e-09\n",
      "Iteration 58660: loss = 1.7724784e-10,3.9969796e-09\n",
      "Iteration 58665: loss = 1.7724648e-10,3.9965897e-09\n",
      "Iteration 58670: loss = 1.77243e-10,3.996205e-09\n",
      "Iteration 58675: loss = 1.7723943e-10,3.9958223e-09\n",
      "Iteration 58680: loss = 1.7723378e-10,3.995443e-09\n",
      "Iteration 58685: loss = 1.7722653e-10,3.9950714e-09\n",
      "Iteration 58690: loss = 1.7721863e-10,3.9947006e-09\n",
      "Iteration 58695: loss = 1.7720936e-10,3.994333e-09\n",
      "Iteration 58700: loss = 1.7719957e-10,3.993968e-09\n",
      "Iteration 58705: loss = 1.7718921e-10,3.993605e-09\n",
      "Iteration 58710: loss = 1.771779e-10,3.993243e-09\n",
      "Iteration 58715: loss = 1.7716556e-10,3.9928842e-09\n",
      "Iteration 58720: loss = 1.7715301e-10,3.9925276e-09\n",
      "Iteration 58725: loss = 1.7713984e-10,3.99217e-09\n",
      "Iteration 58730: loss = 1.7712658e-10,3.991815e-09\n",
      "Iteration 58735: loss = 1.7711212e-10,3.991463e-09\n",
      "Iteration 58740: loss = 1.7709707e-10,3.991111e-09\n",
      "Iteration 58745: loss = 1.7708192e-10,3.9907606e-09\n",
      "Iteration 58750: loss = 1.7706642e-10,3.9904102e-09\n",
      "Iteration 58755: loss = 1.7558599e-10,3.990009e-09\n",
      "Iteration 58760: loss = 1.770854e-10,3.9895656e-09\n",
      "Iteration 58765: loss = 1.7708679e-10,3.9891663e-09\n",
      "Iteration 58770: loss = 1.755973e-10,3.9887906e-09\n",
      "Iteration 58775: loss = 1.770885e-10,3.9883696e-09\n",
      "Iteration 58780: loss = 1.7709184e-10,3.9879646e-09\n",
      "Iteration 58785: loss = 1.7561162e-10,3.9875623e-09\n",
      "Iteration 58790: loss = 1.7561534e-10,3.9871555e-09\n",
      "Iteration 58795: loss = 1.7710867e-10,3.986729e-09\n",
      "Iteration 58800: loss = 1.7562334e-10,3.986341e-09\n",
      "Iteration 58805: loss = 1.7711538e-10,3.9859183e-09\n",
      "Iteration 58810: loss = 1.7563216e-10,3.985523e-09\n",
      "Iteration 58815: loss = 1.7712369e-10,3.9851016e-09\n",
      "Iteration 58820: loss = 1.756413e-10,3.9847055e-09\n",
      "Iteration 58825: loss = 1.7564657e-10,3.9842942e-09\n",
      "Iteration 58830: loss = 1.7565115e-10,3.9838843e-09\n",
      "Iteration 58835: loss = 1.7565327e-10,3.9834815e-09\n",
      "Iteration 58840: loss = 1.7714656e-10,3.9830557e-09\n",
      "Iteration 58845: loss = 1.7565988e-10,3.9826697e-09\n",
      "Iteration 58850: loss = 1.7715597e-10,3.982235e-09\n",
      "Iteration 58855: loss = 1.7567085e-10,3.981846e-09\n",
      "Iteration 58860: loss = 1.7716202e-10,3.981424e-09\n",
      "Iteration 58865: loss = 1.7716491e-10,3.981019e-09\n",
      "Iteration 58870: loss = 1.7568609e-10,3.9806123e-09\n",
      "Iteration 58875: loss = 1.7568826e-10,3.9802086e-09\n",
      "Iteration 58880: loss = 1.771795e-10,3.9797867e-09\n",
      "Iteration 58885: loss = 1.756968e-10,3.9793906e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58890: loss = 1.7718788e-10,3.9789683e-09\n",
      "Iteration 58895: loss = 1.7570721e-10,3.978567e-09\n",
      "Iteration 58900: loss = 1.757114e-10,3.978157e-09\n",
      "Iteration 58905: loss = 1.7571633e-10,3.9777457e-09\n",
      "Iteration 58910: loss = 1.7720449e-10,3.9773336e-09\n",
      "Iteration 58915: loss = 1.757237e-10,3.9769295e-09\n",
      "Iteration 58920: loss = 1.7721301e-10,3.9765133e-09\n",
      "Iteration 58925: loss = 1.757323e-10,3.97611e-09\n",
      "Iteration 58930: loss = 1.757386e-10,3.9756953e-09\n",
      "Iteration 58935: loss = 1.7574142e-10,3.97529e-09\n",
      "Iteration 58940: loss = 1.7574608e-10,3.974878e-09\n",
      "Iteration 58945: loss = 1.7574685e-10,3.9744794e-09\n",
      "Iteration 58950: loss = 1.7575454e-10,3.974059e-09\n",
      "Iteration 58955: loss = 1.7575809e-10,3.973652e-09\n",
      "Iteration 58960: loss = 1.7575878e-10,3.973251e-09\n",
      "Iteration 58965: loss = 1.7576475e-10,3.972837e-09\n",
      "Iteration 58970: loss = 1.7577356e-10,3.9724144e-09\n",
      "Iteration 58975: loss = 1.7577716e-10,3.9720054e-09\n",
      "Iteration 58980: loss = 1.757809e-10,3.971597e-09\n",
      "Iteration 58985: loss = 1.7578518e-10,3.971186e-09\n",
      "Iteration 58990: loss = 1.7579023e-10,3.9707735e-09\n",
      "Iteration 58995: loss = 1.7579442e-10,3.970363e-09\n",
      "Iteration 59000: loss = 1.7579836e-10,3.9699533e-09\n",
      "Iteration 59005: loss = 1.7580122e-10,3.969548e-09\n",
      "Iteration 59010: loss = 1.7580254e-10,3.9691446e-09\n",
      "Iteration 59015: loss = 1.7580287e-10,3.9687458e-09\n",
      "Iteration 59020: loss = 1.7580022e-10,3.9683545e-09\n",
      "Iteration 59025: loss = 1.7579661e-10,3.967968e-09\n",
      "Iteration 59030: loss = 1.7579162e-10,3.967584e-09\n",
      "Iteration 59035: loss = 1.7578607e-10,3.967202e-09\n",
      "Iteration 59040: loss = 1.7577868e-10,3.966825e-09\n",
      "Iteration 59045: loss = 1.7577055e-10,3.966449e-09\n",
      "Iteration 59050: loss = 1.7576218e-10,3.9660755e-09\n",
      "Iteration 59055: loss = 1.7575284e-10,3.9657047e-09\n",
      "Iteration 59060: loss = 1.7574202e-10,3.965336e-09\n",
      "Iteration 59065: loss = 1.7573147e-10,3.964969e-09\n",
      "Iteration 59070: loss = 1.7571967e-10,3.9646038e-09\n",
      "Iteration 59075: loss = 1.7570694e-10,3.964242e-09\n",
      "Iteration 59080: loss = 1.7569424e-10,3.9638803e-09\n",
      "Iteration 59085: loss = 1.7567918e-10,3.9635246e-09\n",
      "Iteration 59090: loss = 1.7566415e-10,3.9631693e-09\n",
      "Iteration 59095: loss = 1.7564782e-10,3.9628176e-09\n",
      "Iteration 59100: loss = 1.756311e-10,3.9624672e-09\n",
      "Iteration 59105: loss = 1.7561418e-10,3.9621177e-09\n",
      "Iteration 59110: loss = 1.7559731e-10,3.961768e-09\n",
      "Iteration 59115: loss = 1.7558037e-10,3.9614187e-09\n",
      "Iteration 59120: loss = 1.7556305e-10,3.9610697e-09\n",
      "Iteration 59125: loss = 1.7555211e-10,3.9607015e-09\n",
      "Iteration 59130: loss = 1.7554903e-10,3.9603116e-09\n",
      "Iteration 59135: loss = 1.7554884e-10,3.9599115e-09\n",
      "Iteration 59140: loss = 1.7555109e-10,3.9595056e-09\n",
      "Iteration 59145: loss = 1.7555363e-10,3.959099e-09\n",
      "Iteration 59150: loss = 1.7555578e-10,3.9586925e-09\n",
      "Iteration 59155: loss = 1.7555636e-10,3.9582906e-09\n",
      "Iteration 59160: loss = 1.7555618e-10,3.95789e-09\n",
      "Iteration 59165: loss = 1.7555539e-10,3.957493e-09\n",
      "Iteration 59170: loss = 1.7555289e-10,3.957099e-09\n",
      "Iteration 59175: loss = 1.7554974e-10,3.956708e-09\n",
      "Iteration 59180: loss = 1.7554443e-10,3.956323e-09\n",
      "Iteration 59185: loss = 1.7553785e-10,3.9559427e-09\n",
      "Iteration 59190: loss = 1.755297e-10,3.9555657e-09\n",
      "Iteration 59195: loss = 1.7552104e-10,3.9551917e-09\n",
      "Iteration 59200: loss = 1.7551156e-10,3.9548183e-09\n",
      "Iteration 59205: loss = 1.7550111e-10,3.9544483e-09\n",
      "Iteration 59210: loss = 1.7548975e-10,3.9540815e-09\n",
      "Iteration 59215: loss = 1.7547784e-10,3.9537134e-09\n",
      "Iteration 59220: loss = 1.7546642e-10,3.9533483e-09\n",
      "Iteration 59225: loss = 1.754528e-10,3.952986e-09\n",
      "Iteration 59230: loss = 1.7543879e-10,3.952625e-09\n",
      "Iteration 59235: loss = 1.7542495e-10,3.952266e-09\n",
      "Iteration 59240: loss = 1.7541063e-10,3.9519064e-09\n",
      "Iteration 59245: loss = 1.7539603e-10,3.9515475e-09\n",
      "Iteration 59250: loss = 1.7538028e-10,3.9511923e-09\n",
      "Iteration 59255: loss = 1.7536399e-10,3.9508374e-09\n",
      "Iteration 59260: loss = 1.7534764e-10,3.9504835e-09\n",
      "Iteration 59265: loss = 1.753312e-10,3.9501313e-09\n",
      "Iteration 59270: loss = 1.7531465e-10,3.949778e-09\n",
      "Iteration 59275: loss = 1.7529761e-10,3.9494252e-09\n",
      "Iteration 59280: loss = 1.752809e-10,3.949074e-09\n",
      "Iteration 59285: loss = 1.7526265e-10,3.948724e-09\n",
      "Iteration 59290: loss = 1.752443e-10,3.9483767e-09\n",
      "Iteration 59295: loss = 1.7522561e-10,3.948028e-09\n",
      "Iteration 59300: loss = 1.7520706e-10,3.94768e-09\n",
      "Iteration 59305: loss = 1.7518853e-10,3.947333e-09\n",
      "Iteration 59310: loss = 1.7516948e-10,3.946986e-09\n",
      "Iteration 59315: loss = 1.7515067e-10,3.946639e-09\n",
      "Iteration 59320: loss = 1.7513146e-10,3.946292e-09\n",
      "Iteration 59325: loss = 1.7511248e-10,3.9459453e-09\n",
      "Iteration 59330: loss = 1.7509345e-10,3.945599e-09\n",
      "Iteration 59335: loss = 1.7507783e-10,3.9452432e-09\n",
      "Iteration 59340: loss = 1.7507033e-10,3.9448618e-09\n",
      "Iteration 59345: loss = 1.7506667e-10,3.9444696e-09\n",
      "Iteration 59350: loss = 1.7506603e-10,3.94407e-09\n",
      "Iteration 59355: loss = 1.7506528e-10,3.9436694e-09\n",
      "Iteration 59360: loss = 1.7506474e-10,3.9432693e-09\n",
      "Iteration 59365: loss = 1.750638e-10,3.942869e-09\n",
      "Iteration 59370: loss = 1.7506113e-10,3.9424743e-09\n",
      "Iteration 59375: loss = 1.7505808e-10,3.942081e-09\n",
      "Iteration 59380: loss = 1.7505385e-10,3.94169e-09\n",
      "Iteration 59385: loss = 1.750483e-10,3.9413024e-09\n",
      "Iteration 59390: loss = 1.7504263e-10,3.9409174e-09\n",
      "Iteration 59395: loss = 1.7503506e-10,3.9405346e-09\n",
      "Iteration 59400: loss = 1.7502738e-10,3.9401553e-09\n",
      "Iteration 59405: loss = 1.7501879e-10,3.9397756e-09\n",
      "Iteration 59410: loss = 1.7500912e-10,3.939401e-09\n",
      "Iteration 59415: loss = 1.749988e-10,3.9390278e-09\n",
      "Iteration 59420: loss = 1.7498641e-10,3.93866e-09\n",
      "Iteration 59425: loss = 1.7497366e-10,3.938294e-09\n",
      "Iteration 59430: loss = 1.7495966e-10,3.9379318e-09\n",
      "Iteration 59435: loss = 1.7494532e-10,3.9375707e-09\n",
      "Iteration 59440: loss = 1.7493014e-10,3.937211e-09\n",
      "Iteration 59445: loss = 1.7491557e-10,3.9368517e-09\n",
      "Iteration 59450: loss = 1.7342973e-10,3.9364845e-09\n",
      "Iteration 59455: loss = 1.7492181e-10,3.9360257e-09\n",
      "Iteration 59460: loss = 1.7493024e-10,3.9355967e-09\n",
      "Iteration 59465: loss = 1.7492674e-10,3.9352024e-09\n",
      "Iteration 59470: loss = 1.734503e-10,3.934807e-09\n",
      "Iteration 59475: loss = 1.7493534e-10,3.9343684e-09\n",
      "Iteration 59480: loss = 1.734609e-10,3.9339683e-09\n",
      "Iteration 59485: loss = 1.7494063e-10,3.9335446e-09\n",
      "Iteration 59490: loss = 1.7494366e-10,3.933131e-09\n",
      "Iteration 59495: loss = 1.749494e-10,3.93271e-09\n",
      "Iteration 59500: loss = 1.7495462e-10,3.93229e-09\n",
      "Iteration 59505: loss = 1.7348258e-10,3.9318824e-09\n",
      "Iteration 59510: loss = 1.749625e-10,3.9314583e-09\n",
      "Iteration 59515: loss = 1.7349298e-10,3.9310426e-09\n",
      "Iteration 59520: loss = 1.749713e-10,3.9306234e-09\n",
      "Iteration 59525: loss = 1.7497863e-10,3.930197e-09\n",
      "Iteration 59530: loss = 1.7350453e-10,3.9297943e-09\n",
      "Iteration 59535: loss = 1.7351016e-10,3.9293733e-09\n",
      "Iteration 59540: loss = 1.7498951e-10,3.928951e-09\n",
      "Iteration 59545: loss = 1.7499435e-10,3.9285304e-09\n",
      "Iteration 59550: loss = 1.750004e-10,3.9281076e-09\n",
      "Iteration 59555: loss = 1.7352753e-10,3.9277026e-09\n",
      "Iteration 59560: loss = 1.7500994e-10,3.92727e-09\n",
      "Iteration 59565: loss = 1.7353824e-10,3.92686e-09\n",
      "Iteration 59570: loss = 1.7354196e-10,3.926444e-09\n",
      "Iteration 59575: loss = 1.7354744e-10,3.926022e-09\n",
      "Iteration 59580: loss = 1.750259e-10,3.925602e-09\n",
      "Iteration 59585: loss = 1.7503116e-10,3.9251806e-09\n",
      "Iteration 59590: loss = 1.7356051e-10,3.924768e-09\n",
      "Iteration 59595: loss = 1.7356468e-10,3.9243497e-09\n",
      "Iteration 59600: loss = 1.7356988e-10,3.9239296e-09\n",
      "Iteration 59605: loss = 1.7357492e-10,3.9235086e-09\n",
      "Iteration 59610: loss = 1.7357783e-10,3.923094e-09\n",
      "Iteration 59615: loss = 1.7358331e-10,3.922672e-09\n",
      "Iteration 59620: loss = 1.7506463e-10,3.922243e-09\n",
      "Iteration 59625: loss = 1.7359171e-10,3.921836e-09\n",
      "Iteration 59630: loss = 1.7359779e-10,3.9214108e-09\n",
      "Iteration 59635: loss = 1.7507669e-10,3.9209898e-09\n",
      "Iteration 59640: loss = 1.736073e-10,3.920572e-09\n",
      "Iteration 59645: loss = 1.7508682e-10,3.9201478e-09\n",
      "Iteration 59650: loss = 1.736149e-10,3.919739e-09\n",
      "Iteration 59655: loss = 1.7361867e-10,3.9193204e-09\n",
      "Iteration 59660: loss = 1.736261e-10,3.918892e-09\n",
      "Iteration 59665: loss = 1.7363043e-10,3.9184744e-09\n",
      "Iteration 59670: loss = 1.7363357e-10,3.918057e-09\n",
      "Iteration 59675: loss = 1.7363809e-10,3.9176373e-09\n",
      "Iteration 59680: loss = 1.7511868e-10,3.91721e-09\n",
      "Iteration 59685: loss = 1.7364159e-10,3.9168153e-09\n",
      "Iteration 59690: loss = 1.7364095e-10,3.9164094e-09\n",
      "Iteration 59695: loss = 1.736422e-10,3.915999e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59700: loss = 1.7364331e-10,3.9155883e-09\n",
      "Iteration 59705: loss = 1.7364442e-10,3.9151784e-09\n",
      "Iteration 59710: loss = 1.7364528e-10,3.9147703e-09\n",
      "Iteration 59715: loss = 1.7364422e-10,3.9143657e-09\n",
      "Iteration 59720: loss = 1.7364253e-10,3.913962e-09\n",
      "Iteration 59725: loss = 1.7363938e-10,3.9135646e-09\n",
      "Iteration 59730: loss = 1.7363559e-10,3.9131685e-09\n",
      "Iteration 59735: loss = 1.7363096e-10,3.912775e-09\n",
      "Iteration 59740: loss = 1.736236e-10,3.9123886e-09\n",
      "Iteration 59745: loss = 1.7361575e-10,3.912005e-09\n",
      "Iteration 59750: loss = 1.7360637e-10,3.9116244e-09\n",
      "Iteration 59755: loss = 1.7359629e-10,3.9112464e-09\n",
      "Iteration 59760: loss = 1.735855e-10,3.9108703e-09\n",
      "Iteration 59765: loss = 1.7357421e-10,3.9104955e-09\n",
      "Iteration 59770: loss = 1.7356155e-10,3.9101242e-09\n",
      "Iteration 59775: loss = 1.7354866e-10,3.9097534e-09\n",
      "Iteration 59780: loss = 1.7353562e-10,3.909385e-09\n",
      "Iteration 59785: loss = 1.7352243e-10,3.909016e-09\n",
      "Iteration 59790: loss = 1.7350753e-10,3.908651e-09\n",
      "Iteration 59795: loss = 1.7349218e-10,3.9082866e-09\n",
      "Iteration 59800: loss = 1.7347686e-10,3.907923e-09\n",
      "Iteration 59805: loss = 1.7346143e-10,3.9075596e-09\n",
      "Iteration 59810: loss = 1.7344591e-10,3.907197e-09\n",
      "Iteration 59815: loss = 1.7342967e-10,3.906836e-09\n",
      "Iteration 59820: loss = 1.7341222e-10,3.9064783e-09\n",
      "Iteration 59825: loss = 1.7339608e-10,3.9061163e-09\n",
      "Iteration 59830: loss = 1.733883e-10,3.905731e-09\n",
      "Iteration 59835: loss = 1.7338607e-10,3.9053294e-09\n",
      "Iteration 59840: loss = 1.7338593e-10,3.9049204e-09\n",
      "Iteration 59845: loss = 1.7338793e-10,3.9045065e-09\n",
      "Iteration 59850: loss = 1.7339004e-10,3.904091e-09\n",
      "Iteration 59855: loss = 1.733907e-10,3.90368e-09\n",
      "Iteration 59860: loss = 1.7339045e-10,3.903272e-09\n",
      "Iteration 59865: loss = 1.7338926e-10,3.9028665e-09\n",
      "Iteration 59870: loss = 1.7338693e-10,3.902465e-09\n",
      "Iteration 59875: loss = 1.7338386e-10,3.902065e-09\n",
      "Iteration 59880: loss = 1.7337869e-10,3.901669e-09\n",
      "Iteration 59885: loss = 1.7337377e-10,3.901275e-09\n",
      "Iteration 59890: loss = 1.733673e-10,3.900885e-09\n",
      "Iteration 59895: loss = 1.7335951e-10,3.9004977e-09\n",
      "Iteration 59900: loss = 1.7335178e-10,3.9001105e-09\n",
      "Iteration 59905: loss = 1.7334216e-10,3.8997285e-09\n",
      "Iteration 59910: loss = 1.7333139e-10,3.8993515e-09\n",
      "Iteration 59915: loss = 1.7331925e-10,3.8989767e-09\n",
      "Iteration 59920: loss = 1.7330666e-10,3.898605e-09\n",
      "Iteration 59925: loss = 1.7329249e-10,3.8982355e-09\n",
      "Iteration 59930: loss = 1.7327867e-10,3.8978674e-09\n",
      "Iteration 59935: loss = 1.7326447e-10,3.8974983e-09\n",
      "Iteration 59940: loss = 1.7324986e-10,3.897131e-09\n",
      "Iteration 59945: loss = 1.7323405e-10,3.896767e-09\n",
      "Iteration 59950: loss = 1.7321818e-10,3.896404e-09\n",
      "Iteration 59955: loss = 1.7320177e-10,3.8960413e-09\n",
      "Iteration 59960: loss = 1.7318537e-10,3.8956793e-09\n",
      "Iteration 59965: loss = 1.7316888e-10,3.8953174e-09\n",
      "Iteration 59970: loss = 1.7315228e-10,3.8949564e-09\n",
      "Iteration 59975: loss = 1.7313521e-10,3.8945953e-09\n",
      "Iteration 59980: loss = 1.7311703e-10,3.894237e-09\n",
      "Iteration 59985: loss = 1.7309885e-10,3.8938808e-09\n",
      "Iteration 59990: loss = 1.7308067e-10,3.8935233e-09\n",
      "Iteration 59995: loss = 1.73062e-10,3.893167e-09\n",
      "Iteration 60000: loss = 1.730433e-10,3.892811e-09\n",
      "Iteration 60005: loss = 1.7302447e-10,3.8924552e-09\n",
      "Iteration 60010: loss = 1.7300565e-10,3.8920986e-09\n",
      "Iteration 60015: loss = 1.7298696e-10,3.891744e-09\n",
      "Iteration 60020: loss = 1.729679e-10,3.8913885e-09\n",
      "Iteration 60025: loss = 1.7294872e-10,3.8910333e-09\n",
      "Iteration 60030: loss = 1.7292938e-10,3.890678e-09\n",
      "Iteration 60035: loss = 1.7291411e-10,3.890312e-09\n",
      "Iteration 60040: loss = 1.7290687e-10,3.889923e-09\n",
      "Iteration 60045: loss = 1.7290443e-10,3.8895176e-09\n",
      "Iteration 60050: loss = 1.7290448e-10,3.889106e-09\n",
      "Iteration 60055: loss = 1.7290515e-10,3.888693e-09\n",
      "Iteration 60060: loss = 1.729058e-10,3.8882795e-09\n",
      "Iteration 60065: loss = 1.7290523e-10,3.887869e-09\n",
      "Iteration 60070: loss = 1.7290357e-10,3.887462e-09\n",
      "Iteration 60075: loss = 1.7290175e-10,3.8870556e-09\n",
      "Iteration 60080: loss = 1.7289788e-10,3.8866554e-09\n",
      "Iteration 60085: loss = 1.7289205e-10,3.8862598e-09\n",
      "Iteration 60090: loss = 1.728857e-10,3.8858667e-09\n",
      "Iteration 60095: loss = 1.7287756e-10,3.8854786e-09\n",
      "Iteration 60100: loss = 1.7286923e-10,3.8850914e-09\n",
      "Iteration 60105: loss = 1.7286028e-10,3.884707e-09\n",
      "Iteration 60110: loss = 1.7284947e-10,3.884324e-09\n",
      "Iteration 60115: loss = 1.7283903e-10,3.8839434e-09\n",
      "Iteration 60120: loss = 1.7282793e-10,3.8835632e-09\n",
      "Iteration 60125: loss = 1.7281554e-10,3.8831867e-09\n",
      "Iteration 60130: loss = 1.7280262e-10,3.8828123e-09\n",
      "Iteration 60135: loss = 1.7278938e-10,3.8824384e-09\n",
      "Iteration 60140: loss = 1.7131226e-10,3.8820653e-09\n",
      "Iteration 60145: loss = 1.7278586e-10,3.8816235e-09\n",
      "Iteration 60150: loss = 1.7279245e-10,3.881191e-09\n",
      "Iteration 60155: loss = 1.7132702e-10,3.8807833e-09\n",
      "Iteration 60160: loss = 1.7280256e-10,3.8803356e-09\n",
      "Iteration 60165: loss = 1.728016e-10,3.8799253e-09\n",
      "Iteration 60170: loss = 1.7280888e-10,3.8794914e-09\n",
      "Iteration 60175: loss = 1.7281772e-10,3.8790535e-09\n",
      "Iteration 60180: loss = 1.7134975e-10,3.878653e-09\n",
      "Iteration 60185: loss = 1.7282514e-10,3.8782053e-09\n",
      "Iteration 60190: loss = 1.7136016e-10,3.8777945e-09\n",
      "Iteration 60195: loss = 1.7283364e-10,3.877353e-09\n",
      "Iteration 60200: loss = 1.7137004e-10,3.87694e-09\n",
      "Iteration 60205: loss = 1.7283686e-10,3.8765164e-09\n",
      "Iteration 60210: loss = 1.7135791e-10,3.876148e-09\n",
      "Iteration 60215: loss = 1.7281858e-10,3.8757437e-09\n",
      "Iteration 60220: loss = 1.7133973e-10,3.8753742e-09\n",
      "Iteration 60225: loss = 1.7132927e-10,3.874992e-09\n",
      "Iteration 60230: loss = 1.727849e-10,3.8746006e-09\n",
      "Iteration 60235: loss = 1.7277717e-10,3.8742103e-09\n",
      "Iteration 60240: loss = 1.7276534e-10,3.8738306e-09\n",
      "Iteration 60245: loss = 1.7275498e-10,3.8734473e-09\n",
      "Iteration 60250: loss = 1.7274597e-10,3.8730597e-09\n",
      "Iteration 60255: loss = 1.7273556e-10,3.872676e-09\n",
      "Iteration 60260: loss = 1.72726e-10,3.8722905e-09\n",
      "Iteration 60265: loss = 1.7124989e-10,3.8719117e-09\n",
      "Iteration 60270: loss = 1.727087e-10,3.8715116e-09\n",
      "Iteration 60275: loss = 1.7123018e-10,3.8711416e-09\n",
      "Iteration 60280: loss = 1.7269042e-10,3.8707366e-09\n",
      "Iteration 60285: loss = 1.7120828e-10,3.8703774e-09\n",
      "Iteration 60290: loss = 1.7267088e-10,3.8699652e-09\n",
      "Iteration 60295: loss = 1.7265793e-10,3.869588e-09\n",
      "Iteration 60300: loss = 1.726473e-10,3.869205e-09\n",
      "Iteration 60305: loss = 1.7117098e-10,3.868828e-09\n",
      "Iteration 60310: loss = 1.7263023e-10,3.868426e-09\n",
      "Iteration 60315: loss = 1.7115215e-10,3.8680534e-09\n",
      "Iteration 60320: loss = 1.7114306e-10,3.8676644e-09\n",
      "Iteration 60325: loss = 1.7113311e-10,3.8672785e-09\n",
      "Iteration 60330: loss = 1.7112282e-10,3.8668944e-09\n",
      "Iteration 60335: loss = 1.7257828e-10,3.866503e-09\n",
      "Iteration 60340: loss = 1.7256986e-10,3.8661123e-09\n",
      "Iteration 60345: loss = 1.710918e-10,3.86574e-09\n",
      "Iteration 60350: loss = 1.7255201e-10,3.8653343e-09\n",
      "Iteration 60355: loss = 1.7255054e-10,3.864923e-09\n",
      "Iteration 60360: loss = 1.7253489e-10,3.8645536e-09\n",
      "Iteration 60365: loss = 1.7105241e-10,3.8641934e-09\n",
      "Iteration 60370: loss = 1.7251622e-10,3.863778e-09\n",
      "Iteration 60375: loss = 1.7250194e-10,3.863404e-09\n",
      "Iteration 60380: loss = 1.7248993e-10,3.8630237e-09\n",
      "Iteration 60385: loss = 1.7248353e-10,3.8626267e-09\n",
      "Iteration 60390: loss = 1.7100538e-10,3.8622527e-09\n",
      "Iteration 60395: loss = 1.7099568e-10,3.8618655e-09\n",
      "Iteration 60400: loss = 1.7245105e-10,3.861474e-09\n",
      "Iteration 60405: loss = 1.724429e-10,3.8610817e-09\n",
      "Iteration 60410: loss = 1.7096678e-10,3.8607024e-09\n",
      "Iteration 60415: loss = 1.7242448e-10,3.8603036e-09\n",
      "Iteration 60420: loss = 1.7094491e-10,3.8599346e-09\n",
      "Iteration 60425: loss = 1.7240648e-10,3.8595243e-09\n",
      "Iteration 60430: loss = 1.709261e-10,3.8591574e-09\n",
      "Iteration 60435: loss = 1.7238783e-10,3.858747e-09\n",
      "Iteration 60440: loss = 1.7090686e-10,3.8583803e-09\n",
      "Iteration 60445: loss = 1.7236496e-10,3.8579793e-09\n",
      "Iteration 60450: loss = 1.7088715e-10,3.8576053e-09\n",
      "Iteration 60455: loss = 1.7234562e-10,3.8572043e-09\n",
      "Iteration 60460: loss = 1.7086933e-10,3.8568246e-09\n",
      "Iteration 60465: loss = 1.7232397e-10,3.8564343e-09\n",
      "Iteration 60470: loss = 1.7231623e-10,3.8560404e-09\n",
      "Iteration 60475: loss = 1.723045e-10,3.855658e-09\n",
      "Iteration 60480: loss = 1.7229539e-10,3.855267e-09\n",
      "Iteration 60485: loss = 1.7228645e-10,3.854877e-09\n",
      "Iteration 60490: loss = 1.7081046e-10,3.8544963e-09\n",
      "Iteration 60495: loss = 1.7080153e-10,3.8541046e-09\n",
      "Iteration 60500: loss = 1.7079145e-10,3.8537173e-09\n",
      "Iteration 60505: loss = 1.7224683e-10,3.8533243e-09\n",
      "Iteration 60510: loss = 1.7077101e-10,3.8529424e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60515: loss = 1.7076274e-10,3.85255e-09\n",
      "Iteration 60520: loss = 1.7221737e-10,3.8521595e-09\n",
      "Iteration 60525: loss = 1.7220853e-10,3.851767e-09\n",
      "Iteration 60530: loss = 1.7219877e-10,3.8513783e-09\n",
      "Iteration 60535: loss = 1.7072325e-10,3.850995e-09\n",
      "Iteration 60540: loss = 1.7071256e-10,3.8506087e-09\n",
      "Iteration 60545: loss = 1.7216979e-10,3.85021e-09\n",
      "Iteration 60550: loss = 1.7069295e-10,3.849831e-09\n",
      "Iteration 60555: loss = 1.7068508e-10,3.8494354e-09\n",
      "Iteration 60560: loss = 1.7067558e-10,3.8490455e-09\n",
      "Iteration 60565: loss = 1.7066548e-10,3.848658e-09\n",
      "Iteration 60570: loss = 1.7212005e-10,3.848266e-09\n",
      "Iteration 60575: loss = 1.7064643e-10,3.847876e-09\n",
      "Iteration 60580: loss = 1.7063669e-10,3.8474868e-09\n",
      "Iteration 60585: loss = 1.7062661e-10,3.8470995e-09\n",
      "Iteration 60590: loss = 1.720815e-10,3.846707e-09\n",
      "Iteration 60595: loss = 1.706066e-10,3.84632e-09\n",
      "Iteration 60600: loss = 1.7059724e-10,3.845929e-09\n",
      "Iteration 60605: loss = 1.7205289e-10,3.845535e-09\n",
      "Iteration 60610: loss = 1.705781e-10,3.845148e-09\n",
      "Iteration 60615: loss = 1.720341e-10,3.8447516e-09\n",
      "Iteration 60620: loss = 1.7202335e-10,3.844365e-09\n",
      "Iteration 60625: loss = 1.7201636e-10,3.843966e-09\n",
      "Iteration 60630: loss = 1.7053801e-10,3.8435917e-09\n",
      "Iteration 60635: loss = 1.719979e-10,3.8431827e-09\n",
      "Iteration 60640: loss = 1.7051711e-10,3.8428136e-09\n",
      "Iteration 60645: loss = 1.7197938e-10,3.8423993e-09\n",
      "Iteration 60650: loss = 1.6904633e-10,3.842024e-09\n",
      "Iteration 60655: loss = 1.7197221e-10,3.8415813e-09\n",
      "Iteration 60660: loss = 1.7051138e-10,3.8411527e-09\n",
      "Iteration 60665: loss = 1.7051623e-10,3.8407197e-09\n",
      "Iteration 60670: loss = 1.7052067e-10,3.8402876e-09\n",
      "Iteration 60675: loss = 1.7052382e-10,3.8398587e-09\n",
      "Iteration 60680: loss = 1.7199486e-10,3.8394177e-09\n",
      "Iteration 60685: loss = 1.7053504e-10,3.8389856e-09\n",
      "Iteration 60690: loss = 1.705386e-10,3.8385553e-09\n",
      "Iteration 60695: loss = 1.7054348e-10,3.8381214e-09\n",
      "Iteration 60700: loss = 1.7054813e-10,3.8376875e-09\n",
      "Iteration 60705: loss = 1.7055295e-10,3.8372536e-09\n",
      "Iteration 60710: loss = 1.7055952e-10,3.836814e-09\n",
      "Iteration 60715: loss = 1.7056094e-10,3.8363903e-09\n",
      "Iteration 60720: loss = 1.705673e-10,3.835951e-09\n",
      "Iteration 60725: loss = 1.7057357e-10,3.8355115e-09\n",
      "Iteration 60730: loss = 1.7057593e-10,3.835085e-09\n",
      "Iteration 60735: loss = 1.7058353e-10,3.834643e-09\n",
      "Iteration 60740: loss = 1.7058693e-10,3.834211e-09\n",
      "Iteration 60745: loss = 1.7059226e-10,3.8337764e-09\n",
      "Iteration 60750: loss = 1.7059387e-10,3.8333514e-09\n",
      "Iteration 60755: loss = 1.7059944e-10,3.832913e-09\n",
      "Iteration 60760: loss = 1.7060699e-10,3.832471e-09\n",
      "Iteration 60765: loss = 1.7060879e-10,3.8320453e-09\n",
      "Iteration 60770: loss = 1.7060885e-10,3.8316235e-09\n",
      "Iteration 60775: loss = 1.7061026e-10,3.8311994e-09\n",
      "Iteration 60780: loss = 1.7061179e-10,3.8307744e-09\n",
      "Iteration 60785: loss = 1.7061312e-10,3.8303503e-09\n",
      "Iteration 60790: loss = 1.7061376e-10,3.8299275e-09\n",
      "Iteration 60795: loss = 1.7061315e-10,3.829508e-09\n",
      "Iteration 60800: loss = 1.7061169e-10,3.8290913e-09\n",
      "Iteration 60805: loss = 1.7060946e-10,3.828677e-09\n",
      "Iteration 60810: loss = 1.7060577e-10,3.828267e-09\n",
      "Iteration 60815: loss = 1.7060135e-10,3.8278585e-09\n",
      "Iteration 60820: loss = 1.7059544e-10,3.8274535e-09\n",
      "Iteration 60825: loss = 1.7058881e-10,3.827051e-09\n",
      "Iteration 60830: loss = 1.7058151e-10,3.8266514e-09\n",
      "Iteration 60835: loss = 1.7057154e-10,3.82626e-09\n",
      "Iteration 60840: loss = 1.705605e-10,3.8258703e-09\n",
      "Iteration 60845: loss = 1.705494e-10,3.8254813e-09\n",
      "Iteration 60850: loss = 1.7053692e-10,3.8250967e-09\n",
      "Iteration 60855: loss = 1.7052415e-10,3.824713e-09\n",
      "Iteration 60860: loss = 1.70511e-10,3.8243297e-09\n",
      "Iteration 60865: loss = 1.704976e-10,3.8239465e-09\n",
      "Iteration 60870: loss = 1.7048334e-10,3.8235672e-09\n",
      "Iteration 60875: loss = 1.704684e-10,3.8231884e-09\n",
      "Iteration 60880: loss = 1.7045314e-10,3.8228114e-09\n",
      "Iteration 60885: loss = 1.704376e-10,3.822435e-09\n",
      "Iteration 60890: loss = 1.7042222e-10,3.8220587e-09\n",
      "Iteration 60895: loss = 1.7040612e-10,3.821683e-09\n",
      "Iteration 60900: loss = 1.703892e-10,3.821311e-09\n",
      "Iteration 60905: loss = 1.7037192e-10,3.8209387e-09\n",
      "Iteration 60910: loss = 1.703545e-10,3.820568e-09\n",
      "Iteration 60915: loss = 1.7033658e-10,3.8201966e-09\n",
      "Iteration 60920: loss = 1.7031888e-10,3.8198262e-09\n",
      "Iteration 60925: loss = 1.7030118e-10,3.819456e-09\n",
      "Iteration 60930: loss = 1.7028384e-10,3.8190846e-09\n",
      "Iteration 60935: loss = 1.702729e-10,3.818693e-09\n",
      "Iteration 60940: loss = 1.7026859e-10,3.8182826e-09\n",
      "Iteration 60945: loss = 1.7026791e-10,3.8178616e-09\n",
      "Iteration 60950: loss = 1.702695e-10,3.817434e-09\n",
      "Iteration 60955: loss = 1.7027074e-10,3.817007e-09\n",
      "Iteration 60960: loss = 1.7027202e-10,3.81658e-09\n",
      "Iteration 60965: loss = 1.7027185e-10,3.816157e-09\n",
      "Iteration 60970: loss = 1.7026967e-10,3.8157397e-09\n",
      "Iteration 60975: loss = 1.702669e-10,3.8153254e-09\n",
      "Iteration 60980: loss = 1.702623e-10,3.8149155e-09\n",
      "Iteration 60985: loss = 1.7025702e-10,3.814508e-09\n",
      "Iteration 60990: loss = 1.7025097e-10,3.8141024e-09\n",
      "Iteration 60995: loss = 1.7024372e-10,3.8137e-09\n",
      "Iteration 61000: loss = 1.702356e-10,3.8132986e-09\n",
      "Iteration 61005: loss = 1.7022712e-10,3.812902e-09\n",
      "Iteration 61010: loss = 1.702171e-10,3.812506e-09\n",
      "Iteration 61015: loss = 1.7020697e-10,3.812113e-09\n",
      "Iteration 61020: loss = 1.7019582e-10,3.8117216e-09\n",
      "Iteration 61025: loss = 1.7018352e-10,3.8113317e-09\n",
      "Iteration 61030: loss = 1.7017004e-10,3.810949e-09\n",
      "Iteration 61035: loss = 1.7015599e-10,3.810566e-09\n",
      "Iteration 61040: loss = 1.7014129e-10,3.810185e-09\n",
      "Iteration 61045: loss = 1.7012534e-10,3.809808e-09\n",
      "Iteration 61050: loss = 1.7010947e-10,3.8094314e-09\n",
      "Iteration 61055: loss = 1.7009322e-10,3.8090544e-09\n",
      "Iteration 61060: loss = 1.7007702e-10,3.8086787e-09\n",
      "Iteration 61065: loss = 1.7006076e-10,3.808303e-09\n",
      "Iteration 61070: loss = 1.7004408e-10,3.8079273e-09\n",
      "Iteration 61075: loss = 1.7002634e-10,3.8075543e-09\n",
      "Iteration 61080: loss = 1.7000808e-10,3.807183e-09\n",
      "Iteration 61085: loss = 1.6999006e-10,3.806812e-09\n",
      "Iteration 61090: loss = 1.699713e-10,3.8064423e-09\n",
      "Iteration 61095: loss = 1.6995332e-10,3.8060715e-09\n",
      "Iteration 61100: loss = 1.6993468e-10,3.805701e-09\n",
      "Iteration 61105: loss = 1.6991615e-10,3.8053307e-09\n",
      "Iteration 61110: loss = 1.6989732e-10,3.804961e-09\n",
      "Iteration 61115: loss = 1.6987868e-10,3.804591e-09\n",
      "Iteration 61120: loss = 1.6985997e-10,3.804222e-09\n",
      "Iteration 61125: loss = 1.6984038e-10,3.8038546e-09\n",
      "Iteration 61130: loss = 1.6982037e-10,3.8034877e-09\n",
      "Iteration 61135: loss = 1.698005e-10,3.8031214e-09\n",
      "Iteration 61140: loss = 1.6978946e-10,3.802728e-09\n",
      "Iteration 61145: loss = 1.697845e-10,3.8023162e-09\n",
      "Iteration 61150: loss = 1.6978319e-10,3.801895e-09\n",
      "Iteration 61155: loss = 1.6978359e-10,3.8014685e-09\n",
      "Iteration 61160: loss = 1.6978374e-10,3.8010413e-09\n",
      "Iteration 61165: loss = 1.6978423e-10,3.8006145e-09\n",
      "Iteration 61170: loss = 1.6978276e-10,3.800193e-09\n",
      "Iteration 61175: loss = 1.697808e-10,3.7997725e-09\n",
      "Iteration 61180: loss = 1.6977832e-10,3.7993537e-09\n",
      "Iteration 61185: loss = 1.6977396e-10,3.79894e-09\n",
      "Iteration 61190: loss = 1.6976921e-10,3.7985264e-09\n",
      "Iteration 61195: loss = 1.6976208e-10,3.798123e-09\n",
      "Iteration 61200: loss = 1.6975392e-10,3.797721e-09\n",
      "Iteration 61205: loss = 1.6829266e-10,3.7973136e-09\n",
      "Iteration 61210: loss = 1.6975926e-10,3.7968517e-09\n",
      "Iteration 61215: loss = 1.6976286e-10,3.796415e-09\n",
      "Iteration 61220: loss = 1.6976476e-10,3.7959826e-09\n",
      "Iteration 61225: loss = 1.6976996e-10,3.7955408e-09\n",
      "Iteration 61230: loss = 1.697742e-10,3.795101e-09\n",
      "Iteration 61235: loss = 1.6832402e-10,3.794663e-09\n",
      "Iteration 61240: loss = 1.6978645e-10,3.7942116e-09\n",
      "Iteration 61245: loss = 1.697904e-10,3.793773e-09\n",
      "Iteration 61250: loss = 1.6833744e-10,3.793341e-09\n",
      "Iteration 61255: loss = 1.6980284e-10,3.792882e-09\n",
      "Iteration 61260: loss = 1.6834663e-10,3.79246e-09\n",
      "Iteration 61265: loss = 1.6980894e-10,3.792009e-09\n",
      "Iteration 61270: loss = 1.6981427e-10,3.7915666e-09\n",
      "Iteration 61275: loss = 1.698184e-10,3.7911274e-09\n",
      "Iteration 61280: loss = 1.6836793e-10,3.790689e-09\n",
      "Iteration 61285: loss = 1.6837337e-10,3.7902463e-09\n",
      "Iteration 61290: loss = 1.6983309e-10,3.789802e-09\n",
      "Iteration 61295: loss = 1.6838188e-10,3.789365e-09\n",
      "Iteration 61300: loss = 1.6838747e-10,3.7889216e-09\n",
      "Iteration 61305: loss = 1.6984823e-10,3.7884758e-09\n",
      "Iteration 61310: loss = 1.6985263e-10,3.7880343e-09\n",
      "Iteration 61315: loss = 1.6840174e-10,3.787597e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61320: loss = 1.6986447e-10,3.787144e-09\n",
      "Iteration 61325: loss = 1.6840955e-10,3.786719e-09\n",
      "Iteration 61330: loss = 1.6841813e-10,3.7862637e-09\n",
      "Iteration 61335: loss = 1.6842283e-10,3.785823e-09\n",
      "Iteration 61340: loss = 1.6842631e-10,3.7853845e-09\n",
      "Iteration 61345: loss = 1.6843299e-10,3.784937e-09\n",
      "Iteration 61350: loss = 1.6843711e-10,3.7844963e-09\n",
      "Iteration 61355: loss = 1.6989905e-10,3.7840446e-09\n",
      "Iteration 61360: loss = 1.6844436e-10,3.7836183e-09\n",
      "Iteration 61365: loss = 1.6990902e-10,3.7831596e-09\n",
      "Iteration 61370: loss = 1.6845397e-10,3.7827332e-09\n",
      "Iteration 61375: loss = 1.6846262e-10,3.782279e-09\n",
      "Iteration 61380: loss = 1.684643e-10,3.7818464e-09\n",
      "Iteration 61385: loss = 1.6846811e-10,3.7814063e-09\n",
      "Iteration 61390: loss = 1.6993384e-10,3.780944e-09\n",
      "Iteration 61395: loss = 1.6848246e-10,3.7805066e-09\n",
      "Iteration 61400: loss = 1.6848807e-10,3.780062e-09\n",
      "Iteration 61405: loss = 1.6849193e-10,3.779621e-09\n",
      "Iteration 61410: loss = 1.684979e-10,3.779175e-09\n",
      "Iteration 61415: loss = 1.6850255e-10,3.778732e-09\n",
      "Iteration 61420: loss = 1.6850825e-10,3.778287e-09\n",
      "Iteration 61425: loss = 1.6851333e-10,3.7778425e-09\n",
      "Iteration 61430: loss = 1.6851569e-10,3.7774064e-09\n",
      "Iteration 61435: loss = 1.6851943e-10,3.7769663e-09\n",
      "Iteration 61440: loss = 1.6852608e-10,3.7765173e-09\n",
      "Iteration 61445: loss = 1.6853287e-10,3.7760692e-09\n",
      "Iteration 61450: loss = 1.6999396e-10,3.7756203e-09\n",
      "Iteration 61455: loss = 1.6853689e-10,3.7751984e-09\n",
      "Iteration 61460: loss = 1.685364e-10,3.7747707e-09\n",
      "Iteration 61465: loss = 1.6853652e-10,3.7743417e-09\n",
      "Iteration 61470: loss = 1.6853642e-10,3.773912e-09\n",
      "Iteration 61475: loss = 1.685359e-10,3.7734846e-09\n",
      "Iteration 61480: loss = 1.6852864e-10,3.7730756e-09\n",
      "Iteration 61485: loss = 1.6851297e-10,3.7726924e-09\n",
      "Iteration 61490: loss = 1.6849962e-10,3.772301e-09\n",
      "Iteration 61495: loss = 1.6849078e-10,3.7718975e-09\n",
      "Iteration 61500: loss = 1.684836e-10,3.7714885e-09\n",
      "Iteration 61505: loss = 1.6847408e-10,3.7710848e-09\n",
      "Iteration 61510: loss = 1.6846437e-10,3.770684e-09\n",
      "Iteration 61515: loss = 1.6845404e-10,3.7702845e-09\n",
      "Iteration 61520: loss = 1.6844448e-10,3.769882e-09\n",
      "Iteration 61525: loss = 1.6843303e-10,3.7694847e-09\n",
      "Iteration 61530: loss = 1.6842189e-10,3.769088e-09\n",
      "Iteration 61535: loss = 1.684121e-10,3.7686867e-09\n",
      "Iteration 61540: loss = 1.6840214e-10,3.7682852e-09\n",
      "Iteration 61545: loss = 1.6839147e-10,3.7678856e-09\n",
      "Iteration 61550: loss = 1.6837814e-10,3.767494e-09\n",
      "Iteration 61555: loss = 1.6836525e-10,3.7671026e-09\n",
      "Iteration 61560: loss = 1.6835146e-10,3.7667105e-09\n",
      "Iteration 61565: loss = 1.6833802e-10,3.76632e-09\n",
      "Iteration 61570: loss = 1.6832385e-10,3.765931e-09\n",
      "Iteration 61575: loss = 1.6830888e-10,3.765545e-09\n",
      "Iteration 61580: loss = 1.6829342e-10,3.7651593e-09\n",
      "Iteration 61585: loss = 1.6827757e-10,3.7647747e-09\n",
      "Iteration 61590: loss = 1.6826203e-10,3.7643897e-09\n",
      "Iteration 61595: loss = 1.682459e-10,3.764005e-09\n",
      "Iteration 61600: loss = 1.6822997e-10,3.7636214e-09\n",
      "Iteration 61605: loss = 1.6821226e-10,3.763242e-09\n",
      "Iteration 61610: loss = 1.6819464e-10,3.7628625e-09\n",
      "Iteration 61615: loss = 1.6817688e-10,3.7624837e-09\n",
      "Iteration 61620: loss = 1.6815917e-10,3.7621035e-09\n",
      "Iteration 61625: loss = 1.6814396e-10,3.7617167e-09\n",
      "Iteration 61630: loss = 1.6813713e-10,3.7613055e-09\n",
      "Iteration 61635: loss = 1.6813546e-10,3.7608783e-09\n",
      "Iteration 61640: loss = 1.6813646e-10,3.7604435e-09\n",
      "Iteration 61645: loss = 1.6813752e-10,3.7600083e-09\n",
      "Iteration 61650: loss = 1.6813907e-10,3.759573e-09\n",
      "Iteration 61655: loss = 1.6813938e-10,3.759138e-09\n",
      "Iteration 61660: loss = 1.6813968e-10,3.758705e-09\n",
      "Iteration 61665: loss = 1.6813863e-10,3.758277e-09\n",
      "Iteration 61670: loss = 1.6813644e-10,3.757849e-09\n",
      "Iteration 61675: loss = 1.681321e-10,3.7574317e-09\n",
      "Iteration 61680: loss = 1.6812642e-10,3.7570156e-09\n",
      "Iteration 61685: loss = 1.6811624e-10,3.756613e-09\n",
      "Iteration 61690: loss = 1.6810492e-10,3.7562136e-09\n",
      "Iteration 61695: loss = 1.6809447e-10,3.7558117e-09\n",
      "Iteration 61700: loss = 1.6808438e-10,3.7554093e-09\n",
      "Iteration 61705: loss = 1.6807385e-10,3.7550065e-09\n",
      "Iteration 61710: loss = 1.6806356e-10,3.7546046e-09\n",
      "Iteration 61715: loss = 1.6805253e-10,3.7542036e-09\n",
      "Iteration 61720: loss = 1.6804079e-10,3.753805e-09\n",
      "Iteration 61725: loss = 1.6802848e-10,3.7534083e-09\n",
      "Iteration 61730: loss = 1.6801577e-10,3.753013e-09\n",
      "Iteration 61735: loss = 1.6800265e-10,3.752618e-09\n",
      "Iteration 61740: loss = 1.6798918e-10,3.7522248e-09\n",
      "Iteration 61745: loss = 1.679744e-10,3.7518344e-09\n",
      "Iteration 61750: loss = 1.6795874e-10,3.751448e-09\n",
      "Iteration 61755: loss = 1.6794245e-10,3.751063e-09\n",
      "Iteration 61760: loss = 1.6792566e-10,3.750679e-09\n",
      "Iteration 61765: loss = 1.6790895e-10,3.7502956e-09\n",
      "Iteration 61770: loss = 1.6789171e-10,3.749913e-09\n",
      "Iteration 61775: loss = 1.6787367e-10,3.7495322e-09\n",
      "Iteration 61780: loss = 1.6785533e-10,3.7491534e-09\n",
      "Iteration 61785: loss = 1.6783673e-10,3.748775e-09\n",
      "Iteration 61790: loss = 1.6781787e-10,3.7483967e-09\n",
      "Iteration 61795: loss = 1.6779927e-10,3.748018e-09\n",
      "Iteration 61800: loss = 1.6778079e-10,3.7476395e-09\n",
      "Iteration 61805: loss = 1.6776165e-10,3.7472616e-09\n",
      "Iteration 61810: loss = 1.6774292e-10,3.7468832e-09\n",
      "Iteration 61815: loss = 1.6772368e-10,3.7465053e-09\n",
      "Iteration 61820: loss = 1.6770464e-10,3.746128e-09\n",
      "Iteration 61825: loss = 1.6768566e-10,3.74575e-09\n",
      "Iteration 61830: loss = 1.6766759e-10,3.7453685e-09\n",
      "Iteration 61835: loss = 1.6765823e-10,3.7449626e-09\n",
      "Iteration 61840: loss = 1.6765449e-10,3.7445385e-09\n",
      "Iteration 61845: loss = 1.6765349e-10,3.7441072e-09\n",
      "Iteration 61850: loss = 1.6765539e-10,3.7436676e-09\n",
      "Iteration 61855: loss = 1.6766306e-10,3.74321e-09\n",
      "Iteration 61860: loss = 1.6766709e-10,3.7427634e-09\n",
      "Iteration 61865: loss = 1.6767086e-10,3.742316e-09\n",
      "Iteration 61870: loss = 1.6767616e-10,3.7418664e-09\n",
      "Iteration 61875: loss = 1.6768142e-10,3.741416e-09\n",
      "Iteration 61880: loss = 1.676861e-10,3.7409675e-09\n",
      "Iteration 61885: loss = 1.6769096e-10,3.740518e-09\n",
      "Iteration 61890: loss = 1.6769568e-10,3.7400683e-09\n",
      "Iteration 61895: loss = 1.6770123e-10,3.7396175e-09\n",
      "Iteration 61900: loss = 1.6626063e-10,3.73917e-09\n",
      "Iteration 61905: loss = 1.6626349e-10,3.7387258e-09\n",
      "Iteration 61910: loss = 1.6771821e-10,3.738261e-09\n",
      "Iteration 61915: loss = 1.6627598e-10,3.7378176e-09\n",
      "Iteration 61920: loss = 1.6628036e-10,3.73737e-09\n",
      "Iteration 61925: loss = 1.677314e-10,3.736916e-09\n",
      "Iteration 61930: loss = 1.662906e-10,3.736469e-09\n",
      "Iteration 61935: loss = 1.6629531e-10,3.736019e-09\n",
      "Iteration 61940: loss = 1.6774948e-10,3.7355563e-09\n",
      "Iteration 61945: loss = 1.6630082e-10,3.735131e-09\n",
      "Iteration 61950: loss = 1.6631203e-10,3.7346637e-09\n",
      "Iteration 61955: loss = 1.6631674e-10,3.7342134e-09\n",
      "Iteration 61960: loss = 1.6632078e-10,3.733766e-09\n",
      "Iteration 61965: loss = 1.6632608e-10,3.7333137e-09\n",
      "Iteration 61970: loss = 1.6633182e-10,3.7328607e-09\n",
      "Iteration 61975: loss = 1.67783e-10,3.732407e-09\n",
      "Iteration 61980: loss = 1.6634026e-10,3.7319636e-09\n",
      "Iteration 61985: loss = 1.6779328e-10,3.7315036e-09\n",
      "Iteration 61990: loss = 1.6634587e-10,3.7310746e-09\n",
      "Iteration 61995: loss = 1.6635504e-10,3.7306114e-09\n",
      "Iteration 62000: loss = 1.6636177e-10,3.7301553e-09\n",
      "Iteration 62005: loss = 1.6636814e-10,3.7297e-09\n",
      "Iteration 62010: loss = 1.6637082e-10,3.729255e-09\n",
      "Iteration 62015: loss = 1.6637318e-10,3.728811e-09\n",
      "Iteration 62020: loss = 1.6638191e-10,3.728349e-09\n",
      "Iteration 62025: loss = 1.6638822e-10,3.727894e-09\n",
      "Iteration 62030: loss = 1.6638735e-10,3.72746e-09\n",
      "Iteration 62035: loss = 1.6639222e-10,3.727009e-09\n",
      "Iteration 62040: loss = 1.6639952e-10,3.7265506e-09\n",
      "Iteration 62045: loss = 1.6640749e-10,3.7260905e-09\n",
      "Iteration 62050: loss = 1.6641553e-10,3.7256305e-09\n",
      "Iteration 62055: loss = 1.6641703e-10,3.7251888e-09\n",
      "Iteration 62060: loss = 1.6641592e-10,3.7247554e-09\n",
      "Iteration 62065: loss = 1.6641849e-10,3.724311e-09\n",
      "Iteration 62070: loss = 1.664207e-10,3.723867e-09\n",
      "Iteration 62075: loss = 1.6642064e-10,3.7234291e-09\n",
      "Iteration 62080: loss = 1.6642003e-10,3.7229955e-09\n",
      "Iteration 62085: loss = 1.6641855e-10,3.7225623e-09\n",
      "Iteration 62090: loss = 1.6641515e-10,3.7221342e-09\n",
      "Iteration 62095: loss = 1.6641151e-10,3.7217083e-09\n",
      "Iteration 62100: loss = 1.6640639e-10,3.7212844e-09\n",
      "Iteration 62105: loss = 1.6640046e-10,3.7208645e-09\n",
      "Iteration 62110: loss = 1.6639379e-10,3.7204462e-09\n",
      "Iteration 62115: loss = 1.6638564e-10,3.7200316e-09\n",
      "Iteration 62120: loss = 1.6637718e-10,3.7196197e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62125: loss = 1.6636813e-10,3.7192076e-09\n",
      "Iteration 62130: loss = 1.6635648e-10,3.7188028e-09\n",
      "Iteration 62135: loss = 1.663445e-10,3.7184018e-09\n",
      "Iteration 62140: loss = 1.6633182e-10,3.7180012e-09\n",
      "Iteration 62145: loss = 1.6631845e-10,3.7176018e-09\n",
      "Iteration 62150: loss = 1.663042e-10,3.7172059e-09\n",
      "Iteration 62155: loss = 1.6628943e-10,3.7168106e-09\n",
      "Iteration 62160: loss = 1.6627462e-10,3.716416e-09\n",
      "Iteration 62165: loss = 1.6625956e-10,3.7160224e-09\n",
      "Iteration 62170: loss = 1.662442e-10,3.7156287e-09\n",
      "Iteration 62175: loss = 1.6622742e-10,3.715239e-09\n",
      "Iteration 62180: loss = 1.6621073e-10,3.7148498e-09\n",
      "Iteration 62185: loss = 1.6619399e-10,3.7144603e-09\n",
      "Iteration 62190: loss = 1.6617692e-10,3.7140724e-09\n",
      "Iteration 62195: loss = 1.6615964e-10,3.7136842e-09\n",
      "Iteration 62200: loss = 1.6614242e-10,3.7132954e-09\n",
      "Iteration 62205: loss = 1.6612489e-10,3.712907e-09\n",
      "Iteration 62210: loss = 1.6610684e-10,3.7125225e-09\n",
      "Iteration 62215: loss = 1.6608814e-10,3.7121382e-09\n",
      "Iteration 62220: loss = 1.6606938e-10,3.7117545e-09\n",
      "Iteration 62225: loss = 1.6605041e-10,3.711371e-09\n",
      "Iteration 62230: loss = 1.6603147e-10,3.7109873e-09\n",
      "Iteration 62235: loss = 1.6601244e-10,3.7106043e-09\n",
      "Iteration 62240: loss = 1.6599327e-10,3.7102208e-09\n",
      "Iteration 62245: loss = 1.6597405e-10,3.7098382e-09\n",
      "Iteration 62250: loss = 1.6595908e-10,3.7094416e-09\n",
      "Iteration 62255: loss = 1.6595209e-10,3.709023e-09\n",
      "Iteration 62260: loss = 1.659496e-10,3.708591e-09\n",
      "Iteration 62265: loss = 1.6594935e-10,3.7081518e-09\n",
      "Iteration 62270: loss = 1.6594935e-10,3.7077117e-09\n",
      "Iteration 62275: loss = 1.659491e-10,3.7072718e-09\n",
      "Iteration 62280: loss = 1.6594902e-10,3.7068322e-09\n",
      "Iteration 62285: loss = 1.6594792e-10,3.7063947e-09\n",
      "Iteration 62290: loss = 1.6594583e-10,3.7059618e-09\n",
      "Iteration 62295: loss = 1.6594132e-10,3.7055337e-09\n",
      "Iteration 62300: loss = 1.6593572e-10,3.7051107e-09\n",
      "Iteration 62305: loss = 1.6592888e-10,3.7046892e-09\n",
      "Iteration 62310: loss = 1.6592222e-10,3.7042702e-09\n",
      "Iteration 62315: loss = 1.6591349e-10,3.7038554e-09\n",
      "Iteration 62320: loss = 1.6590442e-10,3.703442e-09\n",
      "Iteration 62325: loss = 1.6589496e-10,3.7030294e-09\n",
      "Iteration 62330: loss = 1.6588413e-10,3.7026204e-09\n",
      "Iteration 62335: loss = 1.6587244e-10,3.7022123e-09\n",
      "Iteration 62340: loss = 1.658611e-10,3.7018062e-09\n",
      "Iteration 62345: loss = 1.6584868e-10,3.7014014e-09\n",
      "Iteration 62350: loss = 1.6583555e-10,3.7010002e-09\n",
      "Iteration 62355: loss = 1.6582158e-10,3.700599e-09\n",
      "Iteration 62360: loss = 1.6580777e-10,3.700199e-09\n",
      "Iteration 62365: loss = 1.6579305e-10,3.6998011e-09\n",
      "Iteration 62370: loss = 1.657766e-10,3.6994094e-09\n",
      "Iteration 62375: loss = 1.6575953e-10,3.6990189e-09\n",
      "Iteration 62380: loss = 1.6574203e-10,3.698629e-09\n",
      "Iteration 62385: loss = 1.6572459e-10,3.698239e-09\n",
      "Iteration 62390: loss = 1.6570663e-10,3.6978505e-09\n",
      "Iteration 62395: loss = 1.6568924e-10,3.6974606e-09\n",
      "Iteration 62400: loss = 1.6567141e-10,3.697072e-09\n",
      "Iteration 62405: loss = 1.6565375e-10,3.6966832e-09\n",
      "Iteration 62410: loss = 1.6563502e-10,3.6962964e-09\n",
      "Iteration 62415: loss = 1.6561591e-10,3.695912e-09\n",
      "Iteration 62420: loss = 1.6559627e-10,3.6955274e-09\n",
      "Iteration 62425: loss = 1.6557704e-10,3.695143e-09\n",
      "Iteration 62430: loss = 1.6555758e-10,3.6947583e-09\n",
      "Iteration 62435: loss = 1.6553785e-10,3.6943741e-09\n",
      "Iteration 62440: loss = 1.640942e-10,3.6939576e-09\n",
      "Iteration 62445: loss = 1.655444e-10,3.6934715e-09\n",
      "Iteration 62450: loss = 1.6554548e-10,3.693026e-09\n",
      "Iteration 62455: loss = 1.6555186e-10,3.6925647e-09\n",
      "Iteration 62460: loss = 1.6411872e-10,3.6921168e-09\n",
      "Iteration 62465: loss = 1.6556394e-10,3.691645e-09\n",
      "Iteration 62470: loss = 1.6412881e-10,3.6912011e-09\n",
      "Iteration 62475: loss = 1.6557182e-10,3.6907355e-09\n",
      "Iteration 62480: loss = 1.6557668e-10,3.690279e-09\n",
      "Iteration 62485: loss = 1.6558181e-10,3.6898218e-09\n",
      "Iteration 62490: loss = 1.6558761e-10,3.6893608e-09\n",
      "Iteration 62495: loss = 1.6559222e-10,3.6889054e-09\n",
      "Iteration 62500: loss = 1.6559747e-10,3.6884464e-09\n",
      "Iteration 62505: loss = 1.6560374e-10,3.6879857e-09\n",
      "Iteration 62510: loss = 1.6417172e-10,3.6875327e-09\n",
      "Iteration 62515: loss = 1.6417574e-10,3.687078e-09\n",
      "Iteration 62520: loss = 1.6562113e-10,3.6866048e-09\n",
      "Iteration 62525: loss = 1.6418723e-10,3.6861585e-09\n",
      "Iteration 62530: loss = 1.656299e-10,3.685693e-09\n",
      "Iteration 62535: loss = 1.6563458e-10,3.6852361e-09\n",
      "Iteration 62540: loss = 1.6564017e-10,3.6847774e-09\n",
      "Iteration 62545: loss = 1.656388e-10,3.6843368e-09\n",
      "Iteration 62550: loss = 1.6419159e-10,3.6839283e-09\n",
      "Iteration 62555: loss = 1.6561769e-10,3.6835117e-09\n",
      "Iteration 62560: loss = 1.6560821e-10,3.683098e-09\n",
      "Iteration 62565: loss = 1.6560053e-10,3.6826768e-09\n",
      "Iteration 62570: loss = 1.6415198e-10,3.682271e-09\n",
      "Iteration 62575: loss = 1.6557881e-10,3.6818526e-09\n",
      "Iteration 62580: loss = 1.6557024e-10,3.6814338e-09\n",
      "Iteration 62585: loss = 1.6412245e-10,3.6810268e-09\n",
      "Iteration 62590: loss = 1.6554895e-10,3.6806085e-09\n",
      "Iteration 62595: loss = 1.6554014e-10,3.6801908e-09\n",
      "Iteration 62600: loss = 1.6552898e-10,3.6797794e-09\n",
      "Iteration 62605: loss = 1.6551915e-10,3.6793644e-09\n",
      "Iteration 62610: loss = 1.6551156e-10,3.6789432e-09\n",
      "Iteration 62615: loss = 1.6406222e-10,3.6785406e-09\n",
      "Iteration 62620: loss = 1.6549216e-10,3.678112e-09\n",
      "Iteration 62625: loss = 1.6404195e-10,3.6777115e-09\n",
      "Iteration 62630: loss = 1.654719e-10,3.677283e-09\n",
      "Iteration 62635: loss = 1.6546102e-10,3.6768706e-09\n",
      "Iteration 62640: loss = 1.6545243e-10,3.6764507e-09\n",
      "Iteration 62645: loss = 1.654414e-10,3.67604e-09\n",
      "Iteration 62650: loss = 1.6399458e-10,3.6756287e-09\n",
      "Iteration 62655: loss = 1.6542218e-10,3.6752066e-09\n",
      "Iteration 62660: loss = 1.6397513e-10,3.6747967e-09\n",
      "Iteration 62665: loss = 1.6396429e-10,3.6743835e-09\n",
      "Iteration 62670: loss = 1.6539414e-10,3.673955e-09\n",
      "Iteration 62675: loss = 1.6538267e-10,3.6735435e-09\n",
      "Iteration 62680: loss = 1.6393487e-10,3.6731354e-09\n",
      "Iteration 62685: loss = 1.6536279e-10,3.672712e-09\n",
      "Iteration 62690: loss = 1.653536e-10,3.6722936e-09\n",
      "Iteration 62695: loss = 1.6390539e-10,3.6718877e-09\n",
      "Iteration 62700: loss = 1.6533647e-10,3.6714545e-09\n",
      "Iteration 62705: loss = 1.6388337e-10,3.6710612e-09\n",
      "Iteration 62710: loss = 1.6531633e-10,3.6706223e-09\n",
      "Iteration 62715: loss = 1.6386564e-10,3.6702226e-09\n",
      "Iteration 62720: loss = 1.6529488e-10,3.6697954e-09\n",
      "Iteration 62725: loss = 1.6528419e-10,3.6693801e-09\n",
      "Iteration 62730: loss = 1.6383798e-10,3.668967e-09\n",
      "Iteration 62735: loss = 1.6382828e-10,3.6685492e-09\n",
      "Iteration 62740: loss = 1.6381708e-10,3.6681373e-09\n",
      "Iteration 62745: loss = 1.6524761e-10,3.6677053e-09\n",
      "Iteration 62750: loss = 1.6379409e-10,3.667314e-09\n",
      "Iteration 62755: loss = 1.6522739e-10,3.666873e-09\n",
      "Iteration 62760: loss = 1.6377928e-10,3.6664651e-09\n",
      "Iteration 62765: loss = 1.6520559e-10,3.6660452e-09\n",
      "Iteration 62770: loss = 1.6519645e-10,3.6656262e-09\n",
      "Iteration 62775: loss = 1.637503e-10,3.6652117e-09\n",
      "Iteration 62780: loss = 1.6374063e-10,3.6647936e-09\n",
      "Iteration 62785: loss = 1.6373021e-10,3.6643781e-09\n",
      "Iteration 62790: loss = 1.6372004e-10,3.6639618e-09\n",
      "Iteration 62795: loss = 1.651491e-10,3.663533e-09\n",
      "Iteration 62800: loss = 1.6514022e-10,3.663113e-09\n",
      "Iteration 62805: loss = 1.651277e-10,3.6627033e-09\n",
      "Iteration 62810: loss = 1.6368124e-10,3.6622891e-09\n",
      "Iteration 62815: loss = 1.6367226e-10,3.66187e-09\n",
      "Iteration 62820: loss = 1.6366157e-10,3.6614543e-09\n",
      "Iteration 62825: loss = 1.6509105e-10,3.6610233e-09\n",
      "Iteration 62830: loss = 1.6364211e-10,3.6606178e-09\n",
      "Iteration 62835: loss = 1.6363287e-10,3.6601984e-09\n",
      "Iteration 62840: loss = 1.6362327e-10,3.6597787e-09\n",
      "Iteration 62845: loss = 1.6361251e-10,3.6593637e-09\n",
      "Iteration 62850: loss = 1.6504086e-10,3.6589365e-09\n",
      "Iteration 62855: loss = 1.650305e-10,3.6585202e-09\n",
      "Iteration 62860: loss = 1.6358243e-10,3.6581103e-09\n",
      "Iteration 62865: loss = 1.6357386e-10,3.6576886e-09\n",
      "Iteration 62870: loss = 1.6500284e-10,3.6572594e-09\n",
      "Iteration 62875: loss = 1.6499191e-10,3.6568433e-09\n",
      "Iteration 62880: loss = 1.6498118e-10,3.6564276e-09\n",
      "Iteration 62885: loss = 1.649733e-10,3.6560033e-09\n",
      "Iteration 62890: loss = 1.6352468e-10,3.6555954e-09\n",
      "Iteration 62895: loss = 1.6495437e-10,3.6551637e-09\n",
      "Iteration 62900: loss = 1.6350632e-10,3.6547532e-09\n",
      "Iteration 62905: loss = 1.6349622e-10,3.6543355e-09\n",
      "Iteration 62910: loss = 1.6492407e-10,3.65391e-09\n",
      "Iteration 62915: loss = 1.6491443e-10,3.65349e-09\n",
      "Iteration 62920: loss = 1.6346707e-10,3.6530772e-09\n",
      "Iteration 62925: loss = 1.6345829e-10,3.6526564e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62930: loss = 1.6344766e-10,3.652239e-09\n",
      "Iteration 62935: loss = 1.6487628e-10,3.6518104e-09\n",
      "Iteration 62940: loss = 1.6342562e-10,3.6514072e-09\n",
      "Iteration 62945: loss = 1.6485836e-10,3.6509662e-09\n",
      "Iteration 62950: loss = 1.6340355e-10,3.6505752e-09\n",
      "Iteration 62955: loss = 1.6483626e-10,3.650134e-09\n",
      "Iteration 62960: loss = 1.6338784e-10,3.6497243e-09\n",
      "Iteration 62965: loss = 1.648174e-10,3.6492926e-09\n",
      "Iteration 62970: loss = 1.6337255e-10,3.6488723e-09\n",
      "Iteration 62975: loss = 1.6337848e-10,3.6484051e-09\n",
      "Iteration 62980: loss = 1.6338238e-10,3.6479437e-09\n",
      "Iteration 62985: loss = 1.6338926e-10,3.6474743e-09\n",
      "Iteration 62990: loss = 1.633923e-10,3.6470167e-09\n",
      "Iteration 62995: loss = 1.6483759e-10,3.646538e-09\n",
      "Iteration 63000: loss = 1.6340101e-10,3.6460923e-09\n",
      "Iteration 63005: loss = 1.6340669e-10,3.645626e-09\n",
      "Iteration 63010: loss = 1.634165e-10,3.6451482e-09\n",
      "Iteration 63015: loss = 1.6342082e-10,3.6446854e-09\n",
      "Iteration 63020: loss = 1.6342756e-10,3.644216e-09\n",
      "Iteration 63025: loss = 1.6342909e-10,3.6437615e-09\n",
      "Iteration 63030: loss = 1.6343187e-10,3.6433043e-09\n",
      "Iteration 63035: loss = 1.6343644e-10,3.6428414e-09\n",
      "Iteration 63040: loss = 1.6344226e-10,3.6423748e-09\n",
      "Iteration 63045: loss = 1.6344646e-10,3.6419125e-09\n",
      "Iteration 63050: loss = 1.6344974e-10,3.6414538e-09\n",
      "Iteration 63055: loss = 1.6345168e-10,3.6409975e-09\n",
      "Iteration 63060: loss = 1.6345296e-10,3.6405443e-09\n",
      "Iteration 63065: loss = 1.6345279e-10,3.6400947e-09\n",
      "Iteration 63070: loss = 1.6345143e-10,3.6396486e-09\n",
      "Iteration 63075: loss = 1.6344853e-10,3.6392063e-09\n",
      "Iteration 63080: loss = 1.6344535e-10,3.638766e-09\n",
      "Iteration 63085: loss = 1.634398e-10,3.6383323e-09\n",
      "Iteration 63090: loss = 1.6343286e-10,3.6379024e-09\n",
      "Iteration 63095: loss = 1.6342473e-10,3.6374768e-09\n",
      "Iteration 63100: loss = 1.6341571e-10,3.637053e-09\n",
      "Iteration 63105: loss = 1.6340668e-10,3.6366297e-09\n",
      "Iteration 63110: loss = 1.6339585e-10,3.6362113e-09\n",
      "Iteration 63115: loss = 1.633844e-10,3.6357937e-09\n",
      "Iteration 63120: loss = 1.6337275e-10,3.6353778e-09\n",
      "Iteration 63125: loss = 1.633615e-10,3.6349606e-09\n",
      "Iteration 63130: loss = 1.6335651e-10,3.6345245e-09\n",
      "Iteration 63135: loss = 1.633575e-10,3.6340713e-09\n",
      "Iteration 63140: loss = 1.6336098e-10,3.6336099e-09\n",
      "Iteration 63145: loss = 1.6336581e-10,3.633144e-09\n",
      "Iteration 63150: loss = 1.6337041e-10,3.6326784e-09\n",
      "Iteration 63155: loss = 1.6337488e-10,3.6322143e-09\n",
      "Iteration 63160: loss = 1.6337766e-10,3.6317553e-09\n",
      "Iteration 63165: loss = 1.633786e-10,3.6313013e-09\n",
      "Iteration 63170: loss = 1.633788e-10,3.6308498e-09\n",
      "Iteration 63175: loss = 1.6337753e-10,3.6304022e-09\n",
      "Iteration 63180: loss = 1.6337547e-10,3.629957e-09\n",
      "Iteration 63185: loss = 1.633721e-10,3.6295158e-09\n",
      "Iteration 63190: loss = 1.6336767e-10,3.629077e-09\n",
      "Iteration 63195: loss = 1.6336216e-10,3.628641e-09\n",
      "Iteration 63200: loss = 1.6335429e-10,3.6282124e-09\n",
      "Iteration 63205: loss = 1.6334606e-10,3.6277856e-09\n",
      "Iteration 63210: loss = 1.6333629e-10,3.6273635e-09\n",
      "Iteration 63215: loss = 1.6332603e-10,3.6269425e-09\n",
      "Iteration 63220: loss = 1.6331546e-10,3.6265213e-09\n",
      "Iteration 63225: loss = 1.6330369e-10,3.6261043e-09\n",
      "Iteration 63230: loss = 1.6329145e-10,3.6256893e-09\n",
      "Iteration 63235: loss = 1.6327846e-10,3.6252754e-09\n",
      "Iteration 63240: loss = 1.6326507e-10,3.6248622e-09\n",
      "Iteration 63245: loss = 1.6325145e-10,3.6244505e-09\n",
      "Iteration 63250: loss = 1.6323677e-10,3.6240417e-09\n",
      "Iteration 63255: loss = 1.6322176e-10,3.6236327e-09\n",
      "Iteration 63260: loss = 1.6320668e-10,3.6232253e-09\n",
      "Iteration 63265: loss = 1.631912e-10,3.6228183e-09\n",
      "Iteration 63270: loss = 1.6317582e-10,3.6224117e-09\n",
      "Iteration 63275: loss = 1.6315982e-10,3.6220063e-09\n",
      "Iteration 63280: loss = 1.6314279e-10,3.6216037e-09\n",
      "Iteration 63285: loss = 1.6312557e-10,3.6212018e-09\n",
      "Iteration 63290: loss = 1.6310835e-10,3.620799e-09\n",
      "Iteration 63295: loss = 1.6309099e-10,3.6203978e-09\n",
      "Iteration 63300: loss = 1.6307322e-10,3.6199972e-09\n",
      "Iteration 63305: loss = 1.630555e-10,3.6195957e-09\n",
      "Iteration 63310: loss = 1.6303796e-10,3.6191943e-09\n",
      "Iteration 63315: loss = 1.6301982e-10,3.6187957e-09\n",
      "Iteration 63320: loss = 1.6300046e-10,3.6183998e-09\n",
      "Iteration 63325: loss = 1.6297995e-10,3.6180068e-09\n",
      "Iteration 63330: loss = 1.6295913e-10,3.617615e-09\n",
      "Iteration 63335: loss = 1.629389e-10,3.617223e-09\n",
      "Iteration 63340: loss = 1.6291822e-10,3.616831e-09\n",
      "Iteration 63345: loss = 1.6289887e-10,3.6164347e-09\n",
      "Iteration 63350: loss = 1.6288337e-10,3.6160266e-09\n",
      "Iteration 63355: loss = 1.6287599e-10,3.615595e-09\n",
      "Iteration 63360: loss = 1.6287281e-10,3.6151506e-09\n",
      "Iteration 63365: loss = 1.6287131e-10,3.6147014e-09\n",
      "Iteration 63370: loss = 1.6287109e-10,3.6142496e-09\n",
      "Iteration 63375: loss = 1.6287048e-10,3.6137968e-09\n",
      "Iteration 63380: loss = 1.6286882e-10,3.6133483e-09\n",
      "Iteration 63385: loss = 1.6286643e-10,3.612901e-09\n",
      "Iteration 63390: loss = 1.6286374e-10,3.6124552e-09\n",
      "Iteration 63395: loss = 1.6285912e-10,3.6120147e-09\n",
      "Iteration 63400: loss = 1.6285429e-10,3.611575e-09\n",
      "Iteration 63405: loss = 1.6284862e-10,3.611138e-09\n",
      "Iteration 63410: loss = 1.6284134e-10,3.6107044e-09\n",
      "Iteration 63415: loss = 1.6283407e-10,3.6102723e-09\n",
      "Iteration 63420: loss = 1.6282543e-10,3.6098426e-09\n",
      "Iteration 63425: loss = 1.6281604e-10,3.6094159e-09\n",
      "Iteration 63430: loss = 1.6280488e-10,3.608994e-09\n",
      "Iteration 63435: loss = 1.6279307e-10,3.6085752e-09\n",
      "Iteration 63440: loss = 1.6277955e-10,3.60816e-09\n",
      "Iteration 63445: loss = 1.6276615e-10,3.6077452e-09\n",
      "Iteration 63450: loss = 1.6275277e-10,3.6073298e-09\n",
      "Iteration 63455: loss = 1.6273884e-10,3.606917e-09\n",
      "Iteration 63460: loss = 1.6272379e-10,3.606507e-09\n",
      "Iteration 63465: loss = 1.627084e-10,3.6060979e-09\n",
      "Iteration 63470: loss = 1.6269296e-10,3.6056897e-09\n",
      "Iteration 63475: loss = 1.6267687e-10,3.6052803e-09\n",
      "Iteration 63480: loss = 1.6266122e-10,3.6048724e-09\n",
      "Iteration 63485: loss = 1.6122163e-10,3.6044578e-09\n",
      "Iteration 63490: loss = 1.6266412e-10,3.6039536e-09\n",
      "Iteration 63495: loss = 1.6267143e-10,3.6034764e-09\n",
      "Iteration 63500: loss = 1.6124058e-10,3.6030352e-09\n",
      "Iteration 63505: loss = 1.6124634e-10,3.6025627e-09\n",
      "Iteration 63510: loss = 1.6125164e-10,3.602092e-09\n",
      "Iteration 63515: loss = 1.6268731e-10,3.6016057e-09\n",
      "Iteration 63520: loss = 1.6126046e-10,3.6011538e-09\n",
      "Iteration 63525: loss = 1.6269858e-10,3.6006602e-09\n",
      "Iteration 63530: loss = 1.6127148e-10,3.6002077e-09\n",
      "Iteration 63535: loss = 1.6270928e-10,3.5997172e-09\n",
      "Iteration 63540: loss = 1.6128426e-10,3.5992584e-09\n",
      "Iteration 63545: loss = 1.6271849e-10,3.5987775e-09\n",
      "Iteration 63550: loss = 1.6129555e-10,3.5983132e-09\n",
      "Iteration 63555: loss = 1.6273284e-10,3.5978234e-09\n",
      "Iteration 63560: loss = 1.6130358e-10,3.5973773e-09\n",
      "Iteration 63565: loss = 1.6274065e-10,3.596887e-09\n",
      "Iteration 63570: loss = 1.6274411e-10,3.5964205e-09\n",
      "Iteration 63575: loss = 1.6275035e-10,3.595945e-09\n",
      "Iteration 63580: loss = 1.6132762e-10,3.5954804e-09\n",
      "Iteration 63585: loss = 1.6276262e-10,3.594996e-09\n",
      "Iteration 63590: loss = 1.6133993e-10,3.5945311e-09\n",
      "Iteration 63595: loss = 1.6277184e-10,3.5940568e-09\n",
      "Iteration 63600: loss = 1.613482e-10,3.593592e-09\n",
      "Iteration 63605: loss = 1.6135604e-10,3.5931127e-09\n",
      "Iteration 63610: loss = 1.6136004e-10,3.592644e-09\n",
      "Iteration 63615: loss = 1.6136603e-10,3.5921697e-09\n",
      "Iteration 63620: loss = 1.628004e-10,3.591688e-09\n",
      "Iteration 63625: loss = 1.6137501e-10,3.5912289e-09\n",
      "Iteration 63630: loss = 1.6281204e-10,3.5907388e-09\n",
      "Iteration 63635: loss = 1.6138661e-10,3.590281e-09\n",
      "Iteration 63640: loss = 1.6282264e-10,3.589793e-09\n",
      "Iteration 63645: loss = 1.61397e-10,3.5893368e-09\n",
      "Iteration 63650: loss = 1.6140334e-10,3.5888594e-09\n",
      "Iteration 63655: loss = 1.6284002e-10,3.5883703e-09\n",
      "Iteration 63660: loss = 1.6141172e-10,3.5879204e-09\n",
      "Iteration 63665: loss = 1.6141817e-10,3.5874441e-09\n",
      "Iteration 63670: loss = 1.6142909e-10,3.5869538e-09\n",
      "Iteration 63675: loss = 1.6143376e-10,3.586483e-09\n",
      "Iteration 63680: loss = 1.6144057e-10,3.5860053e-09\n",
      "Iteration 63685: loss = 1.6144326e-10,3.5855399e-09\n",
      "Iteration 63690: loss = 1.6144562e-10,3.5850751e-09\n",
      "Iteration 63695: loss = 1.614502e-10,3.5846035e-09\n",
      "Iteration 63700: loss = 1.6145617e-10,3.5841292e-09\n",
      "Iteration 63705: loss = 1.6146129e-10,3.5836554e-09\n",
      "Iteration 63710: loss = 1.6146655e-10,3.5831833e-09\n",
      "Iteration 63715: loss = 1.6147018e-10,3.582715e-09\n",
      "Iteration 63720: loss = 1.6147285e-10,3.5822492e-09\n",
      "Iteration 63725: loss = 1.6147395e-10,3.5817869e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63730: loss = 1.6147426e-10,3.5813281e-09\n",
      "Iteration 63735: loss = 1.6147317e-10,3.5808734e-09\n",
      "Iteration 63740: loss = 1.6147078e-10,3.5804213e-09\n",
      "Iteration 63745: loss = 1.6146566e-10,3.5799788e-09\n",
      "Iteration 63750: loss = 1.6146005e-10,3.5795371e-09\n",
      "Iteration 63755: loss = 1.6145317e-10,3.5790997e-09\n",
      "Iteration 63760: loss = 1.6144512e-10,3.5786645e-09\n",
      "Iteration 63765: loss = 1.6143685e-10,3.5782308e-09\n",
      "Iteration 63770: loss = 1.6142732e-10,3.5778003e-09\n",
      "Iteration 63775: loss = 1.6141695e-10,3.5773717e-09\n",
      "Iteration 63780: loss = 1.6140633e-10,3.5769439e-09\n",
      "Iteration 63785: loss = 1.6139483e-10,3.5765186e-09\n",
      "Iteration 63790: loss = 1.6138262e-10,3.5760959e-09\n",
      "Iteration 63795: loss = 1.6137008e-10,3.5756744e-09\n",
      "Iteration 63800: loss = 1.6135691e-10,3.5752532e-09\n",
      "Iteration 63805: loss = 1.6134367e-10,3.5748335e-09\n",
      "Iteration 63810: loss = 1.6132766e-10,3.5744212e-09\n",
      "Iteration 63815: loss = 1.6131174e-10,3.5740106e-09\n",
      "Iteration 63820: loss = 1.6129524e-10,3.5736e-09\n",
      "Iteration 63825: loss = 1.6128167e-10,3.57318e-09\n",
      "Iteration 63830: loss = 1.6127637e-10,3.5727363e-09\n",
      "Iteration 63835: loss = 1.6127617e-10,3.572278e-09\n",
      "Iteration 63840: loss = 1.6127799e-10,3.5718137e-09\n",
      "Iteration 63845: loss = 1.6127995e-10,3.5713483e-09\n",
      "Iteration 63850: loss = 1.6128184e-10,3.5708827e-09\n",
      "Iteration 63855: loss = 1.612837e-10,3.570418e-09\n",
      "Iteration 63860: loss = 1.6128388e-10,3.5699568e-09\n",
      "Iteration 63865: loss = 1.6128338e-10,3.5694985e-09\n",
      "Iteration 63870: loss = 1.6128225e-10,3.5690426e-09\n",
      "Iteration 63875: loss = 1.612795e-10,3.5685903e-09\n",
      "Iteration 63880: loss = 1.6127621e-10,3.5681404e-09\n",
      "Iteration 63885: loss = 1.6127018e-10,3.5676984e-09\n",
      "Iteration 63890: loss = 1.6126335e-10,3.5672592e-09\n",
      "Iteration 63895: loss = 1.6125516e-10,3.5668226e-09\n",
      "Iteration 63900: loss = 1.6124607e-10,3.5663899e-09\n",
      "Iteration 63905: loss = 1.612368e-10,3.5659569e-09\n",
      "Iteration 63910: loss = 1.612267e-10,3.5655268e-09\n",
      "Iteration 63915: loss = 1.6121515e-10,3.5650995e-09\n",
      "Iteration 63920: loss = 1.6120384e-10,3.564673e-09\n",
      "Iteration 63925: loss = 1.6119178e-10,3.5642476e-09\n",
      "Iteration 63930: loss = 1.6117896e-10,3.5638248e-09\n",
      "Iteration 63935: loss = 1.6116534e-10,3.5634038e-09\n",
      "Iteration 63940: loss = 1.6115166e-10,3.562984e-09\n",
      "Iteration 63945: loss = 1.611377e-10,3.5625647e-09\n",
      "Iteration 63950: loss = 1.611233e-10,3.5621452e-09\n",
      "Iteration 63955: loss = 1.611075e-10,3.561731e-09\n",
      "Iteration 63960: loss = 1.6109185e-10,3.5613157e-09\n",
      "Iteration 63965: loss = 1.6107454e-10,3.5609067e-09\n",
      "Iteration 63970: loss = 1.6105713e-10,3.560498e-09\n",
      "Iteration 63975: loss = 1.6103974e-10,3.5600884e-09\n",
      "Iteration 63980: loss = 1.6102203e-10,3.5596806e-09\n",
      "Iteration 63985: loss = 1.6100392e-10,3.5592718e-09\n",
      "Iteration 63990: loss = 1.6098582e-10,3.5588665e-09\n",
      "Iteration 63995: loss = 1.6096625e-10,3.5584615e-09\n",
      "Iteration 64000: loss = 1.6094732e-10,3.558056e-09\n",
      "Iteration 64005: loss = 1.6092787e-10,3.557652e-09\n",
      "Iteration 64010: loss = 1.6090874e-10,3.5572478e-09\n",
      "Iteration 64015: loss = 1.6088937e-10,3.5568433e-09\n",
      "Iteration 64020: loss = 1.6086993e-10,3.5564391e-09\n",
      "Iteration 64025: loss = 1.6085078e-10,3.5560352e-09\n",
      "Iteration 64030: loss = 1.6083128e-10,3.5556313e-09\n",
      "Iteration 64035: loss = 1.6081185e-10,3.5552263e-09\n",
      "Iteration 64040: loss = 1.6079886e-10,3.5548027e-09\n",
      "Iteration 64045: loss = 1.6079373e-10,3.554357e-09\n",
      "Iteration 64050: loss = 1.6079132e-10,3.5539007e-09\n",
      "Iteration 64055: loss = 1.6079073e-10,3.5534413e-09\n",
      "Iteration 64060: loss = 1.6079028e-10,3.552981e-09\n",
      "Iteration 64065: loss = 1.6078917e-10,3.5525214e-09\n",
      "Iteration 64070: loss = 1.6078827e-10,3.5520622e-09\n",
      "Iteration 64075: loss = 1.607866e-10,3.551604e-09\n",
      "Iteration 64080: loss = 1.6078372e-10,3.5511503e-09\n",
      "Iteration 64085: loss = 1.6078038e-10,3.550698e-09\n",
      "Iteration 64090: loss = 1.6077555e-10,3.5502488e-09\n",
      "Iteration 64095: loss = 1.6076977e-10,3.5498031e-09\n",
      "Iteration 64100: loss = 1.6076372e-10,3.5493588e-09\n",
      "Iteration 64105: loss = 1.607553e-10,3.5489198e-09\n",
      "Iteration 64110: loss = 1.6074579e-10,3.5484864e-09\n",
      "Iteration 64115: loss = 1.6073558e-10,3.5480536e-09\n",
      "Iteration 64120: loss = 1.6072434e-10,3.5476244e-09\n",
      "Iteration 64125: loss = 1.6071243e-10,3.5471963e-09\n",
      "Iteration 64130: loss = 1.6070038e-10,3.546769e-09\n",
      "Iteration 64135: loss = 1.6068791e-10,3.546343e-09\n",
      "Iteration 64140: loss = 1.5925995e-10,3.5459145e-09\n",
      "Iteration 64145: loss = 1.6068957e-10,3.5454122e-09\n",
      "Iteration 64150: loss = 1.6069374e-10,3.5449357e-09\n",
      "Iteration 64155: loss = 1.5927833e-10,3.5444705e-09\n",
      "Iteration 64160: loss = 1.6070467e-10,3.5439758e-09\n",
      "Iteration 64165: loss = 1.6070799e-10,3.5435035e-09\n",
      "Iteration 64170: loss = 1.5929569e-10,3.5430276e-09\n",
      "Iteration 64175: loss = 1.5930175e-10,3.5425463e-09\n",
      "Iteration 64180: loss = 1.6072532e-10,3.5420606e-09\n",
      "Iteration 64185: loss = 1.6073069e-10,3.5415808e-09\n",
      "Iteration 64190: loss = 1.6073702e-10,3.5410985e-09\n",
      "Iteration 64195: loss = 1.6074327e-10,3.5406167e-09\n",
      "Iteration 64200: loss = 1.6074936e-10,3.5401342e-09\n",
      "Iteration 64205: loss = 1.5933548e-10,3.5396632e-09\n",
      "Iteration 64210: loss = 1.5934203e-10,3.5391796e-09\n",
      "Iteration 64215: loss = 1.6076727e-10,3.538688e-09\n",
      "Iteration 64220: loss = 1.5935421e-10,3.5382153e-09\n",
      "Iteration 64225: loss = 1.5935919e-10,3.5377359e-09\n",
      "Iteration 64230: loss = 1.6078384e-10,3.5372474e-09\n",
      "Iteration 64235: loss = 1.607891e-10,3.536768e-09\n",
      "Iteration 64240: loss = 1.5937474e-10,3.536298e-09\n",
      "Iteration 64245: loss = 1.6080154e-10,3.5358019e-09\n",
      "Iteration 64250: loss = 1.5938528e-10,3.5353376e-09\n",
      "Iteration 64255: loss = 1.6081304e-10,3.5348402e-09\n",
      "Iteration 64260: loss = 1.6081819e-10,3.5343586e-09\n",
      "Iteration 64265: loss = 1.5940664e-10,3.5338799e-09\n",
      "Iteration 64270: loss = 1.5940897e-10,3.5334091e-09\n",
      "Iteration 64275: loss = 1.594173e-10,3.5329195e-09\n",
      "Iteration 64280: loss = 1.5942077e-10,3.5324443e-09\n",
      "Iteration 64285: loss = 1.5942876e-10,3.5319567e-09\n",
      "Iteration 64290: loss = 1.5943431e-10,3.5314747e-09\n",
      "Iteration 64295: loss = 1.5944046e-10,3.5309928e-09\n",
      "Iteration 64300: loss = 1.59445e-10,3.5305132e-09\n",
      "Iteration 64305: loss = 1.6087019e-10,3.5300227e-09\n",
      "Iteration 64310: loss = 1.5945802e-10,3.529545e-09\n",
      "Iteration 64315: loss = 1.5946143e-10,3.5290704e-09\n",
      "Iteration 64320: loss = 1.5946511e-10,3.5285947e-09\n",
      "Iteration 64325: loss = 1.5947553e-10,3.5280976e-09\n",
      "Iteration 64330: loss = 1.5948036e-10,3.527619e-09\n",
      "Iteration 64335: loss = 1.5948254e-10,3.5271466e-09\n",
      "Iteration 64340: loss = 1.5948971e-10,3.5266605e-09\n",
      "Iteration 64345: loss = 1.6091344e-10,3.526173e-09\n",
      "Iteration 64350: loss = 1.5948087e-10,3.5257557e-09\n",
      "Iteration 64355: loss = 1.5947134e-10,3.5253185e-09\n",
      "Iteration 64360: loss = 1.5946382e-10,3.5248744e-09\n",
      "Iteration 64365: loss = 1.5945273e-10,3.5244412e-09\n",
      "Iteration 64370: loss = 1.5944433e-10,3.5240006e-09\n",
      "Iteration 64375: loss = 1.5943226e-10,3.5235708e-09\n",
      "Iteration 64380: loss = 1.6084206e-10,3.5231231e-09\n",
      "Iteration 64385: loss = 1.5941076e-10,3.5227021e-09\n",
      "Iteration 64390: loss = 1.5940405e-10,3.5222558e-09\n",
      "Iteration 64395: loss = 1.59393e-10,3.5218215e-09\n",
      "Iteration 64400: loss = 1.5938179e-10,3.5213894e-09\n",
      "Iteration 64405: loss = 1.607924e-10,3.5209393e-09\n",
      "Iteration 64410: loss = 1.6078248e-10,3.5205028e-09\n",
      "Iteration 64415: loss = 1.5935271e-10,3.520076e-09\n",
      "Iteration 64420: loss = 1.5934563e-10,3.5196306e-09\n",
      "Iteration 64425: loss = 1.5933448e-10,3.5191965e-09\n",
      "Iteration 64430: loss = 1.6074338e-10,3.5187524e-09\n",
      "Iteration 64435: loss = 1.5931363e-10,3.5183247e-09\n",
      "Iteration 64440: loss = 1.593055e-10,3.5178822e-09\n",
      "Iteration 64445: loss = 1.5929442e-10,3.517449e-09\n",
      "Iteration 64450: loss = 1.5928375e-10,3.517013e-09\n",
      "Iteration 64455: loss = 1.5927643e-10,3.5165688e-09\n",
      "Iteration 64460: loss = 1.5926445e-10,3.5161367e-09\n",
      "Iteration 64465: loss = 1.5925276e-10,3.515704e-09\n",
      "Iteration 64470: loss = 1.5924384e-10,3.5152636e-09\n",
      "Iteration 64475: loss = 1.5923522e-10,3.5148207e-09\n",
      "Iteration 64480: loss = 1.5922724e-10,3.5143777e-09\n",
      "Iteration 64485: loss = 1.5921381e-10,3.5139502e-09\n",
      "Iteration 64490: loss = 1.59199e-10,3.5135266e-09\n",
      "Iteration 64495: loss = 1.591941e-10,3.5130747e-09\n",
      "Iteration 64500: loss = 1.5918755e-10,3.512626e-09\n",
      "Iteration 64505: loss = 1.5917719e-10,3.5121885e-09\n",
      "Iteration 64510: loss = 1.5916819e-10,3.511748e-09\n",
      "Iteration 64515: loss = 1.5915859e-10,3.511309e-09\n",
      "Iteration 64520: loss = 1.6056582e-10,3.5108678e-09\n",
      "Iteration 64525: loss = 1.6055567e-10,3.5104308e-09\n",
      "Iteration 64530: loss = 1.5912904e-10,3.5099923e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64535: loss = 1.5911512e-10,3.5095664e-09\n",
      "Iteration 64540: loss = 1.5910591e-10,3.5091259e-09\n",
      "Iteration 64545: loss = 1.5909814e-10,3.5086811e-09\n",
      "Iteration 64550: loss = 1.5908637e-10,3.5082486e-09\n",
      "Iteration 64555: loss = 1.590796e-10,3.5077998e-09\n",
      "Iteration 64560: loss = 1.5906826e-10,3.5073646e-09\n",
      "Iteration 64565: loss = 1.5905986e-10,3.5069214e-09\n",
      "Iteration 64570: loss = 1.5904976e-10,3.506483e-09\n",
      "Iteration 64575: loss = 1.5903977e-10,3.5060446e-09\n",
      "Iteration 64580: loss = 1.6044753e-10,3.505602e-09\n",
      "Iteration 64585: loss = 1.5901847e-10,3.5051708e-09\n",
      "Iteration 64590: loss = 1.6042799e-10,3.5047227e-09\n",
      "Iteration 64595: loss = 1.5899983e-10,3.5042889e-09\n",
      "Iteration 64600: loss = 1.5899061e-10,3.5038479e-09\n",
      "Iteration 64605: loss = 1.5897901e-10,3.5034136e-09\n",
      "Iteration 64610: loss = 1.5897145e-10,3.5029666e-09\n",
      "Iteration 64615: loss = 1.5896025e-10,3.502531e-09\n",
      "Iteration 64620: loss = 1.5895009e-10,3.5020924e-09\n",
      "Iteration 64625: loss = 1.6035921e-10,3.501646e-09\n",
      "Iteration 64630: loss = 1.5892819e-10,3.5012202e-09\n",
      "Iteration 64635: loss = 1.5891811e-10,3.500781e-09\n",
      "Iteration 64640: loss = 1.5890934e-10,3.5003374e-09\n",
      "Iteration 64645: loss = 1.5890173e-10,3.499891e-09\n",
      "Iteration 64650: loss = 1.6031061e-10,3.4994445e-09\n",
      "Iteration 64655: loss = 1.5887797e-10,3.4990235e-09\n",
      "Iteration 64660: loss = 1.5886509e-10,3.498591e-09\n",
      "Iteration 64665: loss = 1.5885414e-10,3.4981549e-09\n",
      "Iteration 64670: loss = 1.5884409e-10,3.4977157e-09\n",
      "Iteration 64675: loss = 1.5883415e-10,3.4972756e-09\n",
      "Iteration 64680: loss = 1.5882383e-10,3.4968362e-09\n",
      "Iteration 64685: loss = 1.5881213e-10,3.4963994e-09\n",
      "Iteration 64690: loss = 1.5880058e-10,3.495965e-09\n",
      "Iteration 64695: loss = 1.5878888e-10,3.4955296e-09\n",
      "Iteration 64700: loss = 1.5877673e-10,3.4950955e-09\n",
      "Iteration 64705: loss = 1.5876384e-10,3.4946637e-09\n",
      "Iteration 64710: loss = 1.5874924e-10,3.4942365e-09\n",
      "Iteration 64715: loss = 1.5873476e-10,3.49381e-09\n",
      "Iteration 64720: loss = 1.5872803e-10,3.49336e-09\n",
      "Iteration 64725: loss = 1.5872749e-10,3.492892e-09\n",
      "Iteration 64730: loss = 1.5872942e-10,3.4924155e-09\n",
      "Iteration 64735: loss = 1.6014669e-10,3.4919432e-09\n",
      "Iteration 64740: loss = 1.5871227e-10,3.4915266e-09\n",
      "Iteration 64745: loss = 1.5870151e-10,3.4910879e-09\n",
      "Iteration 64750: loss = 1.5869633e-10,3.4906331e-09\n",
      "Iteration 64755: loss = 1.5728215e-10,3.4901841e-09\n",
      "Iteration 64760: loss = 1.586898e-10,3.4897114e-09\n",
      "Iteration 64765: loss = 1.5868991e-10,3.4892405e-09\n",
      "Iteration 64770: loss = 1.5868973e-10,3.4887702e-09\n",
      "Iteration 64775: loss = 1.5868838e-10,3.4883036e-09\n",
      "Iteration 64780: loss = 1.5868586e-10,3.487839e-09\n",
      "Iteration 64785: loss = 1.5868229e-10,3.4873784e-09\n",
      "Iteration 64790: loss = 1.5867636e-10,3.486927e-09\n",
      "Iteration 64795: loss = 1.5867009e-10,3.4864742e-09\n",
      "Iteration 64800: loss = 1.5866193e-10,3.4860275e-09\n",
      "Iteration 64805: loss = 1.5865344e-10,3.4855816e-09\n",
      "Iteration 64810: loss = 1.586446e-10,3.4851364e-09\n",
      "Iteration 64815: loss = 1.5863434e-10,3.4846965e-09\n",
      "Iteration 64820: loss = 1.5862339e-10,3.484257e-09\n",
      "Iteration 64825: loss = 1.5861253e-10,3.4838186e-09\n",
      "Iteration 64830: loss = 1.5860069e-10,3.4833827e-09\n",
      "Iteration 64835: loss = 1.5858771e-10,3.4829486e-09\n",
      "Iteration 64840: loss = 1.5857482e-10,3.4825154e-09\n",
      "Iteration 64845: loss = 1.5856183e-10,3.482083e-09\n",
      "Iteration 64850: loss = 1.5854817e-10,3.4816514e-09\n",
      "Iteration 64855: loss = 1.5853337e-10,3.4812238e-09\n",
      "Iteration 64860: loss = 1.5851666e-10,3.4808014e-09\n",
      "Iteration 64865: loss = 1.585004e-10,3.480379e-09\n",
      "Iteration 64870: loss = 1.5848362e-10,3.4799568e-09\n",
      "Iteration 64875: loss = 1.5846657e-10,3.4795358e-09\n",
      "Iteration 64880: loss = 1.584494e-10,3.4791143e-09\n",
      "Iteration 64885: loss = 1.5843803e-10,3.4786776e-09\n",
      "Iteration 64890: loss = 1.584342e-10,3.4782162e-09\n",
      "Iteration 64895: loss = 1.5842394e-10,3.4777745e-09\n",
      "Iteration 64900: loss = 1.5840985e-10,3.4773437e-09\n",
      "Iteration 64905: loss = 1.5839857e-10,3.4769059e-09\n",
      "Iteration 64910: loss = 1.5839362e-10,3.476448e-09\n",
      "Iteration 64915: loss = 1.5838182e-10,3.4760097e-09\n",
      "Iteration 64920: loss = 1.569606e-10,3.4755807e-09\n",
      "Iteration 64925: loss = 1.583635e-10,3.4751206e-09\n",
      "Iteration 64930: loss = 1.5835148e-10,3.474684e-09\n",
      "Iteration 64935: loss = 1.5834285e-10,3.4742362e-09\n",
      "Iteration 64940: loss = 1.5833451e-10,3.4737893e-09\n",
      "Iteration 64945: loss = 1.5691178e-10,3.4733632e-09\n",
      "Iteration 64950: loss = 1.5831762e-10,3.4728929e-09\n",
      "Iteration 64955: loss = 1.5831009e-10,3.472443e-09\n",
      "Iteration 64960: loss = 1.5829364e-10,3.47202e-09\n",
      "Iteration 64965: loss = 1.5828383e-10,3.471575e-09\n",
      "Iteration 64970: loss = 1.5827857e-10,3.4711176e-09\n",
      "Iteration 64975: loss = 1.582729e-10,3.4706606e-09\n",
      "Iteration 64980: loss = 1.5826634e-10,3.4702066e-09\n",
      "Iteration 64985: loss = 1.5825874e-10,3.4697571e-09\n",
      "Iteration 64990: loss = 1.582498e-10,3.4693106e-09\n",
      "Iteration 64995: loss = 1.5824053e-10,3.4688654e-09\n",
      "Iteration 65000: loss = 1.582308e-10,3.4684202e-09\n",
      "Iteration 65005: loss = 1.5822021e-10,3.4679788e-09\n",
      "Iteration 65010: loss = 1.5820852e-10,3.4675394e-09\n",
      "Iteration 65015: loss = 1.5819725e-10,3.4671008e-09\n",
      "Iteration 65020: loss = 1.5818508e-10,3.466663e-09\n",
      "Iteration 65025: loss = 1.5817193e-10,3.4662286e-09\n",
      "Iteration 65030: loss = 1.581582e-10,3.465796e-09\n",
      "Iteration 65035: loss = 1.5814815e-10,3.4653524e-09\n",
      "Iteration 65040: loss = 1.5813933e-10,3.4649035e-09\n",
      "Iteration 65045: loss = 1.581279e-10,3.4644643e-09\n",
      "Iteration 65050: loss = 1.5812085e-10,3.4640109e-09\n",
      "Iteration 65055: loss = 1.581102e-10,3.4635685e-09\n",
      "Iteration 65060: loss = 1.5809831e-10,3.46313e-09\n",
      "Iteration 65065: loss = 1.5808928e-10,3.4626833e-09\n",
      "Iteration 65070: loss = 1.5808131e-10,3.4622325e-09\n",
      "Iteration 65075: loss = 1.5807029e-10,3.4617917e-09\n",
      "Iteration 65080: loss = 1.580606e-10,3.461346e-09\n",
      "Iteration 65085: loss = 1.5805014e-10,3.4609027e-09\n",
      "Iteration 65090: loss = 1.5804069e-10,3.4604568e-09\n",
      "Iteration 65095: loss = 1.5803159e-10,3.4600087e-09\n",
      "Iteration 65100: loss = 1.580217e-10,3.4595646e-09\n",
      "Iteration 65105: loss = 1.5801165e-10,3.4591192e-09\n",
      "Iteration 65110: loss = 1.5659431e-10,3.458676e-09\n",
      "Iteration 65115: loss = 1.5658475e-10,3.4582304e-09\n",
      "Iteration 65120: loss = 1.5798278e-10,3.4577816e-09\n",
      "Iteration 65125: loss = 1.5797326e-10,3.4573346e-09\n",
      "Iteration 65130: loss = 1.5796371e-10,3.456889e-09\n",
      "Iteration 65135: loss = 1.5795387e-10,3.4564434e-09\n",
      "Iteration 65140: loss = 1.5653683e-10,3.4559988e-09\n",
      "Iteration 65145: loss = 1.5652692e-10,3.4555523e-09\n",
      "Iteration 65150: loss = 1.5792534e-10,3.4551029e-09\n",
      "Iteration 65155: loss = 1.579168e-10,3.4546541e-09\n",
      "Iteration 65160: loss = 1.5649859e-10,3.4542116e-09\n",
      "Iteration 65165: loss = 1.5649086e-10,3.45376e-09\n",
      "Iteration 65170: loss = 1.5790429e-10,3.4532652e-09\n",
      "Iteration 65175: loss = 1.5650208e-10,3.4527747e-09\n",
      "Iteration 65180: loss = 1.5791678e-10,3.4522774e-09\n",
      "Iteration 65185: loss = 1.5792341e-10,3.4517817e-09\n",
      "Iteration 65190: loss = 1.5651934e-10,3.4512975e-09\n",
      "Iteration 65195: loss = 1.5793568e-10,3.450793e-09\n",
      "Iteration 65200: loss = 1.5653069e-10,3.4503131e-09\n",
      "Iteration 65205: loss = 1.5794625e-10,3.4498113e-09\n",
      "Iteration 65210: loss = 1.5654383e-10,3.4493226e-09\n",
      "Iteration 65215: loss = 1.5654944e-10,3.4488297e-09\n",
      "Iteration 65220: loss = 1.5655681e-10,3.4483314e-09\n",
      "Iteration 65225: loss = 1.5656297e-10,3.4478373e-09\n",
      "Iteration 65230: loss = 1.5656627e-10,3.4473517e-09\n",
      "Iteration 65235: loss = 1.5657081e-10,3.4468624e-09\n",
      "Iteration 65240: loss = 1.5657857e-10,3.4463636e-09\n",
      "Iteration 65245: loss = 1.5799423e-10,3.4458623e-09\n",
      "Iteration 65250: loss = 1.5658881e-10,3.4453815e-09\n",
      "Iteration 65255: loss = 1.5659292e-10,3.4448924e-09\n",
      "Iteration 65260: loss = 1.5659733e-10,3.4444023e-09\n",
      "Iteration 65265: loss = 1.5660191e-10,3.4439127e-09\n",
      "Iteration 65270: loss = 1.5660621e-10,3.4434242e-09\n",
      "Iteration 65275: loss = 1.5660977e-10,3.442937e-09\n",
      "Iteration 65280: loss = 1.5661146e-10,3.4424548e-09\n",
      "Iteration 65285: loss = 1.5661143e-10,3.4419785e-09\n",
      "Iteration 65290: loss = 1.5660945e-10,3.4415077e-09\n",
      "Iteration 65295: loss = 1.5660696e-10,3.4410386e-09\n",
      "Iteration 65300: loss = 1.5660291e-10,3.4405745e-09\n",
      "Iteration 65305: loss = 1.5659835e-10,3.4401109e-09\n",
      "Iteration 65310: loss = 1.5659268e-10,3.4396512e-09\n",
      "Iteration 65315: loss = 1.565859e-10,3.4391947e-09\n",
      "Iteration 65320: loss = 1.5657971e-10,3.4387362e-09\n",
      "Iteration 65325: loss = 1.5658046e-10,3.4382568e-09\n",
      "Iteration 65330: loss = 1.565862e-10,3.4377634e-09\n",
      "Iteration 65335: loss = 1.5659346e-10,3.4372647e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65340: loss = 1.5660061e-10,3.4367658e-09\n",
      "Iteration 65345: loss = 1.5660828e-10,3.4362675e-09\n",
      "Iteration 65350: loss = 1.5661494e-10,3.43577e-09\n",
      "Iteration 65355: loss = 1.5662051e-10,3.4352765e-09\n",
      "Iteration 65360: loss = 1.566246e-10,3.4347867e-09\n",
      "Iteration 65365: loss = 1.5662725e-10,3.4343013e-09\n",
      "Iteration 65370: loss = 1.5662877e-10,3.4338197e-09\n",
      "Iteration 65375: loss = 1.5662745e-10,3.4333467e-09\n",
      "Iteration 65380: loss = 1.5662517e-10,3.4328755e-09\n",
      "Iteration 65385: loss = 1.5662172e-10,3.4324088e-09\n",
      "Iteration 65390: loss = 1.566175e-10,3.431944e-09\n",
      "Iteration 65395: loss = 1.5661168e-10,3.4314833e-09\n",
      "Iteration 65400: loss = 1.5660502e-10,3.4310244e-09\n",
      "Iteration 65405: loss = 1.5659785e-10,3.4305685e-09\n",
      "Iteration 65410: loss = 1.5658926e-10,3.430115e-09\n",
      "Iteration 65415: loss = 1.5658032e-10,3.4296654e-09\n",
      "Iteration 65420: loss = 1.5656915e-10,3.42922e-09\n",
      "Iteration 65425: loss = 1.5655727e-10,3.4287775e-09\n",
      "Iteration 65430: loss = 1.5654512e-10,3.4283365e-09\n",
      "Iteration 65435: loss = 1.5653258e-10,3.427895e-09\n",
      "Iteration 65440: loss = 1.5651926e-10,3.4274565e-09\n",
      "Iteration 65445: loss = 1.565051e-10,3.427021e-09\n",
      "Iteration 65450: loss = 1.5649065e-10,3.426585e-09\n",
      "Iteration 65455: loss = 1.5647615e-10,3.42615e-09\n",
      "Iteration 65460: loss = 1.564613e-10,3.4257155e-09\n",
      "Iteration 65465: loss = 1.5644659e-10,3.4252814e-09\n",
      "Iteration 65470: loss = 1.5643019e-10,3.424851e-09\n",
      "Iteration 65475: loss = 1.5641415e-10,3.4244212e-09\n",
      "Iteration 65480: loss = 1.5639756e-10,3.423991e-09\n",
      "Iteration 65485: loss = 1.563809e-10,3.4235614e-09\n",
      "Iteration 65490: loss = 1.5636409e-10,3.4231324e-09\n",
      "Iteration 65495: loss = 1.563476e-10,3.4227035e-09\n",
      "Iteration 65500: loss = 1.563309e-10,3.422274e-09\n",
      "Iteration 65505: loss = 1.5631292e-10,3.4218484e-09\n",
      "Iteration 65510: loss = 1.5629457e-10,3.4214225e-09\n",
      "Iteration 65515: loss = 1.562764e-10,3.420998e-09\n",
      "Iteration 65520: loss = 1.5625813e-10,3.4205736e-09\n",
      "Iteration 65525: loss = 1.5623984e-10,3.4201488e-09\n",
      "Iteration 65530: loss = 1.562215e-10,3.4197236e-09\n",
      "Iteration 65535: loss = 1.5620305e-10,3.4192995e-09\n",
      "Iteration 65540: loss = 1.5618457e-10,3.4188756e-09\n",
      "Iteration 65545: loss = 1.5616597e-10,3.4184509e-09\n",
      "Iteration 65550: loss = 1.561476e-10,3.4180265e-09\n",
      "Iteration 65555: loss = 1.5612883e-10,3.4176022e-09\n",
      "Iteration 65560: loss = 1.5611529e-10,3.4171637e-09\n",
      "Iteration 65565: loss = 1.5610895e-10,3.4167027e-09\n",
      "Iteration 65570: loss = 1.5610603e-10,3.4162317e-09\n",
      "Iteration 65575: loss = 1.5610548e-10,3.4157537e-09\n",
      "Iteration 65580: loss = 1.5610534e-10,3.4152738e-09\n",
      "Iteration 65585: loss = 1.5610507e-10,3.4147942e-09\n",
      "Iteration 65590: loss = 1.56104e-10,3.4143168e-09\n",
      "Iteration 65595: loss = 1.5610167e-10,3.4138443e-09\n",
      "Iteration 65600: loss = 1.5609769e-10,3.4133762e-09\n",
      "Iteration 65605: loss = 1.5609342e-10,3.4129095e-09\n",
      "Iteration 65610: loss = 1.5608782e-10,3.4124457e-09\n",
      "Iteration 65615: loss = 1.560813e-10,3.411985e-09\n",
      "Iteration 65620: loss = 1.5607447e-10,3.4115248e-09\n",
      "Iteration 65625: loss = 1.5606605e-10,3.4110696e-09\n",
      "Iteration 65630: loss = 1.5605743e-10,3.4106156e-09\n",
      "Iteration 65635: loss = 1.56048e-10,3.4101628e-09\n",
      "Iteration 65640: loss = 1.5603763e-10,3.409712e-09\n",
      "Iteration 65645: loss = 1.5602608e-10,3.4092673e-09\n",
      "Iteration 65650: loss = 1.5601297e-10,3.4088248e-09\n",
      "Iteration 65655: loss = 1.5600048e-10,3.4083831e-09\n",
      "Iteration 65660: loss = 1.5598649e-10,3.4079437e-09\n",
      "Iteration 65665: loss = 1.5597197e-10,3.4075072e-09\n",
      "Iteration 65670: loss = 1.5595704e-10,3.4070702e-09\n",
      "Iteration 65675: loss = 1.5594248e-10,3.4066343e-09\n",
      "Iteration 65680: loss = 1.5592716e-10,3.4061982e-09\n",
      "Iteration 65685: loss = 1.5591169e-10,3.405764e-09\n",
      "Iteration 65690: loss = 1.5589541e-10,3.4053318e-09\n",
      "Iteration 65695: loss = 1.558786e-10,3.4049008e-09\n",
      "Iteration 65700: loss = 1.5586203e-10,3.4044694e-09\n",
      "Iteration 65705: loss = 1.5446495e-10,3.4039866e-09\n",
      "Iteration 65710: loss = 1.5587658e-10,3.4034633e-09\n",
      "Iteration 65715: loss = 1.5447889e-10,3.4029828e-09\n",
      "Iteration 65720: loss = 1.5588324e-10,3.4024807e-09\n",
      "Iteration 65725: loss = 1.5449099e-10,3.4019845e-09\n",
      "Iteration 65730: loss = 1.5589512e-10,3.4014829e-09\n",
      "Iteration 65735: loss = 1.5450258e-10,3.4009864e-09\n",
      "Iteration 65740: loss = 1.5450909e-10,3.4004857e-09\n",
      "Iteration 65745: loss = 1.5591338e-10,3.3999832e-09\n",
      "Iteration 65750: loss = 1.5592026e-10,3.3994807e-09\n",
      "Iteration 65755: loss = 1.5592587e-10,3.3989824e-09\n",
      "Iteration 65760: loss = 1.5453365e-10,3.3984855e-09\n",
      "Iteration 65765: loss = 1.5593916e-10,3.3979797e-09\n",
      "Iteration 65770: loss = 1.5454582e-10,3.3974856e-09\n",
      "Iteration 65775: loss = 1.5455122e-10,3.3969882e-09\n",
      "Iteration 65780: loss = 1.5595943e-10,3.3964742e-09\n",
      "Iteration 65785: loss = 1.5456453e-10,3.3959842e-09\n",
      "Iteration 65790: loss = 1.545697e-10,3.3954877e-09\n",
      "Iteration 65795: loss = 1.5597584e-10,3.3949792e-09\n",
      "Iteration 65800: loss = 1.5458382e-10,3.3944803e-09\n",
      "Iteration 65805: loss = 1.545878e-10,3.3939869e-09\n",
      "Iteration 65810: loss = 1.5459352e-10,3.393488e-09\n",
      "Iteration 65815: loss = 1.5460237e-10,3.3929786e-09\n",
      "Iteration 65820: loss = 1.5600628e-10,3.392477e-09\n",
      "Iteration 65825: loss = 1.5461435e-10,3.3919798e-09\n",
      "Iteration 65830: loss = 1.5461928e-10,3.3914818e-09\n",
      "Iteration 65835: loss = 1.5462719e-10,3.3909762e-09\n",
      "Iteration 65840: loss = 1.5603198e-10,3.3904728e-09\n",
      "Iteration 65845: loss = 1.5463658e-10,3.3899836e-09\n",
      "Iteration 65850: loss = 1.5604378e-10,3.3894718e-09\n",
      "Iteration 65855: loss = 1.5464917e-10,3.3889804e-09\n",
      "Iteration 65860: loss = 1.5605657e-10,3.3884686e-09\n",
      "Iteration 65865: loss = 1.5466041e-10,3.3879826e-09\n",
      "Iteration 65870: loss = 1.5467007e-10,3.3874703e-09\n",
      "Iteration 65875: loss = 1.5467728e-10,3.3869676e-09\n",
      "Iteration 65880: loss = 1.546809e-10,3.3864738e-09\n",
      "Iteration 65885: loss = 1.5468837e-10,3.3859693e-09\n",
      "Iteration 65890: loss = 1.5469494e-10,3.3854661e-09\n",
      "Iteration 65895: loss = 1.5469967e-10,3.3849703e-09\n",
      "Iteration 65900: loss = 1.5470726e-10,3.384465e-09\n",
      "Iteration 65905: loss = 1.5471364e-10,3.3839613e-09\n",
      "Iteration 65910: loss = 1.5471473e-10,3.3834753e-09\n",
      "Iteration 65915: loss = 1.5471742e-10,3.3829848e-09\n",
      "Iteration 65920: loss = 1.5472075e-10,3.3824925e-09\n",
      "Iteration 65925: loss = 1.5472353e-10,3.3820007e-09\n",
      "Iteration 65930: loss = 1.5472604e-10,3.3815106e-09\n",
      "Iteration 65935: loss = 1.5472741e-10,3.381023e-09\n",
      "Iteration 65940: loss = 1.5472783e-10,3.3805385e-09\n",
      "Iteration 65945: loss = 1.5472719e-10,3.3800565e-09\n",
      "Iteration 65950: loss = 1.5472502e-10,3.3795795e-09\n",
      "Iteration 65955: loss = 1.54721e-10,3.3791085e-09\n",
      "Iteration 65960: loss = 1.5471517e-10,3.3786416e-09\n",
      "Iteration 65965: loss = 1.5470966e-10,3.378176e-09\n",
      "Iteration 65970: loss = 1.5470235e-10,3.377713e-09\n",
      "Iteration 65975: loss = 1.5469453e-10,3.3772534e-09\n",
      "Iteration 65980: loss = 1.5468622e-10,3.3767942e-09\n",
      "Iteration 65985: loss = 1.5467629e-10,3.3763392e-09\n",
      "Iteration 65990: loss = 1.5466618e-10,3.3758851e-09\n",
      "Iteration 65995: loss = 1.5465573e-10,3.3754328e-09\n",
      "Iteration 66000: loss = 1.5464442e-10,3.3749816e-09\n",
      "Iteration 66005: loss = 1.5463247e-10,3.3745335e-09\n",
      "Iteration 66010: loss = 1.5461886e-10,3.3740903e-09\n",
      "Iteration 66015: loss = 1.5460459e-10,3.3736482e-09\n",
      "Iteration 66020: loss = 1.545901e-10,3.373207e-09\n",
      "Iteration 66025: loss = 1.5457477e-10,3.3727692e-09\n",
      "Iteration 66030: loss = 1.5456235e-10,3.3723213e-09\n",
      "Iteration 66035: loss = 1.5455821e-10,3.3718492e-09\n",
      "Iteration 66040: loss = 1.545588e-10,3.3713627e-09\n",
      "Iteration 66045: loss = 1.5456146e-10,3.370872e-09\n",
      "Iteration 66050: loss = 1.5456462e-10,3.370377e-09\n",
      "Iteration 66055: loss = 1.5456811e-10,3.369883e-09\n",
      "Iteration 66060: loss = 1.5456954e-10,3.3693937e-09\n",
      "Iteration 66065: loss = 1.5457086e-10,3.3689058e-09\n",
      "Iteration 66070: loss = 1.5457119e-10,3.3684209e-09\n",
      "Iteration 66075: loss = 1.5457004e-10,3.3679386e-09\n",
      "Iteration 66080: loss = 1.5456818e-10,3.3674592e-09\n",
      "Iteration 66085: loss = 1.5456363e-10,3.3669882e-09\n",
      "Iteration 66090: loss = 1.5455899e-10,3.366519e-09\n",
      "Iteration 66095: loss = 1.5455232e-10,3.3660537e-09\n",
      "Iteration 66100: loss = 1.545452e-10,3.3655896e-09\n",
      "Iteration 66105: loss = 1.545372e-10,3.3651282e-09\n",
      "Iteration 66110: loss = 1.5452796e-10,3.3646699e-09\n",
      "Iteration 66115: loss = 1.5451866e-10,3.3642125e-09\n",
      "Iteration 66120: loss = 1.545087e-10,3.3637568e-09\n",
      "Iteration 66125: loss = 1.5449728e-10,3.363306e-09\n",
      "Iteration 66130: loss = 1.5448577e-10,3.3628544e-09\n",
      "Iteration 66135: loss = 1.5447364e-10,3.3624037e-09\n",
      "Iteration 66140: loss = 1.544611e-10,3.3619565e-09\n",
      "Iteration 66145: loss = 1.5444632e-10,3.3615155e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66150: loss = 1.5443123e-10,3.3610745e-09\n",
      "Iteration 66155: loss = 1.5441602e-10,3.3606342e-09\n",
      "Iteration 66160: loss = 1.5440062e-10,3.360195e-09\n",
      "Iteration 66165: loss = 1.5438505e-10,3.359756e-09\n",
      "Iteration 66170: loss = 1.5436925e-10,3.359317e-09\n",
      "Iteration 66175: loss = 1.5435238e-10,3.358882e-09\n",
      "Iteration 66180: loss = 1.5433531e-10,3.3584469e-09\n",
      "Iteration 66185: loss = 1.5431807e-10,3.3580119e-09\n",
      "Iteration 66190: loss = 1.54301e-10,3.357578e-09\n",
      "Iteration 66195: loss = 1.5428407e-10,3.357143e-09\n",
      "Iteration 66200: loss = 1.5426642e-10,3.3567087e-09\n",
      "Iteration 66205: loss = 1.5424918e-10,3.3562744e-09\n",
      "Iteration 66210: loss = 1.5423172e-10,3.3558405e-09\n",
      "Iteration 66215: loss = 1.542134e-10,3.3554082e-09\n",
      "Iteration 66220: loss = 1.5419484e-10,3.354978e-09\n",
      "Iteration 66225: loss = 1.5417602e-10,3.3545478e-09\n",
      "Iteration 66230: loss = 1.5415717e-10,3.354117e-09\n",
      "Iteration 66235: loss = 1.5413819e-10,3.353687e-09\n",
      "Iteration 66240: loss = 1.5411962e-10,3.3532568e-09\n",
      "Iteration 66245: loss = 1.5410072e-10,3.352826e-09\n",
      "Iteration 66250: loss = 1.5408437e-10,3.3523886e-09\n",
      "Iteration 66255: loss = 1.5407602e-10,3.3519265e-09\n",
      "Iteration 66260: loss = 1.5407327e-10,3.3514485e-09\n",
      "Iteration 66265: loss = 1.5407291e-10,3.350964e-09\n",
      "Iteration 66270: loss = 1.5407327e-10,3.3504746e-09\n",
      "Iteration 66275: loss = 1.5407402e-10,3.349986e-09\n",
      "Iteration 66280: loss = 1.5407402e-10,3.3495e-09\n",
      "Iteration 66285: loss = 1.5407263e-10,3.3490162e-09\n",
      "Iteration 66290: loss = 1.5407071e-10,3.348534e-09\n",
      "Iteration 66295: loss = 1.5406791e-10,3.3480567e-09\n",
      "Iteration 66300: loss = 1.5406314e-10,3.3475844e-09\n",
      "Iteration 66305: loss = 1.5405692e-10,3.3471157e-09\n",
      "Iteration 66310: loss = 1.5404993e-10,3.3466507e-09\n",
      "Iteration 66315: loss = 1.54042e-10,3.3461864e-09\n",
      "Iteration 66320: loss = 1.5403322e-10,3.3457246e-09\n",
      "Iteration 66325: loss = 1.5402361e-10,3.345267e-09\n",
      "Iteration 66330: loss = 1.5401387e-10,3.3448084e-09\n",
      "Iteration 66335: loss = 1.5400366e-10,3.3443521e-09\n",
      "Iteration 66340: loss = 1.5399176e-10,3.3438998e-09\n",
      "Iteration 66345: loss = 1.5259195e-10,3.3434473e-09\n",
      "Iteration 66350: loss = 1.53992e-10,3.3429235e-09\n",
      "Iteration 66355: loss = 1.5400672e-10,3.3423917e-09\n",
      "Iteration 66360: loss = 1.5400824e-10,3.3419e-09\n",
      "Iteration 66365: loss = 1.5261876e-10,3.3414178e-09\n",
      "Iteration 66370: loss = 1.5401445e-10,3.340907e-09\n",
      "Iteration 66375: loss = 1.540216e-10,3.3403984e-09\n",
      "Iteration 66380: loss = 1.5402864e-10,3.3398893e-09\n",
      "Iteration 66385: loss = 1.5403458e-10,3.3393834e-09\n",
      "Iteration 66390: loss = 1.5265043e-10,3.338884e-09\n",
      "Iteration 66395: loss = 1.5404915e-10,3.3383656e-09\n",
      "Iteration 66400: loss = 1.5266126e-10,3.337878e-09\n",
      "Iteration 66405: loss = 1.5405978e-10,3.3373575e-09\n",
      "Iteration 66410: loss = 1.526763e-10,3.336857e-09\n",
      "Iteration 66415: loss = 1.5407202e-10,3.3363454e-09\n",
      "Iteration 66420: loss = 1.5268943e-10,3.3358418e-09\n",
      "Iteration 66425: loss = 1.52695e-10,3.3353378e-09\n",
      "Iteration 66430: loss = 1.5270173e-10,3.33483e-09\n",
      "Iteration 66435: loss = 1.5270707e-10,3.3343253e-09\n",
      "Iteration 66440: loss = 1.527153e-10,3.3338123e-09\n",
      "Iteration 66445: loss = 1.5272017e-10,3.3333105e-09\n",
      "Iteration 66450: loss = 1.5411658e-10,3.3327978e-09\n",
      "Iteration 66455: loss = 1.5272981e-10,3.3323058e-09\n",
      "Iteration 66460: loss = 1.5273861e-10,3.331791e-09\n",
      "Iteration 66465: loss = 1.5413619e-10,3.3312744e-09\n",
      "Iteration 66470: loss = 1.5274686e-10,3.3307899e-09\n",
      "Iteration 66475: loss = 1.5275191e-10,3.3302863e-09\n",
      "Iteration 66480: loss = 1.5274264e-10,3.3298249e-09\n",
      "Iteration 66485: loss = 1.5272782e-10,3.3293805e-09\n",
      "Iteration 66490: loss = 1.5272368e-10,3.3289047e-09\n",
      "Iteration 66495: loss = 1.5271429e-10,3.3284442e-09\n",
      "Iteration 66500: loss = 1.5270485e-10,3.3279832e-09\n",
      "Iteration 66505: loss = 1.5269473e-10,3.3275245e-09\n",
      "Iteration 66510: loss = 1.5268628e-10,3.3270617e-09\n",
      "Iteration 66515: loss = 1.5267472e-10,3.326607e-09\n",
      "Iteration 66520: loss = 1.5266645e-10,3.326143e-09\n",
      "Iteration 66525: loss = 1.5404567e-10,3.3256797e-09\n",
      "Iteration 66530: loss = 1.5264563e-10,3.3252265e-09\n",
      "Iteration 66535: loss = 1.5263847e-10,3.3247582e-09\n",
      "Iteration 66540: loss = 1.5262779e-10,3.3243015e-09\n",
      "Iteration 66545: loss = 1.5261749e-10,3.3238443e-09\n",
      "Iteration 66550: loss = 1.5399791e-10,3.3233765e-09\n",
      "Iteration 66555: loss = 1.5259329e-10,3.3229384e-09\n",
      "Iteration 66560: loss = 1.5258349e-10,3.322477e-09\n",
      "Iteration 66565: loss = 1.5257821e-10,3.322004e-09\n",
      "Iteration 66570: loss = 1.525698e-10,3.3215397e-09\n",
      "Iteration 66575: loss = 1.5255422e-10,3.3210965e-09\n",
      "Iteration 66580: loss = 1.5254435e-10,3.3206358e-09\n",
      "Iteration 66585: loss = 1.5253761e-10,3.3201677e-09\n",
      "Iteration 66590: loss = 1.5253238e-10,3.3196925e-09\n",
      "Iteration 66595: loss = 1.5252193e-10,3.3192342e-09\n",
      "Iteration 66600: loss = 1.5251259e-10,3.3187737e-09\n",
      "Iteration 66605: loss = 1.5250438e-10,3.3183074e-09\n",
      "Iteration 66610: loss = 1.5249217e-10,3.3178549e-09\n",
      "Iteration 66615: loss = 1.5247975e-10,3.3174015e-09\n",
      "Iteration 66620: loss = 1.5246965e-10,3.3169423e-09\n",
      "Iteration 66625: loss = 1.5246059e-10,3.3164795e-09\n",
      "Iteration 66630: loss = 1.5245166e-10,3.3160164e-09\n",
      "Iteration 66635: loss = 1.5244243e-10,3.3155536e-09\n",
      "Iteration 66640: loss = 1.5382746e-10,3.3150718e-09\n",
      "Iteration 66645: loss = 1.5242319e-10,3.3146301e-09\n",
      "Iteration 66650: loss = 1.524153e-10,3.3141645e-09\n",
      "Iteration 66655: loss = 1.5240974e-10,3.3136907e-09\n",
      "Iteration 66660: loss = 1.5378897e-10,3.3132257e-09\n",
      "Iteration 66665: loss = 1.523864e-10,3.3127803e-09\n",
      "Iteration 66670: loss = 1.5238005e-10,3.3123084e-09\n",
      "Iteration 66675: loss = 1.5236803e-10,3.311854e-09\n",
      "Iteration 66680: loss = 1.523608e-10,3.3113863e-09\n",
      "Iteration 66685: loss = 1.5235034e-10,3.310926e-09\n",
      "Iteration 66690: loss = 1.5234065e-10,3.3104646e-09\n",
      "Iteration 66695: loss = 1.5233216e-10,3.3099992e-09\n",
      "Iteration 66700: loss = 1.5231971e-10,3.3095457e-09\n",
      "Iteration 66705: loss = 1.5231438e-10,3.309073e-09\n",
      "Iteration 66710: loss = 1.5230102e-10,3.3086214e-09\n",
      "Iteration 66715: loss = 1.5229097e-10,3.3081613e-09\n",
      "Iteration 66720: loss = 1.5228575e-10,3.3076857e-09\n",
      "Iteration 66725: loss = 1.5227213e-10,3.307236e-09\n",
      "Iteration 66730: loss = 1.5226086e-10,3.3067784e-09\n",
      "Iteration 66735: loss = 1.5225447e-10,3.3063068e-09\n",
      "Iteration 66740: loss = 1.5363709e-10,3.3058318e-09\n",
      "Iteration 66745: loss = 1.5223162e-10,3.3053922e-09\n",
      "Iteration 66750: loss = 1.5222106e-10,3.3049323e-09\n",
      "Iteration 66755: loss = 1.5221344e-10,3.3044647e-09\n",
      "Iteration 66760: loss = 1.522087e-10,3.303988e-09\n",
      "Iteration 66765: loss = 1.5219977e-10,3.3035246e-09\n",
      "Iteration 66770: loss = 1.521894e-10,3.3030636e-09\n",
      "Iteration 66775: loss = 1.5218114e-10,3.3025973e-09\n",
      "Iteration 66780: loss = 1.5217176e-10,3.302135e-09\n",
      "Iteration 66785: loss = 1.521581e-10,3.3016843e-09\n",
      "Iteration 66790: loss = 1.5214734e-10,3.3012255e-09\n",
      "Iteration 66795: loss = 1.5213747e-10,3.3007637e-09\n",
      "Iteration 66800: loss = 1.521276e-10,3.3003011e-09\n",
      "Iteration 66805: loss = 1.5211794e-10,3.2998384e-09\n",
      "Iteration 66810: loss = 1.5210815e-10,3.2993759e-09\n",
      "Iteration 66815: loss = 1.5209835e-10,3.298914e-09\n",
      "Iteration 66820: loss = 1.520883e-10,3.2984533e-09\n",
      "Iteration 66825: loss = 1.5207795e-10,3.297992e-09\n",
      "Iteration 66830: loss = 1.5206626e-10,3.2975354e-09\n",
      "Iteration 66835: loss = 1.5205415e-10,3.2970795e-09\n",
      "Iteration 66840: loss = 1.5204189e-10,3.2966239e-09\n",
      "Iteration 66845: loss = 1.5202949e-10,3.2961696e-09\n",
      "Iteration 66850: loss = 1.5201618e-10,3.2957181e-09\n",
      "Iteration 66855: loss = 1.5200215e-10,3.2952674e-09\n",
      "Iteration 66860: loss = 1.5198769e-10,3.2948184e-09\n",
      "Iteration 66865: loss = 1.5059493e-10,3.2943686e-09\n",
      "Iteration 66870: loss = 1.519868e-10,3.2938368e-09\n",
      "Iteration 66875: loss = 1.5338343e-10,3.2933187e-09\n",
      "Iteration 66880: loss = 1.5198541e-10,3.292856e-09\n",
      "Iteration 66885: loss = 1.5199236e-10,3.2923437e-09\n",
      "Iteration 66890: loss = 1.5199732e-10,3.2918372e-09\n",
      "Iteration 66895: loss = 1.5199915e-10,3.291339e-09\n",
      "Iteration 66900: loss = 1.5199986e-10,3.2908454e-09\n",
      "Iteration 66905: loss = 1.5199915e-10,3.290356e-09\n",
      "Iteration 66910: loss = 1.5199747e-10,3.2898688e-09\n",
      "Iteration 66915: loss = 1.519941e-10,3.2893857e-09\n",
      "Iteration 66920: loss = 1.5199027e-10,3.2889051e-09\n",
      "Iteration 66925: loss = 1.519836e-10,3.2884329e-09\n",
      "Iteration 66930: loss = 1.5197625e-10,3.2879623e-09\n",
      "Iteration 66935: loss = 1.519677e-10,3.287496e-09\n",
      "Iteration 66940: loss = 1.5195835e-10,3.2870309e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66945: loss = 1.5194888e-10,3.286568e-09\n",
      "Iteration 66950: loss = 1.5193798e-10,3.286106e-09\n",
      "Iteration 66955: loss = 1.519269e-10,3.285648e-09\n",
      "Iteration 66960: loss = 1.5191533e-10,3.28519e-09\n",
      "Iteration 66965: loss = 1.5190324e-10,3.284732e-09\n",
      "Iteration 66970: loss = 1.5189054e-10,3.2842784e-09\n",
      "Iteration 66975: loss = 1.5187691e-10,3.2838252e-09\n",
      "Iteration 66980: loss = 1.5186331e-10,3.283374e-09\n",
      "Iteration 66985: loss = 1.5184926e-10,3.282922e-09\n",
      "Iteration 66990: loss = 1.518352e-10,3.2824712e-09\n",
      "Iteration 66995: loss = 1.5182008e-10,3.2820233e-09\n",
      "Iteration 67000: loss = 1.5180308e-10,3.2815815e-09\n",
      "Iteration 67005: loss = 1.5178624e-10,3.2811387e-09\n",
      "Iteration 67010: loss = 1.5176911e-10,3.2806968e-09\n",
      "Iteration 67015: loss = 1.5175201e-10,3.2802563e-09\n",
      "Iteration 67020: loss = 1.5173472e-10,3.2798155e-09\n",
      "Iteration 67025: loss = 1.5171704e-10,3.279374e-09\n",
      "Iteration 67030: loss = 1.5170361e-10,3.278923e-09\n",
      "Iteration 67035: loss = 1.5169878e-10,3.2784435e-09\n",
      "Iteration 67040: loss = 1.5168919e-10,3.2779783e-09\n",
      "Iteration 67045: loss = 1.5167616e-10,3.277524e-09\n",
      "Iteration 67050: loss = 1.516662e-10,3.2770602e-09\n",
      "Iteration 67055: loss = 1.5165835e-10,3.2765899e-09\n",
      "Iteration 67060: loss = 1.5164721e-10,3.27613e-09\n",
      "Iteration 67065: loss = 1.5163831e-10,3.2756633e-09\n",
      "Iteration 67070: loss = 1.516297e-10,3.2751963e-09\n",
      "Iteration 67075: loss = 1.5161962e-10,3.274733e-09\n",
      "Iteration 67080: loss = 1.5023076e-10,3.2742709e-09\n",
      "Iteration 67085: loss = 1.5160066e-10,3.2738028e-09\n",
      "Iteration 67090: loss = 1.5159178e-10,3.2733367e-09\n",
      "Iteration 67095: loss = 1.5158286e-10,3.272869e-09\n",
      "Iteration 67100: loss = 1.5157309e-10,3.2724041e-09\n",
      "Iteration 67105: loss = 1.5156376e-10,3.2719387e-09\n",
      "Iteration 67110: loss = 1.5155407e-10,3.2714735e-09\n",
      "Iteration 67115: loss = 1.5154528e-10,3.2710066e-09\n",
      "Iteration 67120: loss = 1.5153784e-10,3.270535e-09\n",
      "Iteration 67125: loss = 1.5153197e-10,3.2700582e-09\n",
      "Iteration 67130: loss = 1.5152436e-10,3.2695873e-09\n",
      "Iteration 67135: loss = 1.5151641e-10,3.2691176e-09\n",
      "Iteration 67140: loss = 1.5150807e-10,3.2686487e-09\n",
      "Iteration 67145: loss = 1.5149831e-10,3.268184e-09\n",
      "Iteration 67150: loss = 1.5148788e-10,3.2677203e-09\n",
      "Iteration 67155: loss = 1.514777e-10,3.2672576e-09\n",
      "Iteration 67160: loss = 1.5146676e-10,3.2667968e-09\n",
      "Iteration 67165: loss = 1.5145435e-10,3.2663392e-09\n",
      "Iteration 67170: loss = 1.5144187e-10,3.2658822e-09\n",
      "Iteration 67175: loss = 1.5143546e-10,3.265407e-09\n",
      "Iteration 67180: loss = 1.5142639e-10,3.2649403e-09\n",
      "Iteration 67185: loss = 1.5141477e-10,3.264481e-09\n",
      "Iteration 67190: loss = 1.5140682e-10,3.26401e-09\n",
      "Iteration 67195: loss = 1.513975e-10,3.263544e-09\n",
      "Iteration 67200: loss = 1.5138692e-10,3.2630811e-09\n",
      "Iteration 67205: loss = 1.5137878e-10,3.262611e-09\n",
      "Iteration 67210: loss = 1.4998816e-10,3.2621525e-09\n",
      "Iteration 67215: loss = 1.5135886e-10,3.2616818e-09\n",
      "Iteration 67220: loss = 1.5135108e-10,3.26121e-09\n",
      "Iteration 67225: loss = 1.5134066e-10,3.2607468e-09\n",
      "Iteration 67230: loss = 1.513341e-10,3.2602718e-09\n",
      "Iteration 67235: loss = 1.5132778e-10,3.2597967e-09\n",
      "Iteration 67240: loss = 1.513145e-10,3.259341e-09\n",
      "Iteration 67245: loss = 1.5130504e-10,3.2588754e-09\n",
      "Iteration 67250: loss = 1.5129627e-10,3.2584069e-09\n",
      "Iteration 67255: loss = 1.4990427e-10,3.2579526e-09\n",
      "Iteration 67260: loss = 1.5127755e-10,3.257473e-09\n",
      "Iteration 67265: loss = 1.498867e-10,3.2570147e-09\n",
      "Iteration 67270: loss = 1.5126052e-10,3.2565342e-09\n",
      "Iteration 67275: loss = 1.4986751e-10,3.256082e-09\n",
      "Iteration 67280: loss = 1.5123977e-10,3.2556065e-09\n",
      "Iteration 67285: loss = 1.4985561e-10,3.2551284e-09\n",
      "Iteration 67290: loss = 1.5124209e-10,3.2546088e-09\n",
      "Iteration 67295: loss = 1.5124955e-10,3.2540919e-09\n",
      "Iteration 67300: loss = 1.4987649e-10,3.2535805e-09\n",
      "Iteration 67305: loss = 1.5126189e-10,3.253064e-09\n",
      "Iteration 67310: loss = 1.5126896e-10,3.252548e-09\n",
      "Iteration 67315: loss = 1.5127544e-10,3.2520333e-09\n",
      "Iteration 67320: loss = 1.5128239e-10,3.2515182e-09\n",
      "Iteration 67325: loss = 1.499088e-10,3.2510068e-09\n",
      "Iteration 67330: loss = 1.499147e-10,3.2504943e-09\n",
      "Iteration 67335: loss = 1.5130266e-10,3.2499714e-09\n",
      "Iteration 67340: loss = 1.4992428e-10,3.2494754e-09\n",
      "Iteration 67345: loss = 1.513144e-10,3.248944e-09\n",
      "Iteration 67350: loss = 1.5132127e-10,3.2484293e-09\n",
      "Iteration 67355: loss = 1.5132749e-10,3.247915e-09\n",
      "Iteration 67360: loss = 1.4995348e-10,3.2474072e-09\n",
      "Iteration 67365: loss = 1.4995949e-10,3.2468932e-09\n",
      "Iteration 67370: loss = 1.4996496e-10,3.2463814e-09\n",
      "Iteration 67375: loss = 1.4997369e-10,3.2458594e-09\n",
      "Iteration 67380: loss = 1.5136002e-10,3.2453407e-09\n",
      "Iteration 67385: loss = 1.4998673e-10,3.244829e-09\n",
      "Iteration 67390: loss = 1.4999306e-10,3.2443153e-09\n",
      "Iteration 67395: loss = 1.5137959e-10,3.2437955e-09\n",
      "Iteration 67400: loss = 1.5138688e-10,3.2432776e-09\n",
      "Iteration 67405: loss = 1.5139305e-10,3.2427634e-09\n",
      "Iteration 67410: loss = 1.500194e-10,3.2422534e-09\n",
      "Iteration 67415: loss = 1.5002678e-10,3.2417367e-09\n",
      "Iteration 67420: loss = 1.5003297e-10,3.2412224e-09\n",
      "Iteration 67425: loss = 1.5003739e-10,3.2407135e-09\n",
      "Iteration 67430: loss = 1.5004628e-10,3.2401917e-09\n",
      "Iteration 67435: loss = 1.5005146e-10,3.2396807e-09\n",
      "Iteration 67440: loss = 1.5005712e-10,3.2391672e-09\n",
      "Iteration 67445: loss = 1.500647e-10,3.238649e-09\n",
      "Iteration 67450: loss = 1.500728e-10,3.2381289e-09\n",
      "Iteration 67455: loss = 1.5007989e-10,3.2376126e-09\n",
      "Iteration 67460: loss = 1.5008605e-10,3.2370975e-09\n",
      "Iteration 67465: loss = 1.5009122e-10,3.2365857e-09\n",
      "Iteration 67470: loss = 1.5009383e-10,3.236082e-09\n",
      "Iteration 67475: loss = 1.500955e-10,3.2355822e-09\n",
      "Iteration 67480: loss = 1.5009548e-10,3.235086e-09\n",
      "Iteration 67485: loss = 1.5009423e-10,3.2345937e-09\n",
      "Iteration 67490: loss = 1.5009215e-10,3.234104e-09\n",
      "Iteration 67495: loss = 1.5008865e-10,3.2336178e-09\n",
      "Iteration 67500: loss = 1.5008435e-10,3.2331342e-09\n",
      "Iteration 67505: loss = 1.5007846e-10,3.232656e-09\n",
      "Iteration 67510: loss = 1.5007051e-10,3.232183e-09\n",
      "Iteration 67515: loss = 1.5006223e-10,3.231712e-09\n",
      "Iteration 67520: loss = 1.5005332e-10,3.2312428e-09\n",
      "Iteration 67525: loss = 1.5004294e-10,3.230776e-09\n",
      "Iteration 67530: loss = 1.5003254e-10,3.2303116e-09\n",
      "Iteration 67535: loss = 1.5002197e-10,3.2298466e-09\n",
      "Iteration 67540: loss = 1.5001049e-10,3.2293845e-09\n",
      "Iteration 67545: loss = 1.4999814e-10,3.2289245e-09\n",
      "Iteration 67550: loss = 1.4998561e-10,3.2284655e-09\n",
      "Iteration 67555: loss = 1.4997242e-10,3.2280076e-09\n",
      "Iteration 67560: loss = 1.4995973e-10,3.2275493e-09\n",
      "Iteration 67565: loss = 1.4994922e-10,3.227084e-09\n",
      "Iteration 67570: loss = 1.4994675e-10,3.2265943e-09\n",
      "Iteration 67575: loss = 1.4994816e-10,3.2260945e-09\n",
      "Iteration 67580: loss = 1.499512e-10,3.225589e-09\n",
      "Iteration 67585: loss = 1.4995416e-10,3.2250829e-09\n",
      "Iteration 67590: loss = 1.4995753e-10,3.224578e-09\n",
      "Iteration 67595: loss = 1.4995959e-10,3.2240752e-09\n",
      "Iteration 67600: loss = 1.4996053e-10,3.2235759e-09\n",
      "Iteration 67605: loss = 1.4996078e-10,3.2230782e-09\n",
      "Iteration 67610: loss = 1.499597e-10,3.2225846e-09\n",
      "Iteration 67615: loss = 1.499579e-10,3.2220937e-09\n",
      "Iteration 67620: loss = 1.4995456e-10,3.2216065e-09\n",
      "Iteration 67625: loss = 1.499505e-10,3.2211211e-09\n",
      "Iteration 67630: loss = 1.4994413e-10,3.2206438e-09\n",
      "Iteration 67635: loss = 1.4993685e-10,3.220169e-09\n",
      "Iteration 67640: loss = 1.4992892e-10,3.2196956e-09\n",
      "Iteration 67645: loss = 1.4991997e-10,3.2192262e-09\n",
      "Iteration 67650: loss = 1.4991015e-10,3.2187575e-09\n",
      "Iteration 67655: loss = 1.4990031e-10,3.21829e-09\n",
      "Iteration 67660: loss = 1.4988961e-10,3.2178242e-09\n",
      "Iteration 67665: loss = 1.498778e-10,3.2173622e-09\n",
      "Iteration 67670: loss = 1.4986615e-10,3.216901e-09\n",
      "Iteration 67675: loss = 1.4985398e-10,3.2164402e-09\n",
      "Iteration 67680: loss = 1.4984124e-10,3.21598e-09\n",
      "Iteration 67685: loss = 1.4982764e-10,3.215523e-09\n",
      "Iteration 67690: loss = 1.4981366e-10,3.2150669e-09\n",
      "Iteration 67695: loss = 1.4979855e-10,3.2146152e-09\n",
      "Iteration 67700: loss = 1.4978303e-10,3.214165e-09\n",
      "Iteration 67705: loss = 1.4976721e-10,3.213715e-09\n",
      "Iteration 67710: loss = 1.4975117e-10,3.2132657e-09\n",
      "Iteration 67715: loss = 1.4973417e-10,3.2128207e-09\n",
      "Iteration 67720: loss = 1.4971678e-10,3.2123746e-09\n",
      "Iteration 67725: loss = 1.4969952e-10,3.2119287e-09\n",
      "Iteration 67730: loss = 1.496822e-10,3.2114829e-09\n",
      "Iteration 67735: loss = 1.4966507e-10,3.2110377e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67740: loss = 1.4964754e-10,3.2105922e-09\n",
      "Iteration 67745: loss = 1.496301e-10,3.2101468e-09\n",
      "Iteration 67750: loss = 1.4961284e-10,3.2097016e-09\n",
      "Iteration 67755: loss = 1.4959507e-10,3.2092566e-09\n",
      "Iteration 67760: loss = 1.4957754e-10,3.2088114e-09\n",
      "Iteration 67765: loss = 1.4955946e-10,3.208368e-09\n",
      "Iteration 67770: loss = 1.4954084e-10,3.2079268e-09\n",
      "Iteration 67775: loss = 1.4952208e-10,3.2074854e-09\n",
      "Iteration 67780: loss = 1.4950317e-10,3.2070442e-09\n",
      "Iteration 67785: loss = 1.4948424e-10,3.206603e-09\n",
      "Iteration 67790: loss = 1.4946878e-10,3.2061518e-09\n",
      "Iteration 67795: loss = 1.494608e-10,3.205678e-09\n",
      "Iteration 67800: loss = 1.4945813e-10,3.2051877e-09\n",
      "Iteration 67805: loss = 1.4945815e-10,3.2046898e-09\n",
      "Iteration 67810: loss = 1.4945879e-10,3.204191e-09\n",
      "Iteration 67815: loss = 1.4945883e-10,3.2036915e-09\n",
      "Iteration 67820: loss = 1.4945921e-10,3.2031937e-09\n",
      "Iteration 67825: loss = 1.4945857e-10,3.2026974e-09\n",
      "Iteration 67830: loss = 1.4945671e-10,3.202205e-09\n",
      "Iteration 67835: loss = 1.494542e-10,3.2017136e-09\n",
      "Iteration 67840: loss = 1.4945073e-10,3.2012264e-09\n",
      "Iteration 67845: loss = 1.4944496e-10,3.2007463e-09\n",
      "Iteration 67850: loss = 1.494385e-10,3.2002678e-09\n",
      "Iteration 67855: loss = 1.4943037e-10,3.1997933e-09\n",
      "Iteration 67860: loss = 1.4942214e-10,3.1993204e-09\n",
      "Iteration 67865: loss = 1.4941333e-10,3.1988483e-09\n",
      "Iteration 67870: loss = 1.4940343e-10,3.1983807e-09\n",
      "Iteration 67875: loss = 1.4939289e-10,3.1979135e-09\n",
      "Iteration 67880: loss = 1.4938224e-10,3.1974472e-09\n",
      "Iteration 67885: loss = 1.4937074e-10,3.1969836e-09\n",
      "Iteration 67890: loss = 1.4799491e-10,3.1965053e-09\n",
      "Iteration 67895: loss = 1.4938314e-10,3.1959497e-09\n",
      "Iteration 67900: loss = 1.4939154e-10,3.1954268e-09\n",
      "Iteration 67905: loss = 1.4801983e-10,3.194937e-09\n",
      "Iteration 67910: loss = 1.4939804e-10,3.19441e-09\n",
      "Iteration 67915: loss = 1.4803345e-10,3.1938994e-09\n",
      "Iteration 67920: loss = 1.4804008e-10,3.1933813e-09\n",
      "Iteration 67925: loss = 1.4804682e-10,3.1928626e-09\n",
      "Iteration 67930: loss = 1.4942324e-10,3.1923415e-09\n",
      "Iteration 67935: loss = 1.494325e-10,3.1918173e-09\n",
      "Iteration 67940: loss = 1.4806632e-10,3.1913103e-09\n",
      "Iteration 67945: loss = 1.4944408e-10,3.1907854e-09\n",
      "Iteration 67950: loss = 1.4945194e-10,3.1902638e-09\n",
      "Iteration 67955: loss = 1.4808617e-10,3.1897556e-09\n",
      "Iteration 67960: loss = 1.4809225e-10,3.1892384e-09\n",
      "Iteration 67965: loss = 1.4810009e-10,3.1887168e-09\n",
      "Iteration 67970: loss = 1.4810665e-10,3.1881993e-09\n",
      "Iteration 67975: loss = 1.481136e-10,3.1876808e-09\n",
      "Iteration 67980: loss = 1.4811898e-10,3.1871654e-09\n",
      "Iteration 67985: loss = 1.4949715e-10,3.18664e-09\n",
      "Iteration 67990: loss = 1.4812868e-10,3.1861402e-09\n",
      "Iteration 67995: loss = 1.4813654e-10,3.1856184e-09\n",
      "Iteration 68000: loss = 1.4951726e-10,3.1850853e-09\n",
      "Iteration 68005: loss = 1.4814831e-10,3.1845857e-09\n",
      "Iteration 68010: loss = 1.4815404e-10,3.184071e-09\n",
      "Iteration 68015: loss = 1.4816325e-10,3.183544e-09\n",
      "Iteration 68020: loss = 1.481733e-10,3.1830163e-09\n",
      "Iteration 68025: loss = 1.481784e-10,3.1825027e-09\n",
      "Iteration 68030: loss = 1.481822e-10,3.181994e-09\n",
      "Iteration 68035: loss = 1.4818703e-10,3.1814804e-09\n",
      "Iteration 68040: loss = 1.4819246e-10,3.1809662e-09\n",
      "Iteration 68045: loss = 1.4819622e-10,3.1804563e-09\n",
      "Iteration 68050: loss = 1.4819995e-10,3.1799479e-09\n",
      "Iteration 68055: loss = 1.4820203e-10,3.179443e-09\n",
      "Iteration 68060: loss = 1.4820321e-10,3.1789407e-09\n",
      "Iteration 68065: loss = 1.4820321e-10,3.178442e-09\n",
      "Iteration 68070: loss = 1.4820217e-10,3.1779464e-09\n",
      "Iteration 68075: loss = 1.4820031e-10,3.1774539e-09\n",
      "Iteration 68080: loss = 1.481962e-10,3.1769665e-09\n",
      "Iteration 68085: loss = 1.4819106e-10,3.1764844e-09\n",
      "Iteration 68090: loss = 1.4818403e-10,3.1760055e-09\n",
      "Iteration 68095: loss = 1.4817676e-10,3.1755285e-09\n",
      "Iteration 68100: loss = 1.4816885e-10,3.1750542e-09\n",
      "Iteration 68105: loss = 1.4815955e-10,3.174583e-09\n",
      "Iteration 68110: loss = 1.4815048e-10,3.1741123e-09\n",
      "Iteration 68115: loss = 1.4814051e-10,3.173642e-09\n",
      "Iteration 68120: loss = 1.4812962e-10,3.1731764e-09\n",
      "Iteration 68125: loss = 1.4811806e-10,3.1727108e-09\n",
      "Iteration 68130: loss = 1.4810642e-10,3.172247e-09\n",
      "Iteration 68135: loss = 1.48094e-10,3.1717844e-09\n",
      "Iteration 68140: loss = 1.4807965e-10,3.1713285e-09\n",
      "Iteration 68145: loss = 1.4806491e-10,3.1708742e-09\n",
      "Iteration 68150: loss = 1.4805006e-10,3.1704204e-09\n",
      "Iteration 68155: loss = 1.4803493e-10,3.1699665e-09\n",
      "Iteration 68160: loss = 1.4801993e-10,3.1695127e-09\n",
      "Iteration 68165: loss = 1.4800443e-10,3.1690595e-09\n",
      "Iteration 68170: loss = 1.4798836e-10,3.1686103e-09\n",
      "Iteration 68175: loss = 1.4797168e-10,3.16816e-09\n",
      "Iteration 68180: loss = 1.4795497e-10,3.1677112e-09\n",
      "Iteration 68185: loss = 1.4793805e-10,3.1672616e-09\n",
      "Iteration 68190: loss = 1.4792127e-10,3.1668135e-09\n",
      "Iteration 68195: loss = 1.4790442e-10,3.1663654e-09\n",
      "Iteration 68200: loss = 1.4788747e-10,3.1659166e-09\n",
      "Iteration 68205: loss = 1.4787128e-10,3.1654674e-09\n",
      "Iteration 68210: loss = 1.4786175e-10,3.1649956e-09\n",
      "Iteration 68215: loss = 1.4785843e-10,3.1645067e-09\n",
      "Iteration 68220: loss = 1.478588e-10,3.1640068e-09\n",
      "Iteration 68225: loss = 1.4786006e-10,3.1635035e-09\n",
      "Iteration 68230: loss = 1.4786226e-10,3.1629983e-09\n",
      "Iteration 68235: loss = 1.4786311e-10,3.1624965e-09\n",
      "Iteration 68240: loss = 1.4786349e-10,3.1619964e-09\n",
      "Iteration 68245: loss = 1.4786337e-10,3.1614973e-09\n",
      "Iteration 68250: loss = 1.4786178e-10,3.161003e-09\n",
      "Iteration 68255: loss = 1.4785831e-10,3.1605136e-09\n",
      "Iteration 68260: loss = 1.4785363e-10,3.1600296e-09\n",
      "Iteration 68265: loss = 1.4784803e-10,3.1595477e-09\n",
      "Iteration 68270: loss = 1.4784197e-10,3.159066e-09\n",
      "Iteration 68275: loss = 1.478341e-10,3.1585905e-09\n",
      "Iteration 68280: loss = 1.4782597e-10,3.158115e-09\n",
      "Iteration 68285: loss = 1.4781763e-10,3.1576406e-09\n",
      "Iteration 68290: loss = 1.4780815e-10,3.1571707e-09\n",
      "Iteration 68295: loss = 1.4779795e-10,3.1567016e-09\n",
      "Iteration 68300: loss = 1.4778731e-10,3.1562344e-09\n",
      "Iteration 68305: loss = 1.4777553e-10,3.15577e-09\n",
      "Iteration 68310: loss = 1.4776168e-10,3.1553113e-09\n",
      "Iteration 68315: loss = 1.4774817e-10,3.1548533e-09\n",
      "Iteration 68320: loss = 1.477342e-10,3.1543952e-09\n",
      "Iteration 68325: loss = 1.4772016e-10,3.1539384e-09\n",
      "Iteration 68330: loss = 1.477059e-10,3.1534817e-09\n",
      "Iteration 68335: loss = 1.4769082e-10,3.153028e-09\n",
      "Iteration 68340: loss = 1.476748e-10,3.1525758e-09\n",
      "Iteration 68345: loss = 1.4765926e-10,3.1521235e-09\n",
      "Iteration 68350: loss = 1.4764343e-10,3.1516718e-09\n",
      "Iteration 68355: loss = 1.4762734e-10,3.1512197e-09\n",
      "Iteration 68360: loss = 1.4761153e-10,3.1507683e-09\n",
      "Iteration 68365: loss = 1.4759531e-10,3.150317e-09\n",
      "Iteration 68370: loss = 1.4757916e-10,3.1498668e-09\n",
      "Iteration 68375: loss = 1.4756198e-10,3.1494174e-09\n",
      "Iteration 68380: loss = 1.4754435e-10,3.1489709e-09\n",
      "Iteration 68385: loss = 1.4752714e-10,3.1485237e-09\n",
      "Iteration 68390: loss = 1.4750946e-10,3.148077e-09\n",
      "Iteration 68395: loss = 1.4749178e-10,3.1476302e-09\n",
      "Iteration 68400: loss = 1.4747382e-10,3.147184e-09\n",
      "Iteration 68405: loss = 1.4745657e-10,3.146737e-09\n",
      "Iteration 68410: loss = 1.474386e-10,3.1462912e-09\n",
      "Iteration 68415: loss = 1.4742056e-10,3.1458454e-09\n",
      "Iteration 68420: loss = 1.460533e-10,3.1453689e-09\n",
      "Iteration 68425: loss = 1.4743541e-10,3.1448006e-09\n",
      "Iteration 68430: loss = 1.47448e-10,3.144265e-09\n",
      "Iteration 68435: loss = 1.4744524e-10,3.1437728e-09\n",
      "Iteration 68440: loss = 1.4745033e-10,3.143258e-09\n",
      "Iteration 68445: loss = 1.4609658e-10,3.1427418e-09\n",
      "Iteration 68450: loss = 1.474632e-10,3.1422198e-09\n",
      "Iteration 68455: loss = 1.4746969e-10,3.141703e-09\n",
      "Iteration 68460: loss = 1.4747668e-10,3.1411818e-09\n",
      "Iteration 68465: loss = 1.4748379e-10,3.140662e-09\n",
      "Iteration 68470: loss = 1.4749017e-10,3.1401417e-09\n",
      "Iteration 68475: loss = 1.4749818e-10,3.1396192e-09\n",
      "Iteration 68480: loss = 1.4750488e-10,3.1390996e-09\n",
      "Iteration 68485: loss = 1.4615059e-10,3.1385838e-09\n",
      "Iteration 68490: loss = 1.4615548e-10,3.1380702e-09\n",
      "Iteration 68495: loss = 1.4752465e-10,3.1375422e-09\n",
      "Iteration 68500: loss = 1.4753056e-10,3.1370249e-09\n",
      "Iteration 68505: loss = 1.4753741e-10,3.1365053e-09\n",
      "Iteration 68510: loss = 1.4618436e-10,3.1359866e-09\n",
      "Iteration 68515: loss = 1.4755122e-10,3.1354654e-09\n",
      "Iteration 68520: loss = 1.4755766e-10,3.1349463e-09\n",
      "Iteration 68525: loss = 1.4756456e-10,3.1344254e-09\n",
      "Iteration 68530: loss = 1.462105e-10,3.13391e-09\n",
      "Iteration 68535: loss = 1.4621572e-10,3.1333958e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68540: loss = 1.4758532e-10,3.1328669e-09\n",
      "Iteration 68545: loss = 1.4623046e-10,3.132353e-09\n",
      "Iteration 68550: loss = 1.4623729e-10,3.1318326e-09\n",
      "Iteration 68555: loss = 1.4624478e-10,3.1313112e-09\n",
      "Iteration 68560: loss = 1.4624828e-10,3.1308016e-09\n",
      "Iteration 68565: loss = 1.4625721e-10,3.1302756e-09\n",
      "Iteration 68570: loss = 1.4762087e-10,3.1297638e-09\n",
      "Iteration 68575: loss = 1.476119e-10,3.129291e-09\n",
      "Iteration 68580: loss = 1.4624187e-10,3.1288232e-09\n",
      "Iteration 68585: loss = 1.4623232e-10,3.1283527e-09\n",
      "Iteration 68590: loss = 1.475849e-10,3.127874e-09\n",
      "Iteration 68595: loss = 1.4757619e-10,3.1274001e-09\n",
      "Iteration 68600: loss = 1.475689e-10,3.1269227e-09\n",
      "Iteration 68605: loss = 1.461961e-10,3.1264629e-09\n",
      "Iteration 68610: loss = 1.4755118e-10,3.1259768e-09\n",
      "Iteration 68615: loss = 1.4618022e-10,3.125511e-09\n",
      "Iteration 68620: loss = 1.4753365e-10,3.125031e-09\n",
      "Iteration 68625: loss = 1.4616132e-10,3.1245688e-09\n",
      "Iteration 68630: loss = 1.4751479e-10,3.1240879e-09\n",
      "Iteration 68635: loss = 1.4750608e-10,3.123615e-09\n",
      "Iteration 68640: loss = 1.4613337e-10,3.1231548e-09\n",
      "Iteration 68645: loss = 1.4749009e-10,3.122664e-09\n",
      "Iteration 68650: loss = 1.4611505e-10,3.1222105e-09\n",
      "Iteration 68655: loss = 1.4747163e-10,3.1217204e-09\n",
      "Iteration 68660: loss = 1.4609923e-10,3.1212581e-09\n",
      "Iteration 68665: loss = 1.4609207e-10,3.1207819e-09\n",
      "Iteration 68670: loss = 1.4744332e-10,3.1203067e-09\n",
      "Iteration 68675: loss = 1.4607053e-10,3.1198468e-09\n",
      "Iteration 68680: loss = 1.4742947e-10,3.1193486e-09\n",
      "Iteration 68685: loss = 1.4605296e-10,3.1188996e-09\n",
      "Iteration 68690: loss = 1.4740924e-10,3.1184109e-09\n",
      "Iteration 68695: loss = 1.4739902e-10,3.1179421e-09\n",
      "Iteration 68700: loss = 1.4739054e-10,3.1174676e-09\n",
      "Iteration 68705: loss = 1.4602179e-10,3.116996e-09\n",
      "Iteration 68710: loss = 1.460099e-10,3.1165324e-09\n",
      "Iteration 68715: loss = 1.473648e-10,3.1160465e-09\n",
      "Iteration 68720: loss = 1.459943e-10,3.1155807e-09\n",
      "Iteration 68725: loss = 1.4598492e-10,3.1151086e-09\n",
      "Iteration 68730: loss = 1.4597752e-10,3.114632e-09\n",
      "Iteration 68735: loss = 1.4596871e-10,3.114159e-09\n",
      "Iteration 68740: loss = 1.4595732e-10,3.1136937e-09\n",
      "Iteration 68745: loss = 1.4595077e-10,3.1132146e-09\n",
      "Iteration 68750: loss = 1.4730263e-10,3.1127374e-09\n",
      "Iteration 68755: loss = 1.4593367e-10,3.112266e-09\n",
      "Iteration 68760: loss = 1.4592129e-10,3.1118041e-09\n",
      "Iteration 68765: loss = 1.4591331e-10,3.1113288e-09\n",
      "Iteration 68770: loss = 1.4590601e-10,3.110852e-09\n",
      "Iteration 68775: loss = 1.4589625e-10,3.1103817e-09\n",
      "Iteration 68780: loss = 1.4725154e-10,3.1098952e-09\n",
      "Iteration 68785: loss = 1.4587846e-10,3.1094365e-09\n",
      "Iteration 68790: loss = 1.4587132e-10,3.1089578e-09\n",
      "Iteration 68795: loss = 1.4586328e-10,3.1084832e-09\n",
      "Iteration 68800: loss = 1.4585233e-10,3.108016e-09\n",
      "Iteration 68805: loss = 1.4584543e-10,3.1075373e-09\n",
      "Iteration 68810: loss = 1.4583611e-10,3.1070666e-09\n",
      "Iteration 68815: loss = 1.458272e-10,3.106594e-09\n",
      "Iteration 68820: loss = 1.4581644e-10,3.1061276e-09\n",
      "Iteration 68825: loss = 1.4717187e-10,3.105641e-09\n",
      "Iteration 68830: loss = 1.4580061e-10,3.1051766e-09\n",
      "Iteration 68835: loss = 1.457918e-10,3.1047034e-09\n",
      "Iteration 68840: loss = 1.4578017e-10,3.1042389e-09\n",
      "Iteration 68845: loss = 1.4713715e-10,3.1037477e-09\n",
      "Iteration 68850: loss = 1.4712763e-10,3.1032767e-09\n",
      "Iteration 68855: loss = 1.4711916e-10,3.1028038e-09\n",
      "Iteration 68860: loss = 1.4711075e-10,3.1023295e-09\n",
      "Iteration 68865: loss = 1.4574077e-10,3.1018623e-09\n",
      "Iteration 68870: loss = 1.4709255e-10,3.101386e-09\n",
      "Iteration 68875: loss = 1.4572275e-10,3.1009186e-09\n",
      "Iteration 68880: loss = 1.4707514e-10,3.1004403e-09\n",
      "Iteration 68885: loss = 1.4706623e-10,3.0999678e-09\n",
      "Iteration 68890: loss = 1.4569788e-10,3.0994942e-09\n",
      "Iteration 68895: loss = 1.4568878e-10,3.0990228e-09\n",
      "Iteration 68900: loss = 1.4704059e-10,3.0985479e-09\n",
      "Iteration 68905: loss = 1.4566996e-10,3.0980822e-09\n",
      "Iteration 68910: loss = 1.4566233e-10,3.0976057e-09\n",
      "Iteration 68915: loss = 1.4565166e-10,3.0971385e-09\n",
      "Iteration 68920: loss = 1.4564581e-10,3.0966576e-09\n",
      "Iteration 68925: loss = 1.4563573e-10,3.096189e-09\n",
      "Iteration 68930: loss = 1.4698932e-10,3.0957077e-09\n",
      "Iteration 68935: loss = 1.4561966e-10,3.0952394e-09\n",
      "Iteration 68940: loss = 1.4560819e-10,3.0947747e-09\n",
      "Iteration 68945: loss = 1.4696228e-10,3.094292e-09\n",
      "Iteration 68950: loss = 1.4559233e-10,3.0938243e-09\n",
      "Iteration 68955: loss = 1.4558411e-10,3.0933498e-09\n",
      "Iteration 68960: loss = 1.4693612e-10,3.0928737e-09\n",
      "Iteration 68965: loss = 1.4556467e-10,3.0924105e-09\n",
      "Iteration 68970: loss = 1.4555757e-10,3.0919334e-09\n",
      "Iteration 68975: loss = 1.4554913e-10,3.0914604e-09\n",
      "Iteration 68980: loss = 1.4554026e-10,3.090988e-09\n",
      "Iteration 68985: loss = 1.4418301e-10,3.0905118e-09\n",
      "Iteration 68990: loss = 1.455403e-10,3.0899896e-09\n",
      "Iteration 68995: loss = 1.4690778e-10,3.0894682e-09\n",
      "Iteration 69000: loss = 1.4554774e-10,3.0889704e-09\n",
      "Iteration 69005: loss = 1.4554954e-10,3.088467e-09\n",
      "Iteration 69010: loss = 1.4555268e-10,3.087959e-09\n",
      "Iteration 69015: loss = 1.4555478e-10,3.0874543e-09\n",
      "Iteration 69020: loss = 1.4555633e-10,3.0869516e-09\n",
      "Iteration 69025: loss = 1.4555755e-10,3.0864493e-09\n",
      "Iteration 69030: loss = 1.4555714e-10,3.0859522e-09\n",
      "Iteration 69035: loss = 1.4555598e-10,3.0854572e-09\n",
      "Iteration 69040: loss = 1.455537e-10,3.0849656e-09\n",
      "Iteration 69045: loss = 1.4555039e-10,3.0844776e-09\n",
      "Iteration 69050: loss = 1.4554626e-10,3.0839913e-09\n",
      "Iteration 69055: loss = 1.455403e-10,3.0835094e-09\n",
      "Iteration 69060: loss = 1.455331e-10,3.0830325e-09\n",
      "Iteration 69065: loss = 1.4552527e-10,3.082559e-09\n",
      "Iteration 69070: loss = 1.4551597e-10,3.0820884e-09\n",
      "Iteration 69075: loss = 1.4550683e-10,3.081618e-09\n",
      "Iteration 69080: loss = 1.4549661e-10,3.0811493e-09\n",
      "Iteration 69085: loss = 1.4548594e-10,3.0806833e-09\n",
      "Iteration 69090: loss = 1.4547445e-10,3.08022e-09\n",
      "Iteration 69095: loss = 1.4546263e-10,3.0797564e-09\n",
      "Iteration 69100: loss = 1.4545064e-10,3.0792942e-09\n",
      "Iteration 69105: loss = 1.4543804e-10,3.0788336e-09\n",
      "Iteration 69110: loss = 1.4542428e-10,3.0783758e-09\n",
      "Iteration 69115: loss = 1.4541073e-10,3.0779184e-09\n",
      "Iteration 69120: loss = 1.4539676e-10,3.0774616e-09\n",
      "Iteration 69125: loss = 1.4538197e-10,3.0770078e-09\n",
      "Iteration 69130: loss = 1.4536644e-10,3.0765568e-09\n",
      "Iteration 69135: loss = 1.4535333e-10,3.0760972e-09\n",
      "Iteration 69140: loss = 1.4534869e-10,3.0756133e-09\n",
      "Iteration 69145: loss = 1.4534864e-10,3.0751153e-09\n",
      "Iteration 69150: loss = 1.4535008e-10,3.0746135e-09\n",
      "Iteration 69155: loss = 1.4535219e-10,3.0741099e-09\n",
      "Iteration 69160: loss = 1.4535398e-10,3.0736067e-09\n",
      "Iteration 69165: loss = 1.4535545e-10,3.0731038e-09\n",
      "Iteration 69170: loss = 1.453566e-10,3.0726028e-09\n",
      "Iteration 69175: loss = 1.4535607e-10,3.0721055e-09\n",
      "Iteration 69180: loss = 1.4535496e-10,3.071611e-09\n",
      "Iteration 69185: loss = 1.4535266e-10,3.0711196e-09\n",
      "Iteration 69190: loss = 1.4534951e-10,3.0706317e-09\n",
      "Iteration 69195: loss = 1.4534449e-10,3.0701486e-09\n",
      "Iteration 69200: loss = 1.4533767e-10,3.0696714e-09\n",
      "Iteration 69205: loss = 1.453307e-10,3.0691956e-09\n",
      "Iteration 69210: loss = 1.4532295e-10,3.0687206e-09\n",
      "Iteration 69215: loss = 1.4531364e-10,3.0682505e-09\n",
      "Iteration 69220: loss = 1.4530437e-10,3.0677807e-09\n",
      "Iteration 69225: loss = 1.4529443e-10,3.0673128e-09\n",
      "Iteration 69230: loss = 1.4528369e-10,3.066846e-09\n",
      "Iteration 69235: loss = 1.4527236e-10,3.0663827e-09\n",
      "Iteration 69240: loss = 1.4526037e-10,3.0659209e-09\n",
      "Iteration 69245: loss = 1.4524833e-10,3.065459e-09\n",
      "Iteration 69250: loss = 1.4523584e-10,3.0649994e-09\n",
      "Iteration 69255: loss = 1.4522096e-10,3.0645457e-09\n",
      "Iteration 69260: loss = 1.4520589e-10,3.0640934e-09\n",
      "Iteration 69265: loss = 1.451909e-10,3.0636422e-09\n",
      "Iteration 69270: loss = 1.4517561e-10,3.0631915e-09\n",
      "Iteration 69275: loss = 1.4516027e-10,3.0627396e-09\n",
      "Iteration 69280: loss = 1.4514467e-10,3.0622895e-09\n",
      "Iteration 69285: loss = 1.451292e-10,3.0618392e-09\n",
      "Iteration 69290: loss = 1.4511331e-10,3.0613896e-09\n",
      "Iteration 69295: loss = 1.4509646e-10,3.0609428e-09\n",
      "Iteration 69300: loss = 1.4507902e-10,3.0604976e-09\n",
      "Iteration 69305: loss = 1.4506225e-10,3.060051e-09\n",
      "Iteration 69310: loss = 1.4504462e-10,3.0596057e-09\n",
      "Iteration 69315: loss = 1.450276e-10,3.0591598e-09\n",
      "Iteration 69320: loss = 1.4501025e-10,3.0587153e-09\n",
      "Iteration 69325: loss = 1.4499293e-10,3.05827e-09\n",
      "Iteration 69330: loss = 1.4497557e-10,3.0578255e-09\n",
      "Iteration 69335: loss = 1.4495824e-10,3.057381e-09\n",
      "Iteration 69340: loss = 1.4494052e-10,3.0569365e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69345: loss = 1.4492331e-10,3.0564922e-09\n",
      "Iteration 69350: loss = 1.4491387e-10,3.0560223e-09\n",
      "Iteration 69355: loss = 1.4490702e-10,3.0555467e-09\n",
      "Iteration 69360: loss = 1.448953e-10,3.0550846e-09\n",
      "Iteration 69365: loss = 1.4489113e-10,3.0546004e-09\n",
      "Iteration 69370: loss = 1.4488385e-10,3.0541258e-09\n",
      "Iteration 69375: loss = 1.4487016e-10,3.05367e-09\n",
      "Iteration 69380: loss = 1.4486594e-10,3.0531857e-09\n",
      "Iteration 69385: loss = 1.4486524e-10,3.0526908e-09\n",
      "Iteration 69390: loss = 1.4486425e-10,3.0521965e-09\n",
      "Iteration 69395: loss = 1.4486336e-10,3.051703e-09\n",
      "Iteration 69400: loss = 1.44861e-10,3.051212e-09\n",
      "Iteration 69405: loss = 1.4485697e-10,3.0507288e-09\n",
      "Iteration 69410: loss = 1.4485228e-10,3.0502463e-09\n",
      "Iteration 69415: loss = 1.4484712e-10,3.049765e-09\n",
      "Iteration 69420: loss = 1.448407e-10,3.049288e-09\n",
      "Iteration 69425: loss = 1.4348549e-10,3.0488079e-09\n",
      "Iteration 69430: loss = 1.4484815e-10,3.0482727e-09\n",
      "Iteration 69435: loss = 1.4485228e-10,3.0477643e-09\n",
      "Iteration 69440: loss = 1.4485987e-10,3.0472447e-09\n",
      "Iteration 69445: loss = 1.4487075e-10,3.0467155e-09\n",
      "Iteration 69450: loss = 1.4352011e-10,3.046221e-09\n",
      "Iteration 69455: loss = 1.4488148e-10,3.0456901e-09\n",
      "Iteration 69460: loss = 1.4353384e-10,3.0451872e-09\n",
      "Iteration 69465: loss = 1.4354008e-10,3.044673e-09\n",
      "Iteration 69470: loss = 1.4489945e-10,3.044148e-09\n",
      "Iteration 69475: loss = 1.449051e-10,3.0436358e-09\n",
      "Iteration 69480: loss = 1.4491257e-10,3.043117e-09\n",
      "Iteration 69485: loss = 1.435667e-10,3.042608e-09\n",
      "Iteration 69490: loss = 1.4357436e-10,3.0420892e-09\n",
      "Iteration 69495: loss = 1.4493187e-10,3.041571e-09\n",
      "Iteration 69500: loss = 1.4493977e-10,3.041051e-09\n",
      "Iteration 69505: loss = 1.4359211e-10,3.0405485e-09\n",
      "Iteration 69510: loss = 1.449531e-10,3.0400207e-09\n",
      "Iteration 69515: loss = 1.4360765e-10,3.0395113e-09\n",
      "Iteration 69520: loss = 1.4496608e-10,3.0389908e-09\n",
      "Iteration 69525: loss = 1.4497172e-10,3.0384775e-09\n",
      "Iteration 69530: loss = 1.449799e-10,3.0379583e-09\n",
      "Iteration 69535: loss = 1.436334e-10,3.0374512e-09\n",
      "Iteration 69540: loss = 1.4364049e-10,3.036936e-09\n",
      "Iteration 69545: loss = 1.4499986e-10,3.036413e-09\n",
      "Iteration 69550: loss = 1.4365349e-10,3.0359062e-09\n",
      "Iteration 69555: loss = 1.4366153e-10,3.0353868e-09\n",
      "Iteration 69560: loss = 1.4366748e-10,3.0348741e-09\n",
      "Iteration 69565: loss = 1.436707e-10,3.0343696e-09\n",
      "Iteration 69570: loss = 1.4367656e-10,3.0338572e-09\n",
      "Iteration 69575: loss = 1.4368404e-10,3.0333405e-09\n",
      "Iteration 69580: loss = 1.436912e-10,3.0328235e-09\n",
      "Iteration 69585: loss = 1.4369837e-10,3.0323086e-09\n",
      "Iteration 69590: loss = 1.43704e-10,3.031796e-09\n",
      "Iteration 69595: loss = 1.4370878e-10,3.0312874e-09\n",
      "Iteration 69600: loss = 1.4371246e-10,3.0307814e-09\n",
      "Iteration 69605: loss = 1.4371478e-10,3.030281e-09\n",
      "Iteration 69610: loss = 1.4371561e-10,3.0297826e-09\n",
      "Iteration 69615: loss = 1.4371453e-10,3.0292917e-09\n",
      "Iteration 69620: loss = 1.4371172e-10,3.0288057e-09\n",
      "Iteration 69625: loss = 1.4370834e-10,3.0283223e-09\n",
      "Iteration 69630: loss = 1.4370334e-10,3.0278415e-09\n",
      "Iteration 69635: loss = 1.4369787e-10,3.0273644e-09\n",
      "Iteration 69640: loss = 1.4369138e-10,3.026889e-09\n",
      "Iteration 69645: loss = 1.4368408e-10,3.0264165e-09\n",
      "Iteration 69650: loss = 1.4367606e-10,3.0259453e-09\n",
      "Iteration 69655: loss = 1.4366709e-10,3.025479e-09\n",
      "Iteration 69660: loss = 1.4365646e-10,3.025017e-09\n",
      "Iteration 69665: loss = 1.4364486e-10,3.0245566e-09\n",
      "Iteration 69670: loss = 1.436332e-10,3.0240979e-09\n",
      "Iteration 69675: loss = 1.4362127e-10,3.0236402e-09\n",
      "Iteration 69680: loss = 1.4360926e-10,3.0231824e-09\n",
      "Iteration 69685: loss = 1.4359595e-10,3.0227283e-09\n",
      "Iteration 69690: loss = 1.4358248e-10,3.0222749e-09\n",
      "Iteration 69695: loss = 1.4356873e-10,3.0218226e-09\n",
      "Iteration 69700: loss = 1.4355465e-10,3.02137e-09\n",
      "Iteration 69705: loss = 1.4354073e-10,3.0209186e-09\n",
      "Iteration 69710: loss = 1.4352615e-10,3.0204674e-09\n",
      "Iteration 69715: loss = 1.4351102e-10,3.0200182e-09\n",
      "Iteration 69720: loss = 1.4349562e-10,3.0195721e-09\n",
      "Iteration 69725: loss = 1.4347983e-10,3.0191254e-09\n",
      "Iteration 69730: loss = 1.4346402e-10,3.0186784e-09\n",
      "Iteration 69735: loss = 1.4344805e-10,3.0182326e-09\n",
      "Iteration 69740: loss = 1.4343213e-10,3.017787e-09\n",
      "Iteration 69745: loss = 1.4341539e-10,3.0173428e-09\n",
      "Iteration 69750: loss = 1.4339807e-10,3.016902e-09\n",
      "Iteration 69755: loss = 1.4338063e-10,3.0164613e-09\n",
      "Iteration 69760: loss = 1.4336328e-10,3.0160212e-09\n",
      "Iteration 69765: loss = 1.4334561e-10,3.0155813e-09\n",
      "Iteration 69770: loss = 1.4332714e-10,3.0151441e-09\n",
      "Iteration 69775: loss = 1.4330835e-10,3.0147076e-09\n",
      "Iteration 69780: loss = 1.4329025e-10,3.0142688e-09\n",
      "Iteration 69785: loss = 1.4328021e-10,3.0138063e-09\n",
      "Iteration 69790: loss = 1.4327704e-10,3.013323e-09\n",
      "Iteration 69795: loss = 1.4327015e-10,3.01285e-09\n",
      "Iteration 69800: loss = 1.432607e-10,3.0123863e-09\n",
      "Iteration 69805: loss = 1.4325811e-10,3.0119023e-09\n",
      "Iteration 69810: loss = 1.4325686e-10,3.0114125e-09\n",
      "Iteration 69815: loss = 1.4325642e-10,3.0109213e-09\n",
      "Iteration 69820: loss = 1.4325603e-10,3.010431e-09\n",
      "Iteration 69825: loss = 1.4325498e-10,3.0099416e-09\n",
      "Iteration 69830: loss = 1.4325291e-10,3.0094558e-09\n",
      "Iteration 69835: loss = 1.4324979e-10,3.0089722e-09\n",
      "Iteration 69840: loss = 1.432464e-10,3.00849e-09\n",
      "Iteration 69845: loss = 1.4324135e-10,3.0080127e-09\n",
      "Iteration 69850: loss = 1.4323502e-10,3.0075398e-09\n",
      "Iteration 69855: loss = 1.432277e-10,3.00707e-09\n",
      "Iteration 69860: loss = 1.4321903e-10,3.0066036e-09\n",
      "Iteration 69865: loss = 1.4320989e-10,3.0061393e-09\n",
      "Iteration 69870: loss = 1.4320052e-10,3.0056757e-09\n",
      "Iteration 69875: loss = 1.4319049e-10,3.0052152e-09\n",
      "Iteration 69880: loss = 1.4317912e-10,3.0047567e-09\n",
      "Iteration 69885: loss = 1.4316777e-10,3.0042995e-09\n",
      "Iteration 69890: loss = 1.4315578e-10,3.003843e-09\n",
      "Iteration 69895: loss = 1.4314396e-10,3.0033873e-09\n",
      "Iteration 69900: loss = 1.4313098e-10,3.0029348e-09\n",
      "Iteration 69905: loss = 1.4311728e-10,3.0024836e-09\n",
      "Iteration 69910: loss = 1.4310314e-10,3.0020353e-09\n",
      "Iteration 69915: loss = 1.4308811e-10,3.0015896e-09\n",
      "Iteration 69920: loss = 1.430727e-10,3.0011447e-09\n",
      "Iteration 69925: loss = 1.4305741e-10,3.0007006e-09\n",
      "Iteration 69930: loss = 1.4304806e-10,3.0002367e-09\n",
      "Iteration 69935: loss = 1.4304447e-10,2.9997578e-09\n",
      "Iteration 69940: loss = 1.4303583e-10,2.999291e-09\n",
      "Iteration 69945: loss = 1.4302434e-10,2.9988352e-09\n",
      "Iteration 69950: loss = 1.4301695e-10,2.998367e-09\n",
      "Iteration 69955: loss = 1.4300973e-10,2.997898e-09\n",
      "Iteration 69960: loss = 1.4300026e-10,2.9974359e-09\n",
      "Iteration 69965: loss = 1.4299419e-10,2.9969638e-09\n",
      "Iteration 69970: loss = 1.4298467e-10,2.9965013e-09\n",
      "Iteration 69975: loss = 1.4297719e-10,2.9960336e-09\n",
      "Iteration 69980: loss = 1.4296907e-10,2.9955678e-09\n",
      "Iteration 69985: loss = 1.4296152e-10,2.9950997e-09\n",
      "Iteration 69990: loss = 1.4295337e-10,2.9946343e-09\n",
      "Iteration 69995: loss = 1.429584e-10,2.9941292e-09\n",
      "Iteration 70000: loss = 1.4296483e-10,2.9936196e-09\n",
      "Iteration 70005: loss = 1.4162982e-10,2.9931126e-09\n",
      "Iteration 70010: loss = 1.4297866e-10,2.992598e-09\n",
      "Iteration 70015: loss = 1.4298472e-10,2.99209e-09\n",
      "Iteration 70020: loss = 1.4299262e-10,2.9915763e-09\n",
      "Iteration 70025: loss = 1.416564e-10,2.9910738e-09\n",
      "Iteration 70030: loss = 1.4166317e-10,2.9905638e-09\n",
      "Iteration 70035: loss = 1.4301192e-10,2.9900498e-09\n",
      "Iteration 70040: loss = 1.4301828e-10,2.98954e-09\n",
      "Iteration 70045: loss = 1.4302447e-10,2.9890332e-09\n",
      "Iteration 70050: loss = 1.4168806e-10,2.988531e-09\n",
      "Iteration 70055: loss = 1.4303873e-10,2.9880116e-09\n",
      "Iteration 70060: loss = 1.4170316e-10,2.9875076e-09\n",
      "Iteration 70065: loss = 1.417097e-10,2.9869995e-09\n",
      "Iteration 70070: loss = 1.4305768e-10,2.986488e-09\n",
      "Iteration 70075: loss = 1.4172281e-10,2.9859821e-09\n",
      "Iteration 70080: loss = 1.4172853e-10,2.9854763e-09\n",
      "Iteration 70085: loss = 1.4173483e-10,2.984969e-09\n",
      "Iteration 70090: loss = 1.4308416e-10,2.9844545e-09\n",
      "Iteration 70095: loss = 1.4174927e-10,2.983949e-09\n",
      "Iteration 70100: loss = 1.4175362e-10,2.9834475e-09\n",
      "Iteration 70105: loss = 1.4176259e-10,2.9829308e-09\n",
      "Iteration 70110: loss = 1.4176904e-10,2.982425e-09\n",
      "Iteration 70115: loss = 1.4177538e-10,2.981917e-09\n",
      "Iteration 70120: loss = 1.4178142e-10,2.9814111e-09\n",
      "Iteration 70125: loss = 1.4313069e-10,2.9808978e-09\n",
      "Iteration 70130: loss = 1.4179015e-10,2.9804097e-09\n",
      "Iteration 70135: loss = 1.4179814e-10,2.979898e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70140: loss = 1.4315059e-10,2.9793747e-09\n",
      "Iteration 70145: loss = 1.4180943e-10,2.978889e-09\n",
      "Iteration 70150: loss = 1.4181366e-10,2.9783893e-09\n",
      "Iteration 70155: loss = 1.4182139e-10,2.977878e-09\n",
      "Iteration 70160: loss = 1.4183099e-10,2.9773615e-09\n",
      "Iteration 70165: loss = 1.418408e-10,2.976845e-09\n",
      "Iteration 70170: loss = 1.4184773e-10,2.9763367e-09\n",
      "Iteration 70175: loss = 1.4184844e-10,2.975848e-09\n",
      "Iteration 70180: loss = 1.418499e-10,2.9753562e-09\n",
      "Iteration 70185: loss = 1.4185171e-10,2.9748635e-09\n",
      "Iteration 70190: loss = 1.4185352e-10,2.9743719e-09\n",
      "Iteration 70195: loss = 1.4185476e-10,2.9738818e-09\n",
      "Iteration 70200: loss = 1.4185544e-10,2.973392e-09\n",
      "Iteration 70205: loss = 1.418541e-10,2.9729101e-09\n",
      "Iteration 70210: loss = 1.4185138e-10,2.972432e-09\n",
      "Iteration 70215: loss = 1.4184778e-10,2.9719562e-09\n",
      "Iteration 70220: loss = 1.4184277e-10,2.9714844e-09\n",
      "Iteration 70225: loss = 1.4183747e-10,2.9710137e-09\n",
      "Iteration 70230: loss = 1.4183078e-10,2.9705478e-09\n",
      "Iteration 70235: loss = 1.4182354e-10,2.9700837e-09\n",
      "Iteration 70240: loss = 1.4181613e-10,2.9696197e-09\n",
      "Iteration 70245: loss = 1.4180711e-10,2.96916e-09\n",
      "Iteration 70250: loss = 1.4179631e-10,2.9687075e-09\n",
      "Iteration 70255: loss = 1.4178526e-10,2.968254e-09\n",
      "Iteration 70260: loss = 1.4177405e-10,2.9678022e-09\n",
      "Iteration 70265: loss = 1.4176178e-10,2.9673535e-09\n",
      "Iteration 70270: loss = 1.4174918e-10,2.9669072e-09\n",
      "Iteration 70275: loss = 1.4173615e-10,2.9664609e-09\n",
      "Iteration 70280: loss = 1.4172434e-10,2.9660105e-09\n",
      "Iteration 70285: loss = 1.417199e-10,2.965538e-09\n",
      "Iteration 70290: loss = 1.4172125e-10,2.9650493e-09\n",
      "Iteration 70295: loss = 1.4172523e-10,2.9645524e-09\n",
      "Iteration 70300: loss = 1.4173006e-10,2.9640528e-09\n",
      "Iteration 70305: loss = 1.4173503e-10,2.9635525e-09\n",
      "Iteration 70310: loss = 1.4173908e-10,2.9630567e-09\n",
      "Iteration 70315: loss = 1.4174187e-10,2.9625635e-09\n",
      "Iteration 70320: loss = 1.4174271e-10,2.9620757e-09\n",
      "Iteration 70325: loss = 1.4174191e-10,2.961594e-09\n",
      "Iteration 70330: loss = 1.417411e-10,2.9611127e-09\n",
      "Iteration 70335: loss = 1.4173816e-10,2.960638e-09\n",
      "Iteration 70340: loss = 1.4173461e-10,2.9601641e-09\n",
      "Iteration 70345: loss = 1.4173018e-10,2.9596932e-09\n",
      "Iteration 70350: loss = 1.4172437e-10,2.9592264e-09\n",
      "Iteration 70355: loss = 1.4171807e-10,2.9587608e-09\n",
      "Iteration 70360: loss = 1.4170955e-10,2.9583034e-09\n",
      "Iteration 70365: loss = 1.4170022e-10,2.9578475e-09\n",
      "Iteration 70370: loss = 1.4169044e-10,2.9573926e-09\n",
      "Iteration 70375: loss = 1.4168026e-10,2.9569402e-09\n",
      "Iteration 70380: loss = 1.416691e-10,2.9564908e-09\n",
      "Iteration 70385: loss = 1.4165734e-10,2.956042e-09\n",
      "Iteration 70390: loss = 1.416456e-10,2.955594e-09\n",
      "Iteration 70395: loss = 1.4163376e-10,2.9551472e-09\n",
      "Iteration 70400: loss = 1.416208e-10,2.9547014e-09\n",
      "Iteration 70405: loss = 1.4160735e-10,2.9542588e-09\n",
      "Iteration 70410: loss = 1.4159358e-10,2.9538163e-09\n",
      "Iteration 70415: loss = 1.415799e-10,2.9533755e-09\n",
      "Iteration 70420: loss = 1.4156522e-10,2.952936e-09\n",
      "Iteration 70425: loss = 1.415499e-10,2.9524996e-09\n",
      "Iteration 70430: loss = 1.4153405e-10,2.9520648e-09\n",
      "Iteration 70435: loss = 1.4151814e-10,2.95163e-09\n",
      "Iteration 70440: loss = 1.4150092e-10,2.9511993e-09\n",
      "Iteration 70445: loss = 1.4148435e-10,2.950768e-09\n",
      "Iteration 70450: loss = 1.4146706e-10,2.9503373e-09\n",
      "Iteration 70455: loss = 1.414499e-10,2.9499074e-09\n",
      "Iteration 70460: loss = 1.4143263e-10,2.9494762e-09\n",
      "Iteration 70465: loss = 1.4141545e-10,2.949047e-09\n",
      "Iteration 70470: loss = 1.4139805e-10,2.9486167e-09\n",
      "Iteration 70475: loss = 1.4138062e-10,2.9481872e-09\n",
      "Iteration 70480: loss = 1.4136307e-10,2.9477578e-09\n",
      "Iteration 70485: loss = 1.4134606e-10,2.947329e-09\n",
      "Iteration 70490: loss = 1.4132838e-10,2.9468994e-09\n",
      "Iteration 70495: loss = 1.4131078e-10,2.9464713e-09\n",
      "Iteration 70500: loss = 1.4129355e-10,2.946043e-09\n",
      "Iteration 70505: loss = 1.41276e-10,2.945614e-09\n",
      "Iteration 70510: loss = 1.4125844e-10,2.9451859e-09\n",
      "Iteration 70515: loss = 1.4124048e-10,2.9447587e-09\n",
      "Iteration 70520: loss = 1.412231e-10,2.9443308e-09\n",
      "Iteration 70525: loss = 1.4120517e-10,2.9439036e-09\n",
      "Iteration 70530: loss = 1.4119327e-10,2.9434588e-09\n",
      "Iteration 70535: loss = 1.4118778e-10,2.9429958e-09\n",
      "Iteration 70540: loss = 1.411866e-10,2.9425187e-09\n",
      "Iteration 70545: loss = 1.4118748e-10,2.942036e-09\n",
      "Iteration 70550: loss = 1.4118835e-10,2.9415537e-09\n",
      "Iteration 70555: loss = 1.4118919e-10,2.9410712e-09\n",
      "Iteration 70560: loss = 1.4118927e-10,2.9405898e-09\n",
      "Iteration 70565: loss = 1.4118827e-10,2.9401142e-09\n",
      "Iteration 70570: loss = 1.4118619e-10,2.9396414e-09\n",
      "Iteration 70575: loss = 1.411821e-10,2.9391731e-09\n",
      "Iteration 70580: loss = 1.4117695e-10,2.9387093e-09\n",
      "Iteration 70585: loss = 1.4117103e-10,2.938248e-09\n",
      "Iteration 70590: loss = 1.411647e-10,2.937789e-09\n",
      "Iteration 70595: loss = 1.4115704e-10,2.9373326e-09\n",
      "Iteration 70600: loss = 1.4114855e-10,2.9368787e-09\n",
      "Iteration 70605: loss = 1.4114013e-10,2.936426e-09\n",
      "Iteration 70610: loss = 1.4113025e-10,2.9359761e-09\n",
      "Iteration 70615: loss = 1.3979883e-10,2.9354987e-09\n",
      "Iteration 70620: loss = 1.4115087e-10,2.9349576e-09\n",
      "Iteration 70625: loss = 1.4115781e-10,2.9344587e-09\n",
      "Iteration 70630: loss = 1.4115493e-10,2.9339893e-09\n",
      "Iteration 70635: loss = 1.398246e-10,2.933509e-09\n",
      "Iteration 70640: loss = 1.4117396e-10,2.9329765e-09\n",
      "Iteration 70645: loss = 1.4117711e-10,2.932489e-09\n",
      "Iteration 70650: loss = 1.3984333e-10,2.9320195e-09\n",
      "Iteration 70655: loss = 1.4118662e-10,2.9315053e-09\n",
      "Iteration 70660: loss = 1.3985817e-10,2.9310205e-09\n",
      "Iteration 70665: loss = 1.3986516e-10,2.9305216e-09\n",
      "Iteration 70670: loss = 1.4120387e-10,2.930022e-09\n",
      "Iteration 70675: loss = 1.412102e-10,2.9295273e-09\n",
      "Iteration 70680: loss = 1.4121694e-10,2.9290304e-09\n",
      "Iteration 70685: loss = 1.3989156e-10,2.928536e-09\n",
      "Iteration 70690: loss = 1.3989741e-10,2.9280436e-09\n",
      "Iteration 70695: loss = 1.4123618e-10,2.9275433e-09\n",
      "Iteration 70700: loss = 1.4124245e-10,2.9270482e-09\n",
      "Iteration 70705: loss = 1.3991716e-10,2.9265552e-09\n",
      "Iteration 70710: loss = 1.3992367e-10,2.9260596e-09\n",
      "Iteration 70715: loss = 1.3992983e-10,2.9255651e-09\n",
      "Iteration 70720: loss = 1.3993538e-10,2.925074e-09\n",
      "Iteration 70725: loss = 1.399403e-10,2.9245824e-09\n",
      "Iteration 70730: loss = 1.3994828e-10,2.9240839e-09\n",
      "Iteration 70735: loss = 1.3995419e-10,2.923591e-09\n",
      "Iteration 70740: loss = 1.412944e-10,2.9230878e-09\n",
      "Iteration 70745: loss = 1.3996368e-10,2.9226124e-09\n",
      "Iteration 70750: loss = 1.4130651e-10,2.9221026e-09\n",
      "Iteration 70755: loss = 1.3998082e-10,2.921612e-09\n",
      "Iteration 70760: loss = 1.3998543e-10,2.9211233e-09\n",
      "Iteration 70765: loss = 1.3998992e-10,2.9206348e-09\n",
      "Iteration 70770: loss = 1.3999908e-10,2.920134e-09\n",
      "Iteration 70775: loss = 1.400053e-10,2.9196414e-09\n",
      "Iteration 70780: loss = 1.4001274e-10,2.919145e-09\n",
      "Iteration 70785: loss = 1.4134428e-10,2.9186693e-09\n",
      "Iteration 70790: loss = 1.4000341e-10,2.9182254e-09\n",
      "Iteration 70795: loss = 1.3999583e-10,2.9177734e-09\n",
      "Iteration 70800: loss = 1.3998876e-10,2.917322e-09\n",
      "Iteration 70805: loss = 1.4131367e-10,2.916867e-09\n",
      "Iteration 70810: loss = 1.3997392e-10,2.9164196e-09\n",
      "Iteration 70815: loss = 1.4129911e-10,2.915964e-09\n",
      "Iteration 70820: loss = 1.3995351e-10,2.9155336e-09\n",
      "Iteration 70825: loss = 1.3995227e-10,2.9150657e-09\n",
      "Iteration 70830: loss = 1.3994246e-10,2.9146214e-09\n",
      "Iteration 70835: loss = 1.3993505e-10,2.9141718e-09\n",
      "Iteration 70840: loss = 1.412654e-10,2.9137e-09\n",
      "Iteration 70845: loss = 1.3991933e-10,2.9132732e-09\n",
      "Iteration 70850: loss = 1.3991434e-10,2.9128167e-09\n",
      "Iteration 70855: loss = 1.4124167e-10,2.9123557e-09\n",
      "Iteration 70860: loss = 1.3989497e-10,2.9119307e-09\n",
      "Iteration 70865: loss = 1.3988988e-10,2.9114724e-09\n",
      "Iteration 70870: loss = 1.4121986e-10,2.9110052e-09\n",
      "Iteration 70875: loss = 1.3987789e-10,2.910566e-09\n",
      "Iteration 70880: loss = 1.4120599e-10,2.9101042e-09\n",
      "Iteration 70885: loss = 1.3986128e-10,2.9096727e-09\n",
      "Iteration 70890: loss = 1.3985467e-10,2.9092213e-09\n",
      "Iteration 70895: loss = 1.4118408e-10,2.9087557e-09\n",
      "Iteration 70900: loss = 1.3983953e-10,2.9083258e-09\n",
      "Iteration 70905: loss = 1.3983133e-10,2.9078793e-09\n",
      "Iteration 70910: loss = 1.3982826e-10,2.9074174e-09\n",
      "Iteration 70915: loss = 1.3982161e-10,2.906967e-09\n",
      "Iteration 70920: loss = 1.3980793e-10,2.9065372e-09\n",
      "Iteration 70925: loss = 1.3979996e-10,2.9060916e-09\n",
      "Iteration 70930: loss = 1.3979588e-10,2.9056337e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70935: loss = 1.3979261e-10,2.9051732e-09\n",
      "Iteration 70940: loss = 1.3978671e-10,2.904721e-09\n",
      "Iteration 70945: loss = 1.3977774e-10,2.9042786e-09\n",
      "Iteration 70950: loss = 1.3977051e-10,2.9038318e-09\n",
      "Iteration 70955: loss = 1.3976369e-10,2.903383e-09\n",
      "Iteration 70960: loss = 1.3975777e-10,2.9029317e-09\n",
      "Iteration 70965: loss = 1.397512e-10,2.9024818e-09\n",
      "Iteration 70970: loss = 1.397437e-10,2.9020355e-09\n",
      "Iteration 70975: loss = 1.3973607e-10,2.9015894e-09\n",
      "Iteration 70980: loss = 1.3972826e-10,2.9011449e-09\n",
      "Iteration 70985: loss = 1.3972012e-10,2.9007012e-09\n",
      "Iteration 70990: loss = 1.397109e-10,2.9002616e-09\n",
      "Iteration 70995: loss = 1.3970076e-10,2.8998224e-09\n",
      "Iteration 71000: loss = 1.396906e-10,2.8993847e-09\n",
      "Iteration 71005: loss = 1.3967973e-10,2.8989489e-09\n",
      "Iteration 71010: loss = 1.3966882e-10,2.8985143e-09\n",
      "Iteration 71015: loss = 1.3965552e-10,2.8980855e-09\n",
      "Iteration 71020: loss = 1.3964525e-10,2.8976495e-09\n",
      "Iteration 71025: loss = 1.3964259e-10,2.8971892e-09\n",
      "Iteration 71030: loss = 1.3964452e-10,2.8967166e-09\n",
      "Iteration 71035: loss = 1.396488e-10,2.8962377e-09\n",
      "Iteration 71040: loss = 1.3964446e-10,2.8957836e-09\n",
      "Iteration 71045: loss = 1.3963708e-10,2.8953382e-09\n",
      "Iteration 71050: loss = 1.3963038e-10,2.8948919e-09\n",
      "Iteration 71055: loss = 1.3962398e-10,2.8944451e-09\n",
      "Iteration 71060: loss = 1.3961576e-10,2.8940033e-09\n",
      "Iteration 71065: loss = 1.3960892e-10,2.8935585e-09\n",
      "Iteration 71070: loss = 1.396028e-10,2.8931106e-09\n",
      "Iteration 71075: loss = 1.3959321e-10,2.8926725e-09\n",
      "Iteration 71080: loss = 1.3958727e-10,2.892226e-09\n",
      "Iteration 71085: loss = 1.395826e-10,2.8917746e-09\n",
      "Iteration 71090: loss = 1.3957172e-10,2.8913412e-09\n",
      "Iteration 71095: loss = 1.3956346e-10,2.8909022e-09\n",
      "Iteration 71100: loss = 1.3955624e-10,2.8904583e-09\n",
      "Iteration 71105: loss = 1.3954979e-10,2.8900125e-09\n",
      "Iteration 71110: loss = 1.3954399e-10,2.8895641e-09\n",
      "Iteration 71115: loss = 1.3953722e-10,2.8891205e-09\n",
      "Iteration 71120: loss = 1.3953015e-10,2.8886777e-09\n",
      "Iteration 71125: loss = 1.3952241e-10,2.8882363e-09\n",
      "Iteration 71130: loss = 1.3951444e-10,2.8877953e-09\n",
      "Iteration 71135: loss = 1.3950599e-10,2.8873561e-09\n",
      "Iteration 71140: loss = 1.3949518e-10,2.886926e-09\n",
      "Iteration 71145: loss = 1.3948369e-10,2.8864968e-09\n",
      "Iteration 71150: loss = 1.3947213e-10,2.8860683e-09\n",
      "Iteration 71155: loss = 1.3946018e-10,2.885641e-09\n",
      "Iteration 71160: loss = 1.3944798e-10,2.8852152e-09\n",
      "Iteration 71165: loss = 1.3943457e-10,2.8847922e-09\n",
      "Iteration 71170: loss = 1.3942086e-10,2.884371e-09\n",
      "Iteration 71175: loss = 1.3940653e-10,2.8839502e-09\n",
      "Iteration 71180: loss = 1.3939253e-10,2.8835307e-09\n",
      "Iteration 71185: loss = 1.3937805e-10,2.8831109e-09\n",
      "Iteration 71190: loss = 1.3936365e-10,2.8826925e-09\n",
      "Iteration 71195: loss = 1.3934913e-10,2.8822749e-09\n",
      "Iteration 71200: loss = 1.3933403e-10,2.8818576e-09\n",
      "Iteration 71205: loss = 1.3931895e-10,2.8814418e-09\n",
      "Iteration 71210: loss = 1.3930258e-10,2.8810283e-09\n",
      "Iteration 71215: loss = 1.3928614e-10,2.8806162e-09\n",
      "Iteration 71220: loss = 1.3926921e-10,2.8802056e-09\n",
      "Iteration 71225: loss = 1.3794056e-10,2.8797669e-09\n",
      "Iteration 71230: loss = 1.3927755e-10,2.8792602e-09\n",
      "Iteration 71235: loss = 1.3927944e-10,2.8787939e-09\n",
      "Iteration 71240: loss = 1.3928667e-10,2.8783116e-09\n",
      "Iteration 71245: loss = 1.3796748e-10,2.8778449e-09\n",
      "Iteration 71250: loss = 1.3929928e-10,2.8773541e-09\n",
      "Iteration 71255: loss = 1.3797828e-10,2.8768923e-09\n",
      "Iteration 71260: loss = 1.3931317e-10,2.8763936e-09\n",
      "Iteration 71265: loss = 1.3799037e-10,2.8759375e-09\n",
      "Iteration 71270: loss = 1.3932301e-10,2.8754454e-09\n",
      "Iteration 71275: loss = 1.3799821e-10,2.8749958e-09\n",
      "Iteration 71280: loss = 1.3931643e-10,2.8745473e-09\n",
      "Iteration 71285: loss = 1.3798522e-10,2.8741178e-09\n",
      "Iteration 71290: loss = 1.3930397e-10,2.8736666e-09\n",
      "Iteration 71295: loss = 1.3797165e-10,2.8732408e-09\n",
      "Iteration 71300: loss = 1.3928883e-10,2.8727964e-09\n",
      "Iteration 71305: loss = 1.3928107e-10,2.8723617e-09\n",
      "Iteration 71310: loss = 1.3927497e-10,2.8719216e-09\n",
      "Iteration 71315: loss = 1.3926815e-10,2.8714844e-09\n",
      "Iteration 71320: loss = 1.392628e-10,2.8710432e-09\n",
      "Iteration 71325: loss = 1.3793235e-10,2.870613e-09\n",
      "Iteration 71330: loss = 1.3924802e-10,2.870173e-09\n",
      "Iteration 71335: loss = 1.3924233e-10,2.8697333e-09\n",
      "Iteration 71340: loss = 1.379106e-10,2.869308e-09\n",
      "Iteration 71345: loss = 1.3923003e-10,2.8688567e-09\n",
      "Iteration 71350: loss = 1.3789749e-10,2.868434e-09\n",
      "Iteration 71355: loss = 1.3921436e-10,2.8679898e-09\n",
      "Iteration 71360: loss = 1.3920863e-10,2.8675515e-09\n",
      "Iteration 71365: loss = 1.378785e-10,2.867122e-09\n",
      "Iteration 71370: loss = 1.3919808e-10,2.866672e-09\n",
      "Iteration 71375: loss = 1.378651e-10,2.8662506e-09\n",
      "Iteration 71380: loss = 1.3918203e-10,2.8658091e-09\n",
      "Iteration 71385: loss = 1.3917704e-10,2.86537e-09\n",
      "Iteration 71390: loss = 1.3916797e-10,2.8649423e-09\n",
      "Iteration 71395: loss = 1.391646e-10,2.8644978e-09\n",
      "Iteration 71400: loss = 1.3915741e-10,2.8640652e-09\n",
      "Iteration 71405: loss = 1.3914958e-10,2.8636347e-09\n",
      "Iteration 71410: loss = 1.3914368e-10,2.8631968e-09\n",
      "Iteration 71415: loss = 1.3913612e-10,2.862766e-09\n",
      "Iteration 71420: loss = 1.3912939e-10,2.8623321e-09\n",
      "Iteration 71425: loss = 1.391217e-10,2.8619018e-09\n",
      "Iteration 71430: loss = 1.3911695e-10,2.8614622e-09\n",
      "Iteration 71435: loss = 1.3778613e-10,2.8610363e-09\n",
      "Iteration 71440: loss = 1.3910355e-10,2.8605964e-09\n",
      "Iteration 71445: loss = 1.3777299e-10,2.8601703e-09\n",
      "Iteration 71450: loss = 1.3909308e-10,2.8597218e-09\n",
      "Iteration 71455: loss = 1.3775864e-10,2.8593083e-09\n",
      "Iteration 71460: loss = 1.3907904e-10,2.8588594e-09\n",
      "Iteration 71465: loss = 1.3774532e-10,2.8584428e-09\n",
      "Iteration 71470: loss = 1.3906558e-10,2.8579965e-09\n",
      "Iteration 71475: loss = 1.3905734e-10,2.857568e-09\n",
      "Iteration 71480: loss = 1.3772626e-10,2.857145e-09\n",
      "Iteration 71485: loss = 1.3772135e-10,2.856709e-09\n",
      "Iteration 71490: loss = 1.3771374e-10,2.8562797e-09\n",
      "Iteration 71495: loss = 1.3903133e-10,2.8558422e-09\n",
      "Iteration 71500: loss = 1.3770189e-10,2.8554141e-09\n",
      "Iteration 71505: loss = 1.3901769e-10,2.8549811e-09\n",
      "Iteration 71510: loss = 1.3901248e-10,2.854546e-09\n",
      "Iteration 71515: loss = 1.376823e-10,2.854121e-09\n",
      "Iteration 71520: loss = 1.3767491e-10,2.853693e-09\n",
      "Iteration 71525: loss = 1.3899216e-10,2.8532572e-09\n",
      "Iteration 71530: loss = 1.3766299e-10,2.8528306e-09\n",
      "Iteration 71535: loss = 1.3898026e-10,2.8523939e-09\n",
      "Iteration 71540: loss = 1.3897312e-10,2.8519667e-09\n",
      "Iteration 71545: loss = 1.389675e-10,2.8515328e-09\n",
      "Iteration 71550: loss = 1.3763686e-10,2.8511116e-09\n",
      "Iteration 71555: loss = 1.3763075e-10,2.8506815e-09\n",
      "Iteration 71560: loss = 1.3762531e-10,2.8502491e-09\n",
      "Iteration 71565: loss = 1.3761829e-10,2.8498228e-09\n",
      "Iteration 71570: loss = 1.389344e-10,2.8493896e-09\n",
      "Iteration 71575: loss = 1.3760458e-10,2.8489668e-09\n",
      "Iteration 71580: loss = 1.3892165e-10,2.8485336e-09\n",
      "Iteration 71585: loss = 1.3759169e-10,2.8481102e-09\n",
      "Iteration 71590: loss = 1.3890995e-10,2.8476743e-09\n",
      "Iteration 71595: loss = 1.3757749e-10,2.8472582e-09\n",
      "Iteration 71600: loss = 1.3889806e-10,2.8468148e-09\n",
      "Iteration 71605: loss = 1.3756572e-10,2.8464002e-09\n",
      "Iteration 71610: loss = 1.388841e-10,2.845964e-09\n",
      "Iteration 71615: loss = 1.3755216e-10,2.845548e-09\n",
      "Iteration 71620: loss = 1.388712e-10,2.8451097e-09\n",
      "Iteration 71625: loss = 1.3753892e-10,2.8446956e-09\n",
      "Iteration 71630: loss = 1.3886071e-10,2.8442497e-09\n",
      "Iteration 71635: loss = 1.3752668e-10,2.8438405e-09\n",
      "Iteration 71640: loss = 1.3884587e-10,2.8434033e-09\n",
      "Iteration 71645: loss = 1.375161e-10,2.8429825e-09\n",
      "Iteration 71650: loss = 1.3750989e-10,2.8425569e-09\n",
      "Iteration 71655: loss = 1.3882706e-10,2.8421252e-09\n",
      "Iteration 71660: loss = 1.3749756e-10,2.8417049e-09\n",
      "Iteration 71665: loss = 1.3881574e-10,2.84127e-09\n",
      "Iteration 71670: loss = 1.3748351e-10,2.8408584e-09\n",
      "Iteration 71675: loss = 1.374804e-10,2.840423e-09\n",
      "Iteration 71680: loss = 1.3747174e-10,2.840006e-09\n",
      "Iteration 71685: loss = 1.374668e-10,2.8395772e-09\n",
      "Iteration 71690: loss = 1.3745886e-10,2.8391574e-09\n",
      "Iteration 71695: loss = 1.3745106e-10,2.8387386e-09\n",
      "Iteration 71700: loss = 1.3744927e-10,2.8383007e-09\n",
      "Iteration 71705: loss = 1.3743928e-10,2.8378886e-09\n",
      "Iteration 71710: loss = 1.374315e-10,2.8374691e-09\n",
      "Iteration 71715: loss = 1.3875501e-10,2.8370222e-09\n",
      "Iteration 71720: loss = 1.374227e-10,2.8366138e-09\n",
      "Iteration 71725: loss = 1.387409e-10,2.8361824e-09\n",
      "Iteration 71730: loss = 1.3740857e-10,2.8357734e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71735: loss = 1.3872896e-10,2.8353364e-09\n",
      "Iteration 71740: loss = 1.3872227e-10,2.8349163e-09\n",
      "Iteration 71745: loss = 1.3739267e-10,2.8344997e-09\n",
      "Iteration 71750: loss = 1.3738621e-10,2.8340776e-09\n",
      "Iteration 71755: loss = 1.373787e-10,2.83366e-09\n",
      "Iteration 71760: loss = 1.3869904e-10,2.8332248e-09\n",
      "Iteration 71765: loss = 1.3736653e-10,2.8328164e-09\n",
      "Iteration 71770: loss = 1.3736257e-10,2.8323894e-09\n",
      "Iteration 71775: loss = 1.373569e-10,2.8319667e-09\n",
      "Iteration 71780: loss = 1.3735049e-10,2.8315466e-09\n",
      "Iteration 71785: loss = 1.3866762e-10,2.831122e-09\n",
      "Iteration 71790: loss = 1.3733494e-10,2.8307154e-09\n",
      "Iteration 71795: loss = 1.3865634e-10,2.8302782e-09\n",
      "Iteration 71800: loss = 1.3732475e-10,2.8298692e-09\n",
      "Iteration 71805: loss = 1.3864454e-10,2.8294362e-09\n",
      "Iteration 71810: loss = 1.3732122e-10,2.829004e-09\n",
      "Iteration 71815: loss = 1.3732948e-10,2.8285412e-09\n",
      "Iteration 71820: loss = 1.373323e-10,2.8280944e-09\n",
      "Iteration 71825: loss = 1.3733333e-10,2.8276539e-09\n",
      "Iteration 71830: loss = 1.3733321e-10,2.827217e-09\n",
      "Iteration 71835: loss = 1.3732486e-10,2.8268055e-09\n",
      "Iteration 71840: loss = 1.3731528e-10,2.8263965e-09\n",
      "Iteration 71845: loss = 1.3730984e-10,2.8259768e-09\n",
      "Iteration 71850: loss = 1.373067e-10,2.8255496e-09\n",
      "Iteration 71855: loss = 1.3730013e-10,2.8251343e-09\n",
      "Iteration 71860: loss = 1.372941e-10,2.824716e-09\n",
      "Iteration 71865: loss = 1.3728889e-10,2.8242961e-09\n",
      "Iteration 71870: loss = 1.3728287e-10,2.823879e-09\n",
      "Iteration 71875: loss = 1.3727636e-10,2.8234648e-09\n",
      "Iteration 71880: loss = 1.372692e-10,2.823052e-09\n",
      "Iteration 71885: loss = 1.3726194e-10,2.8226395e-09\n",
      "Iteration 71890: loss = 1.3725471e-10,2.8222273e-09\n",
      "Iteration 71895: loss = 1.3724716e-10,2.821816e-09\n",
      "Iteration 71900: loss = 1.3723918e-10,2.8214053e-09\n",
      "Iteration 71905: loss = 1.3723085e-10,2.8209965e-09\n",
      "Iteration 71910: loss = 1.3722236e-10,2.8205887e-09\n",
      "Iteration 71915: loss = 1.3721325e-10,2.8201834e-09\n",
      "Iteration 71920: loss = 1.3720246e-10,2.8197822e-09\n",
      "Iteration 71925: loss = 1.3719034e-10,2.8193863e-09\n",
      "Iteration 71930: loss = 1.3717787e-10,2.8189904e-09\n",
      "Iteration 71935: loss = 1.3716868e-10,2.8185863e-09\n",
      "Iteration 71940: loss = 1.3716368e-10,2.8181686e-09\n",
      "Iteration 71945: loss = 1.3716024e-10,2.8177476e-09\n",
      "Iteration 71950: loss = 1.3715173e-10,2.817341e-09\n",
      "Iteration 71955: loss = 1.3714607e-10,2.816926e-09\n",
      "Iteration 71960: loss = 1.3714017e-10,2.816512e-09\n",
      "Iteration 71965: loss = 1.3582091e-10,2.8161016e-09\n",
      "Iteration 71970: loss = 1.3712848e-10,2.8156855e-09\n",
      "Iteration 71975: loss = 1.371221e-10,2.8152745e-09\n",
      "Iteration 71980: loss = 1.3711733e-10,2.81486e-09\n",
      "Iteration 71985: loss = 1.3711131e-10,2.8144478e-09\n",
      "Iteration 71990: loss = 1.3710554e-10,2.814036e-09\n",
      "Iteration 71995: loss = 1.3709982e-10,2.813624e-09\n",
      "Iteration 72000: loss = 1.3709352e-10,2.8132132e-09\n",
      "Iteration 72005: loss = 1.3708937e-10,2.812796e-09\n",
      "Iteration 72010: loss = 1.370823e-10,2.812388e-09\n",
      "Iteration 72015: loss = 1.3707659e-10,2.8119764e-09\n",
      "Iteration 72020: loss = 1.370728e-10,2.8115597e-09\n",
      "Iteration 72025: loss = 1.3575144e-10,2.8111593e-09\n",
      "Iteration 72030: loss = 1.3574625e-10,2.8107463e-09\n",
      "Iteration 72035: loss = 1.3705358e-10,2.8103337e-09\n",
      "Iteration 72040: loss = 1.357354e-10,2.809923e-09\n",
      "Iteration 72045: loss = 1.3704203e-10,2.8095115e-09\n",
      "Iteration 72050: loss = 1.3703665e-10,2.8091014e-09\n",
      "Iteration 72055: loss = 1.3703098e-10,2.8086935e-09\n",
      "Iteration 72060: loss = 1.3702602e-10,2.8082818e-09\n",
      "Iteration 72065: loss = 1.3702023e-10,2.8078735e-09\n",
      "Iteration 72070: loss = 1.3701575e-10,2.8074605e-09\n",
      "Iteration 72075: loss = 1.3700896e-10,2.807055e-09\n",
      "Iteration 72080: loss = 1.3700284e-10,2.8066474e-09\n",
      "Iteration 72085: loss = 1.3699752e-10,2.8062384e-09\n",
      "Iteration 72090: loss = 1.3699165e-10,2.8058305e-09\n",
      "Iteration 72095: loss = 1.3567343e-10,2.8054234e-09\n",
      "Iteration 72100: loss = 1.3698108e-10,2.8050122e-09\n",
      "Iteration 72105: loss = 1.3697884e-10,2.8045948e-09\n",
      "Iteration 72110: loss = 1.3697568e-10,2.80418e-09\n",
      "Iteration 72115: loss = 1.369642e-10,2.803792e-09\n",
      "Iteration 72120: loss = 1.369616e-10,2.8033764e-09\n",
      "Iteration 72125: loss = 1.3695743e-10,2.8029659e-09\n",
      "Iteration 72130: loss = 1.3563335e-10,2.802579e-09\n",
      "Iteration 72135: loss = 1.3694723e-10,2.8021512e-09\n",
      "Iteration 72140: loss = 1.3693732e-10,2.801758e-09\n",
      "Iteration 72145: loss = 1.3693378e-10,2.8013454e-09\n",
      "Iteration 72150: loss = 1.3692764e-10,2.8009424e-09\n",
      "Iteration 72155: loss = 1.3692063e-10,2.8005414e-09\n",
      "Iteration 72160: loss = 1.3691542e-10,2.8001343e-09\n",
      "Iteration 72165: loss = 1.3691015e-10,2.7997293e-09\n",
      "Iteration 72170: loss = 1.3690492e-10,2.7993239e-09\n",
      "Iteration 72175: loss = 1.36899e-10,2.7989213e-09\n",
      "Iteration 72180: loss = 1.3557974e-10,2.7985216e-09\n",
      "Iteration 72185: loss = 1.3557365e-10,2.7981197e-09\n",
      "Iteration 72190: loss = 1.3688518e-10,2.797701e-09\n",
      "Iteration 72195: loss = 1.3556234e-10,2.7973135e-09\n",
      "Iteration 72200: loss = 1.3687118e-10,2.7969043e-09\n",
      "Iteration 72205: loss = 1.368679e-10,2.7964944e-09\n",
      "Iteration 72210: loss = 1.3554607e-10,2.7961042e-09\n",
      "Iteration 72215: loss = 1.3554237e-10,2.7956957e-09\n",
      "Iteration 72220: loss = 1.368498e-10,2.7952924e-09\n",
      "Iteration 72225: loss = 1.368448e-10,2.7948897e-09\n",
      "Iteration 72230: loss = 1.3683966e-10,2.7944864e-09\n",
      "Iteration 72235: loss = 1.3551897e-10,2.7940936e-09\n",
      "Iteration 72240: loss = 1.3551449e-10,2.7936897e-09\n",
      "Iteration 72245: loss = 1.3682344e-10,2.793282e-09\n",
      "Iteration 72250: loss = 1.3681818e-10,2.7928801e-09\n",
      "Iteration 72255: loss = 1.3681224e-10,2.7924811e-09\n",
      "Iteration 72260: loss = 1.3680865e-10,2.7920748e-09\n",
      "Iteration 72265: loss = 1.3680253e-10,2.7916767e-09\n",
      "Iteration 72270: loss = 1.3548206e-10,2.7912868e-09\n",
      "Iteration 72275: loss = 1.3547856e-10,2.7908813e-09\n",
      "Iteration 72280: loss = 1.3547317e-10,2.7904818e-09\n",
      "Iteration 72285: loss = 1.3546782e-10,2.790083e-09\n",
      "Iteration 72290: loss = 1.3546114e-10,2.7896871e-09\n",
      "Iteration 72295: loss = 1.3677122e-10,2.7892784e-09\n",
      "Iteration 72300: loss = 1.3676588e-10,2.7888793e-09\n",
      "Iteration 72305: loss = 1.3544384e-10,2.7884943e-09\n",
      "Iteration 72310: loss = 1.3544167e-10,2.7880884e-09\n",
      "Iteration 72315: loss = 1.3543637e-10,2.7876905e-09\n",
      "Iteration 72320: loss = 1.3674399e-10,2.7872908e-09\n",
      "Iteration 72325: loss = 1.354266e-10,2.786893e-09\n",
      "Iteration 72330: loss = 1.3542006e-10,2.7865e-09\n",
      "Iteration 72335: loss = 1.3672999e-10,2.7860931e-09\n",
      "Iteration 72340: loss = 1.3540634e-10,2.7857143e-09\n",
      "Iteration 72345: loss = 1.367186e-10,2.7853024e-09\n",
      "Iteration 72350: loss = 1.3671386e-10,2.7849056e-09\n",
      "Iteration 72355: loss = 1.353938e-10,2.784518e-09\n",
      "Iteration 72360: loss = 1.3670315e-10,2.7841138e-09\n",
      "Iteration 72365: loss = 1.3538526e-10,2.7837201e-09\n",
      "Iteration 72370: loss = 1.3537965e-10,2.7833256e-09\n",
      "Iteration 72375: loss = 1.3537392e-10,2.7829317e-09\n",
      "Iteration 72380: loss = 1.3536931e-10,2.7825362e-09\n",
      "Iteration 72385: loss = 1.3536151e-10,2.7821496e-09\n",
      "Iteration 72390: loss = 1.3535574e-10,2.7817566e-09\n",
      "Iteration 72395: loss = 1.3666808e-10,2.7813465e-09\n",
      "Iteration 72400: loss = 1.3534733e-10,2.7809635e-09\n",
      "Iteration 72405: loss = 1.3665699e-10,2.7805602e-09\n",
      "Iteration 72410: loss = 1.3533415e-10,2.7801836e-09\n",
      "Iteration 72415: loss = 1.366511e-10,2.779761e-09\n",
      "Iteration 72420: loss = 1.3533498e-10,2.7793647e-09\n",
      "Iteration 72425: loss = 1.3533133e-10,2.7789675e-09\n",
      "Iteration 72430: loss = 1.3533094e-10,2.7785616e-09\n",
      "Iteration 72435: loss = 1.3533301e-10,2.7781464e-09\n",
      "Iteration 72440: loss = 1.3533581e-10,2.7777312e-09\n",
      "Iteration 72445: loss = 1.3533763e-10,2.7773188e-09\n",
      "Iteration 72450: loss = 1.3533703e-10,2.7769143e-09\n",
      "Iteration 72455: loss = 1.3533193e-10,2.7765232e-09\n",
      "Iteration 72460: loss = 1.3532377e-10,2.7761415e-09\n",
      "Iteration 72465: loss = 1.3531759e-10,2.7757538e-09\n",
      "Iteration 72470: loss = 1.3531154e-10,2.7753655e-09\n",
      "Iteration 72475: loss = 1.3530561e-10,2.774978e-09\n",
      "Iteration 72480: loss = 1.3529892e-10,2.774592e-09\n",
      "Iteration 72485: loss = 1.3529163e-10,2.774209e-09\n",
      "Iteration 72490: loss = 1.3528309e-10,2.7738303e-09\n",
      "Iteration 72495: loss = 1.352744e-10,2.7734526e-09\n",
      "Iteration 72500: loss = 1.3526505e-10,2.7730758e-09\n",
      "Iteration 72505: loss = 1.3525424e-10,2.772705e-09\n",
      "Iteration 72510: loss = 1.3524297e-10,2.7723353e-09\n",
      "Iteration 72515: loss = 1.3523695e-10,2.7719502e-09\n",
      "Iteration 72520: loss = 1.3523362e-10,2.7715565e-09\n",
      "Iteration 72525: loss = 1.339228e-10,2.7711784e-09\n",
      "Iteration 72530: loss = 1.3522503e-10,2.770777e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72535: loss = 1.3521874e-10,2.770395e-09\n",
      "Iteration 72540: loss = 1.339087e-10,2.770015e-09\n",
      "Iteration 72545: loss = 1.3521297e-10,2.769608e-09\n",
      "Iteration 72550: loss = 1.3521483e-10,2.7692013e-09\n",
      "Iteration 72555: loss = 1.3521627e-10,2.768795e-09\n",
      "Iteration 72560: loss = 1.3521771e-10,2.768391e-09\n",
      "Iteration 72565: loss = 1.352183e-10,2.7679885e-09\n",
      "Iteration 72570: loss = 1.352181e-10,2.7675886e-09\n",
      "Iteration 72575: loss = 1.3521634e-10,2.7671938e-09\n",
      "Iteration 72580: loss = 1.3521088e-10,2.7668103e-09\n",
      "Iteration 72585: loss = 1.3520499e-10,2.7664293e-09\n",
      "Iteration 72590: loss = 1.3519875e-10,2.7660496e-09\n",
      "Iteration 72595: loss = 1.351919e-10,2.7656708e-09\n",
      "Iteration 72600: loss = 1.3518413e-10,2.7652949e-09\n",
      "Iteration 72605: loss = 1.3517533e-10,2.7649225e-09\n",
      "Iteration 72610: loss = 1.351662e-10,2.7645528e-09\n",
      "Iteration 72615: loss = 1.351566e-10,2.7641835e-09\n",
      "Iteration 72620: loss = 1.3514646e-10,2.7638156e-09\n",
      "Iteration 72625: loss = 1.3513525e-10,2.7634508e-09\n",
      "Iteration 72630: loss = 1.351274e-10,2.7630775e-09\n",
      "Iteration 72635: loss = 1.3512423e-10,2.762691e-09\n",
      "Iteration 72640: loss = 1.33815e-10,2.7623135e-09\n",
      "Iteration 72645: loss = 1.3511699e-10,2.76192e-09\n",
      "Iteration 72650: loss = 1.3510933e-10,2.7615452e-09\n",
      "Iteration 72655: loss = 1.3510612e-10,2.761159e-09\n",
      "Iteration 72660: loss = 1.3379632e-10,2.760786e-09\n",
      "Iteration 72665: loss = 1.3509645e-10,2.760399e-09\n",
      "Iteration 72670: loss = 1.3509131e-10,2.7600184e-09\n",
      "Iteration 72675: loss = 1.3508614e-10,2.7596387e-09\n",
      "Iteration 72680: loss = 1.3508221e-10,2.7592568e-09\n",
      "Iteration 72685: loss = 1.337734e-10,2.7588816e-09\n",
      "Iteration 72690: loss = 1.3507405e-10,2.7584939e-09\n",
      "Iteration 72695: loss = 1.337641e-10,2.758121e-09\n",
      "Iteration 72700: loss = 1.3506388e-10,2.7577354e-09\n",
      "Iteration 72705: loss = 1.3505905e-10,2.7573586e-09\n",
      "Iteration 72710: loss = 1.3505504e-10,2.7569782e-09\n",
      "Iteration 72715: loss = 1.3505064e-10,2.7565983e-09\n",
      "Iteration 72720: loss = 1.3374099e-10,2.7562261e-09\n",
      "Iteration 72725: loss = 1.3504288e-10,2.7558378e-09\n",
      "Iteration 72730: loss = 1.337328e-10,2.7554674e-09\n",
      "Iteration 72735: loss = 1.3372863e-10,2.7550888e-09\n",
      "Iteration 72740: loss = 1.350283e-10,2.7547058e-09\n",
      "Iteration 72745: loss = 1.3371955e-10,2.7543337e-09\n",
      "Iteration 72750: loss = 1.3371461e-10,2.7539588e-09\n",
      "Iteration 72755: loss = 1.3501496e-10,2.7535747e-09\n",
      "Iteration 72760: loss = 1.3501049e-10,2.7531972e-09\n",
      "Iteration 72765: loss = 1.3370129e-10,2.7528286e-09\n",
      "Iteration 72770: loss = 1.3369703e-10,2.7524507e-09\n",
      "Iteration 72775: loss = 1.3499678e-10,2.752071e-09\n",
      "Iteration 72780: loss = 1.3368762e-10,2.751701e-09\n",
      "Iteration 72785: loss = 1.3368444e-10,2.7513227e-09\n",
      "Iteration 72790: loss = 1.3498369e-10,2.7509448e-09\n",
      "Iteration 72795: loss = 1.349791e-10,2.7505693e-09\n",
      "Iteration 72800: loss = 1.3367145e-10,2.750196e-09\n",
      "Iteration 72805: loss = 1.3366624e-10,2.7498248e-09\n",
      "Iteration 72810: loss = 1.3496622e-10,2.7494464e-09\n",
      "Iteration 72815: loss = 1.3365532e-10,2.7490834e-09\n",
      "Iteration 72820: loss = 1.3495809e-10,2.7486966e-09\n",
      "Iteration 72825: loss = 1.3495353e-10,2.7483245e-09\n",
      "Iteration 72830: loss = 1.3364508e-10,2.747955e-09\n",
      "Iteration 72835: loss = 1.349446e-10,2.7475786e-09\n",
      "Iteration 72840: loss = 1.3363433e-10,2.7472153e-09\n",
      "Iteration 72845: loss = 1.3493637e-10,2.7468334e-09\n",
      "Iteration 72850: loss = 1.3362632e-10,2.7464704e-09\n",
      "Iteration 72855: loss = 1.3362363e-10,2.7460922e-09\n",
      "Iteration 72860: loss = 1.3361837e-10,2.745724e-09\n",
      "Iteration 72865: loss = 1.3361234e-10,2.7453577e-09\n",
      "Iteration 72870: loss = 1.336105e-10,2.7449802e-09\n",
      "Iteration 72875: loss = 1.3360742e-10,2.744605e-09\n",
      "Iteration 72880: loss = 1.3360382e-10,2.7442328e-09\n",
      "Iteration 72885: loss = 1.3359636e-10,2.7438725e-09\n",
      "Iteration 72890: loss = 1.335912e-10,2.7435036e-09\n",
      "Iteration 72895: loss = 1.3358807e-10,2.7431315e-09\n",
      "Iteration 72900: loss = 1.3358487e-10,2.7427594e-09\n",
      "Iteration 72905: loss = 1.3358131e-10,2.742387e-09\n",
      "Iteration 72910: loss = 1.3357744e-10,2.7420166e-09\n",
      "Iteration 72915: loss = 1.3357297e-10,2.7416482e-09\n",
      "Iteration 72920: loss = 1.3356656e-10,2.7412868e-09\n",
      "Iteration 72925: loss = 1.3355878e-10,2.7409293e-09\n",
      "Iteration 72930: loss = 1.335505e-10,2.7405735e-09\n",
      "Iteration 72935: loss = 1.3354165e-10,2.7402196e-09\n",
      "Iteration 72940: loss = 1.3353156e-10,2.7398694e-09\n",
      "Iteration 72945: loss = 1.335211e-10,2.739521e-09\n",
      "Iteration 72950: loss = 1.3350994e-10,2.739176e-09\n",
      "Iteration 72955: loss = 1.334983e-10,2.738831e-09\n",
      "Iteration 72960: loss = 1.3348632e-10,2.7384872e-09\n",
      "Iteration 72965: loss = 1.3347398e-10,2.738146e-09\n",
      "Iteration 72970: loss = 1.334592e-10,2.737812e-09\n",
      "Iteration 72975: loss = 1.3344416e-10,2.7374794e-09\n",
      "Iteration 72980: loss = 1.3342842e-10,2.7371483e-09\n",
      "Iteration 72985: loss = 1.3341271e-10,2.7368185e-09\n",
      "Iteration 72990: loss = 1.3339695e-10,2.7364881e-09\n",
      "Iteration 72995: loss = 1.3338723e-10,2.7361404e-09\n",
      "Iteration 73000: loss = 1.3338443e-10,2.7357723e-09\n",
      "Iteration 73005: loss = 1.3338519e-10,2.7353932e-09\n",
      "Iteration 73010: loss = 1.3338693e-10,2.7350113e-09\n",
      "Iteration 73015: loss = 1.3338979e-10,2.734628e-09\n",
      "Iteration 73020: loss = 1.3339164e-10,2.7342464e-09\n",
      "Iteration 73025: loss = 1.3339195e-10,2.7338694e-09\n",
      "Iteration 73030: loss = 1.3339187e-10,2.7334957e-09\n",
      "Iteration 73035: loss = 1.3339133e-10,2.7331226e-09\n",
      "Iteration 73040: loss = 1.3338779e-10,2.7327582e-09\n",
      "Iteration 73045: loss = 1.3338321e-10,2.7323985e-09\n",
      "Iteration 73050: loss = 1.3337774e-10,2.7320408e-09\n",
      "Iteration 73055: loss = 1.3337066e-10,2.7316884e-09\n",
      "Iteration 73060: loss = 1.3336314e-10,2.7313383e-09\n",
      "Iteration 73065: loss = 1.3335492e-10,2.7309888e-09\n",
      "Iteration 73070: loss = 1.3334557e-10,2.7306437e-09\n",
      "Iteration 73075: loss = 1.3333541e-10,2.7303018e-09\n",
      "Iteration 73080: loss = 1.3332362e-10,2.7299654e-09\n",
      "Iteration 73085: loss = 1.3331126e-10,2.7296303e-09\n",
      "Iteration 73090: loss = 1.3201065e-10,2.7292817e-09\n",
      "Iteration 73095: loss = 1.3331147e-10,2.7288871e-09\n",
      "Iteration 73100: loss = 1.333144e-10,2.728507e-09\n",
      "Iteration 73105: loss = 1.3331881e-10,2.728123e-09\n",
      "Iteration 73110: loss = 1.333215e-10,2.7277443e-09\n",
      "Iteration 73115: loss = 1.3202961e-10,2.7273699e-09\n",
      "Iteration 73120: loss = 1.3332734e-10,2.7269866e-09\n",
      "Iteration 73125: loss = 1.3333125e-10,2.7266056e-09\n",
      "Iteration 73130: loss = 1.333353e-10,2.7262241e-09\n",
      "Iteration 73135: loss = 1.3333831e-10,2.7258462e-09\n",
      "Iteration 73140: loss = 1.333424e-10,2.725464e-09\n",
      "Iteration 73145: loss = 1.3205073e-10,2.7250913e-09\n",
      "Iteration 73150: loss = 1.3205508e-10,2.7247107e-09\n",
      "Iteration 73155: loss = 1.3205684e-10,2.724337e-09\n",
      "Iteration 73160: loss = 1.3335567e-10,2.7239526e-09\n",
      "Iteration 73165: loss = 1.3335967e-10,2.7235738e-09\n",
      "Iteration 73170: loss = 1.3336356e-10,2.723194e-09\n",
      "Iteration 73175: loss = 1.3206984e-10,2.7228295e-09\n",
      "Iteration 73180: loss = 1.333703e-10,2.7224412e-09\n",
      "Iteration 73185: loss = 1.3207264e-10,2.7220874e-09\n",
      "Iteration 73190: loss = 1.33365e-10,2.7217255e-09\n",
      "Iteration 73195: loss = 1.3206554e-10,2.7213773e-09\n",
      "Iteration 73200: loss = 1.3206251e-10,2.7210207e-09\n",
      "Iteration 73205: loss = 1.320592e-10,2.7206664e-09\n",
      "Iteration 73210: loss = 1.3205588e-10,2.7203109e-09\n",
      "Iteration 73215: loss = 1.3334592e-10,2.719956e-09\n",
      "Iteration 73220: loss = 1.3204869e-10,2.719603e-09\n",
      "Iteration 73225: loss = 1.32045e-10,2.7192497e-09\n",
      "Iteration 73230: loss = 1.333366e-10,2.7188913e-09\n",
      "Iteration 73235: loss = 1.3333366e-10,2.7185365e-09\n",
      "Iteration 73240: loss = 1.3203315e-10,2.7181957e-09\n",
      "Iteration 73245: loss = 1.3203154e-10,2.7178373e-09\n",
      "Iteration 73250: loss = 1.3332266e-10,2.717481e-09\n",
      "Iteration 73255: loss = 1.3331913e-10,2.7171287e-09\n",
      "Iteration 73260: loss = 1.3331596e-10,2.716776e-09\n",
      "Iteration 73265: loss = 1.3331204e-10,2.716426e-09\n",
      "Iteration 73270: loss = 1.3201278e-10,2.716083e-09\n",
      "Iteration 73275: loss = 1.3201053e-10,2.7157285e-09\n",
      "Iteration 73280: loss = 1.3200742e-10,2.7153773e-09\n",
      "Iteration 73285: loss = 1.3329933e-10,2.7150202e-09\n",
      "Iteration 73290: loss = 1.3200051e-10,2.7146758e-09\n",
      "Iteration 73295: loss = 1.3199662e-10,2.7143277e-09\n",
      "Iteration 73300: loss = 1.332895e-10,2.7139686e-09\n",
      "Iteration 73305: loss = 1.3199047e-10,2.713627e-09\n",
      "Iteration 73310: loss = 1.319876e-10,2.7132754e-09\n",
      "Iteration 73315: loss = 1.319826e-10,2.7129312e-09\n",
      "Iteration 73320: loss = 1.3198163e-10,2.712576e-09\n",
      "Iteration 73325: loss = 1.3197726e-10,2.7122309e-09\n",
      "Iteration 73330: loss = 1.3197506e-10,2.7118792e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73335: loss = 1.3196995e-10,2.7115354e-09\n",
      "Iteration 73340: loss = 1.3196842e-10,2.7111833e-09\n",
      "Iteration 73345: loss = 1.3325903e-10,2.7108333e-09\n",
      "Iteration 73350: loss = 1.3196035e-10,2.7104934e-09\n",
      "Iteration 73355: loss = 1.3325327e-10,2.7101368e-09\n",
      "Iteration 73360: loss = 1.3195502e-10,2.709796e-09\n",
      "Iteration 73365: loss = 1.3195219e-10,2.7094476e-09\n",
      "Iteration 73370: loss = 1.3194658e-10,2.709109e-09\n",
      "Iteration 73375: loss = 1.319448e-10,2.7087579e-09\n",
      "Iteration 73380: loss = 1.3194228e-10,2.7084104e-09\n",
      "Iteration 73385: loss = 1.31935e-10,2.7080767e-09\n",
      "Iteration 73390: loss = 1.3193031e-10,2.7077354e-09\n",
      "Iteration 73395: loss = 1.3192795e-10,2.7073885e-09\n",
      "Iteration 73400: loss = 1.3192537e-10,2.7070408e-09\n",
      "Iteration 73405: loss = 1.319223e-10,2.7066958e-09\n",
      "Iteration 73410: loss = 1.3191782e-10,2.706356e-09\n",
      "Iteration 73415: loss = 1.3191256e-10,2.7060185e-09\n",
      "Iteration 73420: loss = 1.319058e-10,2.705685e-09\n",
      "Iteration 73425: loss = 1.3189864e-10,2.7053537e-09\n",
      "Iteration 73430: loss = 1.3189112e-10,2.7050246e-09\n",
      "Iteration 73435: loss = 1.318821e-10,2.7046978e-09\n",
      "Iteration 73440: loss = 1.318722e-10,2.704375e-09\n",
      "Iteration 73445: loss = 1.3186187e-10,2.7040534e-09\n",
      "Iteration 73450: loss = 1.3185049e-10,2.7037368e-09\n",
      "Iteration 73455: loss = 1.3183797e-10,2.703422e-09\n",
      "Iteration 73460: loss = 1.3182513e-10,2.7031102e-09\n",
      "Iteration 73465: loss = 1.3181106e-10,2.702802e-09\n",
      "Iteration 73470: loss = 1.317965e-10,2.7024947e-09\n",
      "Iteration 73475: loss = 1.3178164e-10,2.7021883e-09\n",
      "Iteration 73480: loss = 1.317664e-10,2.7018827e-09\n",
      "Iteration 73485: loss = 1.3175115e-10,2.7015794e-09\n",
      "Iteration 73490: loss = 1.3173555e-10,2.7012754e-09\n",
      "Iteration 73495: loss = 1.317197e-10,2.7009737e-09\n",
      "Iteration 73500: loss = 1.3170366e-10,2.7006728e-09\n",
      "Iteration 73505: loss = 1.3168737e-10,2.7003721e-09\n",
      "Iteration 73510: loss = 1.3167102e-10,2.7000726e-09\n",
      "Iteration 73515: loss = 1.316542e-10,2.6997737e-09\n",
      "Iteration 73520: loss = 1.3163742e-10,2.699476e-09\n",
      "Iteration 73525: loss = 1.3161906e-10,2.699183e-09\n",
      "Iteration 73530: loss = 1.316007e-10,2.698892e-09\n",
      "Iteration 73535: loss = 1.3158198e-10,2.6986005e-09\n",
      "Iteration 73540: loss = 1.3156401e-10,2.6983078e-09\n",
      "Iteration 73545: loss = 1.3155348e-10,2.6979925e-09\n",
      "Iteration 73550: loss = 1.3154942e-10,2.6976588e-09\n",
      "Iteration 73555: loss = 1.3154743e-10,2.6973175e-09\n",
      "Iteration 73560: loss = 1.3154737e-10,2.6969724e-09\n",
      "Iteration 73565: loss = 1.3154747e-10,2.6966276e-09\n",
      "Iteration 73570: loss = 1.3154684e-10,2.6962834e-09\n",
      "Iteration 73575: loss = 1.3154523e-10,2.6959435e-09\n",
      "Iteration 73580: loss = 1.315427e-10,2.6956066e-09\n",
      "Iteration 73585: loss = 1.3153911e-10,2.6952731e-09\n",
      "Iteration 73590: loss = 1.3153487e-10,2.6949414e-09\n",
      "Iteration 73595: loss = 1.3152898e-10,2.694615e-09\n",
      "Iteration 73600: loss = 1.3152171e-10,2.6942932e-09\n",
      "Iteration 73605: loss = 1.3151329e-10,2.693975e-09\n",
      "Iteration 73610: loss = 1.3150321e-10,2.6936624e-09\n",
      "Iteration 73615: loss = 1.3149258e-10,2.6933509e-09\n",
      "Iteration 73620: loss = 1.3148167e-10,2.6930413e-09\n",
      "Iteration 73625: loss = 1.3146989e-10,2.6927331e-09\n",
      "Iteration 73630: loss = 1.3145808e-10,2.692427e-09\n",
      "Iteration 73635: loss = 1.3144554e-10,2.692122e-09\n",
      "Iteration 73640: loss = 1.3143156e-10,2.6918219e-09\n",
      "Iteration 73645: loss = 1.3141772e-10,2.6915223e-09\n",
      "Iteration 73650: loss = 1.3140218e-10,2.6912272e-09\n",
      "Iteration 73655: loss = 1.3138621e-10,2.6909346e-09\n",
      "Iteration 73660: loss = 1.3136987e-10,2.6906437e-09\n",
      "Iteration 73665: loss = 1.3135316e-10,2.6903537e-09\n",
      "Iteration 73670: loss = 1.3133634e-10,2.690065e-09\n",
      "Iteration 73675: loss = 1.3131929e-10,2.6897762e-09\n",
      "Iteration 73680: loss = 1.3130186e-10,2.6894889e-09\n",
      "Iteration 73685: loss = 1.3128439e-10,2.689202e-09\n",
      "Iteration 73690: loss = 1.3126672e-10,2.6889164e-09\n",
      "Iteration 73695: loss = 1.3124869e-10,2.688631e-09\n",
      "Iteration 73700: loss = 1.3123076e-10,2.6883478e-09\n",
      "Iteration 73705: loss = 1.312124e-10,2.6880638e-09\n",
      "Iteration 73710: loss = 1.3119382e-10,2.6877807e-09\n",
      "Iteration 73715: loss = 1.3117532e-10,2.6874991e-09\n",
      "Iteration 73720: loss = 1.3115653e-10,2.687219e-09\n",
      "Iteration 73725: loss = 1.3113777e-10,2.6869373e-09\n",
      "Iteration 73730: loss = 1.3111838e-10,2.6866585e-09\n",
      "Iteration 73735: loss = 1.2982114e-10,2.686362e-09\n",
      "Iteration 73740: loss = 1.3111735e-10,2.6859879e-09\n",
      "Iteration 73745: loss = 1.3111941e-10,2.6856457e-09\n",
      "Iteration 73750: loss = 1.2983234e-10,2.6853186e-09\n",
      "Iteration 73755: loss = 1.3112088e-10,2.6849691e-09\n",
      "Iteration 73760: loss = 1.311229e-10,2.6846274e-09\n",
      "Iteration 73765: loss = 1.3112463e-10,2.684287e-09\n",
      "Iteration 73770: loss = 1.3112823e-10,2.683941e-09\n",
      "Iteration 73775: loss = 1.2984303e-10,2.6836113e-09\n",
      "Iteration 73780: loss = 1.311319e-10,2.6832605e-09\n",
      "Iteration 73785: loss = 1.3113649e-10,2.682913e-09\n",
      "Iteration 73790: loss = 1.2984992e-10,2.6825875e-09\n",
      "Iteration 73795: loss = 1.311401e-10,2.682234e-09\n",
      "Iteration 73800: loss = 1.3114144e-10,2.681897e-09\n",
      "Iteration 73805: loss = 1.2985879e-10,2.6815608e-09\n",
      "Iteration 73810: loss = 1.2986105e-10,2.681221e-09\n",
      "Iteration 73815: loss = 1.3114744e-10,2.6808809e-09\n",
      "Iteration 73820: loss = 1.2986577e-10,2.680542e-09\n",
      "Iteration 73825: loss = 1.2986794e-10,2.6802038e-09\n",
      "Iteration 73830: loss = 1.2987017e-10,2.6798646e-09\n",
      "Iteration 73835: loss = 1.3115652e-10,2.6795248e-09\n",
      "Iteration 73840: loss = 1.311599e-10,2.6791835e-09\n",
      "Iteration 73845: loss = 1.2987417e-10,2.678859e-09\n",
      "Iteration 73850: loss = 1.2987819e-10,2.6785159e-09\n",
      "Iteration 73855: loss = 1.3116526e-10,2.6781755e-09\n",
      "Iteration 73860: loss = 1.3116841e-10,2.6778355e-09\n",
      "Iteration 73865: loss = 1.2988542e-10,2.6775033e-09\n",
      "Iteration 73870: loss = 1.2988541e-10,2.6771745e-09\n",
      "Iteration 73875: loss = 1.311745e-10,2.676828e-09\n",
      "Iteration 73880: loss = 1.298913e-10,2.6764977e-09\n",
      "Iteration 73885: loss = 1.2989167e-10,2.676167e-09\n",
      "Iteration 73890: loss = 1.2989608e-10,2.6758258e-09\n",
      "Iteration 73895: loss = 1.2989561e-10,2.675498e-09\n",
      "Iteration 73900: loss = 1.298996e-10,2.6751579e-09\n",
      "Iteration 73905: loss = 1.2989815e-10,2.6748337e-09\n",
      "Iteration 73910: loss = 1.2990055e-10,2.6745002e-09\n",
      "Iteration 73915: loss = 1.2990604e-10,2.6741547e-09\n",
      "Iteration 73920: loss = 1.2990725e-10,2.6738245e-09\n",
      "Iteration 73925: loss = 1.299068e-10,2.673499e-09\n",
      "Iteration 73930: loss = 1.2991057e-10,2.6731606e-09\n",
      "Iteration 73935: loss = 1.2991253e-10,2.6728286e-09\n",
      "Iteration 73940: loss = 1.2991265e-10,2.672502e-09\n",
      "Iteration 73945: loss = 1.2991495e-10,2.6721687e-09\n",
      "Iteration 73950: loss = 1.2991819e-10,2.6718334e-09\n",
      "Iteration 73955: loss = 1.2992153e-10,2.6714981e-09\n",
      "Iteration 73960: loss = 1.2992335e-10,2.671167e-09\n",
      "Iteration 73965: loss = 1.2992445e-10,2.670839e-09\n",
      "Iteration 73970: loss = 1.2992328e-10,2.6705171e-09\n",
      "Iteration 73975: loss = 1.299207e-10,2.6702012e-09\n",
      "Iteration 73980: loss = 1.2991701e-10,2.6698879e-09\n",
      "Iteration 73985: loss = 1.2991286e-10,2.6695768e-09\n",
      "Iteration 73990: loss = 1.2990668e-10,2.6692704e-09\n",
      "Iteration 73995: loss = 1.299002e-10,2.6689662e-09\n",
      "Iteration 74000: loss = 1.2989247e-10,2.6686657e-09\n",
      "Iteration 74005: loss = 1.2988251e-10,2.668373e-09\n",
      "Iteration 74010: loss = 1.2987185e-10,2.668082e-09\n",
      "Iteration 74015: loss = 1.2986097e-10,2.6677918e-09\n",
      "Iteration 74020: loss = 1.2984995e-10,2.6675029e-09\n",
      "Iteration 74025: loss = 1.2983793e-10,2.6672173e-09\n",
      "Iteration 74030: loss = 1.2982482e-10,2.6669336e-09\n",
      "Iteration 74035: loss = 1.2981131e-10,2.6666522e-09\n",
      "Iteration 74040: loss = 1.2979769e-10,2.6663711e-09\n",
      "Iteration 74045: loss = 1.2978395e-10,2.6660916e-09\n",
      "Iteration 74050: loss = 1.2976957e-10,2.6658127e-09\n",
      "Iteration 74055: loss = 1.2975371e-10,2.6655393e-09\n",
      "Iteration 74060: loss = 1.2973787e-10,2.6652671e-09\n",
      "Iteration 74065: loss = 1.2972166e-10,2.6649958e-09\n",
      "Iteration 74070: loss = 1.297052e-10,2.6647253e-09\n",
      "Iteration 74075: loss = 1.2968872e-10,2.6644553e-09\n",
      "Iteration 74080: loss = 1.2967173e-10,2.6641862e-09\n",
      "Iteration 74085: loss = 1.2965377e-10,2.6639206e-09\n",
      "Iteration 74090: loss = 1.2963561e-10,2.663656e-09\n",
      "Iteration 74095: loss = 1.2962026e-10,2.663383e-09\n",
      "Iteration 74100: loss = 1.2961292e-10,2.6630869e-09\n",
      "Iteration 74105: loss = 1.2961103e-10,2.6627736e-09\n",
      "Iteration 74110: loss = 1.2961142e-10,2.662454e-09\n",
      "Iteration 74115: loss = 1.2961197e-10,2.6621352e-09\n",
      "Iteration 74120: loss = 1.2961197e-10,2.6618165e-09\n",
      "Iteration 74125: loss = 1.2961186e-10,2.6615e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74130: loss = 1.2961084e-10,2.661185e-09\n",
      "Iteration 74135: loss = 1.2960932e-10,2.660873e-09\n",
      "Iteration 74140: loss = 1.2960522e-10,2.660568e-09\n",
      "Iteration 74145: loss = 1.2959998e-10,2.6602682e-09\n",
      "Iteration 74150: loss = 1.2959377e-10,2.65997e-09\n",
      "Iteration 74155: loss = 1.295861e-10,2.659677e-09\n",
      "Iteration 74160: loss = 1.295777e-10,2.6593852e-09\n",
      "Iteration 74165: loss = 1.2956881e-10,2.6590967e-09\n",
      "Iteration 74170: loss = 1.295595e-10,2.65881e-09\n",
      "Iteration 74175: loss = 1.2954872e-10,2.6585258e-09\n",
      "Iteration 74180: loss = 1.2953595e-10,2.6582496e-09\n",
      "Iteration 74185: loss = 1.2952259e-10,2.6579747e-09\n",
      "Iteration 74190: loss = 1.2950933e-10,2.6577007e-09\n",
      "Iteration 74195: loss = 1.2949515e-10,2.6574276e-09\n",
      "Iteration 74200: loss = 1.294811e-10,2.6571567e-09\n",
      "Iteration 74205: loss = 1.2946676e-10,2.656886e-09\n",
      "Iteration 74210: loss = 1.294516e-10,2.6566174e-09\n",
      "Iteration 74215: loss = 1.2943562e-10,2.656352e-09\n",
      "Iteration 74220: loss = 1.2941916e-10,2.6560882e-09\n",
      "Iteration 74225: loss = 1.2940245e-10,2.655825e-09\n",
      "Iteration 74230: loss = 1.2938584e-10,2.6555627e-09\n",
      "Iteration 74235: loss = 1.2937511e-10,2.6552822e-09\n",
      "Iteration 74240: loss = 1.2937093e-10,2.654983e-09\n",
      "Iteration 74245: loss = 1.2937053e-10,2.6546711e-09\n",
      "Iteration 74250: loss = 1.2937215e-10,2.654354e-09\n",
      "Iteration 74255: loss = 1.2937369e-10,2.654038e-09\n",
      "Iteration 74260: loss = 1.2937486e-10,2.6537226e-09\n",
      "Iteration 74265: loss = 1.2937462e-10,2.6534126e-09\n",
      "Iteration 74270: loss = 1.29373e-10,2.6531064e-09\n",
      "Iteration 74275: loss = 1.2937001e-10,2.6528064e-09\n",
      "Iteration 74280: loss = 1.2936593e-10,2.652507e-09\n",
      "Iteration 74285: loss = 1.2936087e-10,2.6522111e-09\n",
      "Iteration 74290: loss = 1.2935447e-10,2.6519205e-09\n",
      "Iteration 74295: loss = 1.293476e-10,2.6516318e-09\n",
      "Iteration 74300: loss = 1.2933873e-10,2.6513485e-09\n",
      "Iteration 74305: loss = 1.2932841e-10,2.6510705e-09\n",
      "Iteration 74310: loss = 1.2931736e-10,2.650795e-09\n",
      "Iteration 74315: loss = 1.2930591e-10,2.65052e-09\n",
      "Iteration 74320: loss = 1.2929355e-10,2.6502474e-09\n",
      "Iteration 74325: loss = 1.2928132e-10,2.6499751e-09\n",
      "Iteration 74330: loss = 1.2926828e-10,2.6497065e-09\n",
      "Iteration 74335: loss = 1.292541e-10,2.649441e-09\n",
      "Iteration 74340: loss = 1.2923967e-10,2.6491755e-09\n",
      "Iteration 74345: loss = 1.2922492e-10,2.6489124e-09\n",
      "Iteration 74350: loss = 1.2920949e-10,2.6486506e-09\n",
      "Iteration 74355: loss = 1.2919299e-10,2.6483935e-09\n",
      "Iteration 74360: loss = 1.2917606e-10,2.6481368e-09\n",
      "Iteration 74365: loss = 1.2915904e-10,2.647882e-09\n",
      "Iteration 74370: loss = 1.2914216e-10,2.6476266e-09\n",
      "Iteration 74375: loss = 1.2912448e-10,2.6473728e-09\n",
      "Iteration 74380: loss = 1.2910686e-10,2.6471203e-09\n",
      "Iteration 74385: loss = 1.2908931e-10,2.6468672e-09\n",
      "Iteration 74390: loss = 1.2907132e-10,2.6466158e-09\n",
      "Iteration 74395: loss = 1.2905317e-10,2.6463638e-09\n",
      "Iteration 74400: loss = 1.2903499e-10,2.6461142e-09\n",
      "Iteration 74405: loss = 1.2901662e-10,2.645864e-09\n",
      "Iteration 74410: loss = 1.2899799e-10,2.645615e-09\n",
      "Iteration 74415: loss = 1.2897909e-10,2.6453657e-09\n",
      "Iteration 74420: loss = 1.2896041e-10,2.6451183e-09\n",
      "Iteration 74425: loss = 1.2894137e-10,2.644871e-09\n",
      "Iteration 74430: loss = 1.2765843e-10,2.6445928e-09\n",
      "Iteration 74435: loss = 1.2894531e-10,2.644251e-09\n",
      "Iteration 74440: loss = 1.2894448e-10,2.6439484e-09\n",
      "Iteration 74445: loss = 1.2893416e-10,2.643676e-09\n",
      "Iteration 74450: loss = 1.2893292e-10,2.6433775e-09\n",
      "Iteration 74455: loss = 1.2765432e-10,2.6430869e-09\n",
      "Iteration 74460: loss = 1.2893385e-10,2.6427676e-09\n",
      "Iteration 74465: loss = 1.2892863e-10,2.6424807e-09\n",
      "Iteration 74470: loss = 1.289296e-10,2.642175e-09\n",
      "Iteration 74475: loss = 1.2893046e-10,2.6418703e-09\n",
      "Iteration 74480: loss = 1.276481e-10,2.6415916e-09\n",
      "Iteration 74485: loss = 1.2892715e-10,2.6412752e-09\n",
      "Iteration 74490: loss = 1.2764749e-10,2.6409908e-09\n",
      "Iteration 74495: loss = 1.2892191e-10,2.640688e-09\n",
      "Iteration 74500: loss = 1.2764567e-10,2.6403932e-09\n",
      "Iteration 74505: loss = 1.2764419e-10,2.6400964e-09\n",
      "Iteration 74510: loss = 1.2892015e-10,2.639791e-09\n",
      "Iteration 74515: loss = 1.2891892e-10,2.6394928e-09\n",
      "Iteration 74520: loss = 1.2891846e-10,2.6391946e-09\n",
      "Iteration 74525: loss = 1.2763922e-10,2.638909e-09\n",
      "Iteration 74530: loss = 1.2891559e-10,2.638602e-09\n",
      "Iteration 74535: loss = 1.2891545e-10,2.638303e-09\n",
      "Iteration 74540: loss = 1.2763865e-10,2.6380114e-09\n",
      "Iteration 74545: loss = 1.2763718e-10,2.6377167e-09\n",
      "Iteration 74550: loss = 1.276361e-10,2.6374194e-09\n",
      "Iteration 74555: loss = 1.276358e-10,2.637122e-09\n",
      "Iteration 74560: loss = 1.2890956e-10,2.6368236e-09\n",
      "Iteration 74565: loss = 1.289092e-10,2.6365259e-09\n",
      "Iteration 74570: loss = 1.27633e-10,2.636234e-09\n",
      "Iteration 74575: loss = 1.2890733e-10,2.635935e-09\n",
      "Iteration 74580: loss = 1.2763181e-10,2.635641e-09\n",
      "Iteration 74585: loss = 1.2763078e-10,2.6353468e-09\n",
      "Iteration 74590: loss = 1.2762975e-10,2.6350524e-09\n",
      "Iteration 74595: loss = 1.2762909e-10,2.634756e-09\n",
      "Iteration 74600: loss = 1.2762863e-10,2.634461e-09\n",
      "Iteration 74605: loss = 1.2762773e-10,2.6341658e-09\n",
      "Iteration 74610: loss = 1.2762365e-10,2.633882e-09\n",
      "Iteration 74615: loss = 1.2762615e-10,2.6335782e-09\n",
      "Iteration 74620: loss = 1.2762054e-10,2.6332978e-09\n",
      "Iteration 74625: loss = 1.2761708e-10,2.6330118e-09\n",
      "Iteration 74630: loss = 1.2762201e-10,2.6327012e-09\n",
      "Iteration 74635: loss = 1.2762144e-10,2.6324072e-09\n",
      "Iteration 74640: loss = 1.2761413e-10,2.6321325e-09\n",
      "Iteration 74645: loss = 1.2761553e-10,2.6318334e-09\n",
      "Iteration 74650: loss = 1.2762108e-10,2.6315214e-09\n",
      "Iteration 74655: loss = 1.276167e-10,2.631239e-09\n",
      "Iteration 74660: loss = 1.276139e-10,2.6309532e-09\n",
      "Iteration 74665: loss = 1.2761601e-10,2.6306517e-09\n",
      "Iteration 74670: loss = 1.2761775e-10,2.6303524e-09\n",
      "Iteration 74675: loss = 1.2761465e-10,2.6300668e-09\n",
      "Iteration 74680: loss = 1.276139e-10,2.629776e-09\n",
      "Iteration 74685: loss = 1.2761346e-10,2.6294833e-09\n",
      "Iteration 74690: loss = 1.2761288e-10,2.6291915e-09\n",
      "Iteration 74695: loss = 1.2761169e-10,2.6289007e-09\n",
      "Iteration 74700: loss = 1.2761042e-10,2.6286124e-09\n",
      "Iteration 74705: loss = 1.2760835e-10,2.6283253e-09\n",
      "Iteration 74710: loss = 1.2760497e-10,2.6280427e-09\n",
      "Iteration 74715: loss = 1.2760058e-10,2.627763e-09\n",
      "Iteration 74720: loss = 1.2759582e-10,2.627485e-09\n",
      "Iteration 74725: loss = 1.2759509e-10,2.6271936e-09\n",
      "Iteration 74730: loss = 1.2759986e-10,2.6268872e-09\n",
      "Iteration 74735: loss = 1.2759828e-10,2.6266003e-09\n",
      "Iteration 74740: loss = 1.2759879e-10,2.6263072e-09\n",
      "Iteration 74745: loss = 1.2759899e-10,2.6260145e-09\n",
      "Iteration 74750: loss = 1.2760032e-10,2.625719e-09\n",
      "Iteration 74755: loss = 1.2759897e-10,2.625432e-09\n",
      "Iteration 74760: loss = 1.2759944e-10,2.62514e-09\n",
      "Iteration 74765: loss = 1.2760107e-10,2.624844e-09\n",
      "Iteration 74770: loss = 1.2760147e-10,2.6245526e-09\n",
      "Iteration 74775: loss = 1.2760186e-10,2.6242608e-09\n",
      "Iteration 74780: loss = 1.2760239e-10,2.6239697e-09\n",
      "Iteration 74785: loss = 1.2760124e-10,2.6236826e-09\n",
      "Iteration 74790: loss = 1.2759961e-10,2.6233975e-09\n",
      "Iteration 74795: loss = 1.2759725e-10,2.6231137e-09\n",
      "Iteration 74800: loss = 1.2759359e-10,2.6228355e-09\n",
      "Iteration 74805: loss = 1.2758776e-10,2.622564e-09\n",
      "Iteration 74810: loss = 1.2758132e-10,2.6222937e-09\n",
      "Iteration 74815: loss = 1.2757366e-10,2.6220282e-09\n",
      "Iteration 74820: loss = 1.2757041e-10,2.6217482e-09\n",
      "Iteration 74825: loss = 1.2757426e-10,2.6214484e-09\n",
      "Iteration 74830: loss = 1.2757122e-10,2.6211686e-09\n",
      "Iteration 74835: loss = 1.275721e-10,2.620878e-09\n",
      "Iteration 74840: loss = 1.275734e-10,2.6205855e-09\n",
      "Iteration 74845: loss = 1.2757266e-10,2.6203004e-09\n",
      "Iteration 74850: loss = 1.2757476e-10,2.620006e-09\n",
      "Iteration 74855: loss = 1.275733e-10,2.6197227e-09\n",
      "Iteration 74860: loss = 1.2757394e-10,2.6194327e-09\n",
      "Iteration 74865: loss = 1.2757381e-10,2.6191462e-09\n",
      "Iteration 74870: loss = 1.2630742e-10,2.6188633e-09\n",
      "Iteration 74875: loss = 1.275747e-10,2.6185691e-09\n",
      "Iteration 74880: loss = 1.2757574e-10,2.61828e-09\n",
      "Iteration 74885: loss = 1.2757707e-10,2.6179894e-09\n",
      "Iteration 74890: loss = 1.2757594e-10,2.6177063e-09\n",
      "Iteration 74895: loss = 1.2757533e-10,2.6174223e-09\n",
      "Iteration 74900: loss = 1.2757641e-10,2.6171323e-09\n",
      "Iteration 74905: loss = 1.2630809e-10,2.6168574e-09\n",
      "Iteration 74910: loss = 1.2757628e-10,2.6165614e-09\n",
      "Iteration 74915: loss = 1.2757641e-10,2.6162759e-09\n",
      "Iteration 74920: loss = 1.2630885e-10,2.6159985e-09\n",
      "Iteration 74925: loss = 1.2757607e-10,2.6157063e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74930: loss = 1.27577e-10,2.6154185e-09\n",
      "Iteration 74935: loss = 1.2757671e-10,2.6151348e-09\n",
      "Iteration 74940: loss = 1.2757688e-10,2.6148488e-09\n",
      "Iteration 74945: loss = 1.2631164e-10,2.6145657e-09\n",
      "Iteration 74950: loss = 1.2631034e-10,2.6142861e-09\n",
      "Iteration 74955: loss = 1.2631163e-10,2.6139975e-09\n",
      "Iteration 74960: loss = 1.2631203e-10,2.6137124e-09\n",
      "Iteration 74965: loss = 1.2631081e-10,2.613433e-09\n",
      "Iteration 74970: loss = 1.275784e-10,2.6131408e-09\n",
      "Iteration 74975: loss = 1.2757868e-10,2.612857e-09\n",
      "Iteration 74980: loss = 1.2631056e-10,2.6125837e-09\n",
      "Iteration 74985: loss = 1.2757893e-10,2.612289e-09\n",
      "Iteration 74990: loss = 1.2757807e-10,2.6120093e-09\n",
      "Iteration 74995: loss = 1.2631282e-10,2.6117288e-09\n",
      "Iteration 75000: loss = 1.2631263e-10,2.6114462e-09\n",
      "Iteration 75005: loss = 1.263121e-10,2.6111664e-09\n",
      "Iteration 75010: loss = 1.2631282e-10,2.6108824e-09\n",
      "Iteration 75015: loss = 1.2757831e-10,2.6105975e-09\n",
      "Iteration 75020: loss = 1.2631168e-10,2.610322e-09\n",
      "Iteration 75025: loss = 1.2631204e-10,2.610038e-09\n",
      "Iteration 75030: loss = 1.263135e-10,2.609753e-09\n",
      "Iteration 75035: loss = 1.2631281e-10,2.6094733e-09\n",
      "Iteration 75040: loss = 1.2631027e-10,2.6092e-09\n",
      "Iteration 75045: loss = 1.2631035e-10,2.6089195e-09\n",
      "Iteration 75050: loss = 1.275788e-10,2.6086266e-09\n",
      "Iteration 75055: loss = 1.263116e-10,2.6083526e-09\n",
      "Iteration 75060: loss = 1.2631175e-10,2.608072e-09\n",
      "Iteration 75065: loss = 1.263075e-10,2.6078055e-09\n",
      "Iteration 75070: loss = 1.2631048e-10,2.607516e-09\n",
      "Iteration 75075: loss = 1.263122e-10,2.6072307e-09\n",
      "Iteration 75080: loss = 1.2630731e-10,2.6069658e-09\n",
      "Iteration 75085: loss = 1.2630812e-10,2.6066833e-09\n",
      "Iteration 75090: loss = 1.2631148e-10,2.606394e-09\n",
      "Iteration 75095: loss = 1.2631295e-10,2.606111e-09\n",
      "Iteration 75100: loss = 1.2631172e-10,2.6058349e-09\n",
      "Iteration 75105: loss = 1.263127e-10,2.6055536e-09\n",
      "Iteration 75110: loss = 1.2631347e-10,2.6052713e-09\n",
      "Iteration 75115: loss = 1.2630788e-10,2.6050109e-09\n",
      "Iteration 75120: loss = 1.2630282e-10,2.604747e-09\n",
      "Iteration 75125: loss = 1.2629892e-10,2.6044809e-09\n",
      "Iteration 75130: loss = 1.2629546e-10,2.6042126e-09\n",
      "Iteration 75135: loss = 1.2629171e-10,2.603946e-09\n",
      "Iteration 75140: loss = 1.2628766e-10,2.60368e-09\n",
      "Iteration 75145: loss = 1.2628264e-10,2.6034184e-09\n",
      "Iteration 75150: loss = 1.2627677e-10,2.603158e-09\n",
      "Iteration 75155: loss = 1.262704e-10,2.6028997e-09\n",
      "Iteration 75160: loss = 1.2626361e-10,2.602643e-09\n",
      "Iteration 75165: loss = 1.2625502e-10,2.6023905e-09\n",
      "Iteration 75170: loss = 1.2624643e-10,2.6021394e-09\n",
      "Iteration 75175: loss = 1.2623623e-10,2.6018923e-09\n",
      "Iteration 75180: loss = 1.2622564e-10,2.6016473e-09\n",
      "Iteration 75185: loss = 1.2621355e-10,2.6014075e-09\n",
      "Iteration 75190: loss = 1.2620088e-10,2.6011688e-09\n",
      "Iteration 75195: loss = 1.2618807e-10,2.60093e-09\n",
      "Iteration 75200: loss = 1.2617525e-10,2.600692e-09\n",
      "Iteration 75205: loss = 1.2616229e-10,2.6004559e-09\n",
      "Iteration 75210: loss = 1.261488e-10,2.60022e-09\n",
      "Iteration 75215: loss = 1.261345e-10,2.5999856e-09\n",
      "Iteration 75220: loss = 1.2611966e-10,2.5997537e-09\n",
      "Iteration 75225: loss = 1.2610464e-10,2.5995235e-09\n",
      "Iteration 75230: loss = 1.2608896e-10,2.5992952e-09\n",
      "Iteration 75235: loss = 1.260723e-10,2.59907e-09\n",
      "Iteration 75240: loss = 1.2605574e-10,2.5988454e-09\n",
      "Iteration 75245: loss = 1.2603862e-10,2.5986209e-09\n",
      "Iteration 75250: loss = 1.2602162e-10,2.5983962e-09\n",
      "Iteration 75255: loss = 1.2600447e-10,2.5981737e-09\n",
      "Iteration 75260: loss = 1.2598741e-10,2.5979505e-09\n",
      "Iteration 75265: loss = 1.2597003e-10,2.5977283e-09\n",
      "Iteration 75270: loss = 1.2595275e-10,2.5975062e-09\n",
      "Iteration 75275: loss = 1.2593535e-10,2.5972842e-09\n",
      "Iteration 75280: loss = 1.2591787e-10,2.5970632e-09\n",
      "Iteration 75285: loss = 1.2589989e-10,2.5968423e-09\n",
      "Iteration 75290: loss = 1.2588224e-10,2.5966218e-09\n",
      "Iteration 75295: loss = 1.2586433e-10,2.596401e-09\n",
      "Iteration 75300: loss = 1.2584661e-10,2.5961815e-09\n",
      "Iteration 75305: loss = 1.2582853e-10,2.5959617e-09\n",
      "Iteration 75310: loss = 1.2581171e-10,2.5957385e-09\n",
      "Iteration 75315: loss = 1.2580317e-10,2.5954925e-09\n",
      "Iteration 75320: loss = 1.2579997e-10,2.5952294e-09\n",
      "Iteration 75325: loss = 1.2579975e-10,2.5949567e-09\n",
      "Iteration 75330: loss = 1.2580079e-10,2.5946814e-09\n",
      "Iteration 75335: loss = 1.2580119e-10,2.5944071e-09\n",
      "Iteration 75340: loss = 1.2580155e-10,2.5941347e-09\n",
      "Iteration 75345: loss = 1.258011e-10,2.5938631e-09\n",
      "Iteration 75350: loss = 1.2579815e-10,2.5936002e-09\n",
      "Iteration 75355: loss = 1.2579408e-10,2.5933413e-09\n",
      "Iteration 75360: loss = 1.2578964e-10,2.593084e-09\n",
      "Iteration 75365: loss = 1.2578319e-10,2.5928308e-09\n",
      "Iteration 75370: loss = 1.2577635e-10,2.59258e-09\n",
      "Iteration 75375: loss = 1.2576887e-10,2.592331e-09\n",
      "Iteration 75380: loss = 1.2576046e-10,2.5920848e-09\n",
      "Iteration 75385: loss = 1.2575059e-10,2.5918445e-09\n",
      "Iteration 75390: loss = 1.2573895e-10,2.5916078e-09\n",
      "Iteration 75395: loss = 1.2572755e-10,2.5913718e-09\n",
      "Iteration 75400: loss = 1.2571556e-10,2.5911375e-09\n",
      "Iteration 75405: loss = 1.2570246e-10,2.5909062e-09\n",
      "Iteration 75410: loss = 1.2568868e-10,2.5906777e-09\n",
      "Iteration 75415: loss = 1.2567462e-10,2.5904496e-09\n",
      "Iteration 75420: loss = 1.2566075e-10,2.5902218e-09\n",
      "Iteration 75425: loss = 1.2564633e-10,2.5899956e-09\n",
      "Iteration 75430: loss = 1.2563138e-10,2.5897695e-09\n",
      "Iteration 75435: loss = 1.2561646e-10,2.5895437e-09\n",
      "Iteration 75440: loss = 1.2560139e-10,2.5893194e-09\n",
      "Iteration 75445: loss = 1.255861e-10,2.5890967e-09\n",
      "Iteration 75450: loss = 1.2556937e-10,2.5888771e-09\n",
      "Iteration 75455: loss = 1.2555275e-10,2.5886595e-09\n",
      "Iteration 75460: loss = 1.2553568e-10,2.5884415e-09\n",
      "Iteration 75465: loss = 1.2551805e-10,2.5882265e-09\n",
      "Iteration 75470: loss = 1.254996e-10,2.588012e-09\n",
      "Iteration 75475: loss = 1.2548132e-10,2.5877984e-09\n",
      "Iteration 75480: loss = 1.2546329e-10,2.5875848e-09\n",
      "Iteration 75485: loss = 1.2544576e-10,2.587369e-09\n",
      "Iteration 75490: loss = 1.2542811e-10,2.5871534e-09\n",
      "Iteration 75495: loss = 1.2541664e-10,2.5869205e-09\n",
      "Iteration 75500: loss = 1.2541239e-10,2.5866658e-09\n",
      "Iteration 75505: loss = 1.2541147e-10,2.5864013e-09\n",
      "Iteration 75510: loss = 1.2541185e-10,2.5861318e-09\n",
      "Iteration 75515: loss = 1.25413e-10,2.5858609e-09\n",
      "Iteration 75520: loss = 1.2541353e-10,2.585592e-09\n",
      "Iteration 75525: loss = 1.2541337e-10,2.5853257e-09\n",
      "Iteration 75530: loss = 1.2541156e-10,2.5850635e-09\n",
      "Iteration 75535: loss = 1.2540954e-10,2.5848026e-09\n",
      "Iteration 75540: loss = 1.254059e-10,2.5845468e-09\n",
      "Iteration 75545: loss = 1.2540011e-10,2.5842979e-09\n",
      "Iteration 75550: loss = 1.2539365e-10,2.584051e-09\n",
      "Iteration 75555: loss = 1.2538597e-10,2.5838072e-09\n",
      "Iteration 75560: loss = 1.2537739e-10,2.5835671e-09\n",
      "Iteration 75565: loss = 1.2536837e-10,2.5833273e-09\n",
      "Iteration 75570: loss = 1.2535882e-10,2.5830909e-09\n",
      "Iteration 75575: loss = 1.2534822e-10,2.5828566e-09\n",
      "Iteration 75580: loss = 1.2533675e-10,2.5826252e-09\n",
      "Iteration 75585: loss = 1.2532367e-10,2.5823994e-09\n",
      "Iteration 75590: loss = 1.2531064e-10,2.5821743e-09\n",
      "Iteration 75595: loss = 1.2529695e-10,2.5819489e-09\n",
      "Iteration 75600: loss = 1.2528324e-10,2.5817253e-09\n",
      "Iteration 75605: loss = 1.25269e-10,2.581503e-09\n",
      "Iteration 75610: loss = 1.2525374e-10,2.5812836e-09\n",
      "Iteration 75615: loss = 1.252385e-10,2.5810656e-09\n",
      "Iteration 75620: loss = 1.2522282e-10,2.5808475e-09\n",
      "Iteration 75625: loss = 1.2520703e-10,2.5806308e-09\n",
      "Iteration 75630: loss = 1.2519089e-10,2.580414e-09\n",
      "Iteration 75635: loss = 1.2517483e-10,2.5801978e-09\n",
      "Iteration 75640: loss = 1.2515883e-10,2.5799824e-09\n",
      "Iteration 75645: loss = 1.2514262e-10,2.5797675e-09\n",
      "Iteration 75650: loss = 1.2512603e-10,2.579553e-09\n",
      "Iteration 75655: loss = 1.2510941e-10,2.5793383e-09\n",
      "Iteration 75660: loss = 1.2384456e-10,2.5791038e-09\n",
      "Iteration 75665: loss = 1.2510624e-10,2.5788207e-09\n",
      "Iteration 75670: loss = 1.2384695e-10,2.5785694e-09\n",
      "Iteration 75675: loss = 1.2510534e-10,2.5782967e-09\n",
      "Iteration 75680: loss = 1.2510504e-10,2.5780333e-09\n",
      "Iteration 75685: loss = 1.2510552e-10,2.5777696e-09\n",
      "Iteration 75690: loss = 1.238495e-10,2.5775084e-09\n",
      "Iteration 75695: loss = 1.2510665e-10,2.5772406e-09\n",
      "Iteration 75700: loss = 1.2510779e-10,2.5769744e-09\n",
      "Iteration 75705: loss = 1.238515e-10,2.5767148e-09\n",
      "Iteration 75710: loss = 1.2385103e-10,2.5764526e-09\n",
      "Iteration 75715: loss = 1.2385214e-10,2.576188e-09\n",
      "Iteration 75720: loss = 1.2385336e-10,2.5759221e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75725: loss = 1.2385397e-10,2.5756588e-09\n",
      "Iteration 75730: loss = 1.2385439e-10,2.5753946e-09\n",
      "Iteration 75735: loss = 1.2385475e-10,2.575132e-09\n",
      "Iteration 75740: loss = 1.2385502e-10,2.5748694e-09\n",
      "Iteration 75745: loss = 1.2385624e-10,2.5746056e-09\n",
      "Iteration 75750: loss = 1.2385508e-10,2.5743463e-09\n",
      "Iteration 75755: loss = 1.238575e-10,2.5740783e-09\n",
      "Iteration 75760: loss = 1.2511397e-10,2.5738127e-09\n",
      "Iteration 75765: loss = 1.2385855e-10,2.573552e-09\n",
      "Iteration 75770: loss = 1.2385803e-10,2.5732936e-09\n",
      "Iteration 75775: loss = 1.2385994e-10,2.573027e-09\n",
      "Iteration 75780: loss = 1.2385983e-10,2.5727667e-09\n",
      "Iteration 75785: loss = 1.238608e-10,2.5725035e-09\n",
      "Iteration 75790: loss = 1.2386009e-10,2.5722453e-09\n",
      "Iteration 75795: loss = 1.238597e-10,2.5719855e-09\n",
      "Iteration 75800: loss = 1.2386299e-10,2.5717153e-09\n",
      "Iteration 75805: loss = 1.2511926e-10,2.571452e-09\n",
      "Iteration 75810: loss = 1.2386285e-10,2.571196e-09\n",
      "Iteration 75815: loss = 1.2386482e-10,2.5709308e-09\n",
      "Iteration 75820: loss = 1.2386393e-10,2.570674e-09\n",
      "Iteration 75825: loss = 1.238643e-10,2.5704134e-09\n",
      "Iteration 75830: loss = 1.2386549e-10,2.57015e-09\n",
      "Iteration 75835: loss = 1.2386629e-10,2.5698885e-09\n",
      "Iteration 75840: loss = 1.2386701e-10,2.569627e-09\n",
      "Iteration 75845: loss = 1.2386737e-10,2.569367e-09\n",
      "Iteration 75850: loss = 1.2386696e-10,2.5691091e-09\n",
      "Iteration 75855: loss = 1.2386499e-10,2.5688567e-09\n",
      "Iteration 75860: loss = 1.2386188e-10,2.568607e-09\n",
      "Iteration 75865: loss = 1.2385683e-10,2.5683637e-09\n",
      "Iteration 75870: loss = 1.2385108e-10,2.5681235e-09\n",
      "Iteration 75875: loss = 1.2384471e-10,2.5678832e-09\n",
      "Iteration 75880: loss = 1.238372e-10,2.5676474e-09\n",
      "Iteration 75885: loss = 1.238289e-10,2.567414e-09\n",
      "Iteration 75890: loss = 1.2382069e-10,2.567181e-09\n",
      "Iteration 75895: loss = 1.2381145e-10,2.5669502e-09\n",
      "Iteration 75900: loss = 1.238001e-10,2.5667277e-09\n",
      "Iteration 75905: loss = 1.2378815e-10,2.566505e-09\n",
      "Iteration 75910: loss = 1.2377631e-10,2.5662836e-09\n",
      "Iteration 75915: loss = 1.2376422e-10,2.5660625e-09\n",
      "Iteration 75920: loss = 1.2375201e-10,2.5658426e-09\n",
      "Iteration 75925: loss = 1.2373812e-10,2.5656262e-09\n",
      "Iteration 75930: loss = 1.2248423e-10,2.565394e-09\n",
      "Iteration 75935: loss = 1.2373723e-10,2.5651157e-09\n",
      "Iteration 75940: loss = 1.2373422e-10,2.564868e-09\n",
      "Iteration 75945: loss = 1.2373022e-10,2.5646232e-09\n",
      "Iteration 75950: loss = 1.2373043e-10,2.5643656e-09\n",
      "Iteration 75955: loss = 1.2373085e-10,2.5641085e-09\n",
      "Iteration 75960: loss = 1.2372882e-10,2.5638578e-09\n",
      "Iteration 75965: loss = 1.2372948e-10,2.5636004e-09\n",
      "Iteration 75970: loss = 1.2247985e-10,2.5633555e-09\n",
      "Iteration 75975: loss = 1.2372625e-10,2.563097e-09\n",
      "Iteration 75980: loss = 1.2372718e-10,2.5628388e-09\n",
      "Iteration 75985: loss = 1.2247615e-10,2.5625986e-09\n",
      "Iteration 75990: loss = 1.2372696e-10,2.5623288e-09\n",
      "Iteration 75995: loss = 1.2247432e-10,2.5620928e-09\n",
      "Iteration 76000: loss = 1.237233e-10,2.561828e-09\n",
      "Iteration 76005: loss = 1.2372146e-10,2.5615776e-09\n",
      "Iteration 76010: loss = 1.2372274e-10,2.5613183e-09\n",
      "Iteration 76015: loss = 1.2247262e-10,2.5610767e-09\n",
      "Iteration 76020: loss = 1.2372114e-10,2.560814e-09\n",
      "Iteration 76025: loss = 1.2371798e-10,2.5605689e-09\n",
      "Iteration 76030: loss = 1.2371783e-10,2.5603144e-09\n",
      "Iteration 76035: loss = 1.2371694e-10,2.560062e-09\n",
      "Iteration 76040: loss = 1.2371555e-10,2.5598121e-09\n",
      "Iteration 76045: loss = 1.2371956e-10,2.5595457e-09\n",
      "Iteration 76050: loss = 1.2371747e-10,2.5592979e-09\n",
      "Iteration 76055: loss = 1.2246683e-10,2.5590576e-09\n",
      "Iteration 76060: loss = 1.2371838e-10,2.5587874e-09\n",
      "Iteration 76065: loss = 1.237115e-10,2.5585536e-09\n",
      "Iteration 76070: loss = 1.2371222e-10,2.5582976e-09\n",
      "Iteration 76075: loss = 1.2370976e-10,2.5580515e-09\n",
      "Iteration 76080: loss = 1.2371225e-10,2.5577904e-09\n",
      "Iteration 76085: loss = 1.2370932e-10,2.5575455e-09\n",
      "Iteration 76090: loss = 1.2371024e-10,2.55729e-09\n",
      "Iteration 76095: loss = 1.2370645e-10,2.5570488e-09\n",
      "Iteration 76100: loss = 1.2370725e-10,2.5567926e-09\n",
      "Iteration 76105: loss = 1.2370781e-10,2.5565379e-09\n",
      "Iteration 76110: loss = 1.2370395e-10,2.556296e-09\n",
      "Iteration 76115: loss = 1.224555e-10,2.5560518e-09\n",
      "Iteration 76120: loss = 1.2370417e-10,2.5557894e-09\n",
      "Iteration 76125: loss = 1.2370151e-10,2.5555467e-09\n",
      "Iteration 76130: loss = 1.2245395e-10,2.5552989e-09\n",
      "Iteration 76135: loss = 1.2369966e-10,2.5550468e-09\n",
      "Iteration 76140: loss = 1.2370038e-10,2.5547928e-09\n",
      "Iteration 76145: loss = 1.2245115e-10,2.5545508e-09\n",
      "Iteration 76150: loss = 1.23699e-10,2.5542926e-09\n",
      "Iteration 76155: loss = 1.2244881e-10,2.5540534e-09\n",
      "Iteration 76160: loss = 1.2369683e-10,2.5537963e-09\n",
      "Iteration 76165: loss = 1.2369418e-10,2.5535516e-09\n",
      "Iteration 76170: loss = 1.2369372e-10,2.553301e-09\n",
      "Iteration 76175: loss = 1.2369268e-10,2.5530533e-09\n",
      "Iteration 76180: loss = 1.2369164e-10,2.5528055e-09\n",
      "Iteration 76185: loss = 1.2369224e-10,2.5525524e-09\n",
      "Iteration 76190: loss = 1.2244021e-10,2.5523197e-09\n",
      "Iteration 76195: loss = 1.2369004e-10,2.552057e-09\n",
      "Iteration 76200: loss = 1.2243837e-10,2.5518243e-09\n",
      "Iteration 76205: loss = 1.2368741e-10,2.5515634e-09\n",
      "Iteration 76210: loss = 1.2243823e-10,2.5513218e-09\n",
      "Iteration 76215: loss = 1.2243746e-10,2.5510751e-09\n",
      "Iteration 76220: loss = 1.2243724e-10,2.550824e-09\n",
      "Iteration 76225: loss = 1.2368367e-10,2.5505722e-09\n",
      "Iteration 76230: loss = 1.2368266e-10,2.5503248e-09\n",
      "Iteration 76235: loss = 1.2243317e-10,2.5500864e-09\n",
      "Iteration 76240: loss = 1.2243238e-10,2.5498383e-09\n",
      "Iteration 76245: loss = 1.2243358e-10,2.5495854e-09\n",
      "Iteration 76250: loss = 1.2243218e-10,2.5493399e-09\n",
      "Iteration 76255: loss = 1.22428e-10,2.549103e-09\n",
      "Iteration 76260: loss = 1.2243065e-10,2.548845e-09\n",
      "Iteration 76265: loss = 1.2242568e-10,2.5486104e-09\n",
      "Iteration 76270: loss = 1.2242247e-10,2.5483708e-09\n",
      "Iteration 76275: loss = 1.2367414e-10,2.548103e-09\n",
      "Iteration 76280: loss = 1.2242456e-10,2.5478655e-09\n",
      "Iteration 76285: loss = 1.2242551e-10,2.5476132e-09\n",
      "Iteration 76290: loss = 1.2242471e-10,2.5473672e-09\n",
      "Iteration 76295: loss = 1.2242411e-10,2.5471207e-09\n",
      "Iteration 76300: loss = 1.2242188e-10,2.5468774e-09\n",
      "Iteration 76305: loss = 1.2242209e-10,2.5466287e-09\n",
      "Iteration 76310: loss = 1.2242019e-10,2.5463855e-09\n",
      "Iteration 76315: loss = 1.2241923e-10,2.5461402e-09\n",
      "Iteration 76320: loss = 1.2241731e-10,2.5458977e-09\n",
      "Iteration 76325: loss = 1.2366476e-10,2.5456435e-09\n",
      "Iteration 76330: loss = 1.2241437e-10,2.54541e-09\n",
      "Iteration 76335: loss = 1.224124e-10,2.5451679e-09\n",
      "Iteration 76340: loss = 1.2241334e-10,2.5449156e-09\n",
      "Iteration 76345: loss = 1.2241468e-10,2.5446651e-09\n",
      "Iteration 76350: loss = 1.2240915e-10,2.544434e-09\n",
      "Iteration 76355: loss = 1.2240392e-10,2.5442017e-09\n",
      "Iteration 76360: loss = 1.2239994e-10,2.5439657e-09\n",
      "Iteration 76365: loss = 1.2239593e-10,2.5437301e-09\n",
      "Iteration 76370: loss = 1.2239183e-10,2.5434947e-09\n",
      "Iteration 76375: loss = 1.2238766e-10,2.5432594e-09\n",
      "Iteration 76380: loss = 1.223832e-10,2.5430258e-09\n",
      "Iteration 76385: loss = 1.223782e-10,2.5427935e-09\n",
      "Iteration 76390: loss = 1.2237246e-10,2.5425633e-09\n",
      "Iteration 76395: loss = 1.2236466e-10,2.5423412e-09\n",
      "Iteration 76400: loss = 1.2235633e-10,2.5421194e-09\n",
      "Iteration 76405: loss = 1.2234785e-10,2.5418971e-09\n",
      "Iteration 76410: loss = 1.2233829e-10,2.5416798e-09\n",
      "Iteration 76415: loss = 1.223282e-10,2.541463e-09\n",
      "Iteration 76420: loss = 1.2231778e-10,2.5412479e-09\n",
      "Iteration 76425: loss = 1.2230747e-10,2.5410334e-09\n",
      "Iteration 76430: loss = 1.222963e-10,2.540819e-09\n",
      "Iteration 76435: loss = 1.2228428e-10,2.5406086e-09\n",
      "Iteration 76440: loss = 1.222716e-10,2.5404019e-09\n",
      "Iteration 76445: loss = 1.2225779e-10,2.540197e-09\n",
      "Iteration 76450: loss = 1.2224423e-10,2.539992e-09\n",
      "Iteration 76455: loss = 1.2223057e-10,2.5397875e-09\n",
      "Iteration 76460: loss = 1.2221646e-10,2.5395832e-09\n",
      "Iteration 76465: loss = 1.2220258e-10,2.5393798e-09\n",
      "Iteration 76470: loss = 1.221881e-10,2.5391782e-09\n",
      "Iteration 76475: loss = 1.2217279e-10,2.5389781e-09\n",
      "Iteration 76480: loss = 1.2215773e-10,2.5387779e-09\n",
      "Iteration 76485: loss = 1.2214255e-10,2.5385785e-09\n",
      "Iteration 76490: loss = 1.2212734e-10,2.5383793e-09\n",
      "Iteration 76495: loss = 1.2211214e-10,2.5381794e-09\n",
      "Iteration 76500: loss = 1.2209639e-10,2.5379805e-09\n",
      "Iteration 76505: loss = 1.2208116e-10,2.537782e-09\n",
      "Iteration 76510: loss = 1.220657e-10,2.5375833e-09\n",
      "Iteration 76515: loss = 1.2205033e-10,2.5373845e-09\n",
      "Iteration 76520: loss = 1.2203465e-10,2.5371865e-09\n",
      "Iteration 76525: loss = 1.2201813e-10,2.536992e-09\n",
      "Iteration 76530: loss = 1.2200145e-10,2.5367977e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76535: loss = 1.2198474e-10,2.5366045e-09\n",
      "Iteration 76540: loss = 1.2196803e-10,2.5364117e-09\n",
      "Iteration 76545: loss = 1.2195092e-10,2.536218e-09\n",
      "Iteration 76550: loss = 1.2193395e-10,2.5360252e-09\n",
      "Iteration 76555: loss = 1.2191724e-10,2.5358322e-09\n",
      "Iteration 76560: loss = 1.2190036e-10,2.5356397e-09\n",
      "Iteration 76565: loss = 1.2188335e-10,2.535446e-09\n",
      "Iteration 76570: loss = 1.218665e-10,2.5352542e-09\n",
      "Iteration 76575: loss = 1.2185387e-10,2.5350484e-09\n",
      "Iteration 76580: loss = 1.2184928e-10,2.5348192e-09\n",
      "Iteration 76585: loss = 1.2184868e-10,2.534577e-09\n",
      "Iteration 76590: loss = 1.2185024e-10,2.5343305e-09\n",
      "Iteration 76595: loss = 1.2185193e-10,2.5340818e-09\n",
      "Iteration 76600: loss = 1.2185354e-10,2.5338338e-09\n",
      "Iteration 76605: loss = 1.218547e-10,2.533588e-09\n",
      "Iteration 76610: loss = 1.2185462e-10,2.5333453e-09\n",
      "Iteration 76615: loss = 1.218536e-10,2.5331053e-09\n",
      "Iteration 76620: loss = 1.218519e-10,2.5328677e-09\n",
      "Iteration 76625: loss = 1.218478e-10,2.532638e-09\n",
      "Iteration 76630: loss = 1.2184252e-10,2.5324112e-09\n",
      "Iteration 76635: loss = 1.2183672e-10,2.532186e-09\n",
      "Iteration 76640: loss = 1.2182956e-10,2.531966e-09\n",
      "Iteration 76645: loss = 1.218217e-10,2.5317468e-09\n",
      "Iteration 76650: loss = 1.2181377e-10,2.531529e-09\n",
      "Iteration 76655: loss = 1.2180458e-10,2.5313143e-09\n",
      "Iteration 76660: loss = 1.2179478e-10,2.5311022e-09\n",
      "Iteration 76665: loss = 1.217842e-10,2.5308913e-09\n",
      "Iteration 76670: loss = 1.2177252e-10,2.530685e-09\n",
      "Iteration 76675: loss = 1.2176081e-10,2.530479e-09\n",
      "Iteration 76680: loss = 1.2174771e-10,2.530277e-09\n",
      "Iteration 76685: loss = 1.2173446e-10,2.5300753e-09\n",
      "Iteration 76690: loss = 1.2172087e-10,2.529875e-09\n",
      "Iteration 76695: loss = 1.2170735e-10,2.5296742e-09\n",
      "Iteration 76700: loss = 1.2169393e-10,2.5294744e-09\n",
      "Iteration 76705: loss = 1.2168032e-10,2.529273e-09\n",
      "Iteration 76710: loss = 1.216744e-10,2.5290503e-09\n",
      "Iteration 76715: loss = 1.2167396e-10,2.528811e-09\n",
      "Iteration 76720: loss = 1.2167695e-10,2.5285611e-09\n",
      "Iteration 76725: loss = 1.2168193e-10,2.5283062e-09\n",
      "Iteration 76730: loss = 1.2168623e-10,2.5280518e-09\n",
      "Iteration 76735: loss = 1.2169014e-10,2.5277989e-09\n",
      "Iteration 76740: loss = 1.2169156e-10,2.5275548e-09\n",
      "Iteration 76745: loss = 1.2169021e-10,2.5273184e-09\n",
      "Iteration 76750: loss = 1.216889e-10,2.5270823e-09\n",
      "Iteration 76755: loss = 1.2168706e-10,2.5268485e-09\n",
      "Iteration 76760: loss = 1.216847e-10,2.5266156e-09\n",
      "Iteration 76765: loss = 1.2168044e-10,2.5263873e-09\n",
      "Iteration 76770: loss = 1.2167635e-10,2.5261602e-09\n",
      "Iteration 76775: loss = 1.2167116e-10,2.525935e-09\n",
      "Iteration 76780: loss = 1.2166392e-10,2.5257183e-09\n",
      "Iteration 76785: loss = 1.2165603e-10,2.525502e-09\n",
      "Iteration 76790: loss = 1.2164818e-10,2.5252873e-09\n",
      "Iteration 76795: loss = 1.2163849e-10,2.5250766e-09\n",
      "Iteration 76800: loss = 1.216286e-10,2.5248674e-09\n",
      "Iteration 76805: loss = 1.216185e-10,2.5246578e-09\n",
      "Iteration 76810: loss = 1.2160795e-10,2.5244502e-09\n",
      "Iteration 76815: loss = 1.2159736e-10,2.524243e-09\n",
      "Iteration 76820: loss = 1.2158562e-10,2.5240396e-09\n",
      "Iteration 76825: loss = 1.2157354e-10,2.523837e-09\n",
      "Iteration 76830: loss = 1.2155993e-10,2.5236382e-09\n",
      "Iteration 76835: loss = 1.2156176e-10,2.5233953e-09\n",
      "Iteration 76840: loss = 1.2156458e-10,2.5231484e-09\n",
      "Iteration 76845: loss = 1.2032474e-10,2.5229197e-09\n",
      "Iteration 76850: loss = 1.2156555e-10,2.5226687e-09\n",
      "Iteration 76855: loss = 1.2156369e-10,2.5224358e-09\n",
      "Iteration 76860: loss = 1.2156459e-10,2.5221953e-09\n",
      "Iteration 76865: loss = 1.2156888e-10,2.5219435e-09\n",
      "Iteration 76870: loss = 1.2032969e-10,2.521714e-09\n",
      "Iteration 76875: loss = 1.2156874e-10,2.5214684e-09\n",
      "Iteration 76880: loss = 1.2033237e-10,2.5212308e-09\n",
      "Iteration 76885: loss = 1.2157243e-10,2.5209819e-09\n",
      "Iteration 76890: loss = 1.215716e-10,2.520747e-09\n",
      "Iteration 76895: loss = 1.2157401e-10,2.5205016e-09\n",
      "Iteration 76900: loss = 1.2033512e-10,2.5202722e-09\n",
      "Iteration 76905: loss = 1.2157647e-10,2.5200206e-09\n",
      "Iteration 76910: loss = 1.2157542e-10,2.5197862e-09\n",
      "Iteration 76915: loss = 1.2157682e-10,2.5195437e-09\n",
      "Iteration 76920: loss = 1.2157857e-10,2.5193019e-09\n",
      "Iteration 76925: loss = 1.2034061e-10,2.5190712e-09\n",
      "Iteration 76930: loss = 1.2158184e-10,2.5188183e-09\n",
      "Iteration 76935: loss = 1.2034192e-10,2.518593e-09\n",
      "Iteration 76940: loss = 1.2158408e-10,2.5183398e-09\n",
      "Iteration 76945: loss = 1.2034591e-10,2.518108e-09\n",
      "Iteration 76950: loss = 1.2034572e-10,2.5178712e-09\n",
      "Iteration 76955: loss = 1.2158526e-10,2.5176259e-09\n",
      "Iteration 76960: loss = 1.215869e-10,2.5173845e-09\n",
      "Iteration 76965: loss = 1.2158688e-10,2.5171483e-09\n",
      "Iteration 76970: loss = 1.203511e-10,2.5169105e-09\n",
      "Iteration 76975: loss = 1.2035228e-10,2.5166709e-09\n",
      "Iteration 76980: loss = 1.2035305e-10,2.516433e-09\n",
      "Iteration 76985: loss = 1.2035441e-10,2.5161924e-09\n",
      "Iteration 76990: loss = 1.203557e-10,2.5159532e-09\n",
      "Iteration 76995: loss = 1.2159325e-10,2.5157147e-09\n",
      "Iteration 77000: loss = 1.2159478e-10,2.5154725e-09\n",
      "Iteration 77005: loss = 1.2035888e-10,2.515237e-09\n",
      "Iteration 77010: loss = 1.215974e-10,2.5149958e-09\n",
      "Iteration 77015: loss = 1.2035993e-10,2.514764e-09\n",
      "Iteration 77020: loss = 1.2036022e-10,2.5145277e-09\n",
      "Iteration 77025: loss = 1.2036021e-10,2.5142923e-09\n",
      "Iteration 77030: loss = 1.2036373e-10,2.5140474e-09\n",
      "Iteration 77035: loss = 1.2036483e-10,2.5138078e-09\n",
      "Iteration 77040: loss = 1.2036266e-10,2.513581e-09\n",
      "Iteration 77045: loss = 1.2160396e-10,2.5133313e-09\n",
      "Iteration 77050: loss = 1.203659e-10,2.5131015e-09\n",
      "Iteration 77055: loss = 1.216074e-10,2.5128515e-09\n",
      "Iteration 77060: loss = 1.203691e-10,2.5126223e-09\n",
      "Iteration 77065: loss = 1.2037128e-10,2.5123812e-09\n",
      "Iteration 77070: loss = 1.2037175e-10,2.5121454e-09\n",
      "Iteration 77075: loss = 1.203734e-10,2.5119073e-09\n",
      "Iteration 77080: loss = 1.2037439e-10,2.5116704e-09\n",
      "Iteration 77085: loss = 1.2037572e-10,2.5114324e-09\n",
      "Iteration 77090: loss = 1.2037515e-10,2.5112008e-09\n",
      "Iteration 77095: loss = 1.2037826e-10,2.5109563e-09\n",
      "Iteration 77100: loss = 1.2037844e-10,2.5107232e-09\n",
      "Iteration 77105: loss = 1.2161767e-10,2.5104816e-09\n",
      "Iteration 77110: loss = 1.2037842e-10,2.510256e-09\n",
      "Iteration 77115: loss = 1.2038005e-10,2.5100175e-09\n",
      "Iteration 77120: loss = 1.203835e-10,2.5097746e-09\n",
      "Iteration 77125: loss = 1.2038207e-10,2.5095448e-09\n",
      "Iteration 77130: loss = 1.2037989e-10,2.509318e-09\n",
      "Iteration 77135: loss = 1.2037976e-10,2.509086e-09\n",
      "Iteration 77140: loss = 1.2038075e-10,2.5088491e-09\n",
      "Iteration 77145: loss = 1.2038086e-10,2.5086169e-09\n",
      "Iteration 77150: loss = 1.2038076e-10,2.5083846e-09\n",
      "Iteration 77155: loss = 1.2037883e-10,2.5081581e-09\n",
      "Iteration 77160: loss = 1.2037689e-10,2.5079316e-09\n",
      "Iteration 77165: loss = 1.2037392e-10,2.507707e-09\n",
      "Iteration 77170: loss = 1.2036994e-10,2.5074864e-09\n",
      "Iteration 77175: loss = 1.2036562e-10,2.5072668e-09\n",
      "Iteration 77180: loss = 1.2036037e-10,2.5070506e-09\n",
      "Iteration 77185: loss = 1.2035291e-10,2.5068403e-09\n",
      "Iteration 77190: loss = 1.2034544e-10,2.5066307e-09\n",
      "Iteration 77195: loss = 1.2033767e-10,2.5064222e-09\n",
      "Iteration 77200: loss = 1.2032836e-10,2.506217e-09\n",
      "Iteration 77205: loss = 1.203193e-10,2.5060132e-09\n",
      "Iteration 77210: loss = 1.2030964e-10,2.5058093e-09\n",
      "Iteration 77215: loss = 1.2030014e-10,2.5056064e-09\n",
      "Iteration 77220: loss = 1.2028954e-10,2.5054068e-09\n",
      "Iteration 77225: loss = 1.2027852e-10,2.5052065e-09\n",
      "Iteration 77230: loss = 1.2026695e-10,2.5050102e-09\n",
      "Iteration 77235: loss = 1.2025486e-10,2.5048155e-09\n",
      "Iteration 77240: loss = 1.2024239e-10,2.5046214e-09\n",
      "Iteration 77245: loss = 1.2022985e-10,2.5044284e-09\n",
      "Iteration 77250: loss = 1.202165e-10,2.5042366e-09\n",
      "Iteration 77255: loss = 1.2020279e-10,2.5040463e-09\n",
      "Iteration 77260: loss = 1.2018904e-10,2.5038562e-09\n",
      "Iteration 77265: loss = 1.2017524e-10,2.503667e-09\n",
      "Iteration 77270: loss = 1.2016145e-10,2.503477e-09\n",
      "Iteration 77275: loss = 1.2014773e-10,2.5032874e-09\n",
      "Iteration 77280: loss = 1.2013386e-10,2.5030977e-09\n",
      "Iteration 77285: loss = 1.2011975e-10,2.5029097e-09\n",
      "Iteration 77290: loss = 1.2010594e-10,2.5027187e-09\n",
      "Iteration 77295: loss = 1.2009212e-10,2.5025302e-09\n",
      "Iteration 77300: loss = 1.2007828e-10,2.5023414e-09\n",
      "Iteration 77305: loss = 1.2006385e-10,2.5021545e-09\n",
      "Iteration 77310: loss = 1.2004846e-10,2.5019693e-09\n",
      "Iteration 77315: loss = 1.2003228e-10,2.5017879e-09\n",
      "Iteration 77320: loss = 1.20016e-10,2.5016065e-09\n",
      "Iteration 77325: loss = 1.1999986e-10,2.5014253e-09\n",
      "Iteration 77330: loss = 1.1998388e-10,2.501244e-09\n",
      "Iteration 77335: loss = 1.1996751e-10,2.5010625e-09\n",
      "Iteration 77340: loss = 1.1995174e-10,2.5008808e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77345: loss = 1.1993545e-10,2.5006996e-09\n",
      "Iteration 77350: loss = 1.1991931e-10,2.5005185e-09\n",
      "Iteration 77355: loss = 1.199034e-10,2.5003368e-09\n",
      "Iteration 77360: loss = 1.1988706e-10,2.5001552e-09\n",
      "Iteration 77365: loss = 1.198714e-10,2.4999744e-09\n",
      "Iteration 77370: loss = 1.1985529e-10,2.4997928e-09\n",
      "Iteration 77375: loss = 1.1983937e-10,2.4996116e-09\n",
      "Iteration 77380: loss = 1.1982305e-10,2.4994298e-09\n",
      "Iteration 77385: loss = 1.1980696e-10,2.4992495e-09\n",
      "Iteration 77390: loss = 1.1979129e-10,2.4990685e-09\n",
      "Iteration 77395: loss = 1.197752e-10,2.4988878e-09\n",
      "Iteration 77400: loss = 1.197591e-10,2.498706e-09\n",
      "Iteration 77405: loss = 1.1974323e-10,2.4985258e-09\n",
      "Iteration 77410: loss = 1.1850583e-10,2.4983278e-09\n",
      "Iteration 77415: loss = 1.1973707e-10,2.4980857e-09\n",
      "Iteration 77420: loss = 1.1850515e-10,2.4978724e-09\n",
      "Iteration 77425: loss = 1.1973529e-10,2.4976334e-09\n",
      "Iteration 77430: loss = 1.18503e-10,2.4974214e-09\n",
      "Iteration 77435: loss = 1.1973163e-10,2.4971873e-09\n",
      "Iteration 77440: loss = 1.1972984e-10,2.4969649e-09\n",
      "Iteration 77445: loss = 1.1972871e-10,2.4967393e-09\n",
      "Iteration 77450: loss = 1.19729e-10,2.49651e-09\n",
      "Iteration 77455: loss = 1.19728e-10,2.4962858e-09\n",
      "Iteration 77460: loss = 1.1972591e-10,2.4960631e-09\n",
      "Iteration 77465: loss = 1.1972763e-10,2.4958307e-09\n",
      "Iteration 77470: loss = 1.1849682e-10,2.4956144e-09\n",
      "Iteration 77475: loss = 1.1849609e-10,2.4953888e-09\n",
      "Iteration 77480: loss = 1.1972277e-10,2.4951614e-09\n",
      "Iteration 77485: loss = 1.1972273e-10,2.4949343e-09\n",
      "Iteration 77490: loss = 1.1972086e-10,2.4947129e-09\n",
      "Iteration 77495: loss = 1.1971997e-10,2.4944882e-09\n",
      "Iteration 77500: loss = 1.1971947e-10,2.4942621e-09\n",
      "Iteration 77505: loss = 1.1971889e-10,2.494036e-09\n",
      "Iteration 77510: loss = 1.197177e-10,2.493813e-09\n",
      "Iteration 77515: loss = 1.1971711e-10,2.493588e-09\n",
      "Iteration 77520: loss = 1.1848714e-10,2.4933695e-09\n",
      "Iteration 77525: loss = 1.1971496e-10,2.4931412e-09\n",
      "Iteration 77530: loss = 1.1971461e-10,2.4929148e-09\n",
      "Iteration 77535: loss = 1.1848506e-10,2.4926954e-09\n",
      "Iteration 77540: loss = 1.1971325e-10,2.4924658e-09\n",
      "Iteration 77545: loss = 1.197117e-10,2.4922433e-09\n",
      "Iteration 77550: loss = 1.1848157e-10,2.4920275e-09\n",
      "Iteration 77555: loss = 1.1970931e-10,2.4917974e-09\n",
      "Iteration 77560: loss = 1.1848052e-10,2.4915765e-09\n",
      "Iteration 77565: loss = 1.1847932e-10,2.4913545e-09\n",
      "Iteration 77570: loss = 1.1847852e-10,2.4911315e-09\n",
      "Iteration 77575: loss = 1.1970537e-10,2.490905e-09\n",
      "Iteration 77580: loss = 1.1847667e-10,2.4906848e-09\n",
      "Iteration 77585: loss = 1.1847447e-10,2.4904654e-09\n",
      "Iteration 77590: loss = 1.1970332e-10,2.4902334e-09\n",
      "Iteration 77595: loss = 1.1970337e-10,2.490007e-09\n",
      "Iteration 77600: loss = 1.1847236e-10,2.4897944e-09\n",
      "Iteration 77605: loss = 1.1970146e-10,2.4895626e-09\n",
      "Iteration 77610: loss = 1.1969876e-10,2.489344e-09\n",
      "Iteration 77615: loss = 1.1846922e-10,2.4891271e-09\n",
      "Iteration 77620: loss = 1.1969768e-10,2.4888973e-09\n",
      "Iteration 77625: loss = 1.1969646e-10,2.4886753e-09\n",
      "Iteration 77630: loss = 1.1846683e-10,2.4884588e-09\n",
      "Iteration 77635: loss = 1.1969385e-10,2.4882334e-09\n",
      "Iteration 77640: loss = 1.1846589e-10,2.488011e-09\n",
      "Iteration 77645: loss = 1.1969228e-10,2.4877882e-09\n",
      "Iteration 77650: loss = 1.1969155e-10,2.4875657e-09\n",
      "Iteration 77655: loss = 1.184628e-10,2.4873463e-09\n",
      "Iteration 77660: loss = 1.1846114e-10,2.4871263e-09\n",
      "Iteration 77665: loss = 1.1845962e-10,2.486907e-09\n",
      "Iteration 77670: loss = 1.1968934e-10,2.4866735e-09\n",
      "Iteration 77675: loss = 1.184559e-10,2.4864684e-09\n",
      "Iteration 77680: loss = 1.1968648e-10,2.486234e-09\n",
      "Iteration 77685: loss = 1.1968526e-10,2.4860123e-09\n",
      "Iteration 77690: loss = 1.1968389e-10,2.4857925e-09\n",
      "Iteration 77695: loss = 1.1845562e-10,2.4855717e-09\n",
      "Iteration 77700: loss = 1.1845443e-10,2.4853524e-09\n",
      "Iteration 77705: loss = 1.1845222e-10,2.4851339e-09\n",
      "Iteration 77710: loss = 1.1845085e-10,2.484915e-09\n",
      "Iteration 77715: loss = 1.1845148e-10,2.4846887e-09\n",
      "Iteration 77720: loss = 1.1845014e-10,2.4844695e-09\n",
      "Iteration 77725: loss = 1.196771e-10,2.4842457e-09\n",
      "Iteration 77730: loss = 1.1844883e-10,2.4840263e-09\n",
      "Iteration 77735: loss = 1.184452e-10,2.483813e-09\n",
      "Iteration 77740: loss = 1.1967484e-10,2.483582e-09\n",
      "Iteration 77745: loss = 1.1844604e-10,2.483365e-09\n",
      "Iteration 77750: loss = 1.1844299e-10,2.4831506e-09\n",
      "Iteration 77755: loss = 1.1967223e-10,2.4829196e-09\n",
      "Iteration 77760: loss = 1.1844282e-10,2.4827047e-09\n",
      "Iteration 77765: loss = 1.1844227e-10,2.482483e-09\n",
      "Iteration 77770: loss = 1.1966879e-10,2.4822613e-09\n",
      "Iteration 77775: loss = 1.1843639e-10,2.4820543e-09\n",
      "Iteration 77780: loss = 1.1966812e-10,2.4818172e-09\n",
      "Iteration 77785: loss = 1.1843738e-10,2.4816054e-09\n",
      "Iteration 77790: loss = 1.1843733e-10,2.4813833e-09\n",
      "Iteration 77795: loss = 1.1843604e-10,2.4811653e-09\n",
      "Iteration 77800: loss = 1.1843557e-10,2.4809441e-09\n",
      "Iteration 77805: loss = 1.1843511e-10,2.4807232e-09\n",
      "Iteration 77810: loss = 1.1843347e-10,2.4805058e-09\n",
      "Iteration 77815: loss = 1.1843239e-10,2.4802864e-09\n",
      "Iteration 77820: loss = 1.184322e-10,2.4800646e-09\n",
      "Iteration 77825: loss = 1.184295e-10,2.4798519e-09\n",
      "Iteration 77830: loss = 1.1843028e-10,2.4796265e-09\n",
      "Iteration 77835: loss = 1.1842717e-10,2.4794136e-09\n",
      "Iteration 77840: loss = 1.1842712e-10,2.479193e-09\n",
      "Iteration 77845: loss = 1.184274e-10,2.4789693e-09\n",
      "Iteration 77850: loss = 1.1842373e-10,2.4787594e-09\n",
      "Iteration 77855: loss = 1.1842266e-10,2.4785405e-09\n",
      "Iteration 77860: loss = 1.1842352e-10,2.4783156e-09\n",
      "Iteration 77865: loss = 1.184243e-10,2.4780913e-09\n",
      "Iteration 77870: loss = 1.1841855e-10,2.4778868e-09\n",
      "Iteration 77875: loss = 1.1841389e-10,2.4776803e-09\n",
      "Iteration 77880: loss = 1.1841048e-10,2.4774687e-09\n",
      "Iteration 77885: loss = 1.184076e-10,2.477256e-09\n",
      "Iteration 77890: loss = 1.1840467e-10,2.4770443e-09\n",
      "Iteration 77895: loss = 1.1840168e-10,2.4768314e-09\n",
      "Iteration 77900: loss = 1.18398e-10,2.4766225e-09\n",
      "Iteration 77905: loss = 1.1839309e-10,2.476415e-09\n",
      "Iteration 77910: loss = 1.183884e-10,2.476208e-09\n",
      "Iteration 77915: loss = 1.1838212e-10,2.476007e-09\n",
      "Iteration 77920: loss = 1.1837427e-10,2.4758091e-09\n",
      "Iteration 77925: loss = 1.1836666e-10,2.4756126e-09\n",
      "Iteration 77930: loss = 1.183583e-10,2.4754165e-09\n",
      "Iteration 77935: loss = 1.1834973e-10,2.4752216e-09\n",
      "Iteration 77940: loss = 1.1834013e-10,2.4750297e-09\n",
      "Iteration 77945: loss = 1.183306e-10,2.4748386e-09\n",
      "Iteration 77950: loss = 1.1832091e-10,2.4746474e-09\n",
      "Iteration 77955: loss = 1.1831094e-10,2.4744573e-09\n",
      "Iteration 77960: loss = 1.1830031e-10,2.474268e-09\n",
      "Iteration 77965: loss = 1.1828882e-10,2.4740827e-09\n",
      "Iteration 77970: loss = 1.1827625e-10,2.473901e-09\n",
      "Iteration 77975: loss = 1.1826352e-10,2.4737186e-09\n",
      "Iteration 77980: loss = 1.1825106e-10,2.4735365e-09\n",
      "Iteration 77985: loss = 1.1823853e-10,2.4733544e-09\n",
      "Iteration 77990: loss = 1.182286e-10,2.4731657e-09\n",
      "Iteration 77995: loss = 1.1822678e-10,2.472952e-09\n",
      "Iteration 78000: loss = 1.1823006e-10,2.472722e-09\n",
      "Iteration 78005: loss = 1.182339e-10,2.4724902e-09\n",
      "Iteration 78010: loss = 1.1823052e-10,2.4722824e-09\n",
      "Iteration 78015: loss = 1.182271e-10,2.4720732e-09\n",
      "Iteration 78020: loss = 1.1822764e-10,2.4718516e-09\n",
      "Iteration 78025: loss = 1.1822902e-10,2.471628e-09\n",
      "Iteration 78030: loss = 1.1822728e-10,2.4714155e-09\n",
      "Iteration 78035: loss = 1.1822651e-10,2.4711986e-09\n",
      "Iteration 78040: loss = 1.1822582e-10,2.4709825e-09\n",
      "Iteration 78045: loss = 1.1822408e-10,2.4707687e-09\n",
      "Iteration 78050: loss = 1.1822222e-10,2.4705553e-09\n",
      "Iteration 78055: loss = 1.1822034e-10,2.470343e-09\n",
      "Iteration 78060: loss = 1.1821817e-10,2.470131e-09\n",
      "Iteration 78065: loss = 1.1821509e-10,2.469922e-09\n",
      "Iteration 78070: loss = 1.1821101e-10,2.4697149e-09\n",
      "Iteration 78075: loss = 1.182066e-10,2.4695095e-09\n",
      "Iteration 78080: loss = 1.1820132e-10,2.4693068e-09\n",
      "Iteration 78085: loss = 1.1819422e-10,2.4691111e-09\n",
      "Iteration 78090: loss = 1.1818647e-10,2.4689153e-09\n",
      "Iteration 78095: loss = 1.1817866e-10,2.468721e-09\n",
      "Iteration 78100: loss = 1.1816982e-10,2.4685298e-09\n",
      "Iteration 78105: loss = 1.1816063e-10,2.4683395e-09\n",
      "Iteration 78110: loss = 1.181512e-10,2.4681497e-09\n",
      "Iteration 78115: loss = 1.1814154e-10,2.4679607e-09\n",
      "Iteration 78120: loss = 1.1813198e-10,2.4677718e-09\n",
      "Iteration 78125: loss = 1.181212e-10,2.4675866e-09\n",
      "Iteration 78130: loss = 1.168938e-10,2.4673985e-09\n",
      "Iteration 78135: loss = 1.1811893e-10,2.467158e-09\n",
      "Iteration 78140: loss = 1.1812067e-10,2.466935e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78145: loss = 1.1689787e-10,2.4667342e-09\n",
      "Iteration 78150: loss = 1.181203e-10,2.4665021e-09\n",
      "Iteration 78155: loss = 1.1690103e-10,2.46629e-09\n",
      "Iteration 78160: loss = 1.1811949e-10,2.4660687e-09\n",
      "Iteration 78165: loss = 1.181203e-10,2.4658502e-09\n",
      "Iteration 78170: loss = 1.1812144e-10,2.4656304e-09\n",
      "Iteration 78175: loss = 1.1690306e-10,2.4654168e-09\n",
      "Iteration 78180: loss = 1.1812439e-10,2.4651876e-09\n",
      "Iteration 78185: loss = 1.1690597e-10,2.4649738e-09\n",
      "Iteration 78190: loss = 1.1812547e-10,2.4647515e-09\n",
      "Iteration 78195: loss = 1.1812669e-10,2.4645308e-09\n",
      "Iteration 78200: loss = 1.1812619e-10,2.4643163e-09\n",
      "Iteration 78205: loss = 1.1812756e-10,2.4640945e-09\n",
      "Iteration 78210: loss = 1.1812867e-10,2.4638762e-09\n",
      "Iteration 78215: loss = 1.1690962e-10,2.463665e-09\n",
      "Iteration 78220: loss = 1.1813149e-10,2.4634352e-09\n",
      "Iteration 78225: loss = 1.1813035e-10,2.463222e-09\n",
      "Iteration 78230: loss = 1.1813141e-10,2.463004e-09\n",
      "Iteration 78235: loss = 1.1813302e-10,2.4627826e-09\n",
      "Iteration 78240: loss = 1.1691498e-10,2.4625688e-09\n",
      "Iteration 78245: loss = 1.1691485e-10,2.4623525e-09\n",
      "Iteration 78250: loss = 1.181344e-10,2.4621305e-09\n",
      "Iteration 78255: loss = 1.1691727e-10,2.4619147e-09\n",
      "Iteration 78260: loss = 1.1691757e-10,2.461698e-09\n",
      "Iteration 78265: loss = 1.1691877e-10,2.4614792e-09\n",
      "Iteration 78270: loss = 1.1813805e-10,2.4612583e-09\n",
      "Iteration 78275: loss = 1.1813871e-10,2.4610396e-09\n",
      "Iteration 78280: loss = 1.1692138e-10,2.4608253e-09\n",
      "Iteration 78285: loss = 1.1692021e-10,2.4606137e-09\n",
      "Iteration 78290: loss = 1.1814098e-10,2.460388e-09\n",
      "Iteration 78295: loss = 1.1692367e-10,2.4601725e-09\n",
      "Iteration 78300: loss = 1.1692451e-10,2.4599547e-09\n",
      "Iteration 78305: loss = 1.1692558e-10,2.4597377e-09\n",
      "Iteration 78310: loss = 1.1814423e-10,2.4595184e-09\n",
      "Iteration 78315: loss = 1.169233e-10,2.4593136e-09\n",
      "Iteration 78320: loss = 1.1692704e-10,2.459089e-09\n",
      "Iteration 78325: loss = 1.169264e-10,2.4588758e-09\n",
      "Iteration 78330: loss = 1.1692146e-10,2.4586755e-09\n",
      "Iteration 78335: loss = 1.1692283e-10,2.4584574e-09\n",
      "Iteration 78340: loss = 1.1692751e-10,2.4582287e-09\n",
      "Iteration 78345: loss = 1.181502e-10,2.4579987e-09\n",
      "Iteration 78350: loss = 1.1692804e-10,2.4577984e-09\n",
      "Iteration 78355: loss = 1.1692818e-10,2.457584e-09\n",
      "Iteration 78360: loss = 1.1693004e-10,2.4573645e-09\n",
      "Iteration 78365: loss = 1.1693214e-10,2.4571443e-09\n",
      "Iteration 78370: loss = 1.1693398e-10,2.456924e-09\n",
      "Iteration 78375: loss = 1.1693586e-10,2.4567046e-09\n",
      "Iteration 78380: loss = 1.1693739e-10,2.4564868e-09\n",
      "Iteration 78385: loss = 1.169383e-10,2.4562692e-09\n",
      "Iteration 78390: loss = 1.1693732e-10,2.456059e-09\n",
      "Iteration 78395: loss = 1.1693407e-10,2.4558555e-09\n",
      "Iteration 78400: loss = 1.1693062e-10,2.4556517e-09\n",
      "Iteration 78405: loss = 1.1692677e-10,2.45545e-09\n",
      "Iteration 78410: loss = 1.1692257e-10,2.455249e-09\n",
      "Iteration 78415: loss = 1.1691802e-10,2.455049e-09\n",
      "Iteration 78420: loss = 1.169111e-10,2.4548565e-09\n",
      "Iteration 78425: loss = 1.1690428e-10,2.4546636e-09\n",
      "Iteration 78430: loss = 1.1689612e-10,2.454474e-09\n",
      "Iteration 78435: loss = 1.1688775e-10,2.4542868e-09\n",
      "Iteration 78440: loss = 1.1687878e-10,2.4541005e-09\n",
      "Iteration 78445: loss = 1.1686986e-10,2.4539144e-09\n",
      "Iteration 78450: loss = 1.1686113e-10,2.4537279e-09\n",
      "Iteration 78455: loss = 1.168519e-10,2.4535423e-09\n",
      "Iteration 78460: loss = 1.16841e-10,2.4533622e-09\n",
      "Iteration 78465: loss = 1.1682975e-10,2.4531834e-09\n",
      "Iteration 78470: loss = 1.1681846e-10,2.453005e-09\n",
      "Iteration 78475: loss = 1.1680716e-10,2.4528255e-09\n",
      "Iteration 78480: loss = 1.1679556e-10,2.4526472e-09\n",
      "Iteration 78485: loss = 1.1678404e-10,2.452469e-09\n",
      "Iteration 78490: loss = 1.1677244e-10,2.4522913e-09\n",
      "Iteration 78495: loss = 1.1675946e-10,2.4521176e-09\n",
      "Iteration 78500: loss = 1.1674679e-10,2.4519435e-09\n",
      "Iteration 78505: loss = 1.1673414e-10,2.451769e-09\n",
      "Iteration 78510: loss = 1.1672151e-10,2.4515956e-09\n",
      "Iteration 78515: loss = 1.1670868e-10,2.4514222e-09\n",
      "Iteration 78520: loss = 1.1669611e-10,2.4512479e-09\n",
      "Iteration 78525: loss = 1.1668347e-10,2.451074e-09\n",
      "Iteration 78530: loss = 1.1667059e-10,2.4509001e-09\n",
      "Iteration 78535: loss = 1.1665803e-10,2.4507267e-09\n",
      "Iteration 78540: loss = 1.1664548e-10,2.4505524e-09\n",
      "Iteration 78545: loss = 1.1663272e-10,2.4503783e-09\n",
      "Iteration 78550: loss = 1.1661845e-10,2.4502098e-09\n",
      "Iteration 78555: loss = 1.1660363e-10,2.4500424e-09\n",
      "Iteration 78560: loss = 1.1658907e-10,2.449875e-09\n",
      "Iteration 78565: loss = 1.1657449e-10,2.4497069e-09\n",
      "Iteration 78570: loss = 1.165598e-10,2.4495388e-09\n",
      "Iteration 78575: loss = 1.1654556e-10,2.4493714e-09\n",
      "Iteration 78580: loss = 1.1653095e-10,2.449203e-09\n",
      "Iteration 78585: loss = 1.1651642e-10,2.4490348e-09\n",
      "Iteration 78590: loss = 1.1650202e-10,2.4488673e-09\n",
      "Iteration 78595: loss = 1.1649214e-10,2.4486853e-09\n",
      "Iteration 78600: loss = 1.1648962e-10,2.4484812e-09\n",
      "Iteration 78605: loss = 1.1649232e-10,2.4482625e-09\n",
      "Iteration 78610: loss = 1.1649649e-10,2.44804e-09\n",
      "Iteration 78615: loss = 1.1650202e-10,2.447812e-09\n",
      "Iteration 78620: loss = 1.165042e-10,2.447595e-09\n",
      "Iteration 78625: loss = 1.1650299e-10,2.4473883e-09\n",
      "Iteration 78630: loss = 1.1650346e-10,2.4471751e-09\n",
      "Iteration 78635: loss = 1.1650425e-10,2.4469635e-09\n",
      "Iteration 78640: loss = 1.1650497e-10,2.4467512e-09\n",
      "Iteration 78645: loss = 1.1650514e-10,2.4465407e-09\n",
      "Iteration 78650: loss = 1.1650274e-10,2.4463374e-09\n",
      "Iteration 78655: loss = 1.1650006e-10,2.4461353e-09\n",
      "Iteration 78660: loss = 1.164967e-10,2.445935e-09\n",
      "Iteration 78665: loss = 1.164922e-10,2.4457385e-09\n",
      "Iteration 78670: loss = 1.1648695e-10,2.445545e-09\n",
      "Iteration 78675: loss = 1.1648149e-10,2.4453517e-09\n",
      "Iteration 78680: loss = 1.1647432e-10,2.4451625e-09\n",
      "Iteration 78685: loss = 1.1646599e-10,2.4449778e-09\n",
      "Iteration 78690: loss = 1.164575e-10,2.4447933e-09\n",
      "Iteration 78695: loss = 1.1644884e-10,2.4446096e-09\n",
      "Iteration 78700: loss = 1.1643993e-10,2.4444264e-09\n",
      "Iteration 78705: loss = 1.1643071e-10,2.444244e-09\n",
      "Iteration 78710: loss = 1.1642183e-10,2.444062e-09\n",
      "Iteration 78715: loss = 1.16411054e-10,2.4438842e-09\n",
      "Iteration 78720: loss = 1.1640053e-10,2.4437077e-09\n",
      "Iteration 78725: loss = 1.1638986e-10,2.4435303e-09\n",
      "Iteration 78730: loss = 1.1637837e-10,2.4433562e-09\n",
      "Iteration 78735: loss = 1.1636534e-10,2.4431848e-09\n",
      "Iteration 78740: loss = 1.163524e-10,2.4430151e-09\n",
      "Iteration 78745: loss = 1.1633942e-10,2.4428455e-09\n",
      "Iteration 78750: loss = 1.1632658e-10,2.4426754e-09\n",
      "Iteration 78755: loss = 1.1631327e-10,2.4425058e-09\n",
      "Iteration 78760: loss = 1.163003e-10,2.442336e-09\n",
      "Iteration 78765: loss = 1.1628729e-10,2.4421667e-09\n",
      "Iteration 78770: loss = 1.1627408e-10,2.441997e-09\n",
      "Iteration 78775: loss = 1.1626115e-10,2.441827e-09\n",
      "Iteration 78780: loss = 1.162482e-10,2.4416575e-09\n",
      "Iteration 78785: loss = 1.1623518e-10,2.4414888e-09\n",
      "Iteration 78790: loss = 1.1622176e-10,2.4413191e-09\n",
      "Iteration 78795: loss = 1.16208786e-10,2.44115e-09\n",
      "Iteration 78800: loss = 1.1619574e-10,2.4409803e-09\n",
      "Iteration 78805: loss = 1.16181814e-10,2.4408136e-09\n",
      "Iteration 78810: loss = 1.16167555e-10,2.440649e-09\n",
      "Iteration 78815: loss = 1.1615342e-10,2.440484e-09\n",
      "Iteration 78820: loss = 1.16139105e-10,2.4403193e-09\n",
      "Iteration 78825: loss = 1.1612483e-10,2.440154e-09\n",
      "Iteration 78830: loss = 1.16110684e-10,2.4399895e-09\n",
      "Iteration 78835: loss = 1.1609646e-10,2.439824e-09\n",
      "Iteration 78840: loss = 1.160822e-10,2.4396596e-09\n",
      "Iteration 78845: loss = 1.1606798e-10,2.4394942e-09\n",
      "Iteration 78850: loss = 1.1605389e-10,2.4393292e-09\n",
      "Iteration 78855: loss = 1.1603971e-10,2.4391642e-09\n",
      "Iteration 78860: loss = 1.1602569e-10,2.438999e-09\n",
      "Iteration 78865: loss = 1.1601172e-10,2.4388342e-09\n",
      "Iteration 78870: loss = 1.16000015e-10,2.4386617e-09\n",
      "Iteration 78875: loss = 1.15996185e-10,2.4384654e-09\n",
      "Iteration 78880: loss = 1.1599741e-10,2.438255e-09\n",
      "Iteration 78885: loss = 1.16001674e-10,2.4380344e-09\n",
      "Iteration 78890: loss = 1.1600535e-10,2.437816e-09\n",
      "Iteration 78895: loss = 1.16002145e-10,2.437618e-09\n",
      "Iteration 78900: loss = 1.1599908e-10,2.4374223e-09\n",
      "Iteration 78905: loss = 1.159988e-10,2.437215e-09\n",
      "Iteration 78910: loss = 1.1599912e-10,2.4370068e-09\n",
      "Iteration 78915: loss = 1.15999245e-10,2.436799e-09\n",
      "Iteration 78920: loss = 1.1599896e-10,2.4365923e-09\n",
      "Iteration 78925: loss = 1.1599748e-10,2.4363909e-09\n",
      "Iteration 78930: loss = 1.159953e-10,2.4361904e-09\n",
      "Iteration 78935: loss = 1.15992604e-10,2.4359914e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78940: loss = 1.15989884e-10,2.4357938e-09\n",
      "Iteration 78945: loss = 1.1598411e-10,2.4356042e-09\n",
      "Iteration 78950: loss = 1.15978345e-10,2.4354159e-09\n",
      "Iteration 78955: loss = 1.1597207e-10,2.4352271e-09\n",
      "Iteration 78960: loss = 1.1596562e-10,2.4350402e-09\n",
      "Iteration 78965: loss = 1.1595868e-10,2.434855e-09\n",
      "Iteration 78970: loss = 1.15950465e-10,2.4346736e-09\n",
      "Iteration 78975: loss = 1.1594212e-10,2.4344933e-09\n",
      "Iteration 78980: loss = 1.1593267e-10,2.4343156e-09\n",
      "Iteration 78985: loss = 1.1592194e-10,2.434142e-09\n",
      "Iteration 78990: loss = 1.1591114e-10,2.433969e-09\n",
      "Iteration 78995: loss = 1.1589995e-10,2.4337963e-09\n",
      "Iteration 79000: loss = 1.15888944e-10,2.433624e-09\n",
      "Iteration 79005: loss = 1.1587779e-10,2.4334523e-09\n",
      "Iteration 79010: loss = 1.1586642e-10,2.4332811e-09\n",
      "Iteration 79015: loss = 1.1585503e-10,2.4331093e-09\n",
      "Iteration 79020: loss = 1.14635745e-10,2.4329379e-09\n",
      "Iteration 79025: loss = 1.1585071e-10,2.4327116e-09\n",
      "Iteration 79030: loss = 1.158528e-10,2.4325e-09\n",
      "Iteration 79035: loss = 1.1584767e-10,2.4323101e-09\n",
      "Iteration 79040: loss = 1.1584383e-10,2.4321167e-09\n",
      "Iteration 79045: loss = 1.1584439e-10,2.4319093e-09\n",
      "Iteration 79050: loss = 1.15842246e-10,2.4317113e-09\n",
      "Iteration 79055: loss = 1.1584259e-10,2.4315046e-09\n",
      "Iteration 79060: loss = 1.15844105e-10,2.4312958e-09\n",
      "Iteration 79065: loss = 1.1584302e-10,2.431094e-09\n",
      "Iteration 79070: loss = 1.1584195e-10,2.4308922e-09\n",
      "Iteration 79075: loss = 1.1584006e-10,2.4306925e-09\n",
      "Iteration 79080: loss = 1.158403e-10,2.4304874e-09\n",
      "Iteration 79085: loss = 1.1584119e-10,2.4302804e-09\n",
      "Iteration 79090: loss = 1.14629715e-10,2.4300864e-09\n",
      "Iteration 79095: loss = 1.15838374e-10,2.4298799e-09\n",
      "Iteration 79100: loss = 1.1583987e-10,2.4296702e-09\n",
      "Iteration 79105: loss = 1.1583746e-10,2.429474e-09\n",
      "Iteration 79110: loss = 1.1462781e-10,2.4292754e-09\n",
      "Iteration 79115: loss = 1.1462734e-10,2.429072e-09\n",
      "Iteration 79120: loss = 1.1583585e-10,2.428866e-09\n",
      "Iteration 79125: loss = 1.1583795e-10,2.4286557e-09\n",
      "Iteration 79130: loss = 1.1462523e-10,2.4284654e-09\n",
      "Iteration 79135: loss = 1.1583892e-10,2.4282447e-09\n",
      "Iteration 79140: loss = 1.158345e-10,2.4280533e-09\n",
      "Iteration 79145: loss = 1.1583359e-10,2.427853e-09\n",
      "Iteration 79150: loss = 1.14624456e-10,2.4276514e-09\n",
      "Iteration 79155: loss = 1.1462444e-10,2.427448e-09\n",
      "Iteration 79160: loss = 1.1583262e-10,2.4272435e-09\n",
      "Iteration 79165: loss = 1.14623366e-10,2.4270441e-09\n",
      "Iteration 79170: loss = 1.1462237e-10,2.4268443e-09\n",
      "Iteration 79175: loss = 1.15831754e-10,2.426636e-09\n",
      "Iteration 79180: loss = 1.1461995e-10,2.4264442e-09\n",
      "Iteration 79185: loss = 1.1583368e-10,2.4262226e-09\n",
      "Iteration 79190: loss = 1.1461656e-10,2.4260478e-09\n",
      "Iteration 79195: loss = 1.15832476e-10,2.42582e-09\n",
      "Iteration 79200: loss = 1.14617676e-10,2.4256384e-09\n",
      "Iteration 79205: loss = 1.1461643e-10,2.4254385e-09\n",
      "Iteration 79210: loss = 1.1583157e-10,2.4252136e-09\n",
      "Iteration 79215: loss = 1.1461324e-10,2.4250426e-09\n",
      "Iteration 79220: loss = 1.1582784e-10,2.4248201e-09\n",
      "Iteration 79225: loss = 1.1461721e-10,2.4246245e-09\n",
      "Iteration 79230: loss = 1.15826015e-10,2.4244184e-09\n",
      "Iteration 79235: loss = 1.14616185e-10,2.4242215e-09\n",
      "Iteration 79240: loss = 1.1461587e-10,2.4240205e-09\n",
      "Iteration 79245: loss = 1.14616316e-10,2.423816e-09\n",
      "Iteration 79250: loss = 1.1461574e-10,2.4236153e-09\n",
      "Iteration 79255: loss = 1.1461521e-10,2.4234144e-09\n",
      "Iteration 79260: loss = 1.1461294e-10,2.4232185e-09\n",
      "Iteration 79265: loss = 1.1582294e-10,2.4230102e-09\n",
      "Iteration 79270: loss = 1.14611605e-10,2.4228188e-09\n",
      "Iteration 79275: loss = 1.14612826e-10,2.4226132e-09\n",
      "Iteration 79280: loss = 1.1461306e-10,2.42241e-09\n",
      "Iteration 79285: loss = 1.1461282e-10,2.4222084e-09\n",
      "Iteration 79290: loss = 1.1461163e-10,2.42201e-09\n",
      "Iteration 79295: loss = 1.15819965e-10,2.4218072e-09\n",
      "Iteration 79300: loss = 1.14611654e-10,2.4216067e-09\n",
      "Iteration 79305: loss = 1.14611494e-10,2.4214049e-09\n",
      "Iteration 79310: loss = 1.1461126e-10,2.4212037e-09\n",
      "Iteration 79315: loss = 1.1461094e-10,2.421003e-09\n",
      "Iteration 79320: loss = 1.1461302e-10,2.4207947e-09\n",
      "Iteration 79325: loss = 1.14613874e-10,2.4205908e-09\n",
      "Iteration 79330: loss = 1.1461346e-10,2.420391e-09\n",
      "Iteration 79335: loss = 1.1461332e-10,2.42019e-09\n",
      "Iteration 79340: loss = 1.1461498e-10,2.419983e-09\n",
      "Iteration 79345: loss = 1.1461459e-10,2.419783e-09\n",
      "Iteration 79350: loss = 1.14616205e-10,2.4195779e-09\n",
      "Iteration 79355: loss = 1.1461618e-10,2.4193763e-09\n",
      "Iteration 79360: loss = 1.1461503e-10,2.4191784e-09\n",
      "Iteration 79365: loss = 1.14616434e-10,2.4189732e-09\n",
      "Iteration 79370: loss = 1.1461505e-10,2.4187765e-09\n",
      "Iteration 79375: loss = 1.1582652e-10,2.4185662e-09\n",
      "Iteration 79380: loss = 1.1461599e-10,2.4183722e-09\n",
      "Iteration 79385: loss = 1.14617614e-10,2.4181661e-09\n",
      "Iteration 79390: loss = 1.1461828e-10,2.4179627e-09\n",
      "Iteration 79395: loss = 1.1461651e-10,2.417769e-09\n",
      "Iteration 79400: loss = 1.1461641e-10,2.4175684e-09\n",
      "Iteration 79405: loss = 1.1461674e-10,2.4173679e-09\n",
      "Iteration 79410: loss = 1.1461727e-10,2.4171656e-09\n",
      "Iteration 79415: loss = 1.1461757e-10,2.4169649e-09\n",
      "Iteration 79420: loss = 1.1461773e-10,2.4167643e-09\n",
      "Iteration 79425: loss = 1.1461738e-10,2.416565e-09\n",
      "Iteration 79430: loss = 1.1461692e-10,2.416367e-09\n",
      "Iteration 79435: loss = 1.1461592e-10,2.4161695e-09\n",
      "Iteration 79440: loss = 1.1461421e-10,2.4159745e-09\n",
      "Iteration 79445: loss = 1.1461054e-10,2.415785e-09\n",
      "Iteration 79450: loss = 1.1460658e-10,2.4155975e-09\n",
      "Iteration 79455: loss = 1.146011e-10,2.415414e-09\n",
      "Iteration 79460: loss = 1.146001e-10,2.415217e-09\n",
      "Iteration 79465: loss = 1.1459981e-10,2.4150177e-09\n",
      "Iteration 79470: loss = 1.1459907e-10,2.4148203e-09\n",
      "Iteration 79475: loss = 1.14599726e-10,2.4146198e-09\n",
      "Iteration 79480: loss = 1.14598865e-10,2.4144222e-09\n",
      "Iteration 79485: loss = 1.145976e-10,2.4142262e-09\n",
      "Iteration 79490: loss = 1.1459827e-10,2.414026e-09\n",
      "Iteration 79495: loss = 1.1339776e-10,2.4138322e-09\n",
      "Iteration 79500: loss = 1.146035e-10,2.4136115e-09\n",
      "Iteration 79505: loss = 1.14600635e-10,2.4134201e-09\n",
      "Iteration 79510: loss = 1.1339638e-10,2.4132394e-09\n",
      "Iteration 79515: loss = 1.1459756e-10,2.4130322e-09\n",
      "Iteration 79520: loss = 1.13395425e-10,2.4128448e-09\n",
      "Iteration 79525: loss = 1.14596894e-10,2.4126359e-09\n",
      "Iteration 79530: loss = 1.1339457e-10,2.4124491e-09\n",
      "Iteration 79535: loss = 1.1459528e-10,2.412243e-09\n",
      "Iteration 79540: loss = 1.1339459e-10,2.412052e-09\n",
      "Iteration 79545: loss = 1.1339399e-10,2.4118547e-09\n",
      "Iteration 79550: loss = 1.1459386e-10,2.4116522e-09\n",
      "Iteration 79555: loss = 1.1459231e-10,2.4114588e-09\n",
      "Iteration 79560: loss = 1.1339293e-10,2.4112627e-09\n",
      "Iteration 79565: loss = 1.1459367e-10,2.4110576e-09\n",
      "Iteration 79570: loss = 1.14591815e-10,2.4108653e-09\n",
      "Iteration 79575: loss = 1.1339063e-10,2.4106748e-09\n",
      "Iteration 79580: loss = 1.1459491e-10,2.410459e-09\n",
      "Iteration 79585: loss = 1.1339054e-10,2.410279e-09\n",
      "Iteration 79590: loss = 1.1459194e-10,2.4100715e-09\n",
      "Iteration 79595: loss = 1.1458943e-10,2.4098812e-09\n",
      "Iteration 79600: loss = 1.1459004e-10,2.409681e-09\n",
      "Iteration 79605: loss = 1.13389166e-10,2.4094902e-09\n",
      "Iteration 79610: loss = 1.14592245e-10,2.4092794e-09\n",
      "Iteration 79615: loss = 1.1338749e-10,2.4091005e-09\n",
      "Iteration 79620: loss = 1.1459161e-10,2.4088855e-09\n",
      "Iteration 79625: loss = 1.1338714e-10,2.4087061e-09\n",
      "Iteration 79630: loss = 1.14590504e-10,2.4084938e-09\n",
      "Iteration 79635: loss = 1.1338432e-10,2.4083193e-09\n",
      "Iteration 79640: loss = 1.1459092e-10,2.408097e-09\n",
      "Iteration 79645: loss = 1.1338507e-10,2.4079219e-09\n",
      "Iteration 79650: loss = 1.14586174e-10,2.407717e-09\n",
      "Iteration 79655: loss = 1.14585896e-10,2.4075204e-09\n",
      "Iteration 79660: loss = 1.1458631e-10,2.4073223e-09\n",
      "Iteration 79665: loss = 1.1458631e-10,2.4071252e-09\n",
      "Iteration 79670: loss = 1.1338239e-10,2.4069444e-09\n",
      "Iteration 79675: loss = 1.14586333e-10,2.40673e-09\n",
      "Iteration 79680: loss = 1.13382005e-10,2.4065514e-09\n",
      "Iteration 79685: loss = 1.1458468e-10,2.4063416e-09\n",
      "Iteration 79690: loss = 1.13384295e-10,2.4061506e-09\n",
      "Iteration 79695: loss = 1.1458375e-10,2.4059505e-09\n",
      "Iteration 79700: loss = 1.14584425e-10,2.4057514e-09\n",
      "Iteration 79705: loss = 1.1338278e-10,2.4055637e-09\n",
      "Iteration 79710: loss = 1.1338156e-10,2.4053721e-09\n",
      "Iteration 79715: loss = 1.14584085e-10,2.4051634e-09\n",
      "Iteration 79720: loss = 1.13378786e-10,2.4049864e-09\n",
      "Iteration 79725: loss = 1.1458298e-10,2.4047742e-09\n",
      "Iteration 79730: loss = 1.13381755e-10,2.4045854e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79735: loss = 1.13381686e-10,2.4043898e-09\n",
      "Iteration 79740: loss = 1.1338003e-10,2.404199e-09\n",
      "Iteration 79745: loss = 1.13379674e-10,2.404003e-09\n",
      "Iteration 79750: loss = 1.13380305e-10,2.4038056e-09\n",
      "Iteration 79755: loss = 1.145794e-10,2.403607e-09\n",
      "Iteration 79760: loss = 1.13380354e-10,2.4034132e-09\n",
      "Iteration 79765: loss = 1.1337633e-10,2.4032296e-09\n",
      "Iteration 79770: loss = 1.13377495e-10,2.4030304e-09\n",
      "Iteration 79775: loss = 1.13379035e-10,2.4028302e-09\n",
      "Iteration 79780: loss = 1.1337795e-10,2.4026376e-09\n",
      "Iteration 79785: loss = 1.14577604e-10,2.402438e-09\n",
      "Iteration 79790: loss = 1.1337473e-10,2.4022555e-09\n",
      "Iteration 79795: loss = 1.1457849e-10,2.4020446e-09\n",
      "Iteration 79800: loss = 1.1337217e-10,2.4018727e-09\n",
      "Iteration 79805: loss = 1.1337498e-10,2.4016689e-09\n",
      "Iteration 79810: loss = 1.1337616e-10,2.4014704e-09\n",
      "Iteration 79815: loss = 1.14576425e-10,2.401269e-09\n",
      "Iteration 79820: loss = 1.1337225e-10,2.4010904e-09\n",
      "Iteration 79825: loss = 1.1337542e-10,2.400887e-09\n",
      "Iteration 79830: loss = 1.13374705e-10,2.4006936e-09\n",
      "Iteration 79835: loss = 1.1337373e-10,2.4005016e-09\n",
      "Iteration 79840: loss = 1.1337432e-10,2.4003044e-09\n",
      "Iteration 79845: loss = 1.1337022e-10,2.4001212e-09\n",
      "Iteration 79850: loss = 1.1337256e-10,2.3999203e-09\n",
      "Iteration 79855: loss = 1.1337369e-10,2.3997213e-09\n",
      "Iteration 79860: loss = 1.1336829e-10,2.3995432e-09\n",
      "Iteration 79865: loss = 1.13369675e-10,2.399344e-09\n",
      "Iteration 79870: loss = 1.1337419e-10,2.3991362e-09\n",
      "Iteration 79875: loss = 1.1336795e-10,2.3989597e-09\n",
      "Iteration 79880: loss = 1.13362965e-10,2.3987803e-09\n",
      "Iteration 79885: loss = 1.1336371e-10,2.398584e-09\n",
      "Iteration 79890: loss = 1.1336593e-10,2.3983822e-09\n",
      "Iteration 79895: loss = 1.13369945e-10,2.3981763e-09\n",
      "Iteration 79900: loss = 1.1337207e-10,2.3979756e-09\n",
      "Iteration 79905: loss = 1.1336756e-10,2.3977949e-09\n",
      "Iteration 79910: loss = 1.1336593e-10,2.3976066e-09\n",
      "Iteration 79915: loss = 1.1336493e-10,2.3974156e-09\n",
      "Iteration 79920: loss = 1.1336413e-10,2.3972246e-09\n",
      "Iteration 79925: loss = 1.13362965e-10,2.3970332e-09\n",
      "Iteration 79930: loss = 1.1336201e-10,2.3968427e-09\n",
      "Iteration 79935: loss = 1.1336077e-10,2.3966527e-09\n",
      "Iteration 79940: loss = 1.1335711e-10,2.3964708e-09\n",
      "Iteration 79945: loss = 1.1335304e-10,2.3962885e-09\n",
      "Iteration 79950: loss = 1.1334902e-10,2.3961073e-09\n",
      "Iteration 79955: loss = 1.1334447e-10,2.3959266e-09\n",
      "Iteration 79960: loss = 1.1333967e-10,2.3957474e-09\n",
      "Iteration 79965: loss = 1.13334016e-10,2.3955706e-09\n",
      "Iteration 79970: loss = 1.1332781e-10,2.3953965e-09\n",
      "Iteration 79975: loss = 1.1331891e-10,2.3952287e-09\n",
      "Iteration 79980: loss = 1.1331027e-10,2.3950613e-09\n",
      "Iteration 79985: loss = 1.1330171e-10,2.3948947e-09\n",
      "Iteration 79990: loss = 1.1329277e-10,2.394728e-09\n",
      "Iteration 79995: loss = 1.13283695e-10,2.394561e-09\n"
     ]
    }
   ],
   "source": [
    "optim = tf.keras.optimizers.Adam(epsilon=1e-08)\n",
    "PINN_solver.train(N=N, optimizer=optim, method = 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train PINN with PCGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN_solver_pcgrad = PINN(x_u, y_u, x_r, init_model())\n",
    "PINN_solver_pcgrad.model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: loss = 0.3108814,5.3529483e-09\n",
      "Iteration 00005: loss = 0.1549211,4.6981725e-09\n",
      "Iteration 00010: loss = 0.054999113,4.924053e-09\n",
      "Iteration 00015: loss = 0.018032636,5.3990554e-09\n",
      "Iteration 00020: loss = 0.0071039572,5.789751e-09\n",
      "Iteration 00025: loss = 0.0035785048,6.0522094e-09\n",
      "Iteration 00030: loss = 0.0022172686,6.2188983e-09\n",
      "Iteration 00035: loss = 0.0015978423,6.322259e-09\n",
      "Iteration 00040: loss = 0.001277594,6.384872e-09\n",
      "Iteration 00045: loss = 0.0010950386,6.421491e-09\n",
      "Iteration 00050: loss = 0.0009824234,6.4416175e-09\n",
      "Iteration 00055: loss = 0.0009079206,6.4513404e-09\n",
      "Iteration 00060: loss = 0.0008552521,6.4545227e-09\n",
      "Iteration 00065: loss = 0.0008155463,6.4536145e-09\n",
      "Iteration 00070: loss = 0.00078375934,6.450141e-09\n",
      "Iteration 00075: loss = 0.00075693865,6.445058e-09\n",
      "Iteration 00080: loss = 0.0007333306,6.4389574e-09\n",
      "Iteration 00085: loss = 0.0007118671,6.432201e-09\n",
      "Iteration 00090: loss = 0.0006919173,6.425009e-09\n",
      "Iteration 00095: loss = 0.00067308196,6.417519e-09\n",
      "Iteration 00100: loss = 0.0006551232,6.4098096e-09\n",
      "Iteration 00105: loss = 0.00063789485,6.4019297e-09\n",
      "Iteration 00110: loss = 0.0006213013,6.393904e-09\n",
      "Iteration 00115: loss = 0.00060528255,6.3857515e-09\n",
      "Iteration 00120: loss = 0.0005898014,6.3774763e-09\n",
      "Iteration 00125: loss = 0.0005748244,6.36908e-09\n",
      "Iteration 00130: loss = 0.0005603376,6.3605676e-09\n",
      "Iteration 00135: loss = 0.00054631877,6.3519345e-09\n",
      "Iteration 00140: loss = 0.00053275115,6.343176e-09\n",
      "Iteration 00145: loss = 0.0005196234,6.334293e-09\n",
      "Iteration 00150: loss = 0.00050691515,6.3252807e-09\n",
      "Iteration 00155: loss = 0.0004946236,6.316135e-09\n",
      "Iteration 00160: loss = 0.00048272786,6.3068533e-09\n",
      "Iteration 00165: loss = 0.00047122268,6.2974337e-09\n",
      "Iteration 00170: loss = 0.00046009046,6.287872e-09\n",
      "Iteration 00175: loss = 0.00044931794,6.278167e-09\n",
      "Iteration 00180: loss = 0.00043890064,6.2683148e-09\n",
      "Iteration 00185: loss = 0.00042881691,6.2583143e-09\n",
      "Iteration 00190: loss = 0.0004190612,6.2481647e-09\n",
      "Iteration 00195: loss = 0.00040962567,6.2378627e-09\n",
      "Iteration 00200: loss = 0.00040049237,6.2274097e-09\n",
      "Iteration 00205: loss = 0.0003916544,6.216802e-09\n",
      "Iteration 00210: loss = 0.00038310152,6.2060446e-09\n",
      "Iteration 00215: loss = 0.0003748235,6.19513e-09\n",
      "Iteration 00220: loss = 0.0003668102,6.184064e-09\n",
      "Iteration 00225: loss = 0.00035905073,6.1728445e-09\n",
      "Iteration 00230: loss = 0.00035154042,6.161475e-09\n",
      "Iteration 00235: loss = 0.00034426586,6.1499534e-09\n",
      "Iteration 00240: loss = 0.0003372243,6.138282e-09\n",
      "Iteration 00245: loss = 0.00033039963,6.126464e-09\n",
      "Iteration 00250: loss = 0.00032378934,6.114497e-09\n",
      "Iteration 00255: loss = 0.00031738493,6.1023875e-09\n",
      "Iteration 00260: loss = 0.00031117967,6.090136e-09\n",
      "Iteration 00265: loss = 0.00030516434,6.077744e-09\n",
      "Iteration 00270: loss = 0.0002993349,6.065214e-09\n",
      "Iteration 00275: loss = 0.00029368096,6.0525513e-09\n",
      "Iteration 00280: loss = 0.0002881995,6.0397554e-09\n",
      "Iteration 00285: loss = 0.00028288423,6.026832e-09\n",
      "Iteration 00290: loss = 0.0002777267,6.0137815e-09\n",
      "Iteration 00295: loss = 0.00027272504,6.0006093e-09\n",
      "Iteration 00300: loss = 0.00026787084,5.987318e-09\n",
      "Iteration 00305: loss = 0.00026316,5.9739103e-09\n",
      "Iteration 00310: loss = 0.00025858617,5.960391e-09\n",
      "Iteration 00315: loss = 0.00025414588,5.9467626e-09\n",
      "Iteration 00320: loss = 0.000249835,5.933029e-09\n",
      "Iteration 00325: loss = 0.00024564698,5.9191927e-09\n",
      "Iteration 00330: loss = 0.00024157968,5.9052585e-09\n",
      "Iteration 00335: loss = 0.00023762738,5.8912306e-09\n",
      "Iteration 00340: loss = 0.00023378496,5.877111e-09\n",
      "Iteration 00345: loss = 0.00023005264,5.862903e-09\n",
      "Iteration 00350: loss = 0.00022642157,5.8486127e-09\n",
      "Iteration 00355: loss = 0.0002228922,5.834242e-09\n",
      "Iteration 00360: loss = 0.00021946018,5.8197944e-09\n",
      "Iteration 00365: loss = 0.00021612225,5.8052727e-09\n",
      "Iteration 00370: loss = 0.0002128723,5.790683e-09\n",
      "Iteration 00375: loss = 0.00020971161,5.7760277e-09\n",
      "Iteration 00380: loss = 0.00020663386,5.7613088e-09\n",
      "Iteration 00385: loss = 0.00020363931,5.746529e-09\n",
      "Iteration 00390: loss = 0.00020072119,5.7316925e-09\n",
      "Iteration 00395: loss = 0.00019788026,5.716806e-09\n",
      "Iteration 00400: loss = 0.00019511371,5.701868e-09\n",
      "Iteration 00405: loss = 0.00019241807,5.6868847e-09\n",
      "Iteration 00410: loss = 0.00018978996,5.671856e-09\n",
      "Iteration 00415: loss = 0.00018722737,5.656788e-09\n",
      "Iteration 00420: loss = 0.00018473198,5.641681e-09\n",
      "Iteration 00425: loss = 0.00018229603,5.6265383e-09\n",
      "Iteration 00430: loss = 0.00017992238,5.611365e-09\n",
      "Iteration 00435: loss = 0.00017760578,5.5961635e-09\n",
      "Iteration 00440: loss = 0.00017534658,5.580933e-09\n",
      "Iteration 00445: loss = 0.00017313928,5.565678e-09\n",
      "Iteration 00450: loss = 0.0001709864,5.5504015e-09\n",
      "Iteration 00455: loss = 0.00016888541,5.5351044e-09\n",
      "Iteration 00460: loss = 0.00016683237,5.5197926e-09\n",
      "Iteration 00465: loss = 0.0001648283,5.5044653e-09\n",
      "Iteration 00470: loss = 0.0001628698,5.489124e-09\n",
      "Iteration 00475: loss = 0.00016095626,5.473773e-09\n",
      "Iteration 00480: loss = 0.00015908692,5.4584115e-09\n",
      "Iteration 00485: loss = 0.00015725891,5.443047e-09\n",
      "Iteration 00490: loss = 0.00015547268,5.427675e-09\n",
      "Iteration 00495: loss = 0.00015372412,5.4123026e-09\n",
      "Iteration 00500: loss = 0.00015201721,5.3969273e-09\n",
      "Iteration 00505: loss = 0.00015034435,5.381554e-09\n",
      "Iteration 00510: loss = 0.00014871034,5.3661835e-09\n",
      "Iteration 00515: loss = 0.00014710837,5.350817e-09\n",
      "Iteration 00520: loss = 0.00014554328,5.3354556e-09\n",
      "Iteration 00525: loss = 0.00014400888,5.3201017e-09\n",
      "Iteration 00530: loss = 0.00014250811,5.304759e-09\n",
      "Iteration 00535: loss = 0.00014103613,5.289424e-09\n",
      "Iteration 00540: loss = 0.00013959664,5.274102e-09\n",
      "Iteration 00545: loss = 0.00013818631,5.2587916e-09\n",
      "Iteration 00550: loss = 0.000136804,5.2434967e-09\n",
      "Iteration 00555: loss = 0.00013544787,5.228218e-09\n",
      "Iteration 00560: loss = 0.00013411941,5.212954e-09\n",
      "Iteration 00565: loss = 0.00013281876,5.19771e-09\n",
      "Iteration 00570: loss = 0.00013154034,5.1824838e-09\n",
      "Iteration 00575: loss = 0.00013028893,5.1672773e-09\n",
      "Iteration 00580: loss = 0.00012906078,5.152094e-09\n",
      "Iteration 00585: loss = 0.00012785621,5.1369327e-09\n",
      "Iteration 00590: loss = 0.00012667345,5.1217937e-09\n",
      "Iteration 00595: loss = 0.0001255136,5.1066795e-09\n",
      "Iteration 00600: loss = 0.00012437413,5.0915903e-09\n",
      "Iteration 00605: loss = 0.00012325618,5.076527e-09\n",
      "Iteration 00610: loss = 0.00012215866,5.061492e-09\n",
      "Iteration 00615: loss = 0.00012108063,5.0464832e-09\n",
      "Iteration 00620: loss = 0.0001200218,5.031505e-09\n",
      "Iteration 00625: loss = 0.00011898129,5.0165547e-09\n",
      "Iteration 00630: loss = 0.00011795934,5.0016364e-09\n",
      "Iteration 00635: loss = 0.0001169563,4.986748e-09\n",
      "Iteration 00640: loss = 0.00011597004,4.9718913e-09\n",
      "Iteration 00645: loss = 0.00011500033,4.957067e-09\n",
      "Iteration 00650: loss = 0.000114046336,4.942276e-09\n",
      "Iteration 00655: loss = 0.00011310838,4.9275184e-09\n",
      "Iteration 00660: loss = 0.000112188,4.9127946e-09\n",
      "Iteration 00665: loss = 0.000111280824,4.8981077e-09\n",
      "Iteration 00670: loss = 0.00011038951,4.8834563e-09\n",
      "Iteration 00675: loss = 0.00010951219,4.8688413e-09\n",
      "Iteration 00680: loss = 0.0001086503,4.854262e-09\n",
      "Iteration 00685: loss = 0.000107800784,4.8397224e-09\n",
      "Iteration 00690: loss = 0.00010696522,4.8252193e-09\n",
      "Iteration 00695: loss = 0.00010614392,4.8107553e-09\n",
      "Iteration 00700: loss = 0.00010533493,4.7963304e-09\n",
      "Iteration 00705: loss = 0.00010453811,4.7819446e-09\n",
      "Iteration 00710: loss = 0.00010375275,4.767599e-09\n",
      "Iteration 00715: loss = 0.000102981605,4.7532955e-09\n",
      "Iteration 00720: loss = 0.000102220336,4.7390305e-09\n",
      "Iteration 00725: loss = 0.00010147066,4.724809e-09\n",
      "Iteration 00730: loss = 0.000100733974,4.710628e-09\n",
      "Iteration 00735: loss = 0.00010000688,4.6964903e-09\n",
      "Iteration 00740: loss = 9.929084e-05,4.682396e-09\n",
      "Iteration 00745: loss = 9.858626e-05,4.668344e-09\n",
      "Iteration 00750: loss = 9.7890734e-05,4.654337e-09\n",
      "Iteration 00755: loss = 9.72059e-05,4.6403725e-09\n",
      "Iteration 00760: loss = 9.6529955e-05,4.6264534e-09\n",
      "Iteration 00765: loss = 9.5864416e-05,4.6125797e-09\n",
      "Iteration 00770: loss = 9.5209136e-05,4.5987494e-09\n",
      "Iteration 00775: loss = 9.456128e-05,4.5849657e-09\n",
      "Iteration 00780: loss = 9.3924486e-05,4.571228e-09\n",
      "Iteration 00785: loss = 9.329552e-05,4.5575366e-09\n",
      "Iteration 00790: loss = 9.2675305e-05,4.54389e-09\n",
      "Iteration 00795: loss = 9.2065246e-05,4.5302926e-09\n",
      "Iteration 00800: loss = 9.1461545e-05,4.5167408e-09\n",
      "Iteration 00805: loss = 9.086742e-05,4.503237e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00810: loss = 9.028099e-05,4.48978e-09\n",
      "Iteration 00815: loss = 8.970177e-05,4.476372e-09\n",
      "Iteration 00820: loss = 8.913069e-05,4.4630126e-09\n",
      "Iteration 00825: loss = 8.856816e-05,4.4497e-09\n",
      "Iteration 00830: loss = 8.801302e-05,4.436438e-09\n",
      "Iteration 00835: loss = 8.746429e-05,4.4232245e-09\n",
      "Iteration 00840: loss = 8.692381e-05,4.4100594e-09\n",
      "Iteration 00845: loss = 8.6390464e-05,4.3969446e-09\n",
      "Iteration 00850: loss = 8.586224e-05,4.383879e-09\n",
      "Iteration 00855: loss = 8.534254e-05,4.370862e-09\n",
      "Iteration 00860: loss = 8.483037e-05,4.3578976e-09\n",
      "Iteration 00865: loss = 8.4324536e-05,4.3449813e-09\n",
      "Iteration 00870: loss = 8.382349e-05,4.3321164e-09\n",
      "Iteration 00875: loss = 8.332967e-05,4.3193022e-09\n",
      "Iteration 00880: loss = 8.2843115e-05,4.3065387e-09\n",
      "Iteration 00885: loss = 8.236259e-05,4.2938253e-09\n",
      "Iteration 00890: loss = 8.188915e-05,4.2811648e-09\n",
      "Iteration 00895: loss = 8.1420214e-05,4.2685535e-09\n",
      "Iteration 00900: loss = 8.095714e-05,4.2559947e-09\n",
      "Iteration 00905: loss = 8.049997e-05,4.2434865e-09\n",
      "Iteration 00910: loss = 8.004951e-05,4.2310315e-09\n",
      "Iteration 00915: loss = 7.960342e-05,4.2186272e-09\n",
      "Iteration 00920: loss = 7.916349e-05,4.2062753e-09\n",
      "Iteration 00925: loss = 7.873064e-05,4.193975e-09\n",
      "Iteration 00930: loss = 7.830047e-05,4.181727e-09\n",
      "Iteration 00935: loss = 7.7877296e-05,4.1695314e-09\n",
      "Iteration 00940: loss = 7.745918e-05,4.157387e-09\n",
      "Iteration 00945: loss = 7.704594e-05,4.1452948e-09\n",
      "Iteration 00950: loss = 7.663815e-05,4.133255e-09\n",
      "Iteration 00955: loss = 7.6235716e-05,4.121269e-09\n",
      "Iteration 00960: loss = 7.583852e-05,4.109334e-09\n",
      "Iteration 00965: loss = 7.5444674e-05,4.097453e-09\n",
      "Iteration 00970: loss = 7.5056974e-05,4.0856247e-09\n",
      "Iteration 00975: loss = 7.467402e-05,4.073848e-09\n",
      "Iteration 00980: loss = 7.429516e-05,4.062123e-09\n",
      "Iteration 00985: loss = 7.392142e-05,4.050452e-09\n",
      "Iteration 00990: loss = 7.3552765e-05,4.0388346e-09\n",
      "Iteration 00995: loss = 7.318806e-05,4.0272683e-09\n",
      "Iteration 01000: loss = 7.282798e-05,4.0157553e-09\n",
      "Iteration 01005: loss = 7.24722e-05,4.004295e-09\n",
      "Iteration 01010: loss = 7.2119925e-05,3.992888e-09\n",
      "Iteration 01015: loss = 7.1773495e-05,3.9815324e-09\n",
      "Iteration 01020: loss = 7.142999e-05,3.97023e-09\n",
      "Iteration 01025: loss = 7.109219e-05,3.9589807e-09\n",
      "Iteration 01030: loss = 7.075775e-05,3.9477834e-09\n",
      "Iteration 01035: loss = 7.042751e-05,3.936638e-09\n",
      "Iteration 01040: loss = 7.010062e-05,3.9255466e-09\n",
      "Iteration 01045: loss = 6.977974e-05,3.9145065e-09\n",
      "Iteration 01050: loss = 6.9461086e-05,3.9035193e-09\n",
      "Iteration 01055: loss = 6.91471e-05,3.892585e-09\n",
      "Iteration 01060: loss = 6.8836154e-05,3.8817025e-09\n",
      "Iteration 01065: loss = 6.853032e-05,3.8708725e-09\n",
      "Iteration 01070: loss = 6.8228e-05,3.860094e-09\n",
      "Iteration 01075: loss = 6.792784e-05,3.8493684e-09\n",
      "Iteration 01080: loss = 6.763305e-05,3.8386943e-09\n",
      "Iteration 01085: loss = 6.734218e-05,3.828072e-09\n",
      "Iteration 01090: loss = 6.705433e-05,3.8175023e-09\n",
      "Iteration 01095: loss = 6.676993e-05,3.806983e-09\n",
      "Iteration 01100: loss = 6.648935e-05,3.7965164e-09\n",
      "Iteration 01105: loss = 6.621173e-05,3.7861008e-09\n",
      "Iteration 01110: loss = 6.593839e-05,3.775737e-09\n",
      "Iteration 01115: loss = 6.5667875e-05,3.765424e-09\n",
      "Iteration 01120: loss = 6.540074e-05,3.7551633e-09\n",
      "Iteration 01125: loss = 6.513819e-05,3.744953e-09\n",
      "Iteration 01130: loss = 6.4878484e-05,3.7347934e-09\n",
      "Iteration 01135: loss = 6.462112e-05,3.7246861e-09\n",
      "Iteration 01140: loss = 6.436787e-05,3.7146273e-09\n",
      "Iteration 01145: loss = 6.411745e-05,3.7046202e-09\n",
      "Iteration 01150: loss = 6.3870604e-05,3.6946632e-09\n",
      "Iteration 01155: loss = 6.362641e-05,3.6847567e-09\n",
      "Iteration 01160: loss = 6.338679e-05,3.6748995e-09\n",
      "Iteration 01165: loss = 6.314888e-05,3.6650936e-09\n",
      "Iteration 01170: loss = 6.291503e-05,3.655337e-09\n",
      "Iteration 01175: loss = 6.2683306e-05,3.6456302e-09\n",
      "Iteration 01180: loss = 6.245468e-05,3.6359737e-09\n",
      "Iteration 01185: loss = 6.2229934e-05,3.6263652e-09\n",
      "Iteration 01190: loss = 6.200825e-05,3.616807e-09\n",
      "Iteration 01195: loss = 6.1788654e-05,3.6072965e-09\n",
      "Iteration 01200: loss = 6.157255e-05,3.597835e-09\n",
      "Iteration 01205: loss = 6.1358915e-05,3.5884231e-09\n",
      "Iteration 01210: loss = 6.114867e-05,3.579059e-09\n",
      "Iteration 01215: loss = 6.0941395e-05,3.5697445e-09\n",
      "Iteration 01220: loss = 6.073659e-05,3.5604761e-09\n",
      "Iteration 01225: loss = 6.053422e-05,3.5512575e-09\n",
      "Iteration 01230: loss = 6.033569e-05,3.5420864e-09\n",
      "Iteration 01235: loss = 6.0139504e-05,3.5329615e-09\n",
      "Iteration 01240: loss = 5.994578e-05,3.523886e-09\n",
      "Iteration 01245: loss = 5.9754017e-05,3.5148569e-09\n",
      "Iteration 01250: loss = 5.9566737e-05,3.5058756e-09\n",
      "Iteration 01255: loss = 5.938105e-05,3.4969398e-09\n",
      "Iteration 01260: loss = 5.9198108e-05,3.4880523e-09\n",
      "Iteration 01265: loss = 5.9017573e-05,3.47921e-09\n",
      "Iteration 01270: loss = 5.8841015e-05,3.4704135e-09\n",
      "Iteration 01275: loss = 5.8665577e-05,3.4616638e-09\n",
      "Iteration 01280: loss = 5.849281e-05,3.4529593e-09\n",
      "Iteration 01285: loss = 5.832362e-05,3.4443017e-09\n",
      "Iteration 01290: loss = 5.815629e-05,3.4356886e-09\n",
      "Iteration 01295: loss = 5.7991612e-05,3.4271215e-09\n",
      "Iteration 01300: loss = 5.782962e-05,3.4185992e-09\n",
      "Iteration 01305: loss = 5.766907e-05,3.4101224e-09\n",
      "Iteration 01310: loss = 5.7511555e-05,3.40169e-09\n",
      "Iteration 01315: loss = 5.735715e-05,3.3933025e-09\n",
      "Iteration 01320: loss = 5.7204492e-05,3.384959e-09\n",
      "Iteration 01325: loss = 5.7053996e-05,3.376659e-09\n",
      "Iteration 01330: loss = 5.6906545e-05,3.3684033e-09\n",
      "Iteration 01335: loss = 5.676087e-05,3.3601912e-09\n",
      "Iteration 01340: loss = 5.661863e-05,3.352021e-09\n",
      "Iteration 01345: loss = 5.647802e-05,3.343896e-09\n",
      "Iteration 01350: loss = 5.6339177e-05,3.3358132e-09\n",
      "Iteration 01355: loss = 5.62037e-05,3.3277727e-09\n",
      "Iteration 01360: loss = 5.6069523e-05,3.3197751e-09\n",
      "Iteration 01365: loss = 5.593832e-05,3.31182e-09\n",
      "Iteration 01370: loss = 5.5809553e-05,3.3039065e-09\n",
      "Iteration 01375: loss = 5.568204e-05,3.2960354e-09\n",
      "Iteration 01380: loss = 5.555832e-05,3.288204e-09\n",
      "Iteration 01385: loss = 5.5434997e-05,3.2804168e-09\n",
      "Iteration 01390: loss = 5.531406e-05,3.2726706e-09\n",
      "Iteration 01395: loss = 5.519652e-05,3.2649636e-09\n",
      "Iteration 01400: loss = 5.508056e-05,3.257298e-09\n",
      "Iteration 01405: loss = 5.4966953e-05,3.249674e-09\n",
      "Iteration 01410: loss = 5.4855074e-05,3.2420893e-09\n",
      "Iteration 01415: loss = 5.4745513e-05,3.2345464e-09\n",
      "Iteration 01420: loss = 5.463762e-05,3.2270417e-09\n",
      "Iteration 01425: loss = 5.45329e-05,3.2195775e-09\n",
      "Iteration 01430: loss = 5.4429383e-05,3.2121532e-09\n",
      "Iteration 01435: loss = 5.4327782e-05,3.2047665e-09\n",
      "Iteration 01440: loss = 5.4228618e-05,3.1974206e-09\n",
      "Iteration 01445: loss = 5.413135e-05,3.1901115e-09\n",
      "Iteration 01450: loss = 5.4036438e-05,3.1828424e-09\n",
      "Iteration 01455: loss = 5.394266e-05,3.1756113e-09\n",
      "Iteration 01460: loss = 5.3851643e-05,3.168418e-09\n",
      "Iteration 01465: loss = 5.376292e-05,3.161263e-09\n",
      "Iteration 01470: loss = 5.3675725e-05,3.154145e-09\n",
      "Iteration 01475: loss = 5.3590862e-05,3.1470646e-09\n",
      "Iteration 01480: loss = 5.3507396e-05,3.140022e-09\n",
      "Iteration 01485: loss = 5.3426334e-05,3.1330147e-09\n",
      "Iteration 01490: loss = 5.33467e-05,3.1260445e-09\n",
      "Iteration 01495: loss = 5.3268584e-05,3.1191105e-09\n",
      "Iteration 01500: loss = 5.3193497e-05,3.112213e-09\n",
      "Iteration 01505: loss = 5.3119453e-05,3.1053524e-09\n",
      "Iteration 01510: loss = 5.3048123e-05,3.0985272e-09\n",
      "Iteration 01515: loss = 5.2977804e-05,3.0917366e-09\n",
      "Iteration 01520: loss = 5.291012e-05,3.084982e-09\n",
      "Iteration 01525: loss = 5.2843472e-05,3.078262e-09\n",
      "Iteration 01530: loss = 5.277904e-05,3.0715774e-09\n",
      "Iteration 01535: loss = 5.2716437e-05,3.0649263e-09\n",
      "Iteration 01540: loss = 5.2655658e-05,3.0583107e-09\n",
      "Iteration 01545: loss = 5.259619e-05,3.0517295e-09\n",
      "Iteration 01550: loss = 5.2539053e-05,3.0451814e-09\n",
      "Iteration 01555: loss = 5.2483974e-05,3.0386678e-09\n",
      "Iteration 01560: loss = 5.2429576e-05,3.0321867e-09\n",
      "Iteration 01565: loss = 5.2378087e-05,3.02574e-09\n",
      "Iteration 01570: loss = 5.2327992e-05,3.0193266e-09\n",
      "Iteration 01575: loss = 5.227882e-05,3.0129457e-09\n",
      "Iteration 01580: loss = 5.223147e-05,3.0065965e-09\n",
      "Iteration 01585: loss = 5.2187057e-05,3.0002807e-09\n",
      "Iteration 01590: loss = 5.214276e-05,2.9939975e-09\n",
      "Iteration 01595: loss = 5.2101794e-05,2.9877463e-09\n",
      "Iteration 01600: loss = 5.2060957e-05,2.9815264e-09\n",
      "Iteration 01605: loss = 5.2022657e-05,2.9753373e-09\n",
      "Iteration 01610: loss = 5.1985207e-05,2.9691811e-09\n",
      "Iteration 01615: loss = 5.194993e-05,2.9630534e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 01620: loss = 5.1917083e-05,2.956959e-09\n",
      "Iteration 01625: loss = 5.1884435e-05,2.9508942e-09\n",
      "Iteration 01630: loss = 5.185376e-05,2.9448586e-09\n",
      "Iteration 01635: loss = 5.1825267e-05,2.9388532e-09\n",
      "Iteration 01640: loss = 5.1797193e-05,2.932878e-09\n",
      "Iteration 01645: loss = 5.1771232e-05,2.926932e-09\n",
      "Iteration 01650: loss = 5.1746476e-05,2.9210157e-09\n",
      "Iteration 01655: loss = 5.1723775e-05,2.9151277e-09\n",
      "Iteration 01660: loss = 5.170275e-05,2.9092693e-09\n",
      "Iteration 01665: loss = 5.168343e-05,2.9034386e-09\n",
      "Iteration 01670: loss = 5.1663665e-05,2.8976357e-09\n",
      "Iteration 01675: loss = 5.1647516e-05,2.8918627e-09\n",
      "Iteration 01680: loss = 5.1631418e-05,2.8861156e-09\n",
      "Iteration 01685: loss = 5.161774e-05,2.8803981e-09\n",
      "Iteration 01690: loss = 5.1604864e-05,2.8747063e-09\n",
      "Iteration 01695: loss = 5.159357e-05,2.869043e-09\n",
      "Iteration 01700: loss = 5.158391e-05,2.863405e-09\n",
      "Iteration 01705: loss = 5.157458e-05,2.8577944e-09\n",
      "Iteration 01710: loss = 5.1568466e-05,2.85221e-09\n",
      "Iteration 01715: loss = 5.156231e-05,2.846652e-09\n",
      "Iteration 01720: loss = 5.1558116e-05,2.8411193e-09\n",
      "Iteration 01725: loss = 5.1554292e-05,2.8356122e-09\n",
      "Iteration 01730: loss = 5.155239e-05,2.8301312e-09\n",
      "Iteration 01735: loss = 5.1550825e-05,2.8246756e-09\n",
      "Iteration 01740: loss = 5.1552033e-05,2.8192448e-09\n",
      "Iteration 01745: loss = 5.1553518e-05,2.8138376e-09\n",
      "Iteration 01750: loss = 5.1556144e-05,2.8084555e-09\n",
      "Iteration 01755: loss = 5.156064e-05,2.8030978e-09\n",
      "Iteration 01760: loss = 5.156627e-05,2.7977638e-09\n",
      "Iteration 01765: loss = 5.157221e-05,2.792453e-09\n",
      "Iteration 01770: loss = 5.1579624e-05,2.7871654e-09\n",
      "Iteration 01775: loss = 5.15885e-05,2.7819016e-09\n",
      "Iteration 01780: loss = 5.1598472e-05,2.7766602e-09\n",
      "Iteration 01785: loss = 5.1609066e-05,2.7714413e-09\n",
      "Iteration 01790: loss = 5.162113e-05,2.766246e-09\n",
      "Iteration 01795: loss = 5.163457e-05,2.7610711e-09\n",
      "Iteration 01800: loss = 5.1647916e-05,2.7559186e-09\n",
      "Iteration 01805: loss = 5.166343e-05,2.7507885e-09\n",
      "Iteration 01810: loss = 5.1679603e-05,2.7456781e-09\n",
      "Iteration 01815: loss = 5.1696337e-05,2.7405902e-09\n",
      "Iteration 01820: loss = 5.1714098e-05,2.735523e-09\n",
      "Iteration 01825: loss = 5.1733223e-05,2.730475e-09\n",
      "Iteration 01830: loss = 5.1752926e-05,2.7254479e-09\n",
      "Iteration 01835: loss = 5.1772742e-05,2.7204414e-09\n",
      "Iteration 01840: loss = 5.1793566e-05,2.7154545e-09\n",
      "Iteration 01845: loss = 5.1815285e-05,2.710487e-09\n",
      "Iteration 01850: loss = 5.183794e-05,2.7055387e-09\n",
      "Iteration 01855: loss = 5.1861905e-05,2.7006093e-09\n",
      "Iteration 01860: loss = 5.1886385e-05,2.6956979e-09\n",
      "Iteration 01865: loss = 5.191096e-05,2.6908045e-09\n",
      "Iteration 01870: loss = 5.1937255e-05,2.6859304e-09\n",
      "Iteration 01875: loss = 5.196277e-05,2.6810738e-09\n",
      "Iteration 01880: loss = 5.1989122e-05,2.6762355e-09\n",
      "Iteration 01885: loss = 5.2016756e-05,2.6714138e-09\n",
      "Iteration 01890: loss = 5.2044794e-05,2.6666114e-09\n",
      "Iteration 01895: loss = 5.2072864e-05,2.6618237e-09\n",
      "Iteration 01900: loss = 5.210092e-05,2.6570537e-09\n",
      "Iteration 01905: loss = 5.2130654e-05,2.6522997e-09\n",
      "Iteration 01910: loss = 5.216031e-05,2.647561e-09\n",
      "Iteration 01915: loss = 5.2189527e-05,2.6428382e-09\n",
      "Iteration 01920: loss = 5.2220723e-05,2.6381302e-09\n",
      "Iteration 01925: loss = 5.2251064e-05,2.6334375e-09\n",
      "Iteration 01930: loss = 5.2282154e-05,2.6287592e-09\n",
      "Iteration 01935: loss = 5.231355e-05,2.6240945e-09\n",
      "Iteration 01940: loss = 5.2345265e-05,2.6194449e-09\n",
      "Iteration 01945: loss = 5.2376883e-05,2.6148088e-09\n",
      "Iteration 01950: loss = 5.240798e-05,2.6101858e-09\n",
      "Iteration 01955: loss = 5.243937e-05,2.6055758e-09\n",
      "Iteration 01960: loss = 5.247218e-05,2.6009797e-09\n",
      "Iteration 01965: loss = 5.2503703e-05,2.596394e-09\n",
      "Iteration 01970: loss = 5.253541e-05,2.5918219e-09\n",
      "Iteration 01975: loss = 5.2567382e-05,2.587261e-09\n",
      "Iteration 01980: loss = 5.2599546e-05,2.582712e-09\n",
      "Iteration 01985: loss = 5.2631516e-05,2.5781743e-09\n",
      "Iteration 01990: loss = 5.2662857e-05,2.5736475e-09\n",
      "Iteration 01995: loss = 5.269524e-05,2.5691307e-09\n",
      "Iteration 02000: loss = 5.2726133e-05,2.5646245e-09\n",
      "Iteration 02005: loss = 5.275724e-05,2.5601274e-09\n",
      "Iteration 02010: loss = 5.278844e-05,2.5556417e-09\n",
      "Iteration 02015: loss = 5.281896e-05,2.551166e-09\n",
      "Iteration 02020: loss = 5.284917e-05,2.5466997e-09\n",
      "Iteration 02025: loss = 5.2879535e-05,2.542242e-09\n",
      "Iteration 02030: loss = 5.2908403e-05,2.5377922e-09\n",
      "Iteration 02035: loss = 5.2938107e-05,2.5333522e-09\n",
      "Iteration 02040: loss = 5.2966367e-05,2.528919e-09\n",
      "Iteration 02045: loss = 5.299384e-05,2.5244944e-09\n",
      "Iteration 02050: loss = 5.302306e-05,2.520078e-09\n",
      "Iteration 02055: loss = 5.3049476e-05,2.5156677e-09\n",
      "Iteration 02060: loss = 5.3075957e-05,2.5112643e-09\n",
      "Iteration 02065: loss = 5.310174e-05,2.506868e-09\n",
      "Iteration 02070: loss = 5.3127966e-05,2.5024782e-09\n",
      "Iteration 02075: loss = 5.31518e-05,2.498094e-09\n",
      "Iteration 02080: loss = 5.3176496e-05,2.4937163e-09\n",
      "Iteration 02085: loss = 5.320001e-05,2.4893447e-09\n",
      "Iteration 02090: loss = 5.3222782e-05,2.484978e-09\n",
      "Iteration 02095: loss = 5.3244792e-05,2.4806162e-09\n",
      "Iteration 02100: loss = 5.3266453e-05,2.4762592e-09\n",
      "Iteration 02105: loss = 5.328693e-05,2.4719071e-09\n",
      "Iteration 02110: loss = 5.3306674e-05,2.4675586e-09\n",
      "Iteration 02115: loss = 5.332562e-05,2.4632159e-09\n",
      "Iteration 02120: loss = 5.3343418e-05,2.4588769e-09\n",
      "Iteration 02125: loss = 5.3361768e-05,2.454542e-09\n",
      "Iteration 02130: loss = 5.3378873e-05,2.4502107e-09\n",
      "Iteration 02135: loss = 5.3394073e-05,2.4458824e-09\n",
      "Iteration 02140: loss = 5.3409414e-05,2.4415572e-09\n",
      "Iteration 02145: loss = 5.3424057e-05,2.4372344e-09\n",
      "Iteration 02150: loss = 5.3438405e-05,2.4329148e-09\n",
      "Iteration 02155: loss = 5.3450884e-05,2.4285964e-09\n",
      "Iteration 02160: loss = 5.346394e-05,2.4242808e-09\n",
      "Iteration 02165: loss = 5.3475178e-05,2.4199656e-09\n",
      "Iteration 02170: loss = 5.348706e-05,2.4156521e-09\n",
      "Iteration 02175: loss = 5.349742e-05,2.4113405e-09\n",
      "Iteration 02180: loss = 5.3508196e-05,2.4070286e-09\n",
      "Iteration 02185: loss = 5.3518055e-05,2.402718e-09\n",
      "Iteration 02190: loss = 5.3526946e-05,2.3984081e-09\n",
      "Iteration 02195: loss = 5.353581e-05,2.3940983e-09\n",
      "Iteration 02200: loss = 5.354462e-05,2.3897888e-09\n",
      "Iteration 02205: loss = 5.355261e-05,2.3854787e-09\n",
      "Iteration 02210: loss = 5.355981e-05,2.3811693e-09\n",
      "Iteration 02215: loss = 5.356831e-05,2.376859e-09\n",
      "Iteration 02220: loss = 5.3575757e-05,2.3725477e-09\n",
      "Iteration 02225: loss = 5.3583324e-05,2.3682363e-09\n",
      "Iteration 02230: loss = 5.3591062e-05,2.3639228e-09\n",
      "Iteration 02235: loss = 5.3598622e-05,2.359609e-09\n",
      "Iteration 02240: loss = 5.360685e-05,2.3552933e-09\n",
      "Iteration 02245: loss = 5.361541e-05,2.350976e-09\n",
      "Iteration 02250: loss = 5.3623517e-05,2.346658e-09\n",
      "Iteration 02255: loss = 5.3632848e-05,2.342337e-09\n",
      "Iteration 02260: loss = 5.3643005e-05,2.3380142e-09\n",
      "Iteration 02265: loss = 5.3653293e-05,2.3336888e-09\n",
      "Iteration 02270: loss = 5.366497e-05,2.329362e-09\n",
      "Iteration 02275: loss = 5.3677093e-05,2.3250335e-09\n",
      "Iteration 02280: loss = 5.3689975e-05,2.3207023e-09\n",
      "Iteration 02285: loss = 5.370393e-05,2.3163675e-09\n",
      "Iteration 02290: loss = 5.371986e-05,2.312031e-09\n",
      "Iteration 02295: loss = 5.373739e-05,2.307691e-09\n",
      "Iteration 02300: loss = 5.375493e-05,2.3033477e-09\n",
      "Iteration 02305: loss = 5.3775086e-05,2.2990012e-09\n",
      "Iteration 02310: loss = 5.3796943e-05,2.2946516e-09\n",
      "Iteration 02315: loss = 5.382069e-05,2.290298e-09\n",
      "Iteration 02320: loss = 5.3846266e-05,2.2859403e-09\n",
      "Iteration 02325: loss = 5.38742e-05,2.2815794e-09\n",
      "Iteration 02330: loss = 5.390366e-05,2.277214e-09\n",
      "Iteration 02335: loss = 5.3936834e-05,2.2728452e-09\n",
      "Iteration 02340: loss = 5.397209e-05,2.2684719e-09\n",
      "Iteration 02345: loss = 5.4009983e-05,2.2640925e-09\n",
      "Iteration 02350: loss = 5.405038e-05,2.2597106e-09\n",
      "Iteration 02355: loss = 5.4094246e-05,2.255324e-09\n",
      "Iteration 02360: loss = 5.4140382e-05,2.2509319e-09\n",
      "Iteration 02365: loss = 5.4190983e-05,2.246535e-09\n",
      "Iteration 02370: loss = 5.424435e-05,2.2421336e-09\n",
      "Iteration 02375: loss = 5.4300956e-05,2.2377262e-09\n",
      "Iteration 02380: loss = 5.4361702e-05,2.2333133e-09\n",
      "Iteration 02385: loss = 5.4425775e-05,2.2288953e-09\n",
      "Iteration 02390: loss = 5.449372e-05,2.2244715e-09\n",
      "Iteration 02395: loss = 5.456708e-05,2.2200424e-09\n",
      "Iteration 02400: loss = 5.4642707e-05,2.2156081e-09\n",
      "Iteration 02405: loss = 5.4723892e-05,2.2111686e-09\n",
      "Iteration 02410: loss = 5.4808206e-05,2.2067228e-09\n",
      "Iteration 02415: loss = 5.4898614e-05,2.2022706e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 02420: loss = 5.4992648e-05,2.1978126e-09\n",
      "Iteration 02425: loss = 5.509199e-05,2.193349e-09\n",
      "Iteration 02430: loss = 5.5196302e-05,2.1888797e-09\n",
      "Iteration 02435: loss = 5.5305587e-05,2.1844033e-09\n",
      "Iteration 02440: loss = 5.541819e-05,2.1799214e-09\n",
      "Iteration 02445: loss = 5.553795e-05,2.1754332e-09\n",
      "Iteration 02450: loss = 5.5662327e-05,2.1709385e-09\n",
      "Iteration 02455: loss = 5.5792338e-05,2.1664366e-09\n",
      "Iteration 02460: loss = 5.5927e-05,2.1619293e-09\n",
      "Iteration 02465: loss = 5.606853e-05,2.157416e-09\n",
      "Iteration 02470: loss = 5.6215213e-05,2.1528956e-09\n",
      "Iteration 02475: loss = 5.636721e-05,2.148368e-09\n",
      "Iteration 02480: loss = 5.6525747e-05,2.1438336e-09\n",
      "Iteration 02485: loss = 5.6689973e-05,2.139293e-09\n",
      "Iteration 02490: loss = 5.6860375e-05,2.1347468e-09\n",
      "Iteration 02495: loss = 5.7035602e-05,2.1301945e-09\n",
      "Iteration 02500: loss = 5.7217938e-05,2.1256354e-09\n",
      "Iteration 02505: loss = 5.7405672e-05,2.1210693e-09\n",
      "Iteration 02510: loss = 5.7600497e-05,2.1164979e-09\n",
      "Iteration 02515: loss = 5.7801677e-05,2.1119198e-09\n",
      "Iteration 02520: loss = 5.8008256e-05,2.107335e-09\n",
      "Iteration 02525: loss = 5.8221613e-05,2.1027446e-09\n",
      "Iteration 02530: loss = 5.8441856e-05,2.0981474e-09\n",
      "Iteration 02535: loss = 5.866797e-05,2.0935453e-09\n",
      "Iteration 02540: loss = 5.8901816e-05,2.088937e-09\n",
      "Iteration 02545: loss = 5.9141283e-05,2.0843232e-09\n",
      "Iteration 02550: loss = 5.9387665e-05,2.0797026e-09\n",
      "Iteration 02555: loss = 5.9640937e-05,2.0750774e-09\n",
      "Iteration 02560: loss = 5.9900805e-05,2.070446e-09\n",
      "Iteration 02565: loss = 6.0168026e-05,2.0658089e-09\n",
      "Iteration 02570: loss = 6.0442024e-05,2.0611666e-09\n",
      "Iteration 02575: loss = 6.0722097e-05,2.0565185e-09\n",
      "Iteration 02580: loss = 6.1008846e-05,2.0518662e-09\n",
      "Iteration 02585: loss = 6.1304054e-05,2.0472082e-09\n",
      "Iteration 02590: loss = 6.1605e-05,2.0425457e-09\n",
      "Iteration 02595: loss = 6.1914114e-05,2.0378788e-09\n",
      "Iteration 02600: loss = 6.2229556e-05,2.033207e-09\n",
      "Iteration 02605: loss = 6.255179e-05,2.028531e-09\n",
      "Iteration 02610: loss = 6.288138e-05,2.02385e-09\n",
      "Iteration 02615: loss = 6.321826e-05,2.0191648e-09\n",
      "Iteration 02620: loss = 6.356343e-05,2.0144757e-09\n",
      "Iteration 02625: loss = 6.391378e-05,2.0097821e-09\n",
      "Iteration 02630: loss = 6.427384e-05,2.005085e-09\n",
      "Iteration 02635: loss = 6.463952e-05,2.0003854e-09\n",
      "Iteration 02640: loss = 6.5012275e-05,1.995682e-09\n",
      "Iteration 02645: loss = 6.539447e-05,1.9909747e-09\n",
      "Iteration 02650: loss = 6.578205e-05,1.9862627e-09\n",
      "Iteration 02655: loss = 6.617811e-05,1.9815487e-09\n",
      "Iteration 02660: loss = 6.658144e-05,1.976832e-09\n",
      "Iteration 02665: loss = 6.699205e-05,1.9721123e-09\n",
      "Iteration 02670: loss = 6.741092e-05,1.9673898e-09\n",
      "Iteration 02675: loss = 6.7836656e-05,1.962666e-09\n",
      "Iteration 02680: loss = 6.8269794e-05,1.9579383e-09\n",
      "Iteration 02685: loss = 6.871141e-05,1.9532078e-09\n",
      "Iteration 02690: loss = 6.916049e-05,1.948476e-09\n",
      "Iteration 02695: loss = 6.9616995e-05,1.9437427e-09\n",
      "Iteration 02700: loss = 7.008028e-05,1.9390072e-09\n",
      "Iteration 02705: loss = 7.05521e-05,1.9342692e-09\n",
      "Iteration 02710: loss = 7.103199e-05,1.9295312e-09\n",
      "Iteration 02715: loss = 7.152015e-05,1.9247905e-09\n",
      "Iteration 02720: loss = 7.2015064e-05,1.9200481e-09\n",
      "Iteration 02725: loss = 7.251734e-05,1.915305e-09\n",
      "Iteration 02730: loss = 7.302704e-05,1.9105593e-09\n",
      "Iteration 02735: loss = 7.354598e-05,1.9058137e-09\n",
      "Iteration 02740: loss = 7.407196e-05,1.9010669e-09\n",
      "Iteration 02745: loss = 7.460634e-05,1.8963187e-09\n",
      "Iteration 02750: loss = 7.514726e-05,1.8915711e-09\n",
      "Iteration 02755: loss = 7.569629e-05,1.886822e-09\n",
      "Iteration 02760: loss = 7.625345e-05,1.8820707e-09\n",
      "Iteration 02765: loss = 7.6818644e-05,1.87732e-09\n",
      "Iteration 02770: loss = 7.739e-05,1.8725703e-09\n",
      "Iteration 02775: loss = 7.7970566e-05,1.8678197e-09\n",
      "Iteration 02780: loss = 7.855843e-05,1.863068e-09\n",
      "Iteration 02785: loss = 7.915395e-05,1.8583163e-09\n",
      "Iteration 02790: loss = 7.975687e-05,1.8535657e-09\n",
      "Iteration 02795: loss = 8.036696e-05,1.8488149e-09\n",
      "Iteration 02800: loss = 8.098439e-05,1.8440649e-09\n",
      "Iteration 02805: loss = 8.160712e-05,1.8393155e-09\n",
      "Iteration 02810: loss = 8.2240265e-05,1.8345665e-09\n",
      "Iteration 02815: loss = 8.287967e-05,1.829819e-09\n",
      "Iteration 02820: loss = 8.352496e-05,1.8250716e-09\n",
      "Iteration 02825: loss = 8.417811e-05,1.8203262e-09\n",
      "Iteration 02830: loss = 8.4837906e-05,1.8155824e-09\n",
      "Iteration 02835: loss = 8.550417e-05,1.8108393e-09\n",
      "Iteration 02840: loss = 8.6177395e-05,1.806097e-09\n",
      "Iteration 02845: loss = 8.68562e-05,1.8013582e-09\n",
      "Iteration 02850: loss = 8.7542365e-05,1.7966197e-09\n",
      "Iteration 02855: loss = 8.823376e-05,1.7918842e-09\n",
      "Iteration 02860: loss = 8.893238e-05,1.7871505e-09\n",
      "Iteration 02865: loss = 8.963552e-05,1.7824204e-09\n",
      "Iteration 02870: loss = 9.034335e-05,1.7776928e-09\n",
      "Iteration 02875: loss = 9.10583e-05,1.7729677e-09\n",
      "Iteration 02880: loss = 9.1777714e-05,1.7682462e-09\n",
      "Iteration 02885: loss = 9.250192e-05,1.7635291e-09\n",
      "Iteration 02890: loss = 9.3231036e-05,1.7588143e-09\n",
      "Iteration 02895: loss = 9.39639e-05,1.7541048e-09\n",
      "Iteration 02900: loss = 9.470137e-05,1.7494001e-09\n",
      "Iteration 02905: loss = 9.5441705e-05,1.7447007e-09\n",
      "Iteration 02910: loss = 9.618569e-05,1.7400044e-09\n",
      "Iteration 02915: loss = 9.693346e-05,1.735315e-09\n",
      "Iteration 02920: loss = 9.768444e-05,1.7306306e-09\n",
      "Iteration 02925: loss = 9.843856e-05,1.7259523e-09\n",
      "Iteration 02930: loss = 9.919289e-05,1.7212808e-09\n",
      "Iteration 02935: loss = 9.99495e-05,1.7166173e-09\n",
      "Iteration 02940: loss = 0.00010070767,1.7119602e-09\n",
      "Iteration 02945: loss = 0.000101464946,1.7073132e-09\n",
      "Iteration 02950: loss = 0.00010222525,1.7026742e-09\n",
      "Iteration 02955: loss = 0.00010298387,1.6980426e-09\n",
      "Iteration 02960: loss = 0.00010374297,1.6934203e-09\n",
      "Iteration 02965: loss = 0.00010449997,1.6888091e-09\n",
      "Iteration 02970: loss = 0.000105256244,1.6842089e-09\n",
      "Iteration 02975: loss = 0.00010601067,1.6796199e-09\n",
      "Iteration 02980: loss = 0.00010676261,1.6750412e-09\n",
      "Iteration 02985: loss = 0.0001075105,1.6704743e-09\n",
      "Iteration 02990: loss = 0.00010825565,1.6659226e-09\n",
      "Iteration 02995: loss = 0.000108996726,1.6613853e-09\n",
      "Iteration 03000: loss = 0.00010973194,1.6568618e-09\n",
      "Iteration 03005: loss = 0.00011046158,1.6523536e-09\n",
      "Iteration 03010: loss = 0.000111184396,1.6478634e-09\n",
      "Iteration 03015: loss = 0.00011190064,1.643388e-09\n",
      "Iteration 03020: loss = 0.00011260988,1.6389307e-09\n",
      "Iteration 03025: loss = 0.00011331105,1.6344918e-09\n",
      "Iteration 03030: loss = 0.00011400235,1.6300725e-09\n",
      "Iteration 03035: loss = 0.00011468571,1.625672e-09\n",
      "Iteration 03040: loss = 0.00011535777,1.6212935e-09\n",
      "Iteration 03045: loss = 0.000116018666,1.6169378e-09\n",
      "Iteration 03050: loss = 0.00011666597,1.612605e-09\n",
      "Iteration 03055: loss = 0.000117300835,1.6082976e-09\n",
      "Iteration 03060: loss = 0.00011792258,1.6040145e-09\n",
      "Iteration 03065: loss = 0.00011852872,1.5997579e-09\n",
      "Iteration 03070: loss = 0.000119119824,1.5955286e-09\n",
      "Iteration 03075: loss = 0.0001196973,1.591326e-09\n",
      "Iteration 03080: loss = 0.000120254525,1.5871542e-09\n",
      "Iteration 03085: loss = 0.00012079531,1.5830139e-09\n",
      "Iteration 03090: loss = 0.0001213146,1.578904e-09\n",
      "Iteration 03095: loss = 0.00012181712,1.5748266e-09\n",
      "Iteration 03100: loss = 0.00012229802,1.5707848e-09\n",
      "Iteration 03105: loss = 0.00012275687,1.5667758e-09\n",
      "Iteration 03110: loss = 0.00012319356,1.5628034e-09\n",
      "Iteration 03115: loss = 0.00012360682,1.5588687e-09\n",
      "Iteration 03120: loss = 0.00012399888,1.554972e-09\n",
      "Iteration 03125: loss = 0.00012436148,1.5511161e-09\n",
      "Iteration 03130: loss = 0.00012470169,1.5472993e-09\n",
      "Iteration 03135: loss = 0.00012501344,1.5435234e-09\n",
      "Iteration 03140: loss = 0.00012529873,1.5397903e-09\n",
      "Iteration 03145: loss = 0.00012555507,1.536104e-09\n",
      "Iteration 03150: loss = 0.00012578139,1.53246e-09\n",
      "Iteration 03155: loss = 0.00012598092,1.5288631e-09\n",
      "Iteration 03160: loss = 0.00012614792,1.5253125e-09\n",
      "Iteration 03165: loss = 0.00012628315,1.5218102e-09\n",
      "Iteration 03170: loss = 0.00012638855,1.5183562e-09\n",
      "Iteration 03175: loss = 0.00012646116,1.5149516e-09\n",
      "Iteration 03180: loss = 0.00012650022,1.511598e-09\n",
      "Iteration 03185: loss = 0.00012650668,1.5082958e-09\n",
      "Iteration 03190: loss = 0.00012648072,1.5050456e-09\n",
      "Iteration 03195: loss = 0.0001264168,1.5018486e-09\n",
      "Iteration 03200: loss = 0.0001263217,1.4987026e-09\n",
      "Iteration 03205: loss = 0.0001261917,1.4956109e-09\n",
      "Iteration 03210: loss = 0.00012602596,1.4925763e-09\n",
      "Iteration 03215: loss = 0.00012582648,1.489594e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 03220: loss = 0.00012559,1.4866687e-09\n",
      "Iteration 03225: loss = 0.00012531893,1.4837983e-09\n",
      "Iteration 03230: loss = 0.00012501341,1.4809827e-09\n",
      "Iteration 03235: loss = 0.00012467394,1.4782234e-09\n",
      "Iteration 03240: loss = 0.00012429859,1.4755183e-09\n",
      "Iteration 03245: loss = 0.00012388967,1.4728693e-09\n",
      "Iteration 03250: loss = 0.00012344573,1.4702755e-09\n",
      "Iteration 03255: loss = 0.00012296932,1.4677382e-09\n",
      "Iteration 03260: loss = 0.00012245832,1.4652555e-09\n",
      "Iteration 03265: loss = 0.00012191485,1.4628294e-09\n",
      "Iteration 03270: loss = 0.00012133942,1.4604568e-09\n",
      "Iteration 03275: loss = 0.00012073229,1.4581394e-09\n",
      "Iteration 03280: loss = 0.00012009385,1.455875e-09\n",
      "Iteration 03285: loss = 0.00011942845,1.4536626e-09\n",
      "Iteration 03290: loss = 0.00011873058,1.4515045e-09\n",
      "Iteration 03295: loss = 0.00011800532,1.4493977e-09\n",
      "Iteration 03300: loss = 0.0001172531,1.4473411e-09\n",
      "Iteration 03305: loss = 0.00011647298,1.4453371e-09\n",
      "Iteration 03310: loss = 0.00011566814,1.4433815e-09\n",
      "Iteration 03315: loss = 0.00011483908,1.4414747e-09\n",
      "Iteration 03320: loss = 0.000113985916,1.4396137e-09\n",
      "Iteration 03325: loss = 0.00011311242,1.4378002e-09\n",
      "Iteration 03330: loss = 0.00011221709,1.4360311e-09\n",
      "Iteration 03335: loss = 0.000111301,1.4343062e-09\n",
      "Iteration 03340: loss = 0.000110366556,1.4326237e-09\n",
      "Iteration 03345: loss = 0.00010941218,1.4309848e-09\n",
      "Iteration 03350: loss = 0.00010844309,1.4293849e-09\n",
      "Iteration 03355: loss = 0.000107458436,1.4278249e-09\n",
      "Iteration 03360: loss = 0.00010646038,1.426304e-09\n",
      "Iteration 03365: loss = 0.000105448016,1.4248182e-09\n",
      "Iteration 03370: loss = 0.000104423445,1.4233692e-09\n",
      "Iteration 03375: loss = 0.000103388134,1.4219563e-09\n",
      "Iteration 03380: loss = 0.00010234479,1.420575e-09\n",
      "Iteration 03385: loss = 0.00010129085,1.4192262e-09\n",
      "Iteration 03390: loss = 0.000100228914,1.4179092e-09\n",
      "Iteration 03395: loss = 9.916124e-05,1.4166234e-09\n",
      "Iteration 03400: loss = 9.808739e-05,1.4153649e-09\n",
      "Iteration 03405: loss = 9.701005e-05,1.4141318e-09\n",
      "Iteration 03410: loss = 9.592639e-05,1.4129277e-09\n",
      "Iteration 03415: loss = 9.484292e-05,1.4117476e-09\n",
      "Iteration 03420: loss = 9.37564e-05,1.4105921e-09\n",
      "Iteration 03425: loss = 9.267011e-05,1.4094583e-09\n",
      "Iteration 03430: loss = 9.1583155e-05,1.408347e-09\n",
      "Iteration 03435: loss = 9.049662e-05,1.4072558e-09\n",
      "Iteration 03440: loss = 8.941186e-05,1.4061841e-09\n",
      "Iteration 03445: loss = 8.83289e-05,1.4051316e-09\n",
      "Iteration 03450: loss = 8.7249435e-05,1.4040966e-09\n",
      "Iteration 03455: loss = 8.6174405e-05,1.403079e-09\n",
      "Iteration 03460: loss = 8.5101696e-05,1.4020779e-09\n",
      "Iteration 03465: loss = 8.4034866e-05,1.4010899e-09\n",
      "Iteration 03470: loss = 8.297382e-05,1.4001157e-09\n",
      "Iteration 03475: loss = 8.191919e-05,1.3991566e-09\n",
      "Iteration 03480: loss = 8.087047e-05,1.3982077e-09\n",
      "Iteration 03485: loss = 7.982848e-05,1.3972713e-09\n",
      "Iteration 03490: loss = 7.879515e-05,1.3963453e-09\n",
      "Iteration 03495: loss = 7.776998e-05,1.3954287e-09\n",
      "Iteration 03500: loss = 7.675196e-05,1.3945227e-09\n",
      "Iteration 03505: loss = 7.574267e-05,1.3936261e-09\n",
      "Iteration 03510: loss = 7.4742005e-05,1.3927368e-09\n",
      "Iteration 03515: loss = 7.375071e-05,1.3918555e-09\n",
      "Iteration 03520: loss = 7.2770104e-05,1.3909793e-09\n",
      "Iteration 03525: loss = 7.179853e-05,1.3901098e-09\n",
      "Iteration 03530: loss = 7.083662e-05,1.3892458e-09\n",
      "Iteration 03535: loss = 6.9885886e-05,1.3883862e-09\n",
      "Iteration 03540: loss = 6.894553e-05,1.3875328e-09\n",
      "Iteration 03545: loss = 6.801586e-05,1.386683e-09\n",
      "Iteration 03550: loss = 6.709625e-05,1.385837e-09\n",
      "Iteration 03555: loss = 6.61881e-05,1.3849943e-09\n",
      "Iteration 03560: loss = 6.529147e-05,1.3841551e-09\n",
      "Iteration 03565: loss = 6.440476e-05,1.3833183e-09\n",
      "Iteration 03570: loss = 6.352949e-05,1.3824846e-09\n",
      "Iteration 03575: loss = 6.26655e-05,1.3816521e-09\n",
      "Iteration 03580: loss = 6.181313e-05,1.3808208e-09\n",
      "Iteration 03585: loss = 6.097165e-05,1.3799911e-09\n",
      "Iteration 03590: loss = 6.0141636e-05,1.3791612e-09\n",
      "Iteration 03595: loss = 5.932302e-05,1.378334e-09\n",
      "Iteration 03600: loss = 5.8514986e-05,1.3775058e-09\n",
      "Iteration 03605: loss = 5.7718702e-05,1.3766789e-09\n",
      "Iteration 03610: loss = 5.6933077e-05,1.3758528e-09\n",
      "Iteration 03615: loss = 5.6159537e-05,1.375024e-09\n",
      "Iteration 03620: loss = 5.5396613e-05,1.3741964e-09\n",
      "Iteration 03625: loss = 5.4645687e-05,1.3733699e-09\n",
      "Iteration 03630: loss = 5.3903965e-05,1.3725419e-09\n",
      "Iteration 03635: loss = 5.3175158e-05,1.3717112e-09\n",
      "Iteration 03640: loss = 5.2456457e-05,1.3708801e-09\n",
      "Iteration 03645: loss = 5.1748237e-05,1.3700492e-09\n",
      "Iteration 03650: loss = 5.1049978e-05,1.3692169e-09\n",
      "Iteration 03655: loss = 5.0363702e-05,1.3683829e-09\n",
      "Iteration 03660: loss = 4.968705e-05,1.367548e-09\n",
      "Iteration 03665: loss = 4.9021783e-05,1.3667111e-09\n",
      "Iteration 03670: loss = 4.8365564e-05,1.3658734e-09\n",
      "Iteration 03675: loss = 4.7720314e-05,1.3650341e-09\n",
      "Iteration 03680: loss = 4.7084515e-05,1.3641931e-09\n",
      "Iteration 03685: loss = 4.6459052e-05,1.3633504e-09\n",
      "Iteration 03690: loss = 4.584422e-05,1.362505e-09\n",
      "Iteration 03695: loss = 4.5237965e-05,1.3616592e-09\n",
      "Iteration 03700: loss = 4.4640998e-05,1.3608101e-09\n",
      "Iteration 03705: loss = 4.405394e-05,1.3599603e-09\n",
      "Iteration 03710: loss = 4.347666e-05,1.3591079e-09\n",
      "Iteration 03715: loss = 4.2908254e-05,1.358254e-09\n",
      "Iteration 03720: loss = 4.2348733e-05,1.3573985e-09\n",
      "Iteration 03725: loss = 4.179895e-05,1.3565409e-09\n",
      "Iteration 03730: loss = 4.125702e-05,1.3556816e-09\n",
      "Iteration 03735: loss = 4.0723928e-05,1.3548204e-09\n",
      "Iteration 03740: loss = 4.0199604e-05,1.3539576e-09\n",
      "Iteration 03745: loss = 3.9683495e-05,1.3530925e-09\n",
      "Iteration 03750: loss = 3.9176248e-05,1.3522258e-09\n",
      "Iteration 03755: loss = 3.8676382e-05,1.3513567e-09\n",
      "Iteration 03760: loss = 3.8185117e-05,1.350486e-09\n",
      "Iteration 03765: loss = 3.7701036e-05,1.3496138e-09\n",
      "Iteration 03770: loss = 3.722613e-05,1.3487387e-09\n",
      "Iteration 03775: loss = 3.675772e-05,1.3478624e-09\n",
      "Iteration 03780: loss = 3.6297246e-05,1.3469844e-09\n",
      "Iteration 03785: loss = 3.5844172e-05,1.3461056e-09\n",
      "Iteration 03790: loss = 3.5398138e-05,1.3452245e-09\n",
      "Iteration 03795: loss = 3.4959623e-05,1.3443423e-09\n",
      "Iteration 03800: loss = 3.4527955e-05,1.343458e-09\n",
      "Iteration 03805: loss = 3.4102653e-05,1.3425726e-09\n",
      "Iteration 03810: loss = 3.368471e-05,1.3416851e-09\n",
      "Iteration 03815: loss = 3.327397e-05,1.3407951e-09\n",
      "Iteration 03820: loss = 3.2868902e-05,1.3399066e-09\n",
      "Iteration 03825: loss = 3.247162e-05,1.339015e-09\n",
      "Iteration 03830: loss = 3.2080272e-05,1.338121e-09\n",
      "Iteration 03835: loss = 3.169445e-05,1.3372277e-09\n",
      "Iteration 03840: loss = 3.1314772e-05,1.3363317e-09\n",
      "Iteration 03845: loss = 3.0941737e-05,1.3354358e-09\n",
      "Iteration 03850: loss = 3.0573716e-05,1.3345377e-09\n",
      "Iteration 03855: loss = 3.0212212e-05,1.3336392e-09\n",
      "Iteration 03860: loss = 2.9856506e-05,1.3327393e-09\n",
      "Iteration 03865: loss = 2.9505956e-05,1.3318372e-09\n",
      "Iteration 03870: loss = 2.916097e-05,1.3309356e-09\n",
      "Iteration 03875: loss = 2.8822224e-05,1.3300321e-09\n",
      "Iteration 03880: loss = 2.848862e-05,1.3291284e-09\n",
      "Iteration 03885: loss = 2.8159324e-05,1.3282226e-09\n",
      "Iteration 03890: loss = 2.7835928e-05,1.3273167e-09\n",
      "Iteration 03895: loss = 2.7517832e-05,1.3264104e-09\n",
      "Iteration 03900: loss = 2.720408e-05,1.3255027e-09\n",
      "Iteration 03905: loss = 2.6895163e-05,1.3245935e-09\n",
      "Iteration 03910: loss = 2.6591106e-05,1.3236847e-09\n",
      "Iteration 03915: loss = 2.6292068e-05,1.3227747e-09\n",
      "Iteration 03920: loss = 2.5997359e-05,1.3218647e-09\n",
      "Iteration 03925: loss = 2.5707583e-05,1.3209533e-09\n",
      "Iteration 03930: loss = 2.5422065e-05,1.3200419e-09\n",
      "Iteration 03935: loss = 2.5140775e-05,1.3191296e-09\n",
      "Iteration 03940: loss = 2.486333e-05,1.3182169e-09\n",
      "Iteration 03945: loss = 2.4591098e-05,1.3173047e-09\n",
      "Iteration 03950: loss = 2.4322355e-05,1.3163916e-09\n",
      "Iteration 03955: loss = 2.4057632e-05,1.3154782e-09\n",
      "Iteration 03960: loss = 2.3796842e-05,1.3145647e-09\n",
      "Iteration 03965: loss = 2.3541028e-05,1.3136503e-09\n",
      "Iteration 03970: loss = 2.3288243e-05,1.312735e-09\n",
      "Iteration 03975: loss = 2.3039005e-05,1.3118195e-09\n",
      "Iteration 03980: loss = 2.279377e-05,1.3109038e-09\n",
      "Iteration 03985: loss = 2.2552216e-05,1.3099886e-09\n",
      "Iteration 03990: loss = 2.2313703e-05,1.3090751e-09\n",
      "Iteration 03995: loss = 2.2079417e-05,1.3081591e-09\n",
      "Iteration 04000: loss = 2.1848648e-05,1.3072439e-09\n",
      "Iteration 04005: loss = 2.162136e-05,1.3063295e-09\n",
      "Iteration 04010: loss = 2.1396258e-05,1.3054143e-09\n",
      "Iteration 04015: loss = 2.117535e-05,1.3044994e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04020: loss = 2.095759e-05,1.3035851e-09\n",
      "Iteration 04025: loss = 2.0742862e-05,1.3026707e-09\n",
      "Iteration 04030: loss = 2.0531237e-05,1.3017563e-09\n",
      "Iteration 04035: loss = 2.0322828e-05,1.3008425e-09\n",
      "Iteration 04040: loss = 2.0117663e-05,1.2999286e-09\n",
      "Iteration 04045: loss = 1.9914673e-05,1.2990147e-09\n",
      "Iteration 04050: loss = 1.9715084e-05,1.2981017e-09\n",
      "Iteration 04055: loss = 1.9518324e-05,1.2971892e-09\n",
      "Iteration 04060: loss = 1.9324656e-05,1.2962763e-09\n",
      "Iteration 04065: loss = 1.9132834e-05,1.2953627e-09\n",
      "Iteration 04070: loss = 1.8944736e-05,1.2944505e-09\n",
      "Iteration 04075: loss = 1.8758823e-05,1.2935389e-09\n",
      "Iteration 04080: loss = 1.8575383e-05,1.2926281e-09\n",
      "Iteration 04085: loss = 1.839434e-05,1.2917176e-09\n",
      "Iteration 04090: loss = 1.8216175e-05,1.2908077e-09\n",
      "Iteration 04095: loss = 1.804035e-05,1.2898994e-09\n",
      "Iteration 04100: loss = 1.786687e-05,1.2889906e-09\n",
      "Iteration 04105: loss = 1.7696186e-05,1.2880814e-09\n",
      "Iteration 04110: loss = 1.752727e-05,1.2871741e-09\n",
      "Iteration 04115: loss = 1.7360831e-05,1.286267e-09\n",
      "Iteration 04120: loss = 1.7196855e-05,1.2853603e-09\n",
      "Iteration 04125: loss = 1.7034856e-05,1.2844543e-09\n",
      "Iteration 04130: loss = 1.6875218e-05,1.2835493e-09\n",
      "Iteration 04135: loss = 1.6717973e-05,1.2826442e-09\n",
      "Iteration 04140: loss = 1.6562377e-05,1.2817408e-09\n",
      "Iteration 04145: loss = 1.6408843e-05,1.2808381e-09\n",
      "Iteration 04150: loss = 1.625807e-05,1.2799356e-09\n",
      "Iteration 04155: loss = 1.6108681e-05,1.2790338e-09\n",
      "Iteration 04160: loss = 1.5961296e-05,1.2781319e-09\n",
      "Iteration 04165: loss = 1.5815669e-05,1.2772325e-09\n",
      "Iteration 04170: loss = 1.5672673e-05,1.2763337e-09\n",
      "Iteration 04175: loss = 1.5530777e-05,1.2754342e-09\n",
      "Iteration 04180: loss = 1.5391231e-05,1.2745361e-09\n",
      "Iteration 04185: loss = 1.5253601e-05,1.2736385e-09\n",
      "Iteration 04190: loss = 1.5117621e-05,1.2727422e-09\n",
      "Iteration 04195: loss = 1.4983291e-05,1.271847e-09\n",
      "Iteration 04200: loss = 1.4850561e-05,1.2709546e-09\n",
      "Iteration 04205: loss = 1.4719876e-05,1.2700613e-09\n",
      "Iteration 04210: loss = 1.4590786e-05,1.269169e-09\n",
      "Iteration 04215: loss = 1.4463055e-05,1.2682777e-09\n",
      "Iteration 04220: loss = 1.4337292e-05,1.2673881e-09\n",
      "Iteration 04225: loss = 1.4212653e-05,1.2664993e-09\n",
      "Iteration 04230: loss = 1.4089717e-05,1.2656111e-09\n",
      "Iteration 04235: loss = 1.3968518e-05,1.2647235e-09\n",
      "Iteration 04240: loss = 1.38490095e-05,1.2638375e-09\n",
      "Iteration 04245: loss = 1.373074e-05,1.2629525e-09\n",
      "Iteration 04250: loss = 1.3613721e-05,1.262068e-09\n",
      "Iteration 04255: loss = 1.3498754e-05,1.2611856e-09\n",
      "Iteration 04260: loss = 1.3384601e-05,1.2603042e-09\n",
      "Iteration 04265: loss = 1.3272461e-05,1.2594228e-09\n",
      "Iteration 04270: loss = 1.3161297e-05,1.2585429e-09\n",
      "Iteration 04275: loss = 1.30515145e-05,1.2576638e-09\n",
      "Iteration 04280: loss = 1.294328e-05,1.256786e-09\n",
      "Iteration 04285: loss = 1.283602e-05,1.255908e-09\n",
      "Iteration 04290: loss = 1.2730064e-05,1.2550323e-09\n",
      "Iteration 04295: loss = 1.2625423e-05,1.2541582e-09\n",
      "Iteration 04300: loss = 1.2522472e-05,1.2532848e-09\n",
      "Iteration 04305: loss = 1.2420044e-05,1.2524126e-09\n",
      "Iteration 04310: loss = 1.2320052e-05,1.2515414e-09\n",
      "Iteration 04315: loss = 1.221977e-05,1.2506715e-09\n",
      "Iteration 04320: loss = 1.2121699e-05,1.2498028e-09\n",
      "Iteration 04325: loss = 1.202448e-05,1.248935e-09\n",
      "Iteration 04330: loss = 1.19280885e-05,1.2480692e-09\n",
      "Iteration 04335: loss = 1.18335e-05,1.2472041e-09\n",
      "Iteration 04340: loss = 1.1739544e-05,1.2463396e-09\n",
      "Iteration 04345: loss = 1.1646752e-05,1.2454765e-09\n",
      "Iteration 04350: loss = 1.1555142e-05,1.2446159e-09\n",
      "Iteration 04355: loss = 1.1464518e-05,1.2437551e-09\n",
      "Iteration 04360: loss = 1.1374862e-05,1.2428957e-09\n",
      "Iteration 04365: loss = 1.12861635e-05,1.242037e-09\n",
      "Iteration 04370: loss = 1.11984045e-05,1.24118e-09\n",
      "Iteration 04375: loss = 1.1112344e-05,1.2403236e-09\n",
      "Iteration 04380: loss = 1.1026256e-05,1.2394689e-09\n",
      "Iteration 04385: loss = 1.0941658e-05,1.2386148e-09\n",
      "Iteration 04390: loss = 1.0858325e-05,1.2377628e-09\n",
      "Iteration 04395: loss = 1.077551e-05,1.2369124e-09\n",
      "Iteration 04400: loss = 1.0693972e-05,1.2360618e-09\n",
      "Iteration 04405: loss = 1.0612751e-05,1.2352135e-09\n",
      "Iteration 04410: loss = 1.0532937e-05,1.2343668e-09\n",
      "Iteration 04415: loss = 1.0454365e-05,1.23352e-09\n",
      "Iteration 04420: loss = 1.0376089e-05,1.2326751e-09\n",
      "Iteration 04425: loss = 1.0298488e-05,1.2318316e-09\n",
      "Iteration 04430: loss = 1.0222055e-05,1.2309903e-09\n",
      "Iteration 04435: loss = 1.0146281e-05,1.2301493e-09\n",
      "Iteration 04440: loss = 1.0071516e-05,1.2293098e-09\n",
      "Iteration 04445: loss = 9.997727e-06,1.2284715e-09\n",
      "Iteration 04450: loss = 9.9242325e-06,1.2276333e-09\n",
      "Iteration 04455: loss = 9.852053e-06,1.2267967e-09\n",
      "Iteration 04460: loss = 9.780483e-06,1.2259623e-09\n",
      "Iteration 04465: loss = 9.7095335e-06,1.2251282e-09\n",
      "Iteration 04470: loss = 9.6393305e-06,1.2242962e-09\n",
      "Iteration 04475: loss = 9.570085e-06,1.2234648e-09\n",
      "Iteration 04480: loss = 9.501432e-06,1.2226348e-09\n",
      "Iteration 04485: loss = 9.433518e-06,1.2218057e-09\n",
      "Iteration 04490: loss = 9.36653e-06,1.2209782e-09\n",
      "Iteration 04495: loss = 9.300107e-06,1.2201519e-09\n",
      "Iteration 04500: loss = 9.234068e-06,1.2193268e-09\n",
      "Iteration 04505: loss = 9.169095e-06,1.2185034e-09\n",
      "Iteration 04510: loss = 9.104849e-06,1.2176808e-09\n",
      "Iteration 04515: loss = 9.041132e-06,1.2168597e-09\n",
      "Iteration 04520: loss = 8.978306e-06,1.2160397e-09\n",
      "Iteration 04525: loss = 8.9158275e-06,1.215221e-09\n",
      "Iteration 04530: loss = 8.854048e-06,1.2144041e-09\n",
      "Iteration 04535: loss = 8.79262e-06,1.2135876e-09\n",
      "Iteration 04540: loss = 8.7322e-06,1.2127737e-09\n",
      "Iteration 04545: loss = 8.672141e-06,1.2119606e-09\n",
      "Iteration 04550: loss = 8.61308e-06,1.2111466e-09\n",
      "Iteration 04555: loss = 8.554355e-06,1.2103356e-09\n",
      "Iteration 04560: loss = 8.496267e-06,1.2095268e-09\n",
      "Iteration 04565: loss = 8.438525e-06,1.2087188e-09\n",
      "Iteration 04570: loss = 8.382063e-06,1.2079121e-09\n",
      "Iteration 04575: loss = 8.3252935e-06,1.2071069e-09\n",
      "Iteration 04580: loss = 8.2694705e-06,1.2063018e-09\n",
      "Iteration 04585: loss = 8.214441e-06,1.2054989e-09\n",
      "Iteration 04590: loss = 8.159558e-06,1.2046977e-09\n",
      "Iteration 04595: loss = 8.105288e-06,1.2038975e-09\n",
      "Iteration 04600: loss = 8.051963e-06,1.2030983e-09\n",
      "Iteration 04605: loss = 7.998948e-06,1.2022988e-09\n",
      "Iteration 04610: loss = 7.9462125e-06,1.2015013e-09\n",
      "Iteration 04615: loss = 7.894225e-06,1.2007062e-09\n",
      "Iteration 04620: loss = 7.84237e-06,1.1999115e-09\n",
      "Iteration 04625: loss = 7.7917475e-06,1.1991178e-09\n",
      "Iteration 04630: loss = 7.740925e-06,1.198326e-09\n",
      "Iteration 04635: loss = 7.69085e-06,1.1975336e-09\n",
      "Iteration 04640: loss = 7.641342e-06,1.196744e-09\n",
      "Iteration 04645: loss = 7.5922585e-06,1.1959557e-09\n",
      "Iteration 04650: loss = 7.5435764e-06,1.1951689e-09\n",
      "Iteration 04655: loss = 7.495477e-06,1.1943823e-09\n",
      "Iteration 04660: loss = 7.4477807e-06,1.1935979e-09\n",
      "Iteration 04665: loss = 7.400499e-06,1.1928136e-09\n",
      "Iteration 04670: loss = 7.353757e-06,1.1920314e-09\n",
      "Iteration 04675: loss = 7.3071114e-06,1.1912503e-09\n",
      "Iteration 04680: loss = 7.261158e-06,1.1904708e-09\n",
      "Iteration 04685: loss = 7.215605e-06,1.1896918e-09\n",
      "Iteration 04690: loss = 7.1707414e-06,1.1889143e-09\n",
      "Iteration 04695: loss = 7.1259524e-06,1.1881383e-09\n",
      "Iteration 04700: loss = 7.0818446e-06,1.1873633e-09\n",
      "Iteration 04705: loss = 7.0378214e-06,1.1865895e-09\n",
      "Iteration 04710: loss = 6.994463e-06,1.185817e-09\n",
      "Iteration 04715: loss = 6.950884e-06,1.1850456e-09\n",
      "Iteration 04720: loss = 6.9085595e-06,1.1842759e-09\n",
      "Iteration 04725: loss = 6.866005e-06,1.1835074e-09\n",
      "Iteration 04730: loss = 6.8242666e-06,1.1827396e-09\n",
      "Iteration 04735: loss = 6.7828764e-06,1.1819727e-09\n",
      "Iteration 04740: loss = 6.7415626e-06,1.181207e-09\n",
      "Iteration 04745: loss = 6.7008755e-06,1.1804432e-09\n",
      "Iteration 04750: loss = 6.6605476e-06,1.1796797e-09\n",
      "Iteration 04755: loss = 6.620423e-06,1.178918e-09\n",
      "Iteration 04760: loss = 6.580933e-06,1.1781566e-09\n",
      "Iteration 04765: loss = 6.5414897e-06,1.1773971e-09\n",
      "Iteration 04770: loss = 6.502236e-06,1.176639e-09\n",
      "Iteration 04775: loss = 6.4638957e-06,1.1758823e-09\n",
      "Iteration 04780: loss = 6.4256005e-06,1.1751257e-09\n",
      "Iteration 04785: loss = 6.3874995e-06,1.1743707e-09\n",
      "Iteration 04790: loss = 6.349863e-06,1.1736173e-09\n",
      "Iteration 04795: loss = 6.312553e-06,1.1728656e-09\n",
      "Iteration 04800: loss = 6.2758404e-06,1.1721145e-09\n",
      "Iteration 04805: loss = 6.2387476e-06,1.1713642e-09\n",
      "Iteration 04810: loss = 6.2023987e-06,1.1706154e-09\n",
      "Iteration 04815: loss = 6.1669193e-06,1.1698676e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 04820: loss = 6.1309215e-06,1.1691206e-09\n",
      "Iteration 04825: loss = 6.0956504e-06,1.1683752e-09\n",
      "Iteration 04830: loss = 6.0605394e-06,1.1676308e-09\n",
      "Iteration 04835: loss = 6.025601e-06,1.1668879e-09\n",
      "Iteration 04840: loss = 5.9911067e-06,1.166146e-09\n",
      "Iteration 04845: loss = 5.9569134e-06,1.1654049e-09\n",
      "Iteration 04850: loss = 5.9230165e-06,1.1646651e-09\n",
      "Iteration 04855: loss = 5.889278e-06,1.1639265e-09\n",
      "Iteration 04860: loss = 5.8562455e-06,1.1631888e-09\n",
      "Iteration 04865: loss = 5.8228284e-06,1.1624522e-09\n",
      "Iteration 04870: loss = 5.7901034e-06,1.1617168e-09\n",
      "Iteration 04875: loss = 5.7575285e-06,1.1609829e-09\n",
      "Iteration 04880: loss = 5.7256516e-06,1.1602493e-09\n",
      "Iteration 04885: loss = 5.6935096e-06,1.159518e-09\n",
      "Iteration 04890: loss = 5.66193e-06,1.1587866e-09\n",
      "Iteration 04895: loss = 5.6304907e-06,1.1580569e-09\n",
      "Iteration 04900: loss = 5.599191e-06,1.1573285e-09\n",
      "Iteration 04905: loss = 5.568308e-06,1.1566005e-09\n",
      "Iteration 04910: loss = 5.537831e-06,1.1558741e-09\n",
      "Iteration 04915: loss = 5.506956e-06,1.1551495e-09\n",
      "Iteration 04920: loss = 5.4770267e-06,1.1544246e-09\n",
      "Iteration 04925: loss = 5.4472243e-06,1.1537005e-09\n",
      "Iteration 04930: loss = 5.4175594e-06,1.1529784e-09\n",
      "Iteration 04935: loss = 5.38802e-06,1.1522578e-09\n",
      "Iteration 04940: loss = 5.3586136e-06,1.151538e-09\n",
      "Iteration 04945: loss = 5.329603e-06,1.1508202e-09\n",
      "Iteration 04950: loss = 5.300973e-06,1.1501035e-09\n",
      "Iteration 04955: loss = 5.2724713e-06,1.1493876e-09\n",
      "Iteration 04960: loss = 5.2440996e-06,1.1486724e-09\n",
      "Iteration 04965: loss = 5.2161154e-06,1.1479578e-09\n",
      "Iteration 04970: loss = 5.188253e-06,1.1472452e-09\n",
      "Iteration 04975: loss = 5.1606435e-06,1.1465328e-09\n",
      "Iteration 04980: loss = 5.1330267e-06,1.145822e-09\n",
      "Iteration 04985: loss = 5.10605e-06,1.1451109e-09\n",
      "Iteration 04990: loss = 5.079188e-06,1.144401e-09\n",
      "Iteration 04995: loss = 5.052178e-06,1.1436937e-09\n",
      "Iteration 05000: loss = 5.025808e-06,1.1429859e-09\n",
      "Iteration 05005: loss = 4.999542e-06,1.1422798e-09\n",
      "Iteration 05010: loss = 4.973394e-06,1.1415745e-09\n",
      "Iteration 05015: loss = 4.9476075e-06,1.1408702e-09\n",
      "Iteration 05020: loss = 4.9219366e-06,1.1401664e-09\n",
      "Iteration 05025: loss = 4.896246e-06,1.1394644e-09\n",
      "Iteration 05030: loss = 4.8707857e-06,1.1387635e-09\n",
      "Iteration 05035: loss = 4.845691e-06,1.1380639e-09\n",
      "Iteration 05040: loss = 4.8206957e-06,1.1373654e-09\n",
      "Iteration 05045: loss = 4.796056e-06,1.1366675e-09\n",
      "Iteration 05050: loss = 4.7715203e-06,1.1359709e-09\n",
      "Iteration 05055: loss = 4.746968e-06,1.135275e-09\n",
      "Iteration 05060: loss = 4.7228855e-06,1.13458e-09\n",
      "Iteration 05065: loss = 4.6986574e-06,1.1338868e-09\n",
      "Iteration 05070: loss = 4.674784e-06,1.1331931e-09\n",
      "Iteration 05075: loss = 4.6512505e-06,1.132501e-09\n",
      "Iteration 05080: loss = 4.627936e-06,1.1318102e-09\n",
      "Iteration 05085: loss = 4.6045902e-06,1.1311205e-09\n",
      "Iteration 05090: loss = 4.581349e-06,1.1304315e-09\n",
      "Iteration 05095: loss = 4.558446e-06,1.1297433e-09\n",
      "Iteration 05100: loss = 4.535877e-06,1.1290561e-09\n",
      "Iteration 05105: loss = 4.5130455e-06,1.1283696e-09\n",
      "Iteration 05110: loss = 4.4906606e-06,1.1276845e-09\n",
      "Iteration 05115: loss = 4.468365e-06,1.1270007e-09\n",
      "Iteration 05120: loss = 4.4464073e-06,1.1263166e-09\n",
      "Iteration 05125: loss = 4.4241747e-06,1.1256355e-09\n",
      "Iteration 05130: loss = 4.4023914e-06,1.1249538e-09\n",
      "Iteration 05135: loss = 4.380931e-06,1.1242739e-09\n",
      "Iteration 05140: loss = 4.35921e-06,1.1235939e-09\n",
      "Iteration 05145: loss = 4.3381597e-06,1.1229153e-09\n",
      "Iteration 05150: loss = 4.316959e-06,1.1222382e-09\n",
      "Iteration 05155: loss = 4.295848e-06,1.1215613e-09\n",
      "Iteration 05160: loss = 4.274934e-06,1.120886e-09\n",
      "Iteration 05165: loss = 4.254223e-06,1.1202114e-09\n",
      "Iteration 05170: loss = 4.2338233e-06,1.1195371e-09\n",
      "Iteration 05175: loss = 4.2131683e-06,1.1188642e-09\n",
      "Iteration 05180: loss = 4.1929306e-06,1.118192e-09\n",
      "Iteration 05185: loss = 4.173005e-06,1.1175219e-09\n",
      "Iteration 05190: loss = 4.1528174e-06,1.1168515e-09\n",
      "Iteration 05195: loss = 4.1330477e-06,1.1161826e-09\n",
      "Iteration 05200: loss = 4.113587e-06,1.1155146e-09\n",
      "Iteration 05205: loss = 4.0938626e-06,1.1148471e-09\n",
      "Iteration 05210: loss = 4.0743303e-06,1.1141805e-09\n",
      "Iteration 05215: loss = 4.0551004e-06,1.1135152e-09\n",
      "Iteration 05220: loss = 4.036057e-06,1.1128505e-09\n",
      "Iteration 05225: loss = 4.016978e-06,1.1121867e-09\n",
      "Iteration 05230: loss = 3.998198e-06,1.1115238e-09\n",
      "Iteration 05235: loss = 3.979156e-06,1.1108615e-09\n",
      "Iteration 05240: loss = 3.960741e-06,1.1102007e-09\n",
      "Iteration 05245: loss = 3.942178e-06,1.1095403e-09\n",
      "Iteration 05250: loss = 3.92402e-06,1.1088808e-09\n",
      "Iteration 05255: loss = 3.905599e-06,1.1082227e-09\n",
      "Iteration 05260: loss = 3.887364e-06,1.1075648e-09\n",
      "Iteration 05265: loss = 3.8695225e-06,1.1069083e-09\n",
      "Iteration 05270: loss = 3.8517583e-06,1.1062521e-09\n",
      "Iteration 05275: loss = 3.8337253e-06,1.1055975e-09\n",
      "Iteration 05280: loss = 3.816313e-06,1.1049426e-09\n",
      "Iteration 05285: loss = 3.7986374e-06,1.1042898e-09\n",
      "Iteration 05290: loss = 3.781567e-06,1.1036381e-09\n",
      "Iteration 05295: loss = 3.7641319e-06,1.1029865e-09\n",
      "Iteration 05300: loss = 3.746878e-06,1.1023354e-09\n",
      "Iteration 05305: loss = 3.73001e-06,1.1016854e-09\n",
      "Iteration 05310: loss = 3.713095e-06,1.1010365e-09\n",
      "Iteration 05315: loss = 3.696142e-06,1.1003882e-09\n",
      "Iteration 05320: loss = 3.6795643e-06,1.0997406e-09\n",
      "Iteration 05325: loss = 3.6629451e-06,1.0990944e-09\n",
      "Iteration 05330: loss = 3.6462823e-06,1.098449e-09\n",
      "Iteration 05335: loss = 3.6302165e-06,1.0978036e-09\n",
      "Iteration 05340: loss = 3.6140043e-06,1.097159e-09\n",
      "Iteration 05345: loss = 3.5977346e-06,1.0965159e-09\n",
      "Iteration 05350: loss = 3.581638e-06,1.095873e-09\n",
      "Iteration 05355: loss = 3.5659193e-06,1.0952309e-09\n",
      "Iteration 05360: loss = 3.5501498e-06,1.0945901e-09\n",
      "Iteration 05365: loss = 3.5343312e-06,1.0939498e-09\n",
      "Iteration 05370: loss = 3.5186797e-06,1.0933107e-09\n",
      "Iteration 05375: loss = 3.5031856e-06,1.0926727e-09\n",
      "Iteration 05380: loss = 3.4876468e-06,1.0920351e-09\n",
      "Iteration 05385: loss = 3.4722632e-06,1.0913986e-09\n",
      "Iteration 05390: loss = 3.4572515e-06,1.0907627e-09\n",
      "Iteration 05395: loss = 3.4419888e-06,1.0901269e-09\n",
      "Iteration 05400: loss = 3.4270888e-06,1.0894925e-09\n",
      "Iteration 05405: loss = 3.4121376e-06,1.088859e-09\n",
      "Iteration 05410: loss = 3.3973417e-06,1.088226e-09\n",
      "Iteration 05415: loss = 3.3824963e-06,1.087594e-09\n",
      "Iteration 05420: loss = 3.3680105e-06,1.0869632e-09\n",
      "Iteration 05425: loss = 3.3532722e-06,1.0863327e-09\n",
      "Iteration 05430: loss = 3.3388988e-06,1.0857031e-09\n",
      "Iteration 05435: loss = 3.3244687e-06,1.0850738e-09\n",
      "Iteration 05440: loss = 3.3101921e-06,1.0844456e-09\n",
      "Iteration 05445: loss = 3.296068e-06,1.0838184e-09\n",
      "Iteration 05450: loss = 3.282097e-06,1.0831915e-09\n",
      "Iteration 05455: loss = 3.267872e-06,1.0825656e-09\n",
      "Iteration 05460: loss = 3.2540036e-06,1.0819401e-09\n",
      "Iteration 05465: loss = 3.240276e-06,1.0813158e-09\n",
      "Iteration 05470: loss = 3.226505e-06,1.080692e-09\n",
      "Iteration 05475: loss = 3.2128794e-06,1.0800689e-09\n",
      "Iteration 05480: loss = 3.1994011e-06,1.0794473e-09\n",
      "Iteration 05485: loss = 3.1858688e-06,1.0788258e-09\n",
      "Iteration 05490: loss = 3.1723819e-06,1.0782053e-09\n",
      "Iteration 05495: loss = 3.1590462e-06,1.0775854e-09\n",
      "Iteration 05500: loss = 3.1458549e-06,1.0769658e-09\n",
      "Iteration 05505: loss = 3.132802e-06,1.0763479e-09\n",
      "Iteration 05510: loss = 3.1197076e-06,1.0757296e-09\n",
      "Iteration 05515: loss = 3.106548e-06,1.0751128e-09\n",
      "Iteration 05520: loss = 3.093938e-06,1.0744964e-09\n",
      "Iteration 05525: loss = 3.081069e-06,1.0738805e-09\n",
      "Iteration 05530: loss = 3.0683414e-06,1.0732658e-09\n",
      "Iteration 05535: loss = 3.0554684e-06,1.0726513e-09\n",
      "Iteration 05540: loss = 3.0431238e-06,1.0720376e-09\n",
      "Iteration 05545: loss = 3.0303318e-06,1.0714246e-09\n",
      "Iteration 05550: loss = 3.0180738e-06,1.0708125e-09\n",
      "Iteration 05555: loss = 3.0055662e-06,1.0702009e-09\n",
      "Iteration 05560: loss = 2.99329e-06,1.0695902e-09\n",
      "Iteration 05565: loss = 2.9811526e-06,1.0689809e-09\n",
      "Iteration 05570: loss = 2.9689597e-06,1.0683717e-09\n",
      "Iteration 05575: loss = 2.9569119e-06,1.0677623e-09\n",
      "Iteration 05580: loss = 2.9449975e-06,1.0671539e-09\n",
      "Iteration 05585: loss = 2.9329306e-06,1.0665464e-09\n",
      "Iteration 05590: loss = 2.9211878e-06,1.0659398e-09\n",
      "Iteration 05595: loss = 2.9093956e-06,1.0653336e-09\n",
      "Iteration 05600: loss = 2.8977345e-06,1.0647282e-09\n",
      "Iteration 05605: loss = 2.8860138e-06,1.0641241e-09\n",
      "Iteration 05610: loss = 2.8743327e-06,1.0635199e-09\n",
      "Iteration 05615: loss = 2.8629786e-06,1.0629163e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 05620: loss = 2.8513775e-06,1.0623132e-09\n",
      "Iteration 05625: loss = 2.8401003e-06,1.0617108e-09\n",
      "Iteration 05630: loss = 2.8287648e-06,1.0611092e-09\n",
      "Iteration 05635: loss = 2.8176494e-06,1.0605091e-09\n",
      "Iteration 05640: loss = 2.8064787e-06,1.0599092e-09\n",
      "Iteration 05645: loss = 2.7952503e-06,1.0593096e-09\n",
      "Iteration 05650: loss = 2.7844392e-06,1.0587103e-09\n",
      "Iteration 05655: loss = 2.7733774e-06,1.0581122e-09\n",
      "Iteration 05660: loss = 2.7624467e-06,1.0575151e-09\n",
      "Iteration 05665: loss = 2.7516442e-06,1.0569183e-09\n",
      "Iteration 05670: loss = 2.740783e-06,1.056322e-09\n",
      "Iteration 05675: loss = 2.7301412e-06,1.055727e-09\n",
      "Iteration 05680: loss = 2.7194374e-06,1.0551326e-09\n",
      "Iteration 05685: loss = 2.70887e-06,1.0545386e-09\n",
      "Iteration 05690: loss = 2.6983328e-06,1.0539449e-09\n",
      "Iteration 05695: loss = 2.6879218e-06,1.0533515e-09\n",
      "Iteration 05700: loss = 2.6774514e-06,1.0527592e-09\n",
      "Iteration 05705: loss = 2.667013e-06,1.0521679e-09\n",
      "Iteration 05710: loss = 2.6567043e-06,1.0515764e-09\n",
      "Iteration 05715: loss = 2.6465168e-06,1.050986e-09\n",
      "Iteration 05720: loss = 2.6361795e-06,1.0503962e-09\n",
      "Iteration 05725: loss = 2.6263324e-06,1.0498064e-09\n",
      "Iteration 05730: loss = 2.616059e-06,1.0492176e-09\n",
      "Iteration 05735: loss = 2.6061896e-06,1.0486292e-09\n",
      "Iteration 05740: loss = 2.59625e-06,1.0480414e-09\n",
      "Iteration 05745: loss = 2.5862553e-06,1.0474547e-09\n",
      "Iteration 05750: loss = 2.5762947e-06,1.046868e-09\n",
      "Iteration 05755: loss = 2.566633e-06,1.0462824e-09\n",
      "Iteration 05760: loss = 2.5569116e-06,1.0456971e-09\n",
      "Iteration 05765: loss = 2.5472202e-06,1.0451124e-09\n",
      "Iteration 05770: loss = 2.5376473e-06,1.0445286e-09\n",
      "Iteration 05775: loss = 2.5280149e-06,1.0439456e-09\n",
      "Iteration 05780: loss = 2.518592e-06,1.0433628e-09\n",
      "Iteration 05785: loss = 2.5091092e-06,1.0427812e-09\n",
      "Iteration 05790: loss = 2.4995627e-06,1.0422001e-09\n",
      "Iteration 05795: loss = 2.490231e-06,1.0416188e-09\n",
      "Iteration 05800: loss = 2.4810145e-06,1.0410379e-09\n",
      "Iteration 05805: loss = 2.4717383e-06,1.0404584e-09\n",
      "Iteration 05810: loss = 2.4626665e-06,1.0398787e-09\n",
      "Iteration 05815: loss = 2.4533545e-06,1.0393006e-09\n",
      "Iteration 05820: loss = 2.4443389e-06,1.0387222e-09\n",
      "Iteration 05825: loss = 2.4351737e-06,1.0381448e-09\n",
      "Iteration 05830: loss = 2.4262974e-06,1.037568e-09\n",
      "Iteration 05835: loss = 2.4173646e-06,1.036992e-09\n",
      "Iteration 05840: loss = 2.4084563e-06,1.0364158e-09\n",
      "Iteration 05845: loss = 2.3995742e-06,1.0358411e-09\n",
      "Iteration 05850: loss = 2.3908062e-06,1.0352668e-09\n",
      "Iteration 05855: loss = 2.381979e-06,1.0346927e-09\n",
      "Iteration 05860: loss = 2.3731784e-06,1.0341191e-09\n",
      "Iteration 05865: loss = 2.364665e-06,1.0335464e-09\n",
      "Iteration 05870: loss = 2.3559187e-06,1.0329734e-09\n",
      "Iteration 05875: loss = 2.3472023e-06,1.0324009e-09\n",
      "Iteration 05880: loss = 2.3388518e-06,1.0318296e-09\n",
      "Iteration 05885: loss = 2.330268e-06,1.0312589e-09\n",
      "Iteration 05890: loss = 2.321798e-06,1.0306883e-09\n",
      "Iteration 05895: loss = 2.3135233e-06,1.030118e-09\n",
      "Iteration 05900: loss = 2.305361e-06,1.0295478e-09\n",
      "Iteration 05905: loss = 2.296796e-06,1.0289788e-09\n",
      "Iteration 05910: loss = 2.2885954e-06,1.0284104e-09\n",
      "Iteration 05915: loss = 2.2804177e-06,1.0278423e-09\n",
      "Iteration 05920: loss = 2.2721804e-06,1.0272755e-09\n",
      "Iteration 05925: loss = 2.2640556e-06,1.0267082e-09\n",
      "Iteration 05930: loss = 2.2559514e-06,1.0261418e-09\n",
      "Iteration 05935: loss = 2.2479555e-06,1.0255756e-09\n",
      "Iteration 05940: loss = 2.239899e-06,1.0250111e-09\n",
      "Iteration 05945: loss = 2.232036e-06,1.0244462e-09\n",
      "Iteration 05950: loss = 2.224025e-06,1.0238821e-09\n",
      "Iteration 05955: loss = 2.2162938e-06,1.0233177e-09\n",
      "Iteration 05960: loss = 2.2084978e-06,1.0227539e-09\n",
      "Iteration 05965: loss = 2.2005588e-06,1.0221913e-09\n",
      "Iteration 05970: loss = 2.1929754e-06,1.021629e-09\n",
      "Iteration 05975: loss = 2.1851686e-06,1.021066e-09\n",
      "Iteration 05980: loss = 2.1774622e-06,1.020505e-09\n",
      "Iteration 05985: loss = 2.1699404e-06,1.0199448e-09\n",
      "Iteration 05990: loss = 2.162281e-06,1.0193848e-09\n",
      "Iteration 05995: loss = 2.1547255e-06,1.0188246e-09\n",
      "Iteration 06000: loss = 2.1472745e-06,1.018265e-09\n",
      "Iteration 06005: loss = 2.1398464e-06,1.0177055e-09\n",
      "Iteration 06010: loss = 2.1322758e-06,1.0171467e-09\n",
      "Iteration 06015: loss = 2.1249705e-06,1.0165886e-09\n",
      "Iteration 06020: loss = 2.1176022e-06,1.0160315e-09\n",
      "Iteration 06025: loss = 2.1102567e-06,1.0154748e-09\n",
      "Iteration 06030: loss = 2.102933e-06,1.0149177e-09\n",
      "Iteration 06035: loss = 2.0957134e-06,1.0143615e-09\n",
      "Iteration 06040: loss = 2.0885932e-06,1.0138054e-09\n",
      "Iteration 06045: loss = 2.0813347e-06,1.0132497e-09\n",
      "Iteration 06050: loss = 2.0742523e-06,1.0126955e-09\n",
      "Iteration 06055: loss = 2.067274e-06,1.0121411e-09\n",
      "Iteration 06060: loss = 2.0600758e-06,1.0115866e-09\n",
      "Iteration 06065: loss = 2.0530576e-06,1.0110331e-09\n",
      "Iteration 06070: loss = 2.0460573e-06,1.0104801e-09\n",
      "Iteration 06075: loss = 2.0391562e-06,1.0099279e-09\n",
      "Iteration 06080: loss = 2.0321957e-06,1.0093765e-09\n",
      "Iteration 06085: loss = 2.0252571e-06,1.0088236e-09\n",
      "Iteration 06090: loss = 2.0184964e-06,1.008273e-09\n",
      "Iteration 06095: loss = 2.0115924e-06,1.0077224e-09\n",
      "Iteration 06100: loss = 2.0047908e-06,1.0071715e-09\n",
      "Iteration 06105: loss = 1.998085e-06,1.0066219e-09\n",
      "Iteration 06110: loss = 1.9914014e-06,1.0060721e-09\n",
      "Iteration 06115: loss = 1.9847364e-06,1.0055226e-09\n",
      "Iteration 06120: loss = 1.9780057e-06,1.0049745e-09\n",
      "Iteration 06125: loss = 1.971376e-06,1.0044265e-09\n",
      "Iteration 06130: loss = 1.964768e-06,1.0038784e-09\n",
      "Iteration 06135: loss = 1.9583304e-06,1.0033308e-09\n",
      "Iteration 06140: loss = 1.951756e-06,1.0027836e-09\n",
      "Iteration 06145: loss = 1.9452737e-06,1.0022381e-09\n",
      "Iteration 06150: loss = 1.938894e-06,1.0016908e-09\n",
      "Iteration 06155: loss = 1.932528e-06,1.0011457e-09\n",
      "Iteration 06160: loss = 1.9260224e-06,1.0006006e-09\n",
      "Iteration 06165: loss = 1.9197694e-06,1.0000561e-09\n",
      "Iteration 06170: loss = 1.9134568e-06,9.99511e-10\n",
      "Iteration 06175: loss = 1.9071646e-06,9.989667e-10\n",
      "Iteration 06180: loss = 1.9007267e-06,9.984239e-10\n",
      "Iteration 06185: loss = 1.8946225e-06,9.978802e-10\n",
      "Iteration 06190: loss = 1.8882984e-06,9.973378e-10\n",
      "Iteration 06195: loss = 1.8822293e-06,9.967949e-10\n",
      "Iteration 06200: loss = 1.876012e-06,9.962542e-10\n",
      "Iteration 06205: loss = 1.8699758e-06,9.957115e-10\n",
      "Iteration 06210: loss = 1.8639504e-06,9.951705e-10\n",
      "Iteration 06215: loss = 1.8577174e-06,9.946294e-10\n",
      "Iteration 06220: loss = 1.8517234e-06,9.940888e-10\n",
      "Iteration 06225: loss = 1.8457504e-06,9.935481e-10\n",
      "Iteration 06230: loss = 1.8397899e-06,9.930092e-10\n",
      "Iteration 06235: loss = 1.8338465e-06,9.9247e-10\n",
      "Iteration 06240: loss = 1.8279974e-06,9.919309e-10\n",
      "Iteration 06245: loss = 1.8222354e-06,9.913925e-10\n",
      "Iteration 06250: loss = 1.816339e-06,9.908547e-10\n",
      "Iteration 06255: loss = 1.8104574e-06,9.903169e-10\n",
      "Iteration 06260: loss = 1.8045936e-06,9.897803e-10\n",
      "Iteration 06265: loss = 1.7988182e-06,9.892434e-10\n",
      "Iteration 06270: loss = 1.793139e-06,9.887062e-10\n",
      "Iteration 06275: loss = 1.7873211e-06,9.881693e-10\n",
      "Iteration 06280: loss = 1.7818141e-06,9.876344e-10\n",
      "Iteration 06285: loss = 1.7760349e-06,9.870971e-10\n",
      "Iteration 06290: loss = 1.7704837e-06,9.865626e-10\n",
      "Iteration 06295: loss = 1.764876e-06,9.860273e-10\n",
      "Iteration 06300: loss = 1.7592843e-06,9.854926e-10\n",
      "Iteration 06305: loss = 1.7535591e-06,9.849579e-10\n",
      "Iteration 06310: loss = 1.7481457e-06,9.844243e-10\n",
      "Iteration 06315: loss = 1.7426734e-06,9.838899e-10\n",
      "Iteration 06320: loss = 1.7372846e-06,9.833571e-10\n",
      "Iteration 06325: loss = 1.7317648e-06,9.828242e-10\n",
      "Iteration 06330: loss = 1.7262633e-06,9.822906e-10\n",
      "Iteration 06335: loss = 1.7209132e-06,9.817599e-10\n",
      "Iteration 06340: loss = 1.715588e-06,9.812269e-10\n",
      "Iteration 06345: loss = 1.7101969e-06,9.806961e-10\n",
      "Iteration 06350: loss = 1.704892e-06,9.801652e-10\n",
      "Iteration 06355: loss = 1.6996074e-06,9.796337e-10\n",
      "Iteration 06360: loss = 1.6943299e-06,9.791041e-10\n",
      "Iteration 06365: loss = 1.6890732e-06,9.785729e-10\n",
      "Iteration 06370: loss = 1.6838248e-06,9.780429e-10\n",
      "Iteration 06375: loss = 1.6786606e-06,9.775138e-10\n",
      "Iteration 06380: loss = 1.673589e-06,9.769838e-10\n",
      "Iteration 06385: loss = 1.6683786e-06,9.764562e-10\n",
      "Iteration 06390: loss = 1.663188e-06,9.759266e-10\n",
      "Iteration 06395: loss = 1.6580075e-06,9.753984e-10\n",
      "Iteration 06400: loss = 1.6530538e-06,9.748715e-10\n",
      "Iteration 06405: loss = 1.6479026e-06,9.743429e-10\n",
      "Iteration 06410: loss = 1.6428982e-06,9.738176e-10\n",
      "Iteration 06415: loss = 1.6379171e-06,9.732897e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 06420: loss = 1.6328019e-06,9.72763e-10\n",
      "Iteration 06425: loss = 1.6279843e-06,9.722374e-10\n",
      "Iteration 06430: loss = 1.623111e-06,9.717107e-10\n",
      "Iteration 06435: loss = 1.6180314e-06,9.711855e-10\n",
      "Iteration 06440: loss = 1.6132549e-06,9.706601e-10\n",
      "Iteration 06445: loss = 1.608203e-06,9.701346e-10\n",
      "Iteration 06450: loss = 1.6034497e-06,9.696095e-10\n",
      "Iteration 06455: loss = 1.5985657e-06,9.690854e-10\n",
      "Iteration 06460: loss = 1.5939095e-06,9.685601e-10\n",
      "Iteration 06465: loss = 1.5891895e-06,9.680365e-10\n",
      "Iteration 06470: loss = 1.5841989e-06,9.675133e-10\n",
      "Iteration 06475: loss = 1.5795072e-06,9.66989e-10\n",
      "Iteration 06480: loss = 1.5748193e-06,9.66467e-10\n",
      "Iteration 06485: loss = 1.5700107e-06,9.65944e-10\n",
      "Iteration 06490: loss = 1.5653491e-06,9.654222e-10\n",
      "Iteration 06495: loss = 1.5607733e-06,9.648993e-10\n",
      "Iteration 06500: loss = 1.5561321e-06,9.643782e-10\n",
      "Iteration 06505: loss = 1.5515134e-06,9.638557e-10\n",
      "Iteration 06510: loss = 1.5468986e-06,9.633345e-10\n",
      "Iteration 06515: loss = 1.542436e-06,9.628139e-10\n",
      "Iteration 06520: loss = 1.5378474e-06,9.622928e-10\n",
      "Iteration 06525: loss = 1.5333394e-06,9.617711e-10\n",
      "Iteration 06530: loss = 1.5287675e-06,9.612522e-10\n",
      "Iteration 06535: loss = 1.5243496e-06,9.60732e-10\n",
      "Iteration 06540: loss = 1.5198051e-06,9.602124e-10\n",
      "Iteration 06545: loss = 1.5152727e-06,9.596925e-10\n",
      "Iteration 06550: loss = 1.5108867e-06,9.59174e-10\n",
      "Iteration 06555: loss = 1.5063737e-06,9.586549e-10\n",
      "Iteration 06560: loss = 1.5020813e-06,9.581355e-10\n",
      "Iteration 06565: loss = 1.4977269e-06,9.576177e-10\n",
      "Iteration 06570: loss = 1.4932484e-06,9.570996e-10\n",
      "Iteration 06575: loss = 1.4890542e-06,9.565815e-10\n",
      "Iteration 06580: loss = 1.4847366e-06,9.560623e-10\n",
      "Iteration 06585: loss = 1.4802891e-06,9.555456e-10\n",
      "Iteration 06590: loss = 1.4761257e-06,9.550283e-10\n",
      "Iteration 06595: loss = 1.4719054e-06,9.545111e-10\n",
      "Iteration 06600: loss = 1.467629e-06,9.539938e-10\n",
      "Iteration 06605: loss = 1.4633583e-06,9.534774e-10\n",
      "Iteration 06610: loss = 1.4592364e-06,9.529618e-10\n",
      "Iteration 06615: loss = 1.454989e-06,9.524451e-10\n",
      "Iteration 06620: loss = 1.4507526e-06,9.519292e-10\n",
      "Iteration 06625: loss = 1.446661e-06,9.514132e-10\n",
      "Iteration 06630: loss = 1.4425049e-06,9.508998e-10\n",
      "Iteration 06635: loss = 1.4384342e-06,9.503841e-10\n",
      "Iteration 06640: loss = 1.4343696e-06,9.498701e-10\n",
      "Iteration 06645: loss = 1.4301899e-06,9.493528e-10\n",
      "Iteration 06650: loss = 1.4261424e-06,9.488405e-10\n",
      "Iteration 06655: loss = 1.4221096e-06,9.483256e-10\n",
      "Iteration 06660: loss = 1.4180855e-06,9.478118e-10\n",
      "Iteration 06665: loss = 1.4141395e-06,9.472972e-10\n",
      "Iteration 06670: loss = 1.4102721e-06,9.467825e-10\n",
      "Iteration 06675: loss = 1.4062753e-06,9.462704e-10\n",
      "Iteration 06680: loss = 1.4021562e-06,9.457573e-10\n",
      "Iteration 06685: loss = 1.3981762e-06,9.452454e-10\n",
      "Iteration 06690: loss = 1.3942132e-06,9.44731e-10\n",
      "Iteration 06695: loss = 1.3905241e-06,9.442181e-10\n",
      "Iteration 06700: loss = 1.3865762e-06,9.437054e-10\n",
      "Iteration 06705: loss = 1.3827013e-06,9.431947e-10\n",
      "Iteration 06710: loss = 1.3789014e-06,9.426835e-10\n",
      "Iteration 06715: loss = 1.3749836e-06,9.421712e-10\n",
      "Iteration 06720: loss = 1.3712006e-06,9.416606e-10\n",
      "Iteration 06725: loss = 1.3673058e-06,9.411472e-10\n",
      "Iteration 06730: loss = 1.3635381e-06,9.40638e-10\n",
      "Iteration 06735: loss = 1.3597837e-06,9.401275e-10\n",
      "Iteration 06740: loss = 1.3560374e-06,9.396174e-10\n",
      "Iteration 06745: loss = 1.3522352e-06,9.39108e-10\n",
      "Iteration 06750: loss = 1.3486425e-06,9.385965e-10\n",
      "Iteration 06755: loss = 1.3447938e-06,9.380865e-10\n",
      "Iteration 06760: loss = 1.3412194e-06,9.375756e-10\n",
      "Iteration 06765: loss = 1.3375138e-06,9.370671e-10\n",
      "Iteration 06770: loss = 1.3336916e-06,9.365584e-10\n",
      "Iteration 06775: loss = 1.3301373e-06,9.360486e-10\n",
      "Iteration 06780: loss = 1.3264607e-06,9.355405e-10\n",
      "Iteration 06785: loss = 1.3229914e-06,9.350298e-10\n",
      "Iteration 06790: loss = 1.3193353e-06,9.345205e-10\n",
      "Iteration 06795: loss = 1.3156865e-06,9.340116e-10\n",
      "Iteration 06800: loss = 1.3121748e-06,9.335029e-10\n",
      "Iteration 06805: loss = 1.3085377e-06,9.329968e-10\n",
      "Iteration 06810: loss = 1.3050427e-06,9.324888e-10\n",
      "Iteration 06815: loss = 1.3015556e-06,9.319805e-10\n",
      "Iteration 06820: loss = 1.2979435e-06,9.314735e-10\n",
      "Iteration 06825: loss = 1.2945402e-06,9.3096464e-10\n",
      "Iteration 06830: loss = 1.291077e-06,9.3045704e-10\n",
      "Iteration 06835: loss = 1.2876208e-06,9.299494e-10\n",
      "Iteration 06840: loss = 1.2840513e-06,9.294407e-10\n",
      "Iteration 06845: loss = 1.2806082e-06,9.289344e-10\n",
      "Iteration 06850: loss = 1.2773029e-06,9.284281e-10\n",
      "Iteration 06855: loss = 1.273878e-06,9.2792196e-10\n",
      "Iteration 06860: loss = 1.2704567e-06,9.274178e-10\n",
      "Iteration 06865: loss = 1.2670495e-06,9.269103e-10\n",
      "Iteration 06870: loss = 1.2637128e-06,9.2640456e-10\n",
      "Iteration 06875: loss = 1.2604429e-06,9.2589925e-10\n",
      "Iteration 06880: loss = 1.2570607e-06,9.25392e-10\n",
      "Iteration 06885: loss = 1.2536823e-06,9.2488606e-10\n",
      "Iteration 06890: loss = 1.2504393e-06,9.2438013e-10\n",
      "Iteration 06895: loss = 1.2470807e-06,9.238731e-10\n",
      "Iteration 06900: loss = 1.2438528e-06,9.2336727e-10\n",
      "Iteration 06905: loss = 1.2405059e-06,9.2286123e-10\n",
      "Iteration 06910: loss = 1.2372956e-06,9.2235397e-10\n",
      "Iteration 06915: loss = 1.2341496e-06,9.218494e-10\n",
      "Iteration 06920: loss = 1.2309454e-06,9.2134567e-10\n",
      "Iteration 06925: loss = 1.2276264e-06,9.208411e-10\n",
      "Iteration 06930: loss = 1.2244356e-06,9.203384e-10\n",
      "Iteration 06935: loss = 1.2212548e-06,9.1983504e-10\n",
      "Iteration 06940: loss = 1.218083e-06,9.193308e-10\n",
      "Iteration 06945: loss = 1.214789e-06,9.188279e-10\n",
      "Iteration 06950: loss = 1.2116316e-06,9.1832497e-10\n",
      "Iteration 06955: loss = 1.2086051e-06,9.178206e-10\n",
      "Iteration 06960: loss = 1.2053983e-06,9.173178e-10\n",
      "Iteration 06965: loss = 1.202381e-06,9.168154e-10\n",
      "Iteration 06970: loss = 1.1992532e-06,9.1631114e-10\n",
      "Iteration 06975: loss = 1.19613e-06,9.158079e-10\n",
      "Iteration 06980: loss = 1.1930092e-06,9.1530605e-10\n",
      "Iteration 06985: loss = 1.189905e-06,9.148017e-10\n",
      "Iteration 06990: loss = 1.1869254e-06,9.1429847e-10\n",
      "Iteration 06995: loss = 1.18395e-06,9.1379626e-10\n",
      "Iteration 07000: loss = 1.1808627e-06,9.132926e-10\n",
      "Iteration 07005: loss = 1.1779053e-06,9.1278923e-10\n",
      "Iteration 07010: loss = 1.174889e-06,9.1228736e-10\n",
      "Iteration 07015: loss = 1.171942e-06,9.117847e-10\n",
      "Iteration 07020: loss = 1.1690053e-06,9.112811e-10\n",
      "Iteration 07025: loss = 1.1659482e-06,9.107791e-10\n",
      "Iteration 07030: loss = 1.1630187e-06,9.102782e-10\n",
      "Iteration 07035: loss = 1.1601032e-06,9.097741e-10\n",
      "Iteration 07040: loss = 1.1570686e-06,9.092718e-10\n",
      "Iteration 07045: loss = 1.1541596e-06,9.0877095e-10\n",
      "Iteration 07050: loss = 1.1512587e-06,9.082684e-10\n",
      "Iteration 07055: loss = 1.1484851e-06,9.07766e-10\n",
      "Iteration 07060: loss = 1.1456538e-06,9.0726476e-10\n",
      "Iteration 07065: loss = 1.1427683e-06,9.067648e-10\n",
      "Iteration 07070: loss = 1.1397724e-06,9.0626284e-10\n",
      "Iteration 07075: loss = 1.1369027e-06,9.0576147e-10\n",
      "Iteration 07080: loss = 1.1341555e-06,9.052614e-10\n",
      "Iteration 07085: loss = 1.1312947e-06,9.047616e-10\n",
      "Iteration 07090: loss = 1.1284457e-06,9.0425945e-10\n",
      "Iteration 07095: loss = 1.1257182e-06,9.037593e-10\n",
      "Iteration 07100: loss = 1.1228732e-06,9.032602e-10\n",
      "Iteration 07105: loss = 1.1200376e-06,9.027607e-10\n",
      "Iteration 07110: loss = 1.1173881e-06,9.0226016e-10\n",
      "Iteration 07115: loss = 1.1145639e-06,9.0176056e-10\n",
      "Iteration 07120: loss = 1.1118619e-06,9.0126256e-10\n",
      "Iteration 07125: loss = 1.1090559e-06,9.007608e-10\n",
      "Iteration 07130: loss = 1.1063752e-06,9.002583e-10\n",
      "Iteration 07135: loss = 1.1036992e-06,8.9975705e-10\n",
      "Iteration 07140: loss = 1.1010242e-06,8.992571e-10\n",
      "Iteration 07145: loss = 1.0982419e-06,8.9875585e-10\n",
      "Iteration 07150: loss = 1.0955837e-06,8.9825414e-10\n",
      "Iteration 07155: loss = 1.0929266e-06,8.9775387e-10\n",
      "Iteration 07160: loss = 1.0902735e-06,8.97255e-10\n",
      "Iteration 07165: loss = 1.0876861e-06,8.967558e-10\n",
      "Iteration 07170: loss = 1.0850505e-06,8.962549e-10\n",
      "Iteration 07175: loss = 1.0822976e-06,8.957555e-10\n",
      "Iteration 07180: loss = 1.0797819e-06,8.9525753e-10\n",
      "Iteration 07185: loss = 1.0771522e-06,8.947609e-10\n",
      "Iteration 07190: loss = 1.0744255e-06,8.942589e-10\n",
      "Iteration 07195: loss = 1.0719344e-06,8.9375796e-10\n",
      "Iteration 07200: loss = 1.0693293e-06,8.9325797e-10\n",
      "Iteration 07205: loss = 1.0668427e-06,8.927594e-10\n",
      "Iteration 07210: loss = 1.064249e-06,8.922596e-10\n",
      "Iteration 07215: loss = 1.0617789e-06,8.917589e-10\n",
      "Iteration 07220: loss = 1.0591945e-06,8.912595e-10\n",
      "Iteration 07225: loss = 1.0566708e-06,8.907615e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 07230: loss = 1.0540919e-06,8.902646e-10\n",
      "Iteration 07235: loss = 1.0516409e-06,8.897642e-10\n",
      "Iteration 07240: loss = 1.0490846e-06,8.8926283e-10\n",
      "Iteration 07245: loss = 1.0466434e-06,8.887627e-10\n",
      "Iteration 07250: loss = 1.0440903e-06,8.882642e-10\n",
      "Iteration 07255: loss = 1.0416553e-06,8.8776675e-10\n",
      "Iteration 07260: loss = 1.0392324e-06,8.8726615e-10\n",
      "Iteration 07265: loss = 1.0368121e-06,8.8676694e-10\n",
      "Iteration 07270: loss = 1.034393e-06,8.8626867e-10\n",
      "Iteration 07275: loss = 1.0319814e-06,8.857696e-10\n",
      "Iteration 07280: loss = 1.0294596e-06,8.8527113e-10\n",
      "Iteration 07285: loss = 1.0271185e-06,8.847706e-10\n",
      "Iteration 07290: loss = 1.0247248e-06,8.8427055e-10\n",
      "Iteration 07295: loss = 1.0224477e-06,8.8377144e-10\n",
      "Iteration 07300: loss = 1.0199459e-06,8.832738e-10\n",
      "Iteration 07305: loss = 1.0175605e-06,8.8277663e-10\n",
      "Iteration 07310: loss = 1.0151867e-06,8.822767e-10\n",
      "Iteration 07315: loss = 1.0129326e-06,8.817758e-10\n",
      "Iteration 07320: loss = 1.0105679e-06,8.8127594e-10\n",
      "Iteration 07325: loss = 1.0082065e-06,8.807773e-10\n",
      "Iteration 07330: loss = 1.0058461e-06,8.8027974e-10\n",
      "Iteration 07335: loss = 1.0034881e-06,8.7978397e-10\n",
      "Iteration 07340: loss = 1.0011448e-06,8.792837e-10\n",
      "Iteration 07345: loss = 9.989191e-07,8.7878294e-10\n",
      "Iteration 07350: loss = 9.966388e-07,8.782833e-10\n",
      "Iteration 07355: loss = 9.944177e-07,8.7778496e-10\n",
      "Iteration 07360: loss = 9.920861e-07,8.772881e-10\n",
      "Iteration 07365: loss = 9.898725e-07,8.7679053e-10\n",
      "Iteration 07370: loss = 9.875521e-07,8.762921e-10\n",
      "Iteration 07375: loss = 9.85354e-07,8.757915e-10\n",
      "Iteration 07380: loss = 9.830471e-07,8.7529173e-10\n",
      "Iteration 07385: loss = 9.808523e-07,8.747935e-10\n",
      "Iteration 07390: loss = 9.786625e-07,8.742953e-10\n",
      "Iteration 07395: loss = 9.764786e-07,8.737965e-10\n",
      "Iteration 07400: loss = 9.741843e-07,8.73299e-10\n",
      "Iteration 07405: loss = 9.720119e-07,8.7279906e-10\n",
      "Iteration 07410: loss = 9.698433e-07,8.722997e-10\n",
      "Iteration 07415: loss = 9.676229e-07,8.717997e-10\n",
      "Iteration 07420: loss = 9.655746e-07,8.712998e-10\n",
      "Iteration 07425: loss = 9.634159e-07,8.708009e-10\n",
      "Iteration 07430: loss = 9.612596e-07,8.703032e-10\n",
      "Iteration 07435: loss = 9.589979e-07,8.698062e-10\n",
      "Iteration 07440: loss = 9.568545e-07,8.693064e-10\n",
      "Iteration 07445: loss = 9.548285e-07,8.6880547e-10\n",
      "Iteration 07450: loss = 9.52695e-07,8.6830554e-10\n",
      "Iteration 07455: loss = 9.505634e-07,8.678067e-10\n",
      "Iteration 07460: loss = 9.4843784e-07,8.673071e-10\n",
      "Iteration 07465: loss = 9.4631446e-07,8.668082e-10\n",
      "Iteration 07470: loss = 9.4419397e-07,8.663101e-10\n",
      "Iteration 07475: loss = 9.4218353e-07,8.658133e-10\n",
      "Iteration 07480: loss = 9.4007305e-07,8.6531443e-10\n",
      "Iteration 07485: loss = 9.381353e-07,8.6481283e-10\n",
      "Iteration 07490: loss = 9.360362e-07,8.643124e-10\n",
      "Iteration 07495: loss = 9.340488e-07,8.6381297e-10\n",
      "Iteration 07500: loss = 9.319569e-07,8.633129e-10\n",
      "Iteration 07505: loss = 9.299769e-07,8.6281277e-10\n",
      "Iteration 07510: loss = 9.278911e-07,8.623135e-10\n",
      "Iteration 07515: loss = 9.259158e-07,8.6181534e-10\n",
      "Iteration 07520: loss = 9.238399e-07,8.6131613e-10\n",
      "Iteration 07525: loss = 9.218707e-07,8.6081764e-10\n",
      "Iteration 07530: loss = 9.1980223e-07,8.6031804e-10\n",
      "Iteration 07535: loss = 9.178494e-07,8.598167e-10\n",
      "Iteration 07540: loss = 9.159016e-07,8.5931506e-10\n",
      "Iteration 07545: loss = 9.139544e-07,8.5881424e-10\n",
      "Iteration 07550: loss = 9.1200917e-07,8.583145e-10\n",
      "Iteration 07555: loss = 9.099638e-07,8.5781326e-10\n",
      "Iteration 07560: loss = 9.0808084e-07,8.5731316e-10\n",
      "Iteration 07565: loss = 9.062516e-07,8.56814e-10\n",
      "Iteration 07570: loss = 9.0421395e-07,8.563147e-10\n",
      "Iteration 07575: loss = 9.022876e-07,8.55815e-10\n",
      "Iteration 07580: loss = 9.0036264e-07,8.5531626e-10\n",
      "Iteration 07585: loss = 8.984401e-07,8.548178e-10\n",
      "Iteration 07590: loss = 8.9653184e-07,8.543147e-10\n",
      "Iteration 07595: loss = 8.9462577e-07,8.538123e-10\n",
      "Iteration 07600: loss = 8.928267e-07,8.5331114e-10\n",
      "Iteration 07605: loss = 8.909281e-07,8.528085e-10\n",
      "Iteration 07610: loss = 8.891372e-07,8.523066e-10\n",
      "Iteration 07615: loss = 8.871364e-07,8.5180607e-10\n",
      "Iteration 07620: loss = 8.8535256e-07,8.5130447e-10\n",
      "Iteration 07625: loss = 8.8346474e-07,8.508033e-10\n",
      "Iteration 07630: loss = 8.815796e-07,8.50303e-10\n",
      "Iteration 07635: loss = 8.798027e-07,8.498025e-10\n",
      "Iteration 07640: loss = 8.779771e-07,8.493017e-10\n",
      "Iteration 07645: loss = 8.7620543e-07,8.4880186e-10\n",
      "Iteration 07650: loss = 8.7433506e-07,8.48301e-10\n",
      "Iteration 07655: loss = 8.724672e-07,8.478007e-10\n",
      "Iteration 07660: loss = 8.706008e-07,8.47301e-10\n",
      "Iteration 07665: loss = 8.688484e-07,8.467988e-10\n",
      "Iteration 07670: loss = 8.6710287e-07,8.4629453e-10\n",
      "Iteration 07675: loss = 8.653599e-07,8.4579144e-10\n",
      "Iteration 07680: loss = 8.6362024e-07,8.4528734e-10\n",
      "Iteration 07685: loss = 8.6188425e-07,8.447836e-10\n",
      "Iteration 07690: loss = 8.6004485e-07,8.4428026e-10\n",
      "Iteration 07695: loss = 8.583165e-07,8.43776e-10\n",
      "Iteration 07700: loss = 8.565891e-07,8.4327245e-10\n",
      "Iteration 07705: loss = 8.5486516e-07,8.427685e-10\n",
      "Iteration 07710: loss = 8.530417e-07,8.4226476e-10\n",
      "Iteration 07715: loss = 8.513221e-07,8.417618e-10\n",
      "Iteration 07720: loss = 8.4960885e-07,8.412577e-10\n",
      "Iteration 07725: loss = 8.480527e-07,8.4075413e-10\n",
      "Iteration 07730: loss = 8.463447e-07,8.4025015e-10\n",
      "Iteration 07735: loss = 8.4453796e-07,8.3974605e-10\n",
      "Iteration 07740: loss = 8.428335e-07,8.3924284e-10\n",
      "Iteration 07745: loss = 8.411368e-07,8.387381e-10\n",
      "Iteration 07750: loss = 8.3954546e-07,8.382343e-10\n",
      "Iteration 07755: loss = 8.378522e-07,8.377302e-10\n",
      "Iteration 07760: loss = 8.361644e-07,8.3722573e-10\n",
      "Iteration 07765: loss = 8.345794e-07,8.367222e-10\n",
      "Iteration 07770: loss = 8.3289865e-07,8.3621693e-10\n",
      "Iteration 07775: loss = 8.312195e-07,8.357123e-10\n",
      "Iteration 07780: loss = 8.2954347e-07,8.3520807e-10\n",
      "Iteration 07785: loss = 8.279733e-07,8.347031e-10\n",
      "Iteration 07790: loss = 8.2630186e-07,8.341988e-10\n",
      "Iteration 07795: loss = 8.247393e-07,8.336929e-10\n",
      "Iteration 07800: loss = 8.230757e-07,8.331881e-10\n",
      "Iteration 07805: loss = 8.2141696e-07,8.3268203e-10\n",
      "Iteration 07810: loss = 8.198627e-07,8.321765e-10\n",
      "Iteration 07815: loss = 8.182601e-07,8.316704e-10\n",
      "Iteration 07820: loss = 8.1671305e-07,8.3116386e-10\n",
      "Iteration 07825: loss = 8.150649e-07,8.3065815e-10\n",
      "Iteration 07830: loss = 8.135241e-07,8.3015106e-10\n",
      "Iteration 07835: loss = 8.119844e-07,8.2964485e-10\n",
      "Iteration 07840: loss = 8.1034926e-07,8.2913704e-10\n",
      "Iteration 07845: loss = 8.0881654e-07,8.2863016e-10\n",
      "Iteration 07850: loss = 8.072879e-07,8.281219e-10\n",
      "Iteration 07855: loss = 8.057623e-07,8.2761414e-10\n",
      "Iteration 07860: loss = 8.042404e-07,8.2710594e-10\n",
      "Iteration 07865: loss = 8.0261935e-07,8.2659773e-10\n",
      "Iteration 07870: loss = 8.011047e-07,8.2608864e-10\n",
      "Iteration 07875: loss = 7.9959074e-07,8.2557944e-10\n",
      "Iteration 07880: loss = 7.980813e-07,8.2507023e-10\n",
      "Iteration 07885: loss = 7.965764e-07,8.2456025e-10\n",
      "Iteration 07890: loss = 7.9506987e-07,8.240518e-10\n",
      "Iteration 07895: loss = 7.9356136e-07,8.235454e-10\n",
      "Iteration 07900: loss = 7.920551e-07,8.230392e-10\n",
      "Iteration 07905: loss = 7.9055377e-07,8.2253193e-10\n",
      "Iteration 07910: loss = 7.8910415e-07,8.2202534e-10\n",
      "Iteration 07915: loss = 7.8771035e-07,8.2151724e-10\n",
      "Iteration 07920: loss = 7.861177e-07,8.210094e-10\n",
      "Iteration 07925: loss = 7.846301e-07,8.2050045e-10\n",
      "Iteration 07930: loss = 7.832435e-07,8.1999163e-10\n",
      "Iteration 07935: loss = 7.816645e-07,8.194817e-10\n",
      "Iteration 07940: loss = 7.802835e-07,8.189721e-10\n",
      "Iteration 07945: loss = 7.788083e-07,8.184613e-10\n",
      "Iteration 07950: loss = 7.774358e-07,8.17951e-10\n",
      "Iteration 07955: loss = 7.759675e-07,8.174396e-10\n",
      "Iteration 07960: loss = 7.745029e-07,8.169279e-10\n",
      "Iteration 07965: loss = 7.7314013e-07,8.1641544e-10\n",
      "Iteration 07970: loss = 7.71682e-07,8.159028e-10\n",
      "Iteration 07975: loss = 7.703232e-07,8.1539114e-10\n",
      "Iteration 07980: loss = 7.6886334e-07,8.1488155e-10\n",
      "Iteration 07985: loss = 7.675044e-07,8.143718e-10\n",
      "Iteration 07990: loss = 7.661501e-07,8.138608e-10\n",
      "Iteration 07995: loss = 7.6460145e-07,8.1335016e-10\n",
      "Iteration 08000: loss = 7.6325495e-07,8.128376e-10\n",
      "Iteration 08005: loss = 7.619088e-07,8.1232576e-10\n",
      "Iteration 08010: loss = 7.6057086e-07,8.1181206e-10\n",
      "Iteration 08015: loss = 7.591827e-07,8.112991e-10\n",
      "Iteration 08020: loss = 7.5784766e-07,8.1078483e-10\n",
      "Iteration 08025: loss = 7.565174e-07,8.102698e-10\n",
      "Iteration 08030: loss = 7.5518324e-07,8.097574e-10\n",
      "Iteration 08035: loss = 7.537542e-07,8.092448e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08040: loss = 7.5242434e-07,8.087331e-10\n",
      "Iteration 08045: loss = 7.5110023e-07,8.082193e-10\n",
      "Iteration 08050: loss = 7.4977805e-07,8.077055e-10\n",
      "Iteration 08055: loss = 7.4845957e-07,8.0719065e-10\n",
      "Iteration 08060: loss = 7.470458e-07,8.0667517e-10\n",
      "Iteration 08065: loss = 7.4573404e-07,8.061597e-10\n",
      "Iteration 08070: loss = 7.4452345e-07,8.056428e-10\n",
      "Iteration 08075: loss = 7.4321764e-07,8.051257e-10\n",
      "Iteration 08080: loss = 7.419081e-07,8.046118e-10\n",
      "Iteration 08085: loss = 7.4060296e-07,8.040969e-10\n",
      "Iteration 08090: loss = 7.3929783e-07,8.0358237e-10\n",
      "Iteration 08095: loss = 7.380006e-07,8.0306595e-10\n",
      "Iteration 08100: loss = 7.367028e-07,8.0254947e-10\n",
      "Iteration 08105: loss = 7.35507e-07,8.020321e-10\n",
      "Iteration 08110: loss = 7.34215e-07,8.0151535e-10\n",
      "Iteration 08115: loss = 7.32931e-07,8.0099577e-10\n",
      "Iteration 08120: loss = 7.316412e-07,8.004796e-10\n",
      "Iteration 08125: loss = 7.303527e-07,7.9996415e-10\n",
      "Iteration 08130: loss = 7.2921466e-07,7.994463e-10\n",
      "Iteration 08135: loss = 7.279364e-07,7.989272e-10\n",
      "Iteration 08140: loss = 7.267549e-07,7.9840895e-10\n",
      "Iteration 08145: loss = 7.2548346e-07,7.978893e-10\n",
      "Iteration 08150: loss = 7.2430834e-07,7.973692e-10\n",
      "Iteration 08155: loss = 7.2303436e-07,7.968523e-10\n",
      "Iteration 08160: loss = 7.2176476e-07,7.963341e-10\n",
      "Iteration 08165: loss = 7.205972e-07,7.958135e-10\n",
      "Iteration 08170: loss = 7.193323e-07,7.952938e-10\n",
      "Iteration 08175: loss = 7.181673e-07,7.947737e-10\n",
      "Iteration 08180: loss = 7.170099e-07,7.942502e-10\n",
      "Iteration 08185: loss = 7.157511e-07,7.9373075e-10\n",
      "Iteration 08190: loss = 7.144943e-07,7.932121e-10\n",
      "Iteration 08195: loss = 7.134333e-07,7.9269025e-10\n",
      "Iteration 08200: loss = 7.121859e-07,7.9216894e-10\n",
      "Iteration 08205: loss = 7.110322e-07,7.916471e-10\n",
      "Iteration 08210: loss = 7.098897e-07,7.91122e-10\n",
      "Iteration 08215: loss = 7.0873745e-07,7.906022e-10\n",
      "Iteration 08220: loss = 7.0749593e-07,7.900809e-10\n",
      "Iteration 08225: loss = 7.064482e-07,7.89557e-10\n",
      "Iteration 08230: loss = 7.0521327e-07,7.8903395e-10\n",
      "Iteration 08235: loss = 7.041735e-07,7.8850854e-10\n",
      "Iteration 08240: loss = 7.0294476e-07,7.879838e-10\n",
      "Iteration 08245: loss = 7.018064e-07,7.8746215e-10\n",
      "Iteration 08250: loss = 7.0067455e-07,7.8693824e-10\n",
      "Iteration 08255: loss = 6.995925e-07,7.864132e-10\n",
      "Iteration 08260: loss = 6.9846266e-07,7.858884e-10\n",
      "Iteration 08265: loss = 6.9734057e-07,7.853612e-10\n",
      "Iteration 08270: loss = 6.9621615e-07,7.848373e-10\n",
      "Iteration 08275: loss = 6.950907e-07,7.843129e-10\n",
      "Iteration 08280: loss = 6.9407037e-07,7.83785e-10\n",
      "Iteration 08285: loss = 6.928605e-07,7.832578e-10\n",
      "Iteration 08290: loss = 6.9183994e-07,7.8273094e-10\n",
      "Iteration 08295: loss = 6.906341e-07,7.8220436e-10\n",
      "Iteration 08300: loss = 6.896143e-07,7.816789e-10\n",
      "Iteration 08305: loss = 6.8860095e-07,7.811497e-10\n",
      "Iteration 08310: loss = 6.8740457e-07,7.8062007e-10\n",
      "Iteration 08315: loss = 6.863892e-07,7.8009443e-10\n",
      "Iteration 08320: loss = 6.852886e-07,7.795654e-10\n",
      "Iteration 08325: loss = 6.842811e-07,7.790371e-10\n",
      "Iteration 08330: loss = 6.8318514e-07,7.7850615e-10\n",
      "Iteration 08335: loss = 6.8218316e-07,7.7797635e-10\n",
      "Iteration 08340: loss = 6.810844e-07,7.774493e-10\n",
      "Iteration 08345: loss = 6.800892e-07,7.7691736e-10\n",
      "Iteration 08350: loss = 6.7899936e-07,7.7638646e-10\n",
      "Iteration 08355: loss = 6.7791444e-07,7.7585505e-10\n",
      "Iteration 08360: loss = 6.7691917e-07,7.753244e-10\n",
      "Iteration 08365: loss = 6.7592464e-07,7.7479484e-10\n",
      "Iteration 08370: loss = 6.7484825e-07,7.742603e-10\n",
      "Iteration 08375: loss = 6.7386367e-07,7.7372786e-10\n",
      "Iteration 08380: loss = 6.7278296e-07,7.731972e-10\n",
      "Iteration 08385: loss = 6.7180076e-07,7.7266404e-10\n",
      "Iteration 08390: loss = 6.708169e-07,7.7213136e-10\n",
      "Iteration 08395: loss = 6.698869e-07,7.715962e-10\n",
      "Iteration 08400: loss = 6.689074e-07,7.710641e-10\n",
      "Iteration 08405: loss = 6.6793353e-07,7.7052953e-10\n",
      "Iteration 08410: loss = 6.668701e-07,7.699935e-10\n",
      "Iteration 08415: loss = 6.6589433e-07,7.6946094e-10\n",
      "Iteration 08420: loss = 6.6492566e-07,7.689251e-10\n",
      "Iteration 08425: loss = 6.6386684e-07,7.683901e-10\n",
      "Iteration 08430: loss = 6.6290494e-07,7.678516e-10\n",
      "Iteration 08435: loss = 6.6193775e-07,7.6731715e-10\n",
      "Iteration 08440: loss = 6.6097476e-07,7.667805e-10\n",
      "Iteration 08445: loss = 6.600173e-07,7.6624196e-10\n",
      "Iteration 08450: loss = 6.590585e-07,7.6570594e-10\n",
      "Iteration 08455: loss = 6.581009e-07,7.6516815e-10\n",
      "Iteration 08460: loss = 6.5723674e-07,7.646312e-10\n",
      "Iteration 08465: loss = 6.562875e-07,7.6409e-10\n",
      "Iteration 08470: loss = 6.5533465e-07,7.6355333e-10\n",
      "Iteration 08475: loss = 6.543874e-07,7.6301304e-10\n",
      "Iteration 08480: loss = 6.534423e-07,7.6247236e-10\n",
      "Iteration 08485: loss = 6.5249714e-07,7.6193263e-10\n",
      "Iteration 08490: loss = 6.516446e-07,7.6139206e-10\n",
      "Iteration 08495: loss = 6.5070407e-07,7.608511e-10\n",
      "Iteration 08500: loss = 6.4985574e-07,7.603092e-10\n",
      "Iteration 08505: loss = 6.4891327e-07,7.5977025e-10\n",
      "Iteration 08510: loss = 6.479818e-07,7.592261e-10\n",
      "Iteration 08515: loss = 6.4704517e-07,7.5868545e-10\n",
      "Iteration 08520: loss = 6.4611396e-07,7.581418e-10\n",
      "Iteration 08525: loss = 6.4536385e-07,7.575988e-10\n",
      "Iteration 08530: loss = 6.44439e-07,7.5705375e-10\n",
      "Iteration 08535: loss = 6.435102e-07,7.565112e-10\n",
      "Iteration 08540: loss = 6.425869e-07,7.55966e-10\n",
      "Iteration 08545: loss = 6.417528e-07,7.554218e-10\n",
      "Iteration 08550: loss = 6.4092046e-07,7.5487694e-10\n",
      "Iteration 08555: loss = 6.4004826e-07,7.5433004e-10\n",
      "Iteration 08560: loss = 6.3921925e-07,7.537849e-10\n",
      "Iteration 08565: loss = 6.3830265e-07,7.5323914e-10\n",
      "Iteration 08570: loss = 6.37569e-07,7.526917e-10\n",
      "Iteration 08575: loss = 6.366559e-07,7.5214485e-10\n",
      "Iteration 08580: loss = 6.3574504e-07,7.5159756e-10\n",
      "Iteration 08585: loss = 6.3501835e-07,7.5104784e-10\n",
      "Iteration 08590: loss = 6.341103e-07,7.5050083e-10\n",
      "Iteration 08595: loss = 6.3329406e-07,7.4995143e-10\n",
      "Iteration 08600: loss = 6.3248234e-07,7.494012e-10\n",
      "Iteration 08605: loss = 6.31669e-07,7.4885165e-10\n",
      "Iteration 08610: loss = 6.3076965e-07,7.48301e-10\n",
      "Iteration 08615: loss = 6.300509e-07,7.477502e-10\n",
      "Iteration 08620: loss = 6.2915336e-07,7.4719975e-10\n",
      "Iteration 08625: loss = 6.284402e-07,7.4664674e-10\n",
      "Iteration 08630: loss = 6.275449e-07,7.4609535e-10\n",
      "Iteration 08635: loss = 6.267442e-07,7.455426e-10\n",
      "Iteration 08640: loss = 6.2594484e-07,7.449889e-10\n",
      "Iteration 08645: loss = 6.251445e-07,7.444363e-10\n",
      "Iteration 08650: loss = 6.244382e-07,7.438813e-10\n",
      "Iteration 08655: loss = 6.235511e-07,7.433289e-10\n",
      "Iteration 08660: loss = 6.228478e-07,7.4277295e-10\n",
      "Iteration 08665: loss = 6.220569e-07,7.422184e-10\n",
      "Iteration 08670: loss = 6.2126696e-07,7.416616e-10\n",
      "Iteration 08675: loss = 6.2047957e-07,7.411052e-10\n",
      "Iteration 08680: loss = 6.1978e-07,7.405489e-10\n",
      "Iteration 08685: loss = 6.189061e-07,7.399919e-10\n",
      "Iteration 08690: loss = 6.18212e-07,7.3943374e-10\n",
      "Iteration 08695: loss = 6.174294e-07,7.3887607e-10\n",
      "Iteration 08700: loss = 6.167388e-07,7.383167e-10\n",
      "Iteration 08705: loss = 6.1596046e-07,7.377581e-10\n",
      "Iteration 08710: loss = 6.151835e-07,7.371977e-10\n",
      "Iteration 08715: loss = 6.1449447e-07,7.366382e-10\n",
      "Iteration 08720: loss = 6.137221e-07,7.3607825e-10\n",
      "Iteration 08725: loss = 6.1303837e-07,7.3551687e-10\n",
      "Iteration 08730: loss = 6.122669e-07,7.349563e-10\n",
      "Iteration 08735: loss = 6.114993e-07,7.3439366e-10\n",
      "Iteration 08740: loss = 6.1081874e-07,7.338308e-10\n",
      "Iteration 08745: loss = 6.1018494e-07,7.332683e-10\n",
      "Iteration 08750: loss = 6.094219e-07,7.3270445e-10\n",
      "Iteration 08755: loss = 6.0874584e-07,7.3214135e-10\n",
      "Iteration 08760: loss = 6.080726e-07,7.3157647e-10\n",
      "Iteration 08765: loss = 6.074038e-07,7.310103e-10\n",
      "Iteration 08770: loss = 6.0664496e-07,7.304458e-10\n",
      "Iteration 08775: loss = 6.0597716e-07,7.2987877e-10\n",
      "Iteration 08780: loss = 6.05308e-07,7.293141e-10\n",
      "Iteration 08785: loss = 6.0464555e-07,7.287452e-10\n",
      "Iteration 08790: loss = 6.0389254e-07,7.281787e-10\n",
      "Iteration 08795: loss = 6.032322e-07,7.2760975e-10\n",
      "Iteration 08800: loss = 6.0256923e-07,7.2704137e-10\n",
      "Iteration 08805: loss = 6.0190985e-07,7.264728e-10\n",
      "Iteration 08810: loss = 6.0125313e-07,7.259026e-10\n",
      "Iteration 08815: loss = 6.0059705e-07,7.2533185e-10\n",
      "Iteration 08820: loss = 5.9985456e-07,7.2476203e-10\n",
      "Iteration 08825: loss = 5.992909e-07,7.241892e-10\n",
      "Iteration 08830: loss = 5.9863834e-07,7.2361844e-10\n",
      "Iteration 08835: loss = 5.979901e-07,7.230445e-10\n",
      "Iteration 08840: loss = 5.97343e-07,7.2247136e-10\n",
      "Iteration 08845: loss = 5.9669554e-07,7.21898e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 08850: loss = 5.960507e-07,7.213237e-10\n",
      "Iteration 08855: loss = 5.954957e-07,7.207482e-10\n",
      "Iteration 08860: loss = 5.9485166e-07,7.2017414e-10\n",
      "Iteration 08865: loss = 5.9421353e-07,7.1959716e-10\n",
      "Iteration 08870: loss = 5.9357103e-07,7.190222e-10\n",
      "Iteration 08875: loss = 5.9301954e-07,7.1844486e-10\n",
      "Iteration 08880: loss = 5.923858e-07,7.178668e-10\n",
      "Iteration 08885: loss = 5.917493e-07,7.172898e-10\n",
      "Iteration 08890: loss = 5.912036e-07,7.167106e-10\n",
      "Iteration 08895: loss = 5.9057044e-07,7.161321e-10\n",
      "Iteration 08900: loss = 5.8994044e-07,7.155519e-10\n",
      "Iteration 08905: loss = 5.8939673e-07,7.1497236e-10\n",
      "Iteration 08910: loss = 5.887709e-07,7.143908e-10\n",
      "Iteration 08915: loss = 5.8822997e-07,7.138103e-10\n",
      "Iteration 08920: loss = 5.8760656e-07,7.132276e-10\n",
      "Iteration 08925: loss = 5.870711e-07,7.126438e-10\n",
      "Iteration 08930: loss = 5.864478e-07,7.1206185e-10\n",
      "Iteration 08935: loss = 5.8591564e-07,7.1147715e-10\n",
      "Iteration 08940: loss = 5.853843e-07,7.108929e-10\n",
      "Iteration 08945: loss = 5.8476934e-07,7.1030704e-10\n",
      "Iteration 08950: loss = 5.842383e-07,7.0972195e-10\n",
      "Iteration 08955: loss = 5.837112e-07,7.0913525e-10\n",
      "Iteration 08960: loss = 5.831001e-07,7.0854783e-10\n",
      "Iteration 08965: loss = 5.8265886e-07,7.079619e-10\n",
      "Iteration 08970: loss = 5.8205154e-07,7.0737277e-10\n",
      "Iteration 08975: loss = 5.8152915e-07,7.0678385e-10\n",
      "Iteration 08980: loss = 5.809211e-07,7.0619566e-10\n",
      "Iteration 08985: loss = 5.805304e-07,7.056055e-10\n",
      "Iteration 08990: loss = 5.8001285e-07,7.050147e-10\n",
      "Iteration 08995: loss = 5.794122e-07,7.044229e-10\n",
      "Iteration 09000: loss = 5.7889946e-07,7.038304e-10\n",
      "Iteration 09005: loss = 5.7847126e-07,7.03238e-10\n",
      "Iteration 09010: loss = 5.779585e-07,7.0264444e-10\n",
      "Iteration 09015: loss = 5.7736383e-07,7.0205036e-10\n",
      "Iteration 09020: loss = 5.7693666e-07,7.014575e-10\n",
      "Iteration 09025: loss = 5.7643075e-07,7.0086176e-10\n",
      "Iteration 09030: loss = 5.759268e-07,7.002646e-10\n",
      "Iteration 09035: loss = 5.754214e-07,6.996687e-10\n",
      "Iteration 09040: loss = 5.749184e-07,6.990714e-10\n",
      "Iteration 09045: loss = 5.7450296e-07,6.984725e-10\n",
      "Iteration 09050: loss = 5.7400183e-07,6.9787437e-10\n",
      "Iteration 09055: loss = 5.735886e-07,6.972751e-10\n",
      "Iteration 09060: loss = 5.7309256e-07,6.966749e-10\n",
      "Iteration 09065: loss = 5.725096e-07,6.9607486e-10\n",
      "Iteration 09070: loss = 5.721018e-07,6.954729e-10\n",
      "Iteration 09075: loss = 5.7160895e-07,6.948708e-10\n",
      "Iteration 09080: loss = 5.712031e-07,6.942681e-10\n",
      "Iteration 09085: loss = 5.707118e-07,6.936652e-10\n",
      "Iteration 09090: loss = 5.703085e-07,6.9306094e-10\n",
      "Iteration 09095: loss = 5.6982384e-07,6.924547e-10\n",
      "Iteration 09100: loss = 5.6942196e-07,6.9184963e-10\n",
      "Iteration 09105: loss = 5.6902246e-07,6.9124356e-10\n",
      "Iteration 09110: loss = 5.6854054e-07,6.9063644e-10\n",
      "Iteration 09115: loss = 5.681441e-07,6.900288e-10\n",
      "Iteration 09120: loss = 5.676643e-07,6.8942047e-10\n",
      "Iteration 09125: loss = 5.6735564e-07,6.888111e-10\n",
      "Iteration 09130: loss = 5.6696433e-07,6.882006e-10\n",
      "Iteration 09135: loss = 5.6649014e-07,6.8758976e-10\n",
      "Iteration 09140: loss = 5.660997e-07,6.869789e-10\n",
      "Iteration 09145: loss = 5.6571054e-07,6.863674e-10\n",
      "Iteration 09150: loss = 5.652403e-07,6.8575395e-10\n",
      "Iteration 09155: loss = 5.6485663e-07,6.851395e-10\n",
      "Iteration 09160: loss = 5.6455735e-07,6.8452566e-10\n",
      "Iteration 09165: loss = 5.640893e-07,6.8391104e-10\n",
      "Iteration 09170: loss = 5.637083e-07,6.8329525e-10\n",
      "Iteration 09175: loss = 5.633299e-07,6.826781e-10\n",
      "Iteration 09180: loss = 5.630363e-07,6.8206063e-10\n",
      "Iteration 09185: loss = 5.6257664e-07,6.814425e-10\n",
      "Iteration 09190: loss = 5.6220176e-07,6.808233e-10\n",
      "Iteration 09195: loss = 5.6191203e-07,6.8020334e-10\n",
      "Iteration 09200: loss = 5.615388e-07,6.795834e-10\n",
      "Iteration 09205: loss = 5.6117125e-07,6.789611e-10\n",
      "Iteration 09210: loss = 5.6088584e-07,6.7833894e-10\n",
      "Iteration 09215: loss = 5.6043564e-07,6.7771555e-10\n",
      "Iteration 09220: loss = 5.6015296e-07,6.770915e-10\n",
      "Iteration 09225: loss = 5.597883e-07,6.76467e-10\n",
      "Iteration 09230: loss = 5.595078e-07,6.7584216e-10\n",
      "Iteration 09235: loss = 5.591469e-07,6.7521577e-10\n",
      "Iteration 09240: loss = 5.588726e-07,6.7458783e-10\n",
      "Iteration 09245: loss = 5.585122e-07,6.739605e-10\n",
      "Iteration 09250: loss = 5.582397e-07,6.7333183e-10\n",
      "Iteration 09255: loss = 5.578834e-07,6.7270245e-10\n",
      "Iteration 09260: loss = 5.57614e-07,6.720714e-10\n",
      "Iteration 09265: loss = 5.5726235e-07,6.714401e-10\n",
      "Iteration 09270: loss = 5.569943e-07,6.708081e-10\n",
      "Iteration 09275: loss = 5.567298e-07,6.701746e-10\n",
      "Iteration 09280: loss = 5.5638174e-07,6.6954053e-10\n",
      "Iteration 09285: loss = 5.5612026e-07,6.6890504e-10\n",
      "Iteration 09290: loss = 5.5585906e-07,6.6827e-10\n",
      "Iteration 09295: loss = 5.5551345e-07,6.6763406e-10\n",
      "Iteration 09300: loss = 5.5525584e-07,6.669969e-10\n",
      "Iteration 09305: loss = 5.549977e-07,6.6635886e-10\n",
      "Iteration 09310: loss = 5.54825e-07,6.6572037e-10\n",
      "Iteration 09315: loss = 5.544885e-07,6.650803e-10\n",
      "Iteration 09320: loss = 5.542346e-07,6.6443956e-10\n",
      "Iteration 09325: loss = 5.539832e-07,6.6379835e-10\n",
      "Iteration 09330: loss = 5.5373533e-07,6.631553e-10\n",
      "Iteration 09335: loss = 5.535704e-07,6.625117e-10\n",
      "Iteration 09340: loss = 5.5332504e-07,6.618667e-10\n",
      "Iteration 09345: loss = 5.5308175e-07,6.61221e-10\n",
      "Iteration 09350: loss = 5.5275484e-07,6.605745e-10\n",
      "Iteration 09355: loss = 5.5259613e-07,6.599275e-10\n",
      "Iteration 09360: loss = 5.5235824e-07,6.5927863e-10\n",
      "Iteration 09365: loss = 5.521172e-07,6.586302e-10\n",
      "Iteration 09370: loss = 5.519625e-07,6.5798084e-10\n",
      "Iteration 09375: loss = 5.517275e-07,6.5732986e-10\n",
      "Iteration 09380: loss = 5.51492e-07,6.5667816e-10\n",
      "Iteration 09385: loss = 5.5134313e-07,6.5602507e-10\n",
      "Iteration 09390: loss = 5.511132e-07,6.553713e-10\n",
      "Iteration 09395: loss = 5.510095e-07,6.5471556e-10\n",
      "Iteration 09400: loss = 5.50866e-07,6.540592e-10\n",
      "Iteration 09405: loss = 5.5064123e-07,6.5340205e-10\n",
      "Iteration 09410: loss = 5.505009e-07,6.527441e-10\n",
      "Iteration 09415: loss = 5.502792e-07,6.5208444e-10\n",
      "Iteration 09420: loss = 5.5014027e-07,6.5142486e-10\n",
      "Iteration 09425: loss = 5.500055e-07,6.507638e-10\n",
      "Iteration 09430: loss = 5.4987e-07,6.5010153e-10\n",
      "Iteration 09435: loss = 5.496552e-07,6.4943834e-10\n",
      "Iteration 09440: loss = 5.4960816e-07,6.4877387e-10\n",
      "Iteration 09445: loss = 5.494782e-07,6.481088e-10\n",
      "Iteration 09450: loss = 5.492686e-07,6.4744216e-10\n",
      "Iteration 09455: loss = 5.492246e-07,6.46775e-10\n",
      "Iteration 09460: loss = 5.490181e-07,6.4610617e-10\n",
      "Iteration 09465: loss = 5.488965e-07,6.454364e-10\n",
      "Iteration 09470: loss = 5.488584e-07,6.447658e-10\n",
      "Iteration 09475: loss = 5.486568e-07,6.440942e-10\n",
      "Iteration 09480: loss = 5.486199e-07,6.4342265e-10\n",
      "Iteration 09485: loss = 5.485054e-07,6.4274874e-10\n",
      "Iteration 09490: loss = 5.484738e-07,6.420736e-10\n",
      "Iteration 09495: loss = 5.482777e-07,6.413984e-10\n",
      "Iteration 09500: loss = 5.4824744e-07,6.4072225e-10\n",
      "Iteration 09505: loss = 5.481392e-07,6.400437e-10\n",
      "Iteration 09510: loss = 5.481164e-07,6.39364e-10\n",
      "Iteration 09515: loss = 5.480951e-07,6.386833e-10\n",
      "Iteration 09520: loss = 5.479061e-07,6.3800243e-10\n",
      "Iteration 09525: loss = 5.478884e-07,6.3731975e-10\n",
      "Iteration 09530: loss = 5.478707e-07,6.3663597e-10\n",
      "Iteration 09535: loss = 5.477725e-07,6.359513e-10\n",
      "Iteration 09540: loss = 5.4775643e-07,6.3526673e-10\n",
      "Iteration 09545: loss = 5.4774574e-07,6.34579e-10\n",
      "Iteration 09550: loss = 5.4773574e-07,6.338906e-10\n",
      "Iteration 09555: loss = 5.4772653e-07,6.332017e-10\n",
      "Iteration 09560: loss = 5.476371e-07,6.325107e-10\n",
      "Iteration 09565: loss = 5.4763217e-07,6.31819e-10\n",
      "Iteration 09570: loss = 5.476287e-07,6.3112615e-10\n",
      "Iteration 09575: loss = 5.47629e-07,6.304312e-10\n",
      "Iteration 09580: loss = 5.4762995e-07,6.2973604e-10\n",
      "Iteration 09585: loss = 5.476326e-07,6.290398e-10\n",
      "Iteration 09590: loss = 5.476385e-07,6.283419e-10\n",
      "Iteration 09595: loss = 5.476426e-07,6.2764355e-10\n",
      "Iteration 09600: loss = 5.4773807e-07,6.26942e-10\n",
      "Iteration 09605: loss = 5.4774796e-07,6.2623995e-10\n",
      "Iteration 09610: loss = 5.4776007e-07,6.255373e-10\n",
      "Iteration 09615: loss = 5.4777536e-07,6.2483335e-10\n",
      "Iteration 09620: loss = 5.4787404e-07,6.2412825e-10\n",
      "Iteration 09625: loss = 5.479749e-07,6.23422e-10\n",
      "Iteration 09630: loss = 5.479944e-07,6.2271416e-10\n",
      "Iteration 09635: loss = 5.4801995e-07,6.220042e-10\n",
      "Iteration 09640: loss = 5.4812654e-07,6.212938e-10\n",
      "Iteration 09645: loss = 5.482371e-07,6.205816e-10\n",
      "Iteration 09650: loss = 5.4834743e-07,6.1986877e-10\n",
      "Iteration 09655: loss = 5.483788e-07,6.191538e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 09660: loss = 5.484942e-07,6.184384e-10\n",
      "Iteration 09665: loss = 5.486125e-07,6.1772104e-10\n",
      "Iteration 09670: loss = 5.4864773e-07,6.170033e-10\n",
      "Iteration 09675: loss = 5.488533e-07,6.1628314e-10\n",
      "Iteration 09680: loss = 5.488941e-07,6.1556193e-10\n",
      "Iteration 09685: loss = 5.491043e-07,6.1483907e-10\n",
      "Iteration 09690: loss = 5.4914796e-07,6.1411587e-10\n",
      "Iteration 09695: loss = 5.4936396e-07,6.133894e-10\n",
      "Iteration 09700: loss = 5.4949663e-07,6.126636e-10\n",
      "Iteration 09705: loss = 5.496316e-07,6.1193495e-10\n",
      "Iteration 09710: loss = 5.4985213e-07,6.112052e-10\n",
      "Iteration 09715: loss = 5.499924e-07,6.1047445e-10\n",
      "Iteration 09720: loss = 5.501336e-07,6.097421e-10\n",
      "Iteration 09725: loss = 5.503603e-07,6.0900884e-10\n",
      "Iteration 09730: loss = 5.5059115e-07,6.082734e-10\n",
      "Iteration 09735: loss = 5.5073855e-07,6.075367e-10\n",
      "Iteration 09740: loss = 5.508883e-07,6.067992e-10\n",
      "Iteration 09745: loss = 5.510833e-07,6.060593e-10\n",
      "Iteration 09750: loss = 5.5132335e-07,6.0531746e-10\n",
      "Iteration 09755: loss = 5.515655e-07,6.0457556e-10\n",
      "Iteration 09760: loss = 5.518095e-07,6.0383076e-10\n",
      "Iteration 09765: loss = 5.52058e-07,6.030849e-10\n",
      "Iteration 09770: loss = 5.523048e-07,6.0233796e-10\n",
      "Iteration 09775: loss = 5.525556e-07,6.015896e-10\n",
      "Iteration 09780: loss = 5.528065e-07,6.008407e-10\n",
      "Iteration 09785: loss = 5.5306504e-07,6.000887e-10\n",
      "Iteration 09790: loss = 5.5332265e-07,5.993357e-10\n",
      "Iteration 09795: loss = 5.535829e-07,5.9858174e-10\n",
      "Iteration 09800: loss = 5.538455e-07,5.9782523e-10\n",
      "Iteration 09805: loss = 5.541957e-07,5.9706795e-10\n",
      "Iteration 09810: loss = 5.544644e-07,5.9630845e-10\n",
      "Iteration 09815: loss = 5.5481775e-07,5.9554783e-10\n",
      "Iteration 09820: loss = 5.550915e-07,5.9478544e-10\n",
      "Iteration 09825: loss = 5.5545144e-07,5.9402094e-10\n",
      "Iteration 09830: loss = 5.5573315e-07,5.93255e-10\n",
      "Iteration 09835: loss = 5.561819e-07,5.9248734e-10\n",
      "Iteration 09840: loss = 5.5646717e-07,5.917178e-10\n",
      "Iteration 09845: loss = 5.5683785e-07,5.9094746e-10\n",
      "Iteration 09850: loss = 5.5721193e-07,5.901755e-10\n",
      "Iteration 09855: loss = 5.575877e-07,5.894014e-10\n",
      "Iteration 09860: loss = 5.5796727e-07,5.886255e-10\n",
      "Iteration 09865: loss = 5.583515e-07,5.878476e-10\n",
      "Iteration 09870: loss = 5.5882055e-07,5.8706884e-10\n",
      "Iteration 09875: loss = 5.5912506e-07,5.8628813e-10\n",
      "Iteration 09880: loss = 5.595991e-07,5.855056e-10\n",
      "Iteration 09885: loss = 5.5999413e-07,5.8472105e-10\n",
      "Iteration 09890: loss = 5.6047435e-07,5.839352e-10\n",
      "Iteration 09895: loss = 5.608771e-07,5.8314653e-10\n",
      "Iteration 09900: loss = 5.6128295e-07,5.8235605e-10\n",
      "Iteration 09905: loss = 5.617731e-07,5.815642e-10\n",
      "Iteration 09910: loss = 5.622705e-07,5.807695e-10\n",
      "Iteration 09915: loss = 5.6276923e-07,5.799736e-10\n",
      "Iteration 09920: loss = 5.6318385e-07,5.791773e-10\n",
      "Iteration 09925: loss = 5.6368793e-07,5.78378e-10\n",
      "Iteration 09930: loss = 5.642811e-07,5.7757665e-10\n",
      "Iteration 09935: loss = 5.6479183e-07,5.767742e-10\n",
      "Iteration 09940: loss = 5.6530746e-07,5.759694e-10\n",
      "Iteration 09945: loss = 5.6582564e-07,5.751623e-10\n",
      "Iteration 09950: loss = 5.6634616e-07,5.7435423e-10\n",
      "Iteration 09955: loss = 5.6695495e-07,5.7354377e-10\n",
      "Iteration 09960: loss = 5.6748195e-07,5.727323e-10\n",
      "Iteration 09965: loss = 5.681832e-07,5.719174e-10\n",
      "Iteration 09970: loss = 5.687188e-07,5.7110117e-10\n",
      "Iteration 09975: loss = 5.693433e-07,5.702831e-10\n",
      "Iteration 09980: loss = 5.6988335e-07,5.6946303e-10\n",
      "Iteration 09985: loss = 5.7060294e-07,5.686396e-10\n",
      "Iteration 09990: loss = 5.7115125e-07,5.6781546e-10\n",
      "Iteration 09995: loss = 5.7178875e-07,5.6699e-10\n",
      "Iteration 10000: loss = 5.725163e-07,5.6616206e-10\n",
      "Iteration 10005: loss = 5.7307716e-07,5.653318e-10\n",
      "Iteration 10010: loss = 5.738138e-07,5.644988e-10\n",
      "Iteration 10015: loss = 5.7447033e-07,5.6366334e-10\n",
      "Iteration 10020: loss = 5.7520987e-07,5.62828e-10\n",
      "Iteration 10025: loss = 5.758719e-07,5.619897e-10\n",
      "Iteration 10030: loss = 5.7662317e-07,5.6114946e-10\n",
      "Iteration 10035: loss = 5.7738015e-07,5.603058e-10\n",
      "Iteration 10040: loss = 5.7805323e-07,5.5946187e-10\n",
      "Iteration 10045: loss = 5.788175e-07,5.586147e-10\n",
      "Iteration 10050: loss = 5.795917e-07,5.577628e-10\n",
      "Iteration 10055: loss = 5.804517e-07,5.569104e-10\n",
      "Iteration 10060: loss = 5.8118866e-07,5.560561e-10\n",
      "Iteration 10065: loss = 5.819722e-07,5.5519894e-10\n",
      "Iteration 10070: loss = 5.8284814e-07,5.54339e-10\n",
      "Iteration 10075: loss = 5.836417e-07,5.5347726e-10\n",
      "Iteration 10080: loss = 5.845256e-07,5.5261373e-10\n",
      "Iteration 10085: loss = 5.8532737e-07,5.517472e-10\n",
      "Iteration 10090: loss = 5.862213e-07,5.508789e-10\n",
      "Iteration 10095: loss = 5.870365e-07,5.500075e-10\n",
      "Iteration 10100: loss = 5.8793927e-07,5.4913374e-10\n",
      "Iteration 10105: loss = 5.8893477e-07,5.482583e-10\n",
      "Iteration 10110: loss = 5.898471e-07,5.4738075e-10\n",
      "Iteration 10115: loss = 5.908514e-07,5.4650107e-10\n",
      "Iteration 10120: loss = 5.916883e-07,5.4561816e-10\n",
      "Iteration 10125: loss = 5.927047e-07,5.44733e-10\n",
      "Iteration 10130: loss = 5.936361e-07,5.438458e-10\n",
      "Iteration 10135: loss = 5.947514e-07,5.429555e-10\n",
      "Iteration 10140: loss = 5.956975e-07,5.420627e-10\n",
      "Iteration 10145: loss = 5.9673505e-07,5.411677e-10\n",
      "Iteration 10150: loss = 5.9777796e-07,5.402693e-10\n",
      "Iteration 10155: loss = 5.9891505e-07,5.393696e-10\n",
      "Iteration 10160: loss = 5.998855e-07,5.3846555e-10\n",
      "Iteration 10165: loss = 6.0094516e-07,5.3755983e-10\n",
      "Iteration 10170: loss = 6.0210243e-07,5.3665095e-10\n",
      "Iteration 10175: loss = 6.032647e-07,5.357393e-10\n",
      "Iteration 10180: loss = 6.0434263e-07,5.348262e-10\n",
      "Iteration 10185: loss = 6.056056e-07,5.3390864e-10\n",
      "Iteration 10190: loss = 6.0678923e-07,5.3298843e-10\n",
      "Iteration 10195: loss = 6.079759e-07,5.320663e-10\n",
      "Iteration 10200: loss = 6.091681e-07,5.3114174e-10\n",
      "Iteration 10205: loss = 6.1045597e-07,5.302145e-10\n",
      "Iteration 10210: loss = 6.1161853e-07,5.292844e-10\n",
      "Iteration 10215: loss = 6.1292286e-07,5.2834964e-10\n",
      "Iteration 10220: loss = 6.1414585e-07,5.2741284e-10\n",
      "Iteration 10225: loss = 6.154623e-07,5.2647353e-10\n",
      "Iteration 10230: loss = 6.168751e-07,5.255314e-10\n",
      "Iteration 10235: loss = 6.1811903e-07,5.245859e-10\n",
      "Iteration 10240: loss = 6.1946065e-07,5.236366e-10\n",
      "Iteration 10245: loss = 6.208983e-07,5.226845e-10\n",
      "Iteration 10250: loss = 6.223434e-07,5.217301e-10\n",
      "Iteration 10255: loss = 6.237056e-07,5.2077237e-10\n",
      "Iteration 10260: loss = 6.251658e-07,5.198115e-10\n",
      "Iteration 10265: loss = 6.266307e-07,5.188485e-10\n",
      "Iteration 10270: loss = 6.281961e-07,5.178822e-10\n",
      "Iteration 10275: loss = 6.296835e-07,5.169112e-10\n",
      "Iteration 10280: loss = 6.3117704e-07,5.159372e-10\n",
      "Iteration 10285: loss = 6.327694e-07,5.1496074e-10\n",
      "Iteration 10290: loss = 6.3436687e-07,5.139815e-10\n",
      "Iteration 10295: loss = 6.359755e-07,5.1299887e-10\n",
      "Iteration 10300: loss = 6.3750446e-07,5.1201265e-10\n",
      "Iteration 10305: loss = 6.392232e-07,5.1102306e-10\n",
      "Iteration 10310: loss = 6.4090705e-07,5.100293e-10\n",
      "Iteration 10315: loss = 6.425518e-07,5.090332e-10\n",
      "Iteration 10320: loss = 6.4429963e-07,5.080334e-10\n",
      "Iteration 10325: loss = 6.460562e-07,5.070305e-10\n",
      "Iteration 10330: loss = 6.478242e-07,5.0602256e-10\n",
      "Iteration 10335: loss = 6.496029e-07,5.050121e-10\n",
      "Iteration 10340: loss = 6.514793e-07,5.039982e-10\n",
      "Iteration 10345: loss = 6.5328186e-07,5.0297944e-10\n",
      "Iteration 10350: loss = 6.5518293e-07,5.0195803e-10\n",
      "Iteration 10355: loss = 6.570933e-07,5.009345e-10\n",
      "Iteration 10360: loss = 6.591138e-07,4.999031e-10\n",
      "Iteration 10365: loss = 6.609573e-07,4.9887017e-10\n",
      "Iteration 10370: loss = 6.629995e-07,4.978327e-10\n",
      "Iteration 10375: loss = 6.6505163e-07,4.967918e-10\n",
      "Iteration 10380: loss = 6.671134e-07,4.957481e-10\n",
      "Iteration 10385: loss = 6.691918e-07,4.946992e-10\n",
      "Iteration 10390: loss = 6.7133186e-07,4.936449e-10\n",
      "Iteration 10395: loss = 6.7353153e-07,4.925868e-10\n",
      "Iteration 10400: loss = 6.75638e-07,4.9153004e-10\n",
      "Iteration 10405: loss = 6.7794537e-07,4.9046794e-10\n",
      "Iteration 10410: loss = 6.8008416e-07,4.894004e-10\n",
      "Iteration 10415: loss = 6.824157e-07,4.8833093e-10\n",
      "Iteration 10420: loss = 6.8476675e-07,4.8725585e-10\n",
      "Iteration 10425: loss = 6.8712876e-07,4.861783e-10\n",
      "Iteration 10430: loss = 6.895196e-07,4.85091e-10\n",
      "Iteration 10435: loss = 6.9191884e-07,4.84002e-10\n",
      "Iteration 10440: loss = 6.9433105e-07,4.829093e-10\n",
      "Iteration 10445: loss = 6.9694926e-07,4.8181137e-10\n",
      "Iteration 10450: loss = 6.9939114e-07,4.807097e-10\n",
      "Iteration 10455: loss = 7.019956e-07,4.796028e-10\n",
      "Iteration 10460: loss = 7.045681e-07,4.784919e-10\n",
      "Iteration 10465: loss = 7.0725156e-07,4.7737614e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10470: loss = 7.0995026e-07,4.7625665e-10\n",
      "Iteration 10475: loss = 7.1266663e-07,4.751324e-10\n",
      "Iteration 10480: loss = 7.1549715e-07,4.7400284e-10\n",
      "Iteration 10485: loss = 7.183467e-07,4.7286886e-10\n",
      "Iteration 10490: loss = 7.212129e-07,4.717305e-10\n",
      "Iteration 10495: loss = 7.2419374e-07,4.7058685e-10\n",
      "Iteration 10500: loss = 7.271003e-07,4.6943865e-10\n",
      "Iteration 10505: loss = 7.300771e-07,4.682848e-10\n",
      "Iteration 10510: loss = 7.331162e-07,4.671263e-10\n",
      "Iteration 10515: loss = 7.3627547e-07,4.659635e-10\n",
      "Iteration 10520: loss = 7.3955357e-07,4.6479462e-10\n",
      "Iteration 10525: loss = 7.4265716e-07,4.6362128e-10\n",
      "Iteration 10530: loss = 7.4598034e-07,4.6244214e-10\n",
      "Iteration 10535: loss = 7.4922656e-07,4.612583e-10\n",
      "Iteration 10540: loss = 7.52691e-07,4.6006848e-10\n",
      "Iteration 10545: loss = 7.5608494e-07,4.5887333e-10\n",
      "Iteration 10550: loss = 7.596506e-07,4.5767218e-10\n",
      "Iteration 10555: loss = 7.631884e-07,4.5646642e-10\n",
      "Iteration 10560: loss = 7.6685245e-07,4.5525406e-10\n",
      "Iteration 10565: loss = 7.7043865e-07,4.5403772e-10\n",
      "Iteration 10570: loss = 7.741534e-07,4.5281517e-10\n",
      "Iteration 10575: loss = 7.779976e-07,4.5158535e-10\n",
      "Iteration 10580: loss = 7.8186685e-07,4.503502e-10\n",
      "Iteration 10585: loss = 7.858668e-07,4.4910947e-10\n",
      "Iteration 10590: loss = 7.897399e-07,4.478636e-10\n",
      "Iteration 10595: loss = 7.9379487e-07,4.4661164e-10\n",
      "Iteration 10600: loss = 7.979805e-07,4.4535314e-10\n",
      "Iteration 10605: loss = 8.021013e-07,4.4408843e-10\n",
      "Iteration 10610: loss = 8.064494e-07,4.4281825e-10\n",
      "Iteration 10615: loss = 8.108297e-07,4.415416e-10\n",
      "Iteration 10620: loss = 8.1524075e-07,4.4025944e-10\n",
      "Iteration 10625: loss = 8.197408e-07,4.3897028e-10\n",
      "Iteration 10630: loss = 8.243278e-07,4.3767465e-10\n",
      "Iteration 10635: loss = 8.2894877e-07,4.363718e-10\n",
      "Iteration 10640: loss = 8.337059e-07,4.3506382e-10\n",
      "Iteration 10645: loss = 8.3861505e-07,4.3374612e-10\n",
      "Iteration 10650: loss = 8.435631e-07,4.324219e-10\n",
      "Iteration 10655: loss = 8.485367e-07,4.310938e-10\n",
      "Iteration 10660: loss = 8.535039e-07,4.2975784e-10\n",
      "Iteration 10665: loss = 8.5878065e-07,4.2841303e-10\n",
      "Iteration 10670: loss = 8.6408653e-07,4.2706416e-10\n",
      "Iteration 10675: loss = 8.6944414e-07,4.257061e-10\n",
      "Iteration 10680: loss = 8.74955e-07,4.2433923e-10\n",
      "Iteration 10685: loss = 8.805653e-07,4.2296555e-10\n",
      "Iteration 10690: loss = 8.86273e-07,4.215848e-10\n",
      "Iteration 10695: loss = 8.921308e-07,4.2019882e-10\n",
      "Iteration 10700: loss = 8.979361e-07,4.188021e-10\n",
      "Iteration 10705: loss = 9.040072e-07,4.173986e-10\n",
      "Iteration 10710: loss = 9.1018507e-07,4.1598675e-10\n",
      "Iteration 10715: loss = 9.164696e-07,4.1456727e-10\n",
      "Iteration 10720: loss = 9.229273e-07,4.131384e-10\n",
      "Iteration 10725: loss = 9.295342e-07,4.1170337e-10\n",
      "Iteration 10730: loss = 9.361044e-07,4.1025847e-10\n",
      "Iteration 10735: loss = 9.4289453e-07,4.088051e-10\n",
      "Iteration 10740: loss = 9.4991236e-07,4.0734432e-10\n",
      "Iteration 10745: loss = 9.569934e-07,4.0587372e-10\n",
      "Iteration 10750: loss = 9.642463e-07,4.0439524e-10\n",
      "Iteration 10755: loss = 9.715178e-07,4.0290749e-10\n",
      "Iteration 10760: loss = 9.790216e-07,4.0141146e-10\n",
      "Iteration 10765: loss = 9.868282e-07,3.9990442e-10\n",
      "Iteration 10770: loss = 9.945908e-07,3.9838957e-10\n",
      "Iteration 10775: loss = 1.0024842e-06,3.968652e-10\n",
      "Iteration 10780: loss = 1.0107444e-06,3.953303e-10\n",
      "Iteration 10785: loss = 1.0191944e-06,3.9378661e-10\n",
      "Iteration 10790: loss = 1.0276698e-06,3.922332e-10\n",
      "Iteration 10795: loss = 1.036408e-06,3.906691e-10\n",
      "Iteration 10800: loss = 1.0453408e-06,3.8909725e-10\n",
      "Iteration 10805: loss = 1.0544774e-06,3.8751485e-10\n",
      "Iteration 10810: loss = 1.0636517e-06,3.8592207e-10\n",
      "Iteration 10815: loss = 1.0732182e-06,3.8431813e-10\n",
      "Iteration 10820: loss = 1.0830023e-06,3.8270398e-10\n",
      "Iteration 10825: loss = 1.0929438e-06,3.810785e-10\n",
      "Iteration 10830: loss = 1.1030536e-06,3.7944312e-10\n",
      "Iteration 10835: loss = 1.1135025e-06,3.777978e-10\n",
      "Iteration 10840: loss = 1.12413e-06,3.7614006e-10\n",
      "Iteration 10845: loss = 1.1350479e-06,3.744718e-10\n",
      "Iteration 10850: loss = 1.146081e-06,3.7279316e-10\n",
      "Iteration 10855: loss = 1.1575439e-06,3.7110245e-10\n",
      "Iteration 10860: loss = 1.1692003e-06,3.6939896e-10\n",
      "Iteration 10865: loss = 1.1810513e-06,3.6768386e-10\n",
      "Iteration 10870: loss = 1.1934692e-06,3.6595627e-10\n",
      "Iteration 10875: loss = 1.2059717e-06,3.6421666e-10\n",
      "Iteration 10880: loss = 1.2189288e-06,3.6246445e-10\n",
      "Iteration 10885: loss = 1.2321656e-06,3.6070003e-10\n",
      "Iteration 10890: loss = 1.2456225e-06,3.589217e-10\n",
      "Iteration 10895: loss = 1.2594337e-06,3.571308e-10\n",
      "Iteration 10900: loss = 1.2737383e-06,3.5532616e-10\n",
      "Iteration 10905: loss = 1.2882768e-06,3.5350833e-10\n",
      "Iteration 10910: loss = 1.3033144e-06,3.516773e-10\n",
      "Iteration 10915: loss = 1.3187408e-06,3.4983216e-10\n",
      "Iteration 10920: loss = 1.3344219e-06,3.4797318e-10\n",
      "Iteration 10925: loss = 1.3506314e-06,3.461004e-10\n",
      "Iteration 10930: loss = 1.3672652e-06,3.442112e-10\n",
      "Iteration 10935: loss = 1.3844318e-06,3.4230985e-10\n",
      "Iteration 10940: loss = 1.4019088e-06,3.4039085e-10\n",
      "Iteration 10945: loss = 1.4200132e-06,3.3845787e-10\n",
      "Iteration 10950: loss = 1.4385745e-06,3.3650904e-10\n",
      "Iteration 10955: loss = 1.4577248e-06,3.3454528e-10\n",
      "Iteration 10960: loss = 1.4772763e-06,3.3256564e-10\n",
      "Iteration 10965: loss = 1.4975909e-06,3.3056885e-10\n",
      "Iteration 10970: loss = 1.5182624e-06,3.2855635e-10\n",
      "Iteration 10975: loss = 1.5396258e-06,3.2652964e-10\n",
      "Iteration 10980: loss = 1.5616724e-06,3.2448405e-10\n",
      "Iteration 10985: loss = 1.584393e-06,3.224218e-10\n",
      "Iteration 10990: loss = 1.607587e-06,3.203422e-10\n",
      "Iteration 10995: loss = 1.6317022e-06,3.1824648e-10\n",
      "Iteration 11000: loss = 1.6564818e-06,3.1613118e-10\n",
      "Iteration 11005: loss = 1.6820577e-06,3.1400016e-10\n",
      "Iteration 11010: loss = 1.7084102e-06,3.1184974e-10\n",
      "Iteration 11015: loss = 1.7355391e-06,3.096818e-10\n",
      "Iteration 11020: loss = 1.763541e-06,3.0749536e-10\n",
      "Iteration 11025: loss = 1.7924407e-06,3.052905e-10\n",
      "Iteration 11030: loss = 1.8222578e-06,3.0306646e-10\n",
      "Iteration 11035: loss = 1.8530399e-06,3.0082184e-10\n",
      "Iteration 11040: loss = 1.8849379e-06,2.9855857e-10\n",
      "Iteration 11045: loss = 1.9178276e-06,2.962754e-10\n",
      "Iteration 11050: loss = 1.9516622e-06,2.9397315e-10\n",
      "Iteration 11055: loss = 1.9866354e-06,2.9165018e-10\n",
      "Iteration 11060: loss = 2.022987e-06,2.8930808e-10\n",
      "Iteration 11065: loss = 2.0604732e-06,2.8694497e-10\n",
      "Iteration 11070: loss = 2.099175e-06,2.84561e-10\n",
      "Iteration 11075: loss = 2.13929e-06,2.8215755e-10\n",
      "Iteration 11080: loss = 2.1807919e-06,2.7973307e-10\n",
      "Iteration 11085: loss = 2.223628e-06,2.7728703e-10\n",
      "Iteration 11090: loss = 2.2679983e-06,2.7482094e-10\n",
      "Iteration 11095: loss = 2.3139507e-06,2.7233316e-10\n",
      "Iteration 11100: loss = 2.361546e-06,2.6982386e-10\n",
      "Iteration 11105: loss = 2.4108197e-06,2.672927e-10\n",
      "Iteration 11110: loss = 2.4617345e-06,2.6473948e-10\n",
      "Iteration 11115: loss = 2.5148806e-06,2.6216393e-10\n",
      "Iteration 11120: loss = 2.5698407e-06,2.5956787e-10\n",
      "Iteration 11125: loss = 2.6265018e-06,2.5695024e-10\n",
      "Iteration 11130: loss = 2.6853631e-06,2.543127e-10\n",
      "Iteration 11135: loss = 2.7466874e-06,2.5165314e-10\n",
      "Iteration 11140: loss = 2.8099903e-06,2.4897168e-10\n",
      "Iteration 11145: loss = 2.8753814e-06,2.4627148e-10\n",
      "Iteration 11150: loss = 2.9435005e-06,2.435513e-10\n",
      "Iteration 11155: loss = 3.013963e-06,2.4081123e-10\n",
      "Iteration 11160: loss = 3.0869205e-06,2.3805227e-10\n",
      "Iteration 11165: loss = 3.162435e-06,2.352748e-10\n",
      "Iteration 11170: loss = 3.2410014e-06,2.3247812e-10\n",
      "Iteration 11175: loss = 3.3221547e-06,2.2966531e-10\n",
      "Iteration 11180: loss = 3.4060643e-06,2.2683591e-10\n",
      "Iteration 11185: loss = 3.49328e-06,2.2398958e-10\n",
      "Iteration 11190: loss = 3.58326e-06,2.2112935e-10\n",
      "Iteration 11195: loss = 3.676276e-06,2.1825453e-10\n",
      "Iteration 11200: loss = 3.7727787e-06,2.1536573e-10\n",
      "Iteration 11205: loss = 3.8723224e-06,2.1246528e-10\n",
      "Iteration 11210: loss = 3.975209e-06,2.0955722e-10\n",
      "Iteration 11215: loss = 4.0813534e-06,2.0663993e-10\n",
      "Iteration 11220: loss = 4.1905837e-06,2.0371554e-10\n",
      "Iteration 11225: loss = 4.303543e-06,2.007852e-10\n",
      "Iteration 11230: loss = 4.4193025e-06,1.9785182e-10\n",
      "Iteration 11235: loss = 4.5386446e-06,1.9491633e-10\n",
      "Iteration 11240: loss = 4.661594e-06,1.9198176e-10\n",
      "Iteration 11245: loss = 4.787192e-06,1.8904969e-10\n",
      "Iteration 11250: loss = 4.916474e-06,1.8612245e-10\n",
      "Iteration 11255: loss = 5.0482154e-06,1.8320492e-10\n",
      "Iteration 11260: loss = 5.18295e-06,1.8029751e-10\n",
      "Iteration 11265: loss = 5.320411e-06,1.774037e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11270: loss = 5.4598836e-06,1.7452587e-10\n",
      "Iteration 11275: loss = 5.602244e-06,1.7166732e-10\n",
      "Iteration 11280: loss = 5.7457564e-06,1.6883112e-10\n",
      "Iteration 11285: loss = 5.891318e-06,1.660201e-10\n",
      "Iteration 11290: loss = 6.0379425e-06,1.6323745e-10\n",
      "Iteration 11295: loss = 6.1859264e-06,1.604842e-10\n",
      "Iteration 11300: loss = 6.3345356e-06,1.5776364e-10\n",
      "Iteration 11305: loss = 6.4839023e-06,1.5507817e-10\n",
      "Iteration 11310: loss = 6.6330226e-06,1.5243079e-10\n",
      "Iteration 11315: loss = 6.782282e-06,1.4982467e-10\n",
      "Iteration 11320: loss = 6.9302077e-06,1.4726487e-10\n",
      "Iteration 11325: loss = 7.077077e-06,1.4475027e-10\n",
      "Iteration 11330: loss = 7.2223997e-06,1.422824e-10\n",
      "Iteration 11335: loss = 7.3659903e-06,1.3986419e-10\n",
      "Iteration 11340: loss = 7.5078365e-06,1.3749615e-10\n",
      "Iteration 11345: loss = 7.647128e-06,1.3518081e-10\n",
      "Iteration 11350: loss = 7.784364e-06,1.3291641e-10\n",
      "Iteration 11355: loss = 7.918471e-06,1.307052e-10\n",
      "Iteration 11360: loss = 8.0491955e-06,1.2854989e-10\n",
      "Iteration 11365: loss = 8.176331e-06,1.2645171e-10\n",
      "Iteration 11370: loss = 8.299578e-06,1.2441002e-10\n",
      "Iteration 11375: loss = 8.418844e-06,1.2242571e-10\n",
      "Iteration 11380: loss = 8.533224e-06,1.2049968e-10\n",
      "Iteration 11385: loss = 8.643636e-06,1.186305e-10\n",
      "Iteration 11390: loss = 8.748796e-06,1.1681803e-10\n",
      "Iteration 11395: loss = 8.849296e-06,1.15062196e-10\n",
      "Iteration 11400: loss = 8.9444875e-06,1.13363555e-10\n",
      "Iteration 11405: loss = 9.034436e-06,1.11722666e-10\n",
      "Iteration 11410: loss = 9.117043e-06,1.1014174e-10\n",
      "Iteration 11415: loss = 9.193119e-06,1.0862262e-10\n",
      "Iteration 11420: loss = 9.260673e-06,1.07165346e-10\n",
      "Iteration 11425: loss = 9.321048e-06,1.05766604e-10\n",
      "Iteration 11430: loss = 9.373442e-06,1.0442707e-10\n",
      "Iteration 11435: loss = 9.416667e-06,1.0314935e-10\n",
      "Iteration 11440: loss = 9.449924e-06,1.0193524e-10\n",
      "Iteration 11445: loss = 9.472017e-06,1.007873e-10\n",
      "Iteration 11450: loss = 9.483399e-06,9.970366e-11\n",
      "Iteration 11455: loss = 9.483526e-06,9.868405e-11\n",
      "Iteration 11460: loss = 9.471446e-06,9.773103e-11\n",
      "Iteration 11465: loss = 9.446954e-06,9.684195e-11\n",
      "Iteration 11470: loss = 9.410323e-06,9.601837e-11\n",
      "Iteration 11475: loss = 9.361542e-06,9.525931e-11\n",
      "Iteration 11480: loss = 9.299828e-06,9.4566355e-11\n",
      "Iteration 11485: loss = 9.225122e-06,9.3939835e-11\n",
      "Iteration 11490: loss = 9.137581e-06,9.337828e-11\n",
      "Iteration 11495: loss = 9.037642e-06,9.2882334e-11\n",
      "Iteration 11500: loss = 8.925973e-06,9.2448924e-11\n",
      "Iteration 11505: loss = 8.803805e-06,9.2076166e-11\n",
      "Iteration 11510: loss = 8.671878e-06,9.176056e-11\n",
      "Iteration 11515: loss = 8.531256e-06,9.1500946e-11\n",
      "Iteration 11520: loss = 8.382775e-06,9.1294305e-11\n",
      "Iteration 11525: loss = 8.227298e-06,9.1139554e-11\n",
      "Iteration 11530: loss = 8.0659065e-06,9.103276e-11\n",
      "Iteration 11535: loss = 7.899814e-06,9.097241e-11\n",
      "Iteration 11540: loss = 7.729648e-06,9.0955236e-11\n",
      "Iteration 11545: loss = 7.5568896e-06,9.0977545e-11\n",
      "Iteration 11550: loss = 7.3821425e-06,9.1036303e-11\n",
      "Iteration 11555: loss = 7.2072585e-06,9.1129465e-11\n",
      "Iteration 11560: loss = 7.031953e-06,9.1253505e-11\n",
      "Iteration 11565: loss = 6.8571203e-06,9.140614e-11\n",
      "Iteration 11570: loss = 6.6841926e-06,9.158331e-11\n",
      "Iteration 11575: loss = 6.5129293e-06,9.1783955e-11\n",
      "Iteration 11580: loss = 6.3449584e-06,9.2004085e-11\n",
      "Iteration 11585: loss = 6.179234e-06,9.224284e-11\n",
      "Iteration 11590: loss = 6.0172497e-06,9.2498675e-11\n",
      "Iteration 11595: loss = 5.8583787e-06,9.27683e-11\n",
      "Iteration 11600: loss = 5.703849e-06,9.304968e-11\n",
      "Iteration 11605: loss = 5.553402e-06,9.334129e-11\n",
      "Iteration 11610: loss = 5.4070315e-06,9.3640644e-11\n",
      "Iteration 11615: loss = 5.2659047e-06,9.394444e-11\n",
      "Iteration 11620: loss = 5.1287025e-06,9.425463e-11\n",
      "Iteration 11625: loss = 4.996022e-06,9.456965e-11\n",
      "Iteration 11630: loss = 4.86753e-06,9.488858e-11\n",
      "Iteration 11635: loss = 4.742887e-06,9.521039e-11\n",
      "Iteration 11640: loss = 4.6226337e-06,9.553505e-11\n",
      "Iteration 11645: loss = 4.506428e-06,9.5860875e-11\n",
      "Iteration 11650: loss = 4.394093e-06,9.618681e-11\n",
      "Iteration 11655: loss = 4.285689e-06,9.6513686e-11\n",
      "Iteration 11660: loss = 4.181128e-06,9.683923e-11\n",
      "Iteration 11665: loss = 4.080089e-06,9.716395e-11\n",
      "Iteration 11670: loss = 3.9826027e-06,9.748766e-11\n",
      "Iteration 11675: loss = 3.888604e-06,9.780973e-11\n",
      "Iteration 11680: loss = 3.7978095e-06,9.812903e-11\n",
      "Iteration 11685: loss = 3.7105729e-06,9.844467e-11\n",
      "Iteration 11690: loss = 3.6260365e-06,9.875907e-11\n",
      "Iteration 11695: loss = 3.5446476e-06,9.9070196e-11\n",
      "Iteration 11700: loss = 3.4663492e-06,9.9377256e-11\n",
      "Iteration 11705: loss = 3.3903882e-06,9.968265e-11\n",
      "Iteration 11710: loss = 3.3170043e-06,9.998416e-11\n",
      "Iteration 11715: loss = 3.2460564e-06,1.00284434e-10\n",
      "Iteration 11720: loss = 3.1776246e-06,1.00580565e-10\n",
      "Iteration 11725: loss = 3.111344e-06,1.00873865e-10\n",
      "Iteration 11730: loss = 3.0473336e-06,1.0116358e-10\n",
      "Iteration 11735: loss = 2.9854646e-06,1.0144976e-10\n",
      "Iteration 11740: loss = 2.9259027e-06,1.0173093e-10\n",
      "Iteration 11745: loss = 2.8679276e-06,1.0200914e-10\n",
      "Iteration 11750: loss = 2.8118939e-06,1.0228453e-10\n",
      "Iteration 11755: loss = 2.757674e-06,1.02556526e-10\n",
      "Iteration 11760: loss = 2.7052222e-06,1.02824e-10\n",
      "Iteration 11765: loss = 2.654805e-06,1.0308704e-10\n",
      "Iteration 11770: loss = 2.6056175e-06,1.03345624e-10\n",
      "Iteration 11775: loss = 2.5580086e-06,1.0360224e-10\n",
      "Iteration 11780: loss = 2.5117708e-06,1.0385628e-10\n",
      "Iteration 11785: loss = 2.4668968e-06,1.0410727e-10\n",
      "Iteration 11790: loss = 2.42357e-06,1.0435335e-10\n",
      "Iteration 11795: loss = 2.3815494e-06,1.0459529e-10\n",
      "Iteration 11800: loss = 2.3405253e-06,1.048347e-10\n",
      "Iteration 11805: loss = 2.300859e-06,1.0507215e-10\n",
      "Iteration 11810: loss = 2.262055e-06,1.05306874e-10\n",
      "Iteration 11815: loss = 2.2246525e-06,1.05539036e-10\n",
      "Iteration 11820: loss = 2.188128e-06,1.0576557e-10\n",
      "Iteration 11825: loss = 2.152635e-06,1.0599237e-10\n",
      "Iteration 11830: loss = 2.118031e-06,1.0621441e-10\n",
      "Iteration 11835: loss = 2.0844059e-06,1.0643456e-10\n",
      "Iteration 11840: loss = 2.0519158e-06,1.0665087e-10\n",
      "Iteration 11845: loss = 2.0202235e-06,1.0686424e-10\n",
      "Iteration 11850: loss = 1.989272e-06,1.0707526e-10\n",
      "Iteration 11855: loss = 1.9590746e-06,1.0728342e-10\n",
      "Iteration 11860: loss = 1.929781e-06,1.0748794e-10\n",
      "Iteration 11865: loss = 1.9013497e-06,1.07689996e-10\n",
      "Iteration 11870: loss = 1.8734287e-06,1.0789074e-10\n",
      "Iteration 11875: loss = 1.8461784e-06,1.0808884e-10\n",
      "Iteration 11880: loss = 1.8197592e-06,1.08283896e-10\n",
      "Iteration 11885: loss = 1.7938029e-06,1.0847802e-10\n",
      "Iteration 11890: loss = 1.7684865e-06,1.08669636e-10\n",
      "Iteration 11895: loss = 1.7438698e-06,1.0885846e-10\n",
      "Iteration 11900: loss = 1.7197777e-06,1.0904535e-10\n",
      "Iteration 11905: loss = 1.6963064e-06,1.0922881e-10\n",
      "Iteration 11910: loss = 1.6733444e-06,1.0941376e-10\n",
      "Iteration 11915: loss = 1.6508365e-06,1.09595416e-10\n",
      "Iteration 11920: loss = 1.6288282e-06,1.0977397e-10\n",
      "Iteration 11925: loss = 1.6072717e-06,1.09952124e-10\n",
      "Iteration 11930: loss = 1.5864271e-06,1.101265e-10\n",
      "Iteration 11935: loss = 1.5659025e-06,1.1029882e-10\n",
      "Iteration 11940: loss = 1.545806e-06,1.1047038e-10\n",
      "Iteration 11945: loss = 1.5262449e-06,1.10638707e-10\n",
      "Iteration 11950: loss = 1.5071026e-06,1.10805934e-10\n",
      "Iteration 11955: loss = 1.4883814e-06,1.1097139e-10\n",
      "Iteration 11960: loss = 1.4699732e-06,1.111365e-10\n",
      "Iteration 11965: loss = 1.4518602e-06,1.1129962e-10\n",
      "Iteration 11970: loss = 1.4344514e-06,1.1145861e-10\n",
      "Iteration 11975: loss = 1.4170178e-06,1.11616515e-10\n",
      "Iteration 11980: loss = 1.4001034e-06,1.11772716e-10\n",
      "Iteration 11985: loss = 1.3836612e-06,1.1192664e-10\n",
      "Iteration 11990: loss = 1.3675663e-06,1.1207977e-10\n",
      "Iteration 11995: loss = 1.3516989e-06,1.1223176e-10\n",
      "Iteration 12000: loss = 1.3360885e-06,1.1238112e-10\n",
      "Iteration 12005: loss = 1.3208333e-06,1.1252887e-10\n",
      "Iteration 12010: loss = 1.3059184e-06,1.12675466e-10\n",
      "Iteration 12015: loss = 1.2910864e-06,1.12821155e-10\n",
      "Iteration 12020: loss = 1.2768529e-06,1.12965026e-10\n",
      "Iteration 12025: loss = 1.2627115e-06,1.131069e-10\n",
      "Iteration 12030: loss = 1.2488932e-06,1.1324755e-10\n",
      "Iteration 12035: loss = 1.2352663e-06,1.13387744e-10\n",
      "Iteration 12040: loss = 1.221823e-06,1.135272e-10\n",
      "Iteration 12045: loss = 1.2087877e-06,1.13662996e-10\n",
      "Iteration 12050: loss = 1.1959152e-06,1.137992e-10\n",
      "Iteration 12055: loss = 1.1832367e-06,1.13933626e-10\n",
      "Iteration 12060: loss = 1.1710074e-06,1.14065465e-10\n",
      "Iteration 12065: loss = 1.1588296e-06,1.1419608e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12070: loss = 1.1469366e-06,1.1432578e-10\n",
      "Iteration 12075: loss = 1.1352172e-06,1.1445393e-10\n",
      "Iteration 12080: loss = 1.1236285e-06,1.1457999e-10\n",
      "Iteration 12085: loss = 1.1124254e-06,1.14705315e-10\n",
      "Iteration 12090: loss = 1.1012494e-06,1.14831186e-10\n",
      "Iteration 12095: loss = 1.0903049e-06,1.149547e-10\n",
      "Iteration 12100: loss = 1.0794868e-06,1.15078974e-10\n",
      "Iteration 12105: loss = 1.0688519e-06,1.15200516e-10\n",
      "Iteration 12110: loss = 1.0584054e-06,1.15322044e-10\n",
      "Iteration 12115: loss = 1.0481185e-06,1.1544222e-10\n",
      "Iteration 12120: loss = 1.0382098e-06,1.15560776e-10\n",
      "Iteration 12125: loss = 1.0282829e-06,1.1567752e-10\n",
      "Iteration 12130: loss = 1.0186044e-06,1.1579421e-10\n",
      "Iteration 12135: loss = 1.0088265e-06,1.15910254e-10\n",
      "Iteration 12140: loss = 9.994718e-07,1.1602533e-10\n",
      "Iteration 12145: loss = 9.901374e-07,1.1613923e-10\n",
      "Iteration 12150: loss = 9.80942e-07,1.1625244e-10\n",
      "Iteration 12155: loss = 9.719228e-07,1.16364966e-10\n",
      "Iteration 12160: loss = 9.6303e-07,1.1647681e-10\n",
      "Iteration 12165: loss = 9.542796e-07,1.1658706e-10\n",
      "Iteration 12170: loss = 9.455907e-07,1.1669678e-10\n",
      "Iteration 12175: loss = 9.371324e-07,1.1680573e-10\n",
      "Iteration 12180: loss = 9.2879736e-07,1.16914e-10\n",
      "Iteration 12185: loss = 9.2042006e-07,1.1702134e-10\n",
      "Iteration 12190: loss = 9.123186e-07,1.1712861e-10\n",
      "Iteration 12195: loss = 9.043528e-07,1.1723317e-10\n",
      "Iteration 12200: loss = 8.964849e-07,1.1733829e-10\n",
      "Iteration 12205: loss = 8.885733e-07,1.1744267e-10\n",
      "Iteration 12210: loss = 8.8094225e-07,1.1754533e-10\n",
      "Iteration 12215: loss = 8.734266e-07,1.1764684e-10\n",
      "Iteration 12220: loss = 8.6600994e-07,1.1774869e-10\n",
      "Iteration 12225: loss = 8.5865446e-07,1.1784922e-10\n",
      "Iteration 12230: loss = 8.5135184e-07,1.1794915e-10\n",
      "Iteration 12235: loss = 8.442566e-07,1.1804893e-10\n",
      "Iteration 12240: loss = 8.37282e-07,1.181458e-10\n",
      "Iteration 12245: loss = 8.303413e-07,1.1824405e-10\n",
      "Iteration 12250: loss = 8.2345423e-07,1.1834089e-10\n",
      "Iteration 12255: loss = 8.1678786e-07,1.184358e-10\n",
      "Iteration 12260: loss = 8.1009875e-07,1.185316e-10\n",
      "Iteration 12265: loss = 8.035623e-07,1.186265e-10\n",
      "Iteration 12270: loss = 7.9707024e-07,1.1872091e-10\n",
      "Iteration 12275: loss = 7.906313e-07,1.1881392e-10\n",
      "Iteration 12280: loss = 7.8437347e-07,1.1890795e-10\n",
      "Iteration 12285: loss = 7.78014e-07,1.190009e-10\n",
      "Iteration 12290: loss = 7.718974e-07,1.1909274e-10\n",
      "Iteration 12295: loss = 7.6582864e-07,1.1918382e-10\n",
      "Iteration 12300: loss = 7.598499e-07,1.1927391e-10\n",
      "Iteration 12305: loss = 7.5385583e-07,1.1936442e-10\n",
      "Iteration 12310: loss = 7.480109e-07,1.1945377e-10\n",
      "Iteration 12315: loss = 7.422991e-07,1.1954265e-10\n",
      "Iteration 12320: loss = 7.365381e-07,1.1962974e-10\n",
      "Iteration 12325: loss = 7.308951e-07,1.1971783e-10\n",
      "Iteration 12330: loss = 7.2533476e-07,1.1980605e-10\n",
      "Iteration 12335: loss = 7.1986733e-07,1.1989253e-10\n",
      "Iteration 12340: loss = 7.144394e-07,1.1997825e-10\n",
      "Iteration 12345: loss = 7.0894276e-07,1.2006415e-10\n",
      "Iteration 12350: loss = 7.0369055e-07,1.2014822e-10\n",
      "Iteration 12355: loss = 6.985027e-07,1.2023275e-10\n",
      "Iteration 12360: loss = 6.934079e-07,1.2031554e-10\n",
      "Iteration 12365: loss = 6.88247e-07,1.2039882e-10\n",
      "Iteration 12370: loss = 6.831208e-07,1.204813e-10\n",
      "Iteration 12375: loss = 6.782116e-07,1.2056278e-10\n",
      "Iteration 12380: loss = 6.7334446e-07,1.2064305e-10\n",
      "Iteration 12385: loss = 6.684536e-07,1.2072378e-10\n",
      "Iteration 12390: loss = 6.636401e-07,1.208042e-10\n",
      "Iteration 12395: loss = 6.589448e-07,1.2088375e-10\n",
      "Iteration 12400: loss = 6.5418277e-07,1.209636e-10\n",
      "Iteration 12405: loss = 6.4955026e-07,1.2104252e-10\n",
      "Iteration 12410: loss = 6.4503143e-07,1.2112061e-10\n",
      "Iteration 12415: loss = 6.404967e-07,1.2119855e-10\n",
      "Iteration 12420: loss = 6.3594416e-07,1.2127635e-10\n",
      "Iteration 12425: loss = 6.315109e-07,1.2135334e-10\n",
      "Iteration 12430: loss = 6.27097e-07,1.2143017e-10\n",
      "Iteration 12435: loss = 6.228113e-07,1.2150549e-10\n",
      "Iteration 12440: loss = 6.1845975e-07,1.2158054e-10\n",
      "Iteration 12445: loss = 6.142261e-07,1.2165478e-10\n",
      "Iteration 12450: loss = 6.1005215e-07,1.217299e-10\n",
      "Iteration 12455: loss = 6.059509e-07,1.2180376e-10\n",
      "Iteration 12460: loss = 6.0179207e-07,1.2187688e-10\n",
      "Iteration 12465: loss = 5.978245e-07,1.2194981e-10\n",
      "Iteration 12470: loss = 5.9379363e-07,1.2202303e-10\n",
      "Iteration 12475: loss = 5.897837e-07,1.2209585e-10\n",
      "Iteration 12480: loss = 5.85799e-07,1.2216755e-10\n",
      "Iteration 12485: loss = 5.819166e-07,1.2224018e-10\n",
      "Iteration 12490: loss = 5.7810263e-07,1.2231173e-10\n",
      "Iteration 12495: loss = 5.7427224e-07,1.2238222e-10\n",
      "Iteration 12500: loss = 5.7054916e-07,1.2245255e-10\n",
      "Iteration 12505: loss = 5.667701e-07,1.2252152e-10\n",
      "Iteration 12510: loss = 5.6317896e-07,1.2258998e-10\n",
      "Iteration 12515: loss = 5.5943934e-07,1.2265851e-10\n",
      "Iteration 12520: loss = 5.558842e-07,1.2272693e-10\n",
      "Iteration 12525: loss = 5.523553e-07,1.2279454e-10\n",
      "Iteration 12530: loss = 5.487907e-07,1.2286298e-10\n",
      "Iteration 12535: loss = 5.4529596e-07,1.2292993e-10\n",
      "Iteration 12540: loss = 5.4182163e-07,1.229969e-10\n",
      "Iteration 12545: loss = 5.384449e-07,1.230636e-10\n",
      "Iteration 12550: loss = 5.35007e-07,1.2312941e-10\n",
      "Iteration 12555: loss = 5.315896e-07,1.2319515e-10\n",
      "Iteration 12560: loss = 5.283519e-07,1.232604e-10\n",
      "Iteration 12565: loss = 5.249706e-07,1.233249e-10\n",
      "Iteration 12570: loss = 5.2176654e-07,1.2338995e-10\n",
      "Iteration 12575: loss = 5.186273e-07,1.2345336e-10\n",
      "Iteration 12580: loss = 5.1538086e-07,1.2351699e-10\n",
      "Iteration 12585: loss = 5.122279e-07,1.2358065e-10\n",
      "Iteration 12590: loss = 5.090913e-07,1.2364408e-10\n",
      "Iteration 12595: loss = 5.0597737e-07,1.2370656e-10\n",
      "Iteration 12600: loss = 5.02874e-07,1.2376881e-10\n",
      "Iteration 12605: loss = 4.9979417e-07,1.2383035e-10\n",
      "Iteration 12610: loss = 4.9688845e-07,1.2389134e-10\n",
      "Iteration 12615: loss = 4.9383874e-07,1.2395268e-10\n",
      "Iteration 12620: loss = 4.9099833e-07,1.240137e-10\n",
      "Iteration 12625: loss = 4.880506e-07,1.2407476e-10\n",
      "Iteration 12630: loss = 4.8504313e-07,1.2413551e-10\n",
      "Iteration 12635: loss = 4.822023e-07,1.2419653e-10\n",
      "Iteration 12640: loss = 4.793754e-07,1.2425685e-10\n",
      "Iteration 12645: loss = 4.7648948e-07,1.2431643e-10\n",
      "Iteration 12650: loss = 4.736976e-07,1.243754e-10\n",
      "Iteration 12655: loss = 4.709243e-07,1.2443391e-10\n",
      "Iteration 12660: loss = 4.6822936e-07,1.2449351e-10\n",
      "Iteration 12665: loss = 4.6547424e-07,1.245524e-10\n",
      "Iteration 12670: loss = 4.6273456e-07,1.2461084e-10\n",
      "Iteration 12675: loss = 4.6012323e-07,1.2466853e-10\n",
      "Iteration 12680: loss = 4.5740737e-07,1.2472708e-10\n",
      "Iteration 12685: loss = 4.5486058e-07,1.2478439e-10\n",
      "Iteration 12690: loss = 4.5217112e-07,1.2484196e-10\n",
      "Iteration 12695: loss = 4.4956732e-07,1.248996e-10\n",
      "Iteration 12700: loss = 4.4705908e-07,1.2495618e-10\n",
      "Iteration 12705: loss = 4.4448632e-07,1.250123e-10\n",
      "Iteration 12710: loss = 4.4199888e-07,1.2506815e-10\n",
      "Iteration 12715: loss = 4.3945462e-07,1.2512372e-10\n",
      "Iteration 12720: loss = 4.3699296e-07,1.2517866e-10\n",
      "Iteration 12725: loss = 4.3454838e-07,1.2523338e-10\n",
      "Iteration 12730: loss = 4.3222315e-07,1.2528764e-10\n",
      "Iteration 12735: loss = 4.298015e-07,1.2534168e-10\n",
      "Iteration 12740: loss = 4.2745913e-07,1.2539597e-10\n",
      "Iteration 12745: loss = 4.250555e-07,1.2544998e-10\n",
      "Iteration 12750: loss = 4.2273572e-07,1.2550394e-10\n",
      "Iteration 12755: loss = 4.2035708e-07,1.2555694e-10\n",
      "Iteration 12760: loss = 4.1806098e-07,1.2560972e-10\n",
      "Iteration 12765: loss = 4.1577016e-07,1.2566326e-10\n",
      "Iteration 12770: loss = 4.1356836e-07,1.2571601e-10\n",
      "Iteration 12775: loss = 4.1122848e-07,1.2576863e-10\n",
      "Iteration 12780: loss = 4.08969e-07,1.2582137e-10\n",
      "Iteration 12785: loss = 4.0679242e-07,1.2587412e-10\n",
      "Iteration 12790: loss = 4.0455382e-07,1.2592648e-10\n",
      "Iteration 12795: loss = 4.0243535e-07,1.2597816e-10\n",
      "Iteration 12800: loss = 4.002862e-07,1.2603041e-10\n",
      "Iteration 12805: loss = 3.9814927e-07,1.2608176e-10\n",
      "Iteration 12810: loss = 3.9595434e-07,1.2613248e-10\n",
      "Iteration 12815: loss = 3.938362e-07,1.2618351e-10\n",
      "Iteration 12820: loss = 3.9180455e-07,1.262336e-10\n",
      "Iteration 12825: loss = 3.897027e-07,1.2628441e-10\n",
      "Iteration 12830: loss = 3.8768647e-07,1.263347e-10\n",
      "Iteration 12835: loss = 3.856052e-07,1.2638493e-10\n",
      "Iteration 12840: loss = 3.8360233e-07,1.264349e-10\n",
      "Iteration 12845: loss = 3.8161468e-07,1.2648388e-10\n",
      "Iteration 12850: loss = 3.7956318e-07,1.2653276e-10\n",
      "Iteration 12855: loss = 3.775956e-07,1.2658094e-10\n",
      "Iteration 12860: loss = 3.7563646e-07,1.2662851e-10\n",
      "Iteration 12865: loss = 3.7368554e-07,1.2667613e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12870: loss = 3.7184233e-07,1.267236e-10\n",
      "Iteration 12875: loss = 3.6990681e-07,1.267713e-10\n",
      "Iteration 12880: loss = 3.6797505e-07,1.268189e-10\n",
      "Iteration 12885: loss = 3.66192e-07,1.268659e-10\n",
      "Iteration 12890: loss = 3.6427568e-07,1.2691365e-10\n",
      "Iteration 12895: loss = 3.6243748e-07,1.2696096e-10\n",
      "Iteration 12900: loss = 3.6053544e-07,1.2700832e-10\n",
      "Iteration 12905: loss = 3.5878443e-07,1.2705438e-10\n",
      "Iteration 12910: loss = 3.5696806e-07,1.2710097e-10\n",
      "Iteration 12915: loss = 3.551609e-07,1.2714743e-10\n",
      "Iteration 12920: loss = 3.533537e-07,1.2719452e-10\n",
      "Iteration 12925: loss = 3.5156253e-07,1.2724051e-10\n",
      "Iteration 12930: loss = 3.497778e-07,1.2728614e-10\n",
      "Iteration 12935: loss = 3.4806496e-07,1.2733195e-10\n",
      "Iteration 12940: loss = 3.4629102e-07,1.2737805e-10\n",
      "Iteration 12945: loss = 3.445922e-07,1.2742354e-10\n",
      "Iteration 12950: loss = 3.429061e-07,1.2746815e-10\n",
      "Iteration 12955: loss = 3.4125364e-07,1.2751306e-10\n",
      "Iteration 12960: loss = 3.3951008e-07,1.2755831e-10\n",
      "Iteration 12965: loss = 3.3783758e-07,1.2760308e-10\n",
      "Iteration 12970: loss = 3.3617877e-07,1.2764685e-10\n",
      "Iteration 12975: loss = 3.345226e-07,1.2769127e-10\n",
      "Iteration 12980: loss = 3.3287208e-07,1.2773517e-10\n",
      "Iteration 12985: loss = 3.3129723e-07,1.2777852e-10\n",
      "Iteration 12990: loss = 3.2965818e-07,1.2782239e-10\n",
      "Iteration 12995: loss = 3.2809024e-07,1.2786626e-10\n",
      "Iteration 13000: loss = 3.2646707e-07,1.2790972e-10\n",
      "Iteration 13005: loss = 3.249141e-07,1.2795291e-10\n",
      "Iteration 13010: loss = 3.233031e-07,1.279965e-10\n",
      "Iteration 13015: loss = 3.218282e-07,1.2803858e-10\n",
      "Iteration 13020: loss = 3.2023095e-07,1.2808148e-10\n",
      "Iteration 13025: loss = 3.1863541e-07,1.2812441e-10\n",
      "Iteration 13030: loss = 3.1718332e-07,1.2816564e-10\n",
      "Iteration 13035: loss = 3.1566464e-07,1.2820783e-10\n",
      "Iteration 13040: loss = 3.1415445e-07,1.2825015e-10\n",
      "Iteration 13045: loss = 3.1268314e-07,1.2829164e-10\n",
      "Iteration 13050: loss = 3.112487e-07,1.2833297e-10\n",
      "Iteration 13055: loss = 3.097571e-07,1.2837421e-10\n",
      "Iteration 13060: loss = 3.0833752e-07,1.2841431e-10\n",
      "Iteration 13065: loss = 3.0685803e-07,1.2845515e-10\n",
      "Iteration 13070: loss = 3.054464e-07,1.2849558e-10\n",
      "Iteration 13075: loss = 3.040428e-07,1.2853532e-10\n",
      "Iteration 13080: loss = 3.0257738e-07,1.2857612e-10\n",
      "Iteration 13085: loss = 3.0118204e-07,1.2861609e-10\n",
      "Iteration 13090: loss = 2.9978995e-07,1.2865692e-10\n",
      "Iteration 13095: loss = 2.9834158e-07,1.286969e-10\n",
      "Iteration 13100: loss = 2.969611e-07,1.28737e-10\n",
      "Iteration 13105: loss = 2.9558348e-07,1.2877722e-10\n",
      "Iteration 13110: loss = 2.942135e-07,1.2881682e-10\n",
      "Iteration 13115: loss = 2.9290888e-07,1.2885683e-10\n",
      "Iteration 13120: loss = 2.9154964e-07,1.2889569e-10\n",
      "Iteration 13125: loss = 2.9025455e-07,1.2893524e-10\n",
      "Iteration 13130: loss = 2.8890457e-07,1.2897403e-10\n",
      "Iteration 13135: loss = 2.8756173e-07,1.2901276e-10\n",
      "Iteration 13140: loss = 2.8627966e-07,1.2905214e-10\n",
      "Iteration 13145: loss = 2.8494438e-07,1.290906e-10\n",
      "Iteration 13150: loss = 2.8366898e-07,1.2913073e-10\n",
      "Iteration 13155: loss = 2.8237628e-07,1.2916851e-10\n",
      "Iteration 13160: loss = 2.811132e-07,1.2920737e-10\n",
      "Iteration 13165: loss = 2.7985823e-07,1.2924474e-10\n",
      "Iteration 13170: loss = 2.7860258e-07,1.2928389e-10\n",
      "Iteration 13175: loss = 2.7735632e-07,1.2932126e-10\n",
      "Iteration 13180: loss = 2.7611355e-07,1.2935913e-10\n",
      "Iteration 13185: loss = 2.7481556e-07,1.2939681e-10\n",
      "Iteration 13190: loss = 2.7364396e-07,1.2943409e-10\n",
      "Iteration 13195: loss = 2.7235328e-07,1.2947171e-10\n",
      "Iteration 13200: loss = 2.7119088e-07,1.2950806e-10\n",
      "Iteration 13205: loss = 2.700265e-07,1.2954564e-10\n",
      "Iteration 13210: loss = 2.688155e-07,1.2958153e-10\n",
      "Iteration 13215: loss = 2.6759966e-07,1.2961945e-10\n",
      "Iteration 13220: loss = 2.6645216e-07,1.2965566e-10\n",
      "Iteration 13225: loss = 2.6524606e-07,1.2969271e-10\n",
      "Iteration 13230: loss = 2.6411035e-07,1.2972809e-10\n",
      "Iteration 13235: loss = 2.629133e-07,1.297647e-10\n",
      "Iteration 13240: loss = 2.6178398e-07,1.2980039e-10\n",
      "Iteration 13245: loss = 2.6059413e-07,1.298373e-10\n",
      "Iteration 13250: loss = 2.5947182e-07,1.2987238e-10\n",
      "Iteration 13255: loss = 2.5834802e-07,1.2990875e-10\n",
      "Iteration 13260: loss = 2.5723023e-07,1.2994464e-10\n",
      "Iteration 13265: loss = 2.5611644e-07,1.2998087e-10\n",
      "Iteration 13270: loss = 2.550026e-07,1.3001723e-10\n",
      "Iteration 13275: loss = 2.5389735e-07,1.3005266e-10\n",
      "Iteration 13280: loss = 2.5276316e-07,1.3008937e-10\n",
      "Iteration 13285: loss = 2.516641e-07,1.3012487e-10\n",
      "Iteration 13290: loss = 2.5056545e-07,1.3016116e-10\n",
      "Iteration 13295: loss = 2.4953462e-07,1.3019552e-10\n",
      "Iteration 13300: loss = 2.4844223e-07,1.3023184e-10\n",
      "Iteration 13305: loss = 2.4736224e-07,1.3026585e-10\n",
      "Iteration 13310: loss = 2.4628005e-07,1.3030142e-10\n",
      "Iteration 13315: loss = 2.4531914e-07,1.303346e-10\n",
      "Iteration 13320: loss = 2.4424412e-07,1.3037023e-10\n",
      "Iteration 13325: loss = 2.431769e-07,1.304044e-10\n",
      "Iteration 13330: loss = 2.4216757e-07,1.3043826e-10\n",
      "Iteration 13335: loss = 2.4111043e-07,1.304715e-10\n",
      "Iteration 13340: loss = 2.401082e-07,1.3050556e-10\n",
      "Iteration 13345: loss = 2.3910908e-07,1.3053934e-10\n",
      "Iteration 13350: loss = 2.380625e-07,1.3057218e-10\n",
      "Iteration 13355: loss = 2.3706416e-07,1.3060725e-10\n",
      "Iteration 13360: loss = 2.360829e-07,1.3063894e-10\n",
      "Iteration 13365: loss = 2.350974e-07,1.306722e-10\n",
      "Iteration 13370: loss = 2.3411455e-07,1.3070553e-10\n",
      "Iteration 13375: loss = 2.3313748e-07,1.3073828e-10\n",
      "Iteration 13380: loss = 2.3216121e-07,1.307716e-10\n",
      "Iteration 13385: loss = 2.3119088e-07,1.3080423e-10\n",
      "Iteration 13390: loss = 2.3021788e-07,1.3083797e-10\n",
      "Iteration 13395: loss = 2.2931262e-07,1.3086852e-10\n",
      "Iteration 13400: loss = 2.2834928e-07,1.3090191e-10\n",
      "Iteration 13405: loss = 2.2738787e-07,1.3093461e-10\n",
      "Iteration 13410: loss = 2.264354e-07,1.3096603e-10\n",
      "Iteration 13415: loss = 2.2553127e-07,1.3099985e-10\n",
      "Iteration 13420: loss = 2.2458505e-07,1.3103081e-10\n",
      "Iteration 13425: loss = 2.2369109e-07,1.3106309e-10\n",
      "Iteration 13430: loss = 2.2271767e-07,1.3109615e-10\n",
      "Iteration 13435: loss = 2.218343e-07,1.3112628e-10\n",
      "Iteration 13440: loss = 2.2094515e-07,1.3115942e-10\n",
      "Iteration 13445: loss = 2.2000886e-07,1.3119167e-10\n",
      "Iteration 13450: loss = 2.1913121e-07,1.3122302e-10\n",
      "Iteration 13455: loss = 2.1824845e-07,1.312565e-10\n",
      "Iteration 13460: loss = 2.1732626e-07,1.3128651e-10\n",
      "Iteration 13465: loss = 2.164536e-07,1.3131864e-10\n",
      "Iteration 13470: loss = 2.1558363e-07,1.3135057e-10\n",
      "Iteration 13475: loss = 2.147212e-07,1.3138085e-10\n",
      "Iteration 13480: loss = 2.1380303e-07,1.3141305e-10\n",
      "Iteration 13485: loss = 2.1294612e-07,1.3144316e-10\n",
      "Iteration 13490: loss = 2.1208915e-07,1.3147432e-10\n",
      "Iteration 13495: loss = 2.1123104e-07,1.3150644e-10\n",
      "Iteration 13500: loss = 2.1038483e-07,1.3153584e-10\n",
      "Iteration 13505: loss = 2.0958521e-07,1.3156692e-10\n",
      "Iteration 13510: loss = 2.0873591e-07,1.3159865e-10\n",
      "Iteration 13515: loss = 2.07843e-07,1.3162865e-10\n",
      "Iteration 13520: loss = 2.0700021e-07,1.3165964e-10\n",
      "Iteration 13525: loss = 2.0616193e-07,1.3168999e-10\n",
      "Iteration 13530: loss = 2.0537789e-07,1.3172011e-10\n",
      "Iteration 13535: loss = 2.045427e-07,1.31751e-10\n",
      "Iteration 13540: loss = 2.0370781e-07,1.3178256e-10\n",
      "Iteration 13545: loss = 2.0288316e-07,1.3181191e-10\n",
      "Iteration 13550: loss = 2.0210234e-07,1.3184352e-10\n",
      "Iteration 13555: loss = 2.0127953e-07,1.3187376e-10\n",
      "Iteration 13560: loss = 2.0045839e-07,1.3190382e-10\n",
      "Iteration 13565: loss = 1.9968518e-07,1.3193535e-10\n",
      "Iteration 13570: loss = 1.9887055e-07,1.3196534e-10\n",
      "Iteration 13575: loss = 1.9805505e-07,1.319954e-10\n",
      "Iteration 13580: loss = 1.9729113e-07,1.3202638e-10\n",
      "Iteration 13585: loss = 1.9653544e-07,1.3205537e-10\n",
      "Iteration 13590: loss = 1.9578097e-07,1.320841e-10\n",
      "Iteration 13595: loss = 1.9497236e-07,1.3211551e-10\n",
      "Iteration 13600: loss = 1.9422436e-07,1.3214366e-10\n",
      "Iteration 13605: loss = 1.9344922e-07,1.3217344e-10\n",
      "Iteration 13610: loss = 1.9269953e-07,1.3220339e-10\n",
      "Iteration 13615: loss = 1.919566e-07,1.3223198e-10\n",
      "Iteration 13620: loss = 1.9121579e-07,1.3226104e-10\n",
      "Iteration 13625: loss = 1.9047245e-07,1.3229115e-10\n",
      "Iteration 13630: loss = 1.8968926e-07,1.3231899e-10\n",
      "Iteration 13635: loss = 1.8900285e-07,1.323475e-10\n",
      "Iteration 13640: loss = 1.8826775e-07,1.323767e-10\n",
      "Iteration 13645: loss = 1.8748965e-07,1.3240509e-10\n",
      "Iteration 13650: loss = 1.8681037e-07,1.3243315e-10\n",
      "Iteration 13655: loss = 1.8608227e-07,1.3246226e-10\n",
      "Iteration 13660: loss = 1.853092e-07,1.3249099e-10\n",
      "Iteration 13665: loss = 1.8463747e-07,1.3251866e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13670: loss = 1.8391673e-07,1.325471e-10\n",
      "Iteration 13675: loss = 1.8324751e-07,1.325753e-10\n",
      "Iteration 13680: loss = 1.8253688e-07,1.3260129e-10\n",
      "Iteration 13685: loss = 1.8182119e-07,1.3262974e-10\n",
      "Iteration 13690: loss = 1.8111005e-07,1.3265795e-10\n",
      "Iteration 13695: loss = 1.8045121e-07,1.326846e-10\n",
      "Iteration 13700: loss = 1.7974696e-07,1.3271136e-10\n",
      "Iteration 13705: loss = 1.7903686e-07,1.3274082e-10\n",
      "Iteration 13710: loss = 1.7838447e-07,1.3276724e-10\n",
      "Iteration 13715: loss = 1.7773168e-07,1.3279466e-10\n",
      "Iteration 13720: loss = 1.7703393e-07,1.3282168e-10\n",
      "Iteration 13725: loss = 1.7637986e-07,1.3285041e-10\n",
      "Iteration 13730: loss = 1.7569134e-07,1.3287586e-10\n",
      "Iteration 13735: loss = 1.7504549e-07,1.329027e-10\n",
      "Iteration 13740: loss = 1.7435218e-07,1.3293078e-10\n",
      "Iteration 13745: loss = 1.7370998e-07,1.3295788e-10\n",
      "Iteration 13750: loss = 1.7307188e-07,1.3298425e-10\n",
      "Iteration 13755: loss = 1.7238783e-07,1.3301085e-10\n",
      "Iteration 13760: loss = 1.7174717e-07,1.3304002e-10\n",
      "Iteration 13765: loss = 1.7111603e-07,1.3306506e-10\n",
      "Iteration 13770: loss = 1.7043784e-07,1.3309158e-10\n",
      "Iteration 13775: loss = 1.6980512e-07,1.3311918e-10\n",
      "Iteration 13780: loss = 1.691753e-07,1.3314622e-10\n",
      "Iteration 13785: loss = 1.6850318e-07,1.3317204e-10\n",
      "Iteration 13790: loss = 1.6792457e-07,1.3319862e-10\n",
      "Iteration 13795: loss = 1.672958e-07,1.3322689e-10\n",
      "Iteration 13800: loss = 1.6667622e-07,1.3325223e-10\n",
      "Iteration 13805: loss = 1.6601099e-07,1.3327886e-10\n",
      "Iteration 13810: loss = 1.6539407e-07,1.3330473e-10\n",
      "Iteration 13815: loss = 1.6477263e-07,1.3333297e-10\n",
      "Iteration 13820: loss = 1.6420609e-07,1.3335752e-10\n",
      "Iteration 13825: loss = 1.6361828e-07,1.333828e-10\n",
      "Iteration 13830: loss = 1.6296026e-07,1.3340963e-10\n",
      "Iteration 13835: loss = 1.6234746e-07,1.3343648e-10\n",
      "Iteration 13840: loss = 1.6178882e-07,1.3346127e-10\n",
      "Iteration 13845: loss = 1.6118538e-07,1.3348578e-10\n",
      "Iteration 13850: loss = 1.6057936e-07,1.3351226e-10\n",
      "Iteration 13855: loss = 1.6001734e-07,1.3354017e-10\n",
      "Iteration 13860: loss = 1.5942156e-07,1.3356354e-10\n",
      "Iteration 13865: loss = 1.58821e-07,1.335895e-10\n",
      "Iteration 13870: loss = 1.5822175e-07,1.3361567e-10\n",
      "Iteration 13875: loss = 1.5762248e-07,1.3364267e-10\n",
      "Iteration 13880: loss = 1.570291e-07,1.3366791e-10\n",
      "Iteration 13885: loss = 1.5648182e-07,1.336924e-10\n",
      "Iteration 13890: loss = 1.5588826e-07,1.3371902e-10\n",
      "Iteration 13895: loss = 1.5534e-07,1.3374585e-10\n",
      "Iteration 13900: loss = 1.5475452e-07,1.3376994e-10\n",
      "Iteration 13905: loss = 1.5421305e-07,1.3379456e-10\n",
      "Iteration 13910: loss = 1.5362907e-07,1.3381977e-10\n",
      "Iteration 13915: loss = 1.5308548e-07,1.338467e-10\n",
      "Iteration 13920: loss = 1.5250617e-07,1.3387098e-10\n",
      "Iteration 13925: loss = 1.5197226e-07,1.338951e-10\n",
      "Iteration 13930: loss = 1.5139155e-07,1.339209e-10\n",
      "Iteration 13935: loss = 1.5085674e-07,1.3394649e-10\n",
      "Iteration 13940: loss = 1.502813e-07,1.3397142e-10\n",
      "Iteration 13945: loss = 1.4975329e-07,1.3399548e-10\n",
      "Iteration 13950: loss = 1.4922753e-07,1.340192e-10\n",
      "Iteration 13955: loss = 1.4869498e-07,1.340459e-10\n",
      "Iteration 13960: loss = 1.4812771e-07,1.3406981e-10\n",
      "Iteration 13965: loss = 1.4760637e-07,1.3409317e-10\n",
      "Iteration 13970: loss = 1.4708287e-07,1.3411815e-10\n",
      "Iteration 13975: loss = 1.465175e-07,1.3414284e-10\n",
      "Iteration 13980: loss = 1.45993e-07,1.341692e-10\n",
      "Iteration 13985: loss = 1.4552047e-07,1.3419202e-10\n",
      "Iteration 13990: loss = 1.4496382e-07,1.34215e-10\n",
      "Iteration 13995: loss = 1.4444544e-07,1.3424051e-10\n",
      "Iteration 14000: loss = 1.4392764e-07,1.3426626e-10\n",
      "Iteration 14005: loss = 1.433759e-07,1.3428876e-10\n",
      "Iteration 14010: loss = 1.4290792e-07,1.3431267e-10\n",
      "Iteration 14015: loss = 1.4239787e-07,1.3433633e-10\n",
      "Iteration 14020: loss = 1.418886e-07,1.3436073e-10\n",
      "Iteration 14025: loss = 1.4134007e-07,1.3438434e-10\n",
      "Iteration 14030: loss = 1.4088003e-07,1.3440624e-10\n",
      "Iteration 14035: loss = 1.4037714e-07,1.3442913e-10\n",
      "Iteration 14040: loss = 1.3987265e-07,1.3445346e-10\n",
      "Iteration 14045: loss = 1.3941145e-07,1.3447801e-10\n",
      "Iteration 14050: loss = 1.3891193e-07,1.3450092e-10\n",
      "Iteration 14055: loss = 1.3837301e-07,1.3452349e-10\n",
      "Iteration 14060: loss = 1.3791824e-07,1.3454637e-10\n",
      "Iteration 14065: loss = 1.3742148e-07,1.3457002e-10\n",
      "Iteration 14070: loss = 1.3692257e-07,1.3459528e-10\n",
      "Iteration 14075: loss = 1.364745e-07,1.3461648e-10\n",
      "Iteration 14080: loss = 1.3598527e-07,1.3463826e-10\n",
      "Iteration 14085: loss = 1.3553448e-07,1.3466211e-10\n",
      "Iteration 14090: loss = 1.350431e-07,1.3468573e-10\n",
      "Iteration 14095: loss = 1.3455328e-07,1.3470923e-10\n",
      "Iteration 14100: loss = 1.3413045e-07,1.3473116e-10\n",
      "Iteration 14105: loss = 1.3364681e-07,1.3475283e-10\n",
      "Iteration 14110: loss = 1.3320467e-07,1.3477473e-10\n",
      "Iteration 14115: loss = 1.3267768e-07,1.3479895e-10\n",
      "Iteration 14120: loss = 1.3219307e-07,1.3482324e-10\n",
      "Iteration 14125: loss = 1.3175764e-07,1.3484369e-10\n",
      "Iteration 14130: loss = 1.3127789e-07,1.3486645e-10\n",
      "Iteration 14135: loss = 1.308408e-07,1.3488906e-10\n",
      "Iteration 14140: loss = 1.3040484e-07,1.3491114e-10\n",
      "Iteration 14145: loss = 1.2992571e-07,1.3493508e-10\n",
      "Iteration 14150: loss = 1.2953619e-07,1.3495569e-10\n",
      "Iteration 14155: loss = 1.2906604e-07,1.3497607e-10\n",
      "Iteration 14160: loss = 1.2863411e-07,1.349986e-10\n",
      "Iteration 14165: loss = 1.2816152e-07,1.3502202e-10\n",
      "Iteration 14170: loss = 1.2772944e-07,1.3504535e-10\n",
      "Iteration 14175: loss = 1.2726203e-07,1.3506693e-10\n",
      "Iteration 14180: loss = 1.268358e-07,1.350884e-10\n",
      "Iteration 14185: loss = 1.2637085e-07,1.351098e-10\n",
      "Iteration 14190: loss = 1.2594653e-07,1.351318e-10\n",
      "Iteration 14195: loss = 1.2555817e-07,1.3515598e-10\n",
      "Iteration 14200: loss = 1.250945e-07,1.3517794e-10\n",
      "Iteration 14205: loss = 1.246763e-07,1.3519812e-10\n",
      "Iteration 14210: loss = 1.2421708e-07,1.3521938e-10\n",
      "Iteration 14215: loss = 1.2379705e-07,1.3524155e-10\n",
      "Iteration 14220: loss = 1.233772e-07,1.3526405e-10\n",
      "Iteration 14225: loss = 1.2295932e-07,1.3528624e-10\n",
      "Iteration 14230: loss = 1.225449e-07,1.3530672e-10\n",
      "Iteration 14235: loss = 1.2213305e-07,1.3532668e-10\n",
      "Iteration 14240: loss = 1.2168088e-07,1.3534747e-10\n",
      "Iteration 14245: loss = 1.2130606e-07,1.3536954e-10\n",
      "Iteration 14250: loss = 1.2089103e-07,1.3539223e-10\n",
      "Iteration 14255: loss = 1.2044299e-07,1.3541282e-10\n",
      "Iteration 14260: loss = 1.2007635e-07,1.3543161e-10\n",
      "Iteration 14265: loss = 1.1966775e-07,1.3545279e-10\n",
      "Iteration 14270: loss = 1.1926154e-07,1.3547359e-10\n",
      "Iteration 14275: loss = 1.1881584e-07,1.3549477e-10\n",
      "Iteration 14280: loss = 1.1844574e-07,1.3551738e-10\n",
      "Iteration 14285: loss = 1.18040454e-07,1.3553868e-10\n",
      "Iteration 14290: loss = 1.1763881e-07,1.3555884e-10\n",
      "Iteration 14295: loss = 1.17238876e-07,1.3557847e-10\n",
      "Iteration 14300: loss = 1.1683636e-07,1.355999e-10\n",
      "Iteration 14305: loss = 1.16434904e-07,1.3562165e-10\n",
      "Iteration 14310: loss = 1.16070545e-07,1.3564397e-10\n",
      "Iteration 14315: loss = 1.1563174e-07,1.3566576e-10\n",
      "Iteration 14320: loss = 1.1523613e-07,1.3568581e-10\n",
      "Iteration 14325: loss = 1.1487794e-07,1.357061e-10\n",
      "Iteration 14330: loss = 1.1448313e-07,1.3572654e-10\n",
      "Iteration 14335: loss = 1.14125235e-07,1.3574768e-10\n",
      "Iteration 14340: loss = 1.1369062e-07,1.3577031e-10\n",
      "Iteration 14345: loss = 1.13298256e-07,1.35791e-10\n",
      "Iteration 14350: loss = 1.12947866e-07,1.3580949e-10\n",
      "Iteration 14355: loss = 1.1255768e-07,1.3582964e-10\n",
      "Iteration 14360: loss = 1.1220502e-07,1.3585041e-10\n",
      "Iteration 14365: loss = 1.11813925e-07,1.3587209e-10\n",
      "Iteration 14370: loss = 1.1146239e-07,1.3589307e-10\n",
      "Iteration 14375: loss = 1.1103657e-07,1.3591439e-10\n",
      "Iteration 14380: loss = 1.1068966e-07,1.3593386e-10\n",
      "Iteration 14385: loss = 1.10306274e-07,1.3595328e-10\n",
      "Iteration 14390: loss = 1.09961e-07,1.3597279e-10\n",
      "Iteration 14395: loss = 1.0957598e-07,1.3599379e-10\n",
      "Iteration 14400: loss = 1.0922768e-07,1.3601584e-10\n",
      "Iteration 14405: loss = 1.088437e-07,1.3603774e-10\n",
      "Iteration 14410: loss = 1.0850507e-07,1.3605518e-10\n",
      "Iteration 14415: loss = 1.0812665e-07,1.3607458e-10\n",
      "Iteration 14420: loss = 1.07785176e-07,1.3609479e-10\n",
      "Iteration 14425: loss = 1.074069e-07,1.3611498e-10\n",
      "Iteration 14430: loss = 1.0706634e-07,1.36135e-10\n",
      "Iteration 14435: loss = 1.0668646e-07,1.3615768e-10\n",
      "Iteration 14440: loss = 1.06348324e-07,1.3617785e-10\n",
      "Iteration 14445: loss = 1.0601269e-07,1.3619648e-10\n",
      "Iteration 14450: loss = 1.0564219e-07,1.3621468e-10\n",
      "Iteration 14455: loss = 1.0530528e-07,1.362351e-10\n",
      "Iteration 14460: loss = 1.0493153e-07,1.3625641e-10\n",
      "Iteration 14465: loss = 1.04595664e-07,1.3627721e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14470: loss = 1.0427922e-07,1.3629796e-10\n",
      "Iteration 14475: loss = 1.0391068e-07,1.3631762e-10\n",
      "Iteration 14480: loss = 1.03579794e-07,1.363368e-10\n",
      "Iteration 14485: loss = 1.0324993e-07,1.3635575e-10\n",
      "Iteration 14490: loss = 1.0288486e-07,1.3637458e-10\n",
      "Iteration 14495: loss = 1.025552e-07,1.3639435e-10\n",
      "Iteration 14500: loss = 1.022232e-07,1.3641624e-10\n",
      "Iteration 14505: loss = 1.0185726e-07,1.3643706e-10\n",
      "Iteration 14510: loss = 1.0153471e-07,1.3645407e-10\n",
      "Iteration 14515: loss = 1.0121113e-07,1.3647158e-10\n",
      "Iteration 14520: loss = 1.0084884e-07,1.3649194e-10\n",
      "Iteration 14525: loss = 1.0052369e-07,1.3651158e-10\n",
      "Iteration 14530: loss = 1.0019985e-07,1.3653147e-10\n",
      "Iteration 14535: loss = 9.987628e-08,1.3655106e-10\n",
      "Iteration 14540: loss = 9.951575e-08,1.3657221e-10\n",
      "Iteration 14545: loss = 9.923156e-08,1.365908e-10\n",
      "Iteration 14550: loss = 9.891276e-08,1.3660865e-10\n",
      "Iteration 14555: loss = 9.859517e-08,1.366264e-10\n",
      "Iteration 14560: loss = 9.82405e-08,1.3664558e-10\n",
      "Iteration 14565: loss = 9.791979e-08,1.3666633e-10\n",
      "Iteration 14570: loss = 9.759978e-08,1.3668713e-10\n",
      "Iteration 14575: loss = 9.728239e-08,1.3670667e-10\n",
      "Iteration 14580: loss = 9.697072e-08,1.3672276e-10\n",
      "Iteration 14585: loss = 9.6655015e-08,1.367423e-10\n",
      "Iteration 14590: loss = 9.634033e-08,1.3676114e-10\n",
      "Iteration 14595: loss = 9.6026916e-08,1.3677998e-10\n",
      "Iteration 14600: loss = 9.571435e-08,1.3679846e-10\n",
      "Iteration 14605: loss = 9.5435304e-08,1.3681864e-10\n",
      "Iteration 14610: loss = 9.508534e-08,1.3684022e-10\n",
      "Iteration 14615: loss = 9.477477e-08,1.3685887e-10\n",
      "Iteration 14620: loss = 9.4467055e-08,1.368757e-10\n",
      "Iteration 14625: loss = 9.415914e-08,1.368932e-10\n",
      "Iteration 14630: loss = 9.388496e-08,1.3691247e-10\n",
      "Iteration 14635: loss = 9.357519e-08,1.3693206e-10\n",
      "Iteration 14640: loss = 9.3267595e-08,1.3695142e-10\n",
      "Iteration 14645: loss = 9.29251e-08,1.3697117e-10\n",
      "Iteration 14650: loss = 9.2652726e-08,1.3699024e-10\n",
      "Iteration 14655: loss = 9.2348905e-08,1.3700804e-10\n",
      "Iteration 14660: loss = 9.204635e-08,1.3702584e-10\n",
      "Iteration 14665: loss = 9.174399e-08,1.3704353e-10\n",
      "Iteration 14670: loss = 9.1476245e-08,1.370612e-10\n",
      "Iteration 14675: loss = 9.117363e-08,1.370799e-10\n",
      "Iteration 14680: loss = 9.0869065e-08,1.3710043e-10\n",
      "Iteration 14685: loss = 9.0566125e-08,1.3712044e-10\n",
      "Iteration 14690: loss = 9.03e-08,1.3713851e-10\n",
      "Iteration 14695: loss = 9.000333e-08,1.3715513e-10\n",
      "Iteration 14700: loss = 8.970593e-08,1.3717284e-10\n",
      "Iteration 14705: loss = 8.944115e-08,1.3719152e-10\n",
      "Iteration 14710: loss = 8.914273e-08,1.3721009e-10\n",
      "Iteration 14715: loss = 8.884633e-08,1.3722833e-10\n",
      "Iteration 14720: loss = 8.8582624e-08,1.3724742e-10\n",
      "Iteration 14725: loss = 8.825126e-08,1.37268e-10\n",
      "Iteration 14730: loss = 8.795499e-08,1.3728728e-10\n",
      "Iteration 14735: loss = 8.769612e-08,1.3730402e-10\n",
      "Iteration 14740: loss = 8.743834e-08,1.3732075e-10\n",
      "Iteration 14745: loss = 8.714692e-08,1.3733789e-10\n",
      "Iteration 14750: loss = 8.688742e-08,1.3735665e-10\n",
      "Iteration 14755: loss = 8.65943e-08,1.3737574e-10\n",
      "Iteration 14760: loss = 8.630233e-08,1.373948e-10\n",
      "Iteration 14765: loss = 8.6043805e-08,1.3741419e-10\n",
      "Iteration 14770: loss = 8.5755595e-08,1.3743119e-10\n",
      "Iteration 14775: loss = 8.550213e-08,1.3744801e-10\n",
      "Iteration 14780: loss = 8.521429e-08,1.374657e-10\n",
      "Iteration 14785: loss = 8.496074e-08,1.3748294e-10\n",
      "Iteration 14790: loss = 8.4674845e-08,1.3750012e-10\n",
      "Iteration 14795: loss = 8.438858e-08,1.3751802e-10\n",
      "Iteration 14800: loss = 8.413628e-08,1.3753572e-10\n",
      "Iteration 14805: loss = 8.3848334e-08,1.375559e-10\n",
      "Iteration 14810: loss = 8.3561304e-08,1.37576e-10\n",
      "Iteration 14815: loss = 8.331225e-08,1.3759228e-10\n",
      "Iteration 14820: loss = 8.3064094e-08,1.3760848e-10\n",
      "Iteration 14825: loss = 8.281649e-08,1.3762474e-10\n",
      "Iteration 14830: loss = 8.253363e-08,1.3764309e-10\n",
      "Iteration 14835: loss = 8.2284096e-08,1.376613e-10\n",
      "Iteration 14840: loss = 8.200295e-08,1.3767984e-10\n",
      "Iteration 14845: loss = 8.175406e-08,1.3769808e-10\n",
      "Iteration 14850: loss = 8.147426e-08,1.377167e-10\n",
      "Iteration 14855: loss = 8.122391e-08,1.3773735e-10\n",
      "Iteration 14860: loss = 8.0979724e-08,1.3775352e-10\n",
      "Iteration 14865: loss = 8.073685e-08,1.3776959e-10\n",
      "Iteration 14870: loss = 8.046112e-08,1.3778591e-10\n",
      "Iteration 14875: loss = 8.021859e-08,1.3780274e-10\n",
      "Iteration 14880: loss = 7.994351e-08,1.3781982e-10\n",
      "Iteration 14885: loss = 7.9729794e-08,1.37839e-10\n",
      "Iteration 14890: loss = 7.945413e-08,1.378578e-10\n",
      "Iteration 14895: loss = 7.921024e-08,1.3787663e-10\n",
      "Iteration 14900: loss = 7.8938825e-08,1.3789285e-10\n",
      "Iteration 14905: loss = 7.8700765e-08,1.3790787e-10\n",
      "Iteration 14910: loss = 7.8462534e-08,1.3792412e-10\n",
      "Iteration 14915: loss = 7.8222826e-08,1.3794153e-10\n",
      "Iteration 14920: loss = 7.7952016e-08,1.3795903e-10\n",
      "Iteration 14925: loss = 7.771314e-08,1.3797631e-10\n",
      "Iteration 14930: loss = 7.747431e-08,1.379941e-10\n",
      "Iteration 14935: loss = 7.723625e-08,1.3801206e-10\n",
      "Iteration 14940: loss = 7.696473e-08,1.38032e-10\n",
      "Iteration 14945: loss = 7.672841e-08,1.3804896e-10\n",
      "Iteration 14950: loss = 7.649439e-08,1.3806485e-10\n",
      "Iteration 14955: loss = 7.626006e-08,1.3808077e-10\n",
      "Iteration 14960: loss = 7.6026744e-08,1.3809658e-10\n",
      "Iteration 14965: loss = 7.579364e-08,1.3811288e-10\n",
      "Iteration 14970: loss = 7.5558866e-08,1.3813106e-10\n",
      "Iteration 14975: loss = 7.532439e-08,1.3814877e-10\n",
      "Iteration 14980: loss = 7.5060285e-08,1.3816642e-10\n",
      "Iteration 14985: loss = 7.48262e-08,1.3818519e-10\n",
      "Iteration 14990: loss = 7.45946e-08,1.3820184e-10\n",
      "Iteration 14995: loss = 7.438161e-08,1.3821642e-10\n",
      "Iteration 15000: loss = 7.4182935e-08,1.382325e-10\n",
      "Iteration 15005: loss = 7.3923026e-08,1.382485e-10\n",
      "Iteration 15010: loss = 7.369476e-08,1.3826414e-10\n",
      "Iteration 15015: loss = 7.349723e-08,1.382801e-10\n",
      "Iteration 15020: loss = 7.323823e-08,1.3829679e-10\n",
      "Iteration 15025: loss = 7.30108e-08,1.383132e-10\n",
      "Iteration 15030: loss = 7.2812334e-08,1.3833076e-10\n",
      "Iteration 15035: loss = 7.255257e-08,1.3834936e-10\n",
      "Iteration 15040: loss = 7.232545e-08,1.3836671e-10\n",
      "Iteration 15045: loss = 7.213158e-08,1.3838174e-10\n",
      "Iteration 15050: loss = 7.187776e-08,1.3839681e-10\n",
      "Iteration 15055: loss = 7.168509e-08,1.3841162e-10\n",
      "Iteration 15060: loss = 7.146133e-08,1.3842752e-10\n",
      "Iteration 15065: loss = 7.1236734e-08,1.3844434e-10\n",
      "Iteration 15070: loss = 7.101281e-08,1.3846126e-10\n",
      "Iteration 15075: loss = 7.0788815e-08,1.3847884e-10\n",
      "Iteration 15080: loss = 7.059448e-08,1.3849656e-10\n",
      "Iteration 15085: loss = 7.034169e-08,1.3851381e-10\n",
      "Iteration 15090: loss = 7.01186e-08,1.3853164e-10\n",
      "Iteration 15095: loss = 6.9927346e-08,1.3854792e-10\n",
      "Iteration 15100: loss = 6.967891e-08,1.3856298e-10\n",
      "Iteration 15105: loss = 6.9489715e-08,1.3857837e-10\n",
      "Iteration 15110: loss = 6.927076e-08,1.3859429e-10\n",
      "Iteration 15115: loss = 6.908238e-08,1.3860968e-10\n",
      "Iteration 15120: loss = 6.8834716e-08,1.386249e-10\n",
      "Iteration 15125: loss = 6.864597e-08,1.3864122e-10\n",
      "Iteration 15130: loss = 6.842645e-08,1.3865921e-10\n",
      "Iteration 15135: loss = 6.820737e-08,1.3867736e-10\n",
      "Iteration 15140: loss = 6.7988516e-08,1.386957e-10\n",
      "Iteration 15145: loss = 6.7774245e-08,1.3870982e-10\n",
      "Iteration 15150: loss = 6.759004e-08,1.3872378e-10\n",
      "Iteration 15155: loss = 6.737694e-08,1.3873766e-10\n",
      "Iteration 15160: loss = 6.716219e-08,1.3875363e-10\n",
      "Iteration 15165: loss = 6.694861e-08,1.3876976e-10\n",
      "Iteration 15170: loss = 6.6762475e-08,1.3878683e-10\n",
      "Iteration 15175: loss = 6.6548516e-08,1.3880318e-10\n",
      "Iteration 15180: loss = 6.6363775e-08,1.3881961e-10\n",
      "Iteration 15185: loss = 6.6121494e-08,1.3883632e-10\n",
      "Iteration 15190: loss = 6.5936995e-08,1.3885332e-10\n",
      "Iteration 15195: loss = 6.57238e-08,1.388716e-10\n",
      "Iteration 15200: loss = 6.554171e-08,1.388866e-10\n",
      "Iteration 15205: loss = 6.533261e-08,1.3890158e-10\n",
      "Iteration 15210: loss = 6.512317e-08,1.3891627e-10\n",
      "Iteration 15215: loss = 6.4944174e-08,1.3893063e-10\n",
      "Iteration 15220: loss = 6.473595e-08,1.3894505e-10\n",
      "Iteration 15225: loss = 6.4556446e-08,1.3896054e-10\n",
      "Iteration 15230: loss = 6.434629e-08,1.389776e-10\n",
      "Iteration 15235: loss = 6.4166535e-08,1.389939e-10\n",
      "Iteration 15240: loss = 6.392823e-08,1.390114e-10\n",
      "Iteration 15245: loss = 6.3748296e-08,1.3902886e-10\n",
      "Iteration 15250: loss = 6.3539694e-08,1.3904602e-10\n",
      "Iteration 15255: loss = 6.3363096e-08,1.3906037e-10\n",
      "Iteration 15260: loss = 6.318746e-08,1.3907439e-10\n",
      "Iteration 15265: loss = 6.298279e-08,1.3908968e-10\n",
      "Iteration 15270: loss = 6.2806535e-08,1.3910506e-10\n",
      "Iteration 15275: loss = 6.2602275e-08,1.3912052e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15280: loss = 6.2398385e-08,1.3913619e-10\n",
      "Iteration 15285: loss = 6.2221865e-08,1.391519e-10\n",
      "Iteration 15290: loss = 6.201921e-08,1.3916719e-10\n",
      "Iteration 15295: loss = 6.1843686e-08,1.3918339e-10\n",
      "Iteration 15300: loss = 6.1667436e-08,1.392012e-10\n",
      "Iteration 15305: loss = 6.146364e-08,1.3921879e-10\n",
      "Iteration 15310: loss = 6.1260316e-08,1.3923647e-10\n",
      "Iteration 15315: loss = 6.10608e-08,1.3925006e-10\n",
      "Iteration 15320: loss = 6.0889725e-08,1.3926385e-10\n",
      "Iteration 15325: loss = 6.07187e-08,1.3927774e-10\n",
      "Iteration 15330: loss = 6.05204e-08,1.3929141e-10\n",
      "Iteration 15335: loss = 6.034821e-08,1.393071e-10\n",
      "Iteration 15340: loss = 6.0176355e-08,1.3932261e-10\n",
      "Iteration 15345: loss = 5.997834e-08,1.3933871e-10\n",
      "Iteration 15350: loss = 5.980632e-08,1.3935528e-10\n",
      "Iteration 15355: loss = 5.963522e-08,1.3937129e-10\n",
      "Iteration 15360: loss = 5.9436985e-08,1.3938735e-10\n",
      "Iteration 15365: loss = 5.9266526e-08,1.3940363e-10\n",
      "Iteration 15370: loss = 5.906631e-08,1.3942249e-10\n",
      "Iteration 15375: loss = 5.8869407e-08,1.3943893e-10\n",
      "Iteration 15380: loss = 5.8702863e-08,1.3945285e-10\n",
      "Iteration 15385: loss = 5.8535164e-08,1.3946735e-10\n",
      "Iteration 15390: loss = 5.834134e-08,1.3948212e-10\n",
      "Iteration 15395: loss = 5.8174567e-08,1.3949623e-10\n",
      "Iteration 15400: loss = 5.800916e-08,1.3951044e-10\n",
      "Iteration 15405: loss = 5.784067e-08,1.3952728e-10\n",
      "Iteration 15410: loss = 5.764635e-08,1.39544e-10\n",
      "Iteration 15415: loss = 5.7478577e-08,1.3956081e-10\n",
      "Iteration 15420: loss = 5.731233e-08,1.3957739e-10\n",
      "Iteration 15425: loss = 5.7117802e-08,1.395946e-10\n",
      "Iteration 15430: loss = 5.695173e-08,1.3961177e-10\n",
      "Iteration 15435: loss = 5.6789776e-08,1.3962437e-10\n",
      "Iteration 15440: loss = 5.6598868e-08,1.396387e-10\n",
      "Iteration 15445: loss = 5.6435606e-08,1.3965368e-10\n",
      "Iteration 15450: loss = 5.627246e-08,1.3966862e-10\n",
      "Iteration 15455: loss = 5.6109617e-08,1.3968349e-10\n",
      "Iteration 15460: loss = 5.5947407e-08,1.3969825e-10\n",
      "Iteration 15465: loss = 5.5757265e-08,1.3971396e-10\n",
      "Iteration 15470: loss = 5.5595176e-08,1.3972941e-10\n",
      "Iteration 15475: loss = 5.543265e-08,1.3974558e-10\n",
      "Iteration 15480: loss = 5.524275e-08,1.3976305e-10\n",
      "Iteration 15485: loss = 5.508008e-08,1.3978027e-10\n",
      "Iteration 15490: loss = 5.4917436e-08,1.3979783e-10\n",
      "Iteration 15495: loss = 5.4757468e-08,1.3981179e-10\n",
      "Iteration 15500: loss = 5.4598654e-08,1.3982583e-10\n",
      "Iteration 15505: loss = 5.441395e-08,1.3983911e-10\n",
      "Iteration 15510: loss = 5.4255988e-08,1.3985268e-10\n",
      "Iteration 15515: loss = 5.4096308e-08,1.3986835e-10\n",
      "Iteration 15520: loss = 5.393746e-08,1.3988433e-10\n",
      "Iteration 15525: loss = 5.3778525e-08,1.3989995e-10\n",
      "Iteration 15530: loss = 5.359392e-08,1.3991534e-10\n",
      "Iteration 15535: loss = 5.3435524e-08,1.3993137e-10\n",
      "Iteration 15540: loss = 5.327666e-08,1.3994768e-10\n",
      "Iteration 15545: loss = 5.3118686e-08,1.3996365e-10\n",
      "Iteration 15550: loss = 5.2934713e-08,1.3998025e-10\n",
      "Iteration 15555: loss = 5.2776258e-08,1.3999862e-10\n",
      "Iteration 15560: loss = 5.2618706e-08,1.4001461e-10\n",
      "Iteration 15565: loss = 5.2464696e-08,1.4002828e-10\n",
      "Iteration 15570: loss = 5.2335405e-08,1.400422e-10\n",
      "Iteration 15575: loss = 5.218112e-08,1.4005673e-10\n",
      "Iteration 15580: loss = 5.2000797e-08,1.4007104e-10\n",
      "Iteration 15585: loss = 5.1847792e-08,1.4008529e-10\n",
      "Iteration 15590: loss = 5.169304e-08,1.4010039e-10\n",
      "Iteration 15595: loss = 5.153837e-08,1.4011681e-10\n",
      "Iteration 15600: loss = 5.1383143e-08,1.4013347e-10\n",
      "Iteration 15605: loss = 5.1229062e-08,1.4015007e-10\n",
      "Iteration 15610: loss = 5.104866e-08,1.4016699e-10\n",
      "Iteration 15615: loss = 5.089503e-08,1.4018389e-10\n",
      "Iteration 15620: loss = 5.0743207e-08,1.4019852e-10\n",
      "Iteration 15625: loss = 5.0617658e-08,1.4021131e-10\n",
      "Iteration 15630: loss = 5.0468554e-08,1.4022344e-10\n",
      "Iteration 15635: loss = 5.031998e-08,1.402354e-10\n",
      "Iteration 15640: loss = 5.0171593e-08,1.4024729e-10\n",
      "Iteration 15645: loss = 5.0023445e-08,1.4025933e-10\n",
      "Iteration 15650: loss = 4.989954e-08,1.4027171e-10\n",
      "Iteration 15655: loss = 4.9750852e-08,1.4028506e-10\n",
      "Iteration 15660: loss = 4.9599944e-08,1.4030162e-10\n",
      "Iteration 15665: loss = 4.9448687e-08,1.4031887e-10\n",
      "Iteration 15670: loss = 4.9297316e-08,1.4033656e-10\n",
      "Iteration 15675: loss = 4.9121425e-08,1.4035431e-10\n",
      "Iteration 15680: loss = 4.89698e-08,1.4037213e-10\n",
      "Iteration 15685: loss = 4.8819956e-08,1.4038928e-10\n",
      "Iteration 15690: loss = 4.8673883e-08,1.4040183e-10\n",
      "Iteration 15695: loss = 4.8552362e-08,1.404148e-10\n",
      "Iteration 15700: loss = 4.8406562e-08,1.4042877e-10\n",
      "Iteration 15705: loss = 4.8260677e-08,1.4044196e-10\n",
      "Iteration 15710: loss = 4.8115282e-08,1.4045502e-10\n",
      "Iteration 15715: loss = 4.7994593e-08,1.4046825e-10\n",
      "Iteration 15720: loss = 4.784952e-08,1.4048146e-10\n",
      "Iteration 15725: loss = 4.7704873e-08,1.4049475e-10\n",
      "Iteration 15730: loss = 4.756086e-08,1.4050763e-10\n",
      "Iteration 15735: loss = 4.741716e-08,1.4052e-10\n",
      "Iteration 15740: loss = 4.7298688e-08,1.4053325e-10\n",
      "Iteration 15745: loss = 4.715127e-08,1.4055122e-10\n",
      "Iteration 15750: loss = 4.7003805e-08,1.4056965e-10\n",
      "Iteration 15755: loss = 4.6856588e-08,1.4058853e-10\n",
      "Iteration 15760: loss = 4.671146e-08,1.4060515e-10\n",
      "Iteration 15765: loss = 4.6568303e-08,1.4061899e-10\n",
      "Iteration 15770: loss = 4.64503e-08,1.4063281e-10\n",
      "Iteration 15775: loss = 4.630758e-08,1.4064681e-10\n",
      "Iteration 15780: loss = 4.6166544e-08,1.4066019e-10\n",
      "Iteration 15785: loss = 4.6024656e-08,1.4067378e-10\n",
      "Iteration 15790: loss = 4.5907573e-08,1.4068774e-10\n",
      "Iteration 15795: loss = 4.5765898e-08,1.4070203e-10\n",
      "Iteration 15800: loss = 4.5624954e-08,1.4071658e-10\n",
      "Iteration 15805: loss = 4.548399e-08,1.4073041e-10\n",
      "Iteration 15810: loss = 4.5344162e-08,1.4074422e-10\n",
      "Iteration 15815: loss = 4.5227765e-08,1.407579e-10\n",
      "Iteration 15820: loss = 4.5087674e-08,1.4077178e-10\n",
      "Iteration 15825: loss = 4.4948454e-08,1.4078584e-10\n",
      "Iteration 15830: loss = 4.4806217e-08,1.4080342e-10\n",
      "Iteration 15835: loss = 4.4700897e-08,1.4081955e-10\n",
      "Iteration 15840: loss = 4.4561727e-08,1.4083483e-10\n",
      "Iteration 15845: loss = 4.4422716e-08,1.4084955e-10\n",
      "Iteration 15850: loss = 4.4284178e-08,1.408645e-10\n",
      "Iteration 15855: loss = 4.4169624e-08,1.4087877e-10\n",
      "Iteration 15860: loss = 4.403147e-08,1.4089284e-10\n",
      "Iteration 15865: loss = 4.3894186e-08,1.409069e-10\n",
      "Iteration 15870: loss = 4.3756955e-08,1.4092116e-10\n",
      "Iteration 15875: loss = 4.364331e-08,1.4093576e-10\n",
      "Iteration 15880: loss = 4.3506436e-08,1.4095046e-10\n",
      "Iteration 15885: loss = 4.3368974e-08,1.4096524e-10\n",
      "Iteration 15890: loss = 4.3232603e-08,1.4098003e-10\n",
      "Iteration 15895: loss = 4.3119694e-08,1.4099466e-10\n",
      "Iteration 15900: loss = 4.2983903e-08,1.4100945e-10\n",
      "Iteration 15905: loss = 4.2848313e-08,1.4102407e-10\n",
      "Iteration 15910: loss = 4.2760472e-08,1.4103681e-10\n",
      "Iteration 15915: loss = 4.262856e-08,1.4104665e-10\n",
      "Iteration 15920: loss = 4.249392e-08,1.4106083e-10\n",
      "Iteration 15925: loss = 4.238205e-08,1.4107593e-10\n",
      "Iteration 15930: loss = 4.2247365e-08,1.4109167e-10\n",
      "Iteration 15935: loss = 4.2112244e-08,1.4110765e-10\n",
      "Iteration 15940: loss = 4.1977568e-08,1.4112339e-10\n",
      "Iteration 15945: loss = 4.186619e-08,1.411391e-10\n",
      "Iteration 15950: loss = 4.1732225e-08,1.4115456e-10\n",
      "Iteration 15955: loss = 4.159847e-08,1.4117034e-10\n",
      "Iteration 15960: loss = 4.148821e-08,1.4118517e-10\n",
      "Iteration 15965: loss = 4.137833e-08,1.4120005e-10\n",
      "Iteration 15970: loss = 4.1246143e-08,1.4121529e-10\n",
      "Iteration 15975: loss = 4.1113115e-08,1.4123133e-10\n",
      "Iteration 15980: loss = 4.100609e-08,1.4124271e-10\n",
      "Iteration 15985: loss = 4.0877552e-08,1.4125352e-10\n",
      "Iteration 15990: loss = 4.0771074e-08,1.4126476e-10\n",
      "Iteration 15995: loss = 4.0665494e-08,1.4127591e-10\n",
      "Iteration 16000: loss = 4.0559282e-08,1.4128725e-10\n",
      "Iteration 16005: loss = 4.043075e-08,1.4129864e-10\n",
      "Iteration 16010: loss = 4.03034e-08,1.413094e-10\n",
      "Iteration 16015: loss = 4.0220858e-08,1.4132019e-10\n",
      "Iteration 16020: loss = 4.009269e-08,1.4133275e-10\n",
      "Iteration 16025: loss = 3.9984474e-08,1.4134846e-10\n",
      "Iteration 16030: loss = 3.985474e-08,1.4136431e-10\n",
      "Iteration 16035: loss = 3.972419e-08,1.413809e-10\n",
      "Iteration 16040: loss = 3.9594198e-08,1.4139787e-10\n",
      "Iteration 16045: loss = 3.9486256e-08,1.4141463e-10\n",
      "Iteration 16050: loss = 3.9357307e-08,1.4143084e-10\n",
      "Iteration 16055: loss = 3.922808e-08,1.4144692e-10\n",
      "Iteration 16060: loss = 3.914345e-08,1.41463e-10\n",
      "Iteration 16065: loss = 3.9015795e-08,1.4147856e-10\n",
      "Iteration 16070: loss = 3.8890608e-08,1.4149033e-10\n",
      "Iteration 16075: loss = 3.8787213e-08,1.41502e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16080: loss = 3.86627e-08,1.415134e-10\n",
      "Iteration 16085: loss = 3.8581845e-08,1.4152568e-10\n",
      "Iteration 16090: loss = 3.8457205e-08,1.415381e-10\n",
      "Iteration 16095: loss = 3.835407e-08,1.4155044e-10\n",
      "Iteration 16100: loss = 3.8230365e-08,1.4156204e-10\n",
      "Iteration 16105: loss = 3.8150684e-08,1.4157378e-10\n",
      "Iteration 16110: loss = 3.8027167e-08,1.4158585e-10\n",
      "Iteration 16115: loss = 3.7925844e-08,1.4159751e-10\n",
      "Iteration 16120: loss = 3.782277e-08,1.4161136e-10\n",
      "Iteration 16125: loss = 3.7696875e-08,1.4162817e-10\n",
      "Iteration 16130: loss = 3.759295e-08,1.416449e-10\n",
      "Iteration 16135: loss = 3.7467828e-08,1.4166127e-10\n",
      "Iteration 16140: loss = 3.734227e-08,1.4167763e-10\n",
      "Iteration 16145: loss = 3.723863e-08,1.4169514e-10\n",
      "Iteration 16150: loss = 3.71136e-08,1.4171257e-10\n",
      "Iteration 16155: loss = 3.7012786e-08,1.4172562e-10\n",
      "Iteration 16160: loss = 3.6912734e-08,1.4173845e-10\n",
      "Iteration 16165: loss = 3.679111e-08,1.4175111e-10\n",
      "Iteration 16170: loss = 3.66914e-08,1.4176368e-10\n",
      "Iteration 16175: loss = 3.659226e-08,1.4177645e-10\n",
      "Iteration 16180: loss = 3.649286e-08,1.4178929e-10\n",
      "Iteration 16185: loss = 3.6372363e-08,1.4180183e-10\n",
      "Iteration 16190: loss = 3.6294995e-08,1.4181394e-10\n",
      "Iteration 16195: loss = 3.6175063e-08,1.4182605e-10\n",
      "Iteration 16200: loss = 3.6076305e-08,1.418389e-10\n",
      "Iteration 16205: loss = 3.5956308e-08,1.4185209e-10\n",
      "Iteration 16210: loss = 3.5879122e-08,1.4186505e-10\n",
      "Iteration 16215: loss = 3.575948e-08,1.4187812e-10\n",
      "Iteration 16220: loss = 3.563935e-08,1.4189369e-10\n",
      "Iteration 16225: loss = 3.5538577e-08,1.4191141e-10\n",
      "Iteration 16230: loss = 3.543817e-08,1.41929e-10\n",
      "Iteration 16235: loss = 3.5339742e-08,1.4194396e-10\n",
      "Iteration 16240: loss = 3.5220765e-08,1.419577e-10\n",
      "Iteration 16245: loss = 3.51229e-08,1.4197175e-10\n",
      "Iteration 16250: loss = 3.5026122e-08,1.4198527e-10\n",
      "Iteration 16255: loss = 3.4929027e-08,1.4199882e-10\n",
      "Iteration 16260: loss = 3.481144e-08,1.4201247e-10\n",
      "Iteration 16265: loss = 3.4714628e-08,1.4202628e-10\n",
      "Iteration 16270: loss = 3.4618633e-08,1.4203969e-10\n",
      "Iteration 16275: loss = 3.45017e-08,1.4205268e-10\n",
      "Iteration 16280: loss = 3.4426463e-08,1.4206566e-10\n",
      "Iteration 16285: loss = 3.4310496e-08,1.4207882e-10\n",
      "Iteration 16290: loss = 3.4214718e-08,1.4209224e-10\n",
      "Iteration 16295: loss = 3.4098562e-08,1.4210626e-10\n",
      "Iteration 16300: loss = 3.4023348e-08,1.4212055e-10\n",
      "Iteration 16305: loss = 3.390786e-08,1.4213418e-10\n",
      "Iteration 16310: loss = 3.381286e-08,1.4214788e-10\n",
      "Iteration 16315: loss = 3.3718464e-08,1.421616e-10\n",
      "Iteration 16320: loss = 3.3622502e-08,1.4217746e-10\n",
      "Iteration 16325: loss = 3.3506833e-08,1.4219168e-10\n",
      "Iteration 16330: loss = 3.343339e-08,1.4220526e-10\n",
      "Iteration 16335: loss = 3.3319186e-08,1.4221878e-10\n",
      "Iteration 16340: loss = 3.3225515e-08,1.4223253e-10\n",
      "Iteration 16345: loss = 3.3131474e-08,1.4224606e-10\n",
      "Iteration 16350: loss = 3.3037615e-08,1.4226084e-10\n",
      "Iteration 16355: loss = 3.292336e-08,1.4227583e-10\n",
      "Iteration 16360: loss = 3.2850327e-08,1.4229037e-10\n",
      "Iteration 16365: loss = 3.273686e-08,1.4230471e-10\n",
      "Iteration 16370: loss = 3.2643445e-08,1.423192e-10\n",
      "Iteration 16375: loss = 3.2571002e-08,1.423332e-10\n",
      "Iteration 16380: loss = 3.245827e-08,1.4234759e-10\n",
      "Iteration 16385: loss = 3.2366078e-08,1.4236137e-10\n",
      "Iteration 16390: loss = 3.2273743e-08,1.4237504e-10\n",
      "Iteration 16395: loss = 3.2181962e-08,1.4238896e-10\n",
      "Iteration 16400: loss = 3.2071526e-08,1.424009e-10\n",
      "Iteration 16405: loss = 3.200186e-08,1.4241172e-10\n",
      "Iteration 16410: loss = 3.1892373e-08,1.4242217e-10\n",
      "Iteration 16415: loss = 3.1823173e-08,1.4243223e-10\n",
      "Iteration 16420: loss = 3.1732835e-08,1.4244492e-10\n",
      "Iteration 16425: loss = 3.1641537e-08,1.4245963e-10\n",
      "Iteration 16430: loss = 3.1550385e-08,1.424743e-10\n",
      "Iteration 16435: loss = 3.1439352e-08,1.4248913e-10\n",
      "Iteration 16440: loss = 3.1368316e-08,1.4250402e-10\n",
      "Iteration 16445: loss = 3.125761e-08,1.4251922e-10\n",
      "Iteration 16450: loss = 3.1166863e-08,1.4253461e-10\n",
      "Iteration 16455: loss = 3.1077104e-08,1.4254951e-10\n",
      "Iteration 16460: loss = 3.0986715e-08,1.4256471e-10\n",
      "Iteration 16465: loss = 3.0876713e-08,1.4258013e-10\n",
      "Iteration 16470: loss = 3.080626e-08,1.4259574e-10\n",
      "Iteration 16475: loss = 3.0716492e-08,1.4261087e-10\n",
      "Iteration 16480: loss = 3.062717e-08,1.4262551e-10\n",
      "Iteration 16485: loss = 3.0538068e-08,1.4264016e-10\n",
      "Iteration 16490: loss = 3.043044e-08,1.426532e-10\n",
      "Iteration 16495: loss = 3.0363335e-08,1.426636e-10\n",
      "Iteration 16500: loss = 3.0276958e-08,1.426746e-10\n",
      "Iteration 16505: loss = 3.0190183e-08,1.4268585e-10\n",
      "Iteration 16510: loss = 3.0104275e-08,1.4269687e-10\n",
      "Iteration 16515: loss = 3.0037352e-08,1.4270748e-10\n",
      "Iteration 16520: loss = 2.9932266e-08,1.4271839e-10\n",
      "Iteration 16525: loss = 2.9865742e-08,1.4272919e-10\n",
      "Iteration 16530: loss = 2.9779926e-08,1.4273986e-10\n",
      "Iteration 16535: loss = 2.96922e-08,1.4275581e-10\n",
      "Iteration 16540: loss = 2.9603909e-08,1.4277214e-10\n",
      "Iteration 16545: loss = 2.9497235e-08,1.4278771e-10\n",
      "Iteration 16550: loss = 2.942906e-08,1.4280324e-10\n",
      "Iteration 16555: loss = 2.9322933e-08,1.4281876e-10\n",
      "Iteration 16560: loss = 2.925483e-08,1.4283442e-10\n",
      "Iteration 16565: loss = 2.9148623e-08,1.4285004e-10\n",
      "Iteration 16570: loss = 2.9061807e-08,1.4286639e-10\n",
      "Iteration 16575: loss = 2.8993812e-08,1.4288262e-10\n",
      "Iteration 16580: loss = 2.8888616e-08,1.4289842e-10\n",
      "Iteration 16585: loss = 2.8822882e-08,1.4291156e-10\n",
      "Iteration 16590: loss = 2.8719986e-08,1.429232e-10\n",
      "Iteration 16595: loss = 2.8654755e-08,1.4293498e-10\n",
      "Iteration 16600: loss = 2.8571387e-08,1.4294588e-10\n",
      "Iteration 16605: loss = 2.8488484e-08,1.4295666e-10\n",
      "Iteration 16610: loss = 2.8405546e-08,1.4296779e-10\n",
      "Iteration 16615: loss = 2.8340986e-08,1.4297909e-10\n",
      "Iteration 16620: loss = 2.8258219e-08,1.4299038e-10\n",
      "Iteration 16625: loss = 2.8175627e-08,1.4300139e-10\n",
      "Iteration 16630: loss = 2.8092712e-08,1.4301378e-10\n",
      "Iteration 16635: loss = 2.8028182e-08,1.4302617e-10\n",
      "Iteration 16640: loss = 2.7945907e-08,1.4303775e-10\n",
      "Iteration 16645: loss = 2.7862328e-08,1.4305225e-10\n",
      "Iteration 16650: loss = 2.777801e-08,1.4306853e-10\n",
      "Iteration 16655: loss = 2.7693355e-08,1.4308497e-10\n",
      "Iteration 16660: loss = 2.7609312e-08,1.4310156e-10\n",
      "Iteration 16665: loss = 2.7525433e-08,1.4311817e-10\n",
      "Iteration 16670: loss = 2.744163e-08,1.4313463e-10\n",
      "Iteration 16675: loss = 2.733921e-08,1.4315078e-10\n",
      "Iteration 16680: loss = 2.7276146e-08,1.4316333e-10\n",
      "Iteration 16685: loss = 2.7213787e-08,1.4317596e-10\n",
      "Iteration 16690: loss = 2.7132359e-08,1.4318854e-10\n",
      "Iteration 16695: loss = 2.7051072e-08,1.432015e-10\n",
      "Iteration 16700: loss = 2.6970108e-08,1.4321383e-10\n",
      "Iteration 16705: loss = 2.6907886e-08,1.4322604e-10\n",
      "Iteration 16710: loss = 2.6808918e-08,1.4323824e-10\n",
      "Iteration 16715: loss = 2.6747022e-08,1.432501e-10\n",
      "Iteration 16720: loss = 2.6666834e-08,1.4326232e-10\n",
      "Iteration 16725: loss = 2.6586747e-08,1.4327456e-10\n",
      "Iteration 16730: loss = 2.6506797e-08,1.4328683e-10\n",
      "Iteration 16735: loss = 2.6445365e-08,1.432987e-10\n",
      "Iteration 16740: loss = 2.6384285e-08,1.4331118e-10\n",
      "Iteration 16745: loss = 2.628647e-08,1.4332385e-10\n",
      "Iteration 16750: loss = 2.6225093e-08,1.4333625e-10\n",
      "Iteration 16755: loss = 2.6144315e-08,1.4335182e-10\n",
      "Iteration 16760: loss = 2.6062887e-08,1.4336918e-10\n",
      "Iteration 16765: loss = 2.598164e-08,1.4338686e-10\n",
      "Iteration 16770: loss = 2.5900766e-08,1.4340353e-10\n",
      "Iteration 16775: loss = 2.5821175e-08,1.4341858e-10\n",
      "Iteration 16780: loss = 2.5760244e-08,1.4343142e-10\n",
      "Iteration 16785: loss = 2.5681663e-08,1.4344514e-10\n",
      "Iteration 16790: loss = 2.5602702e-08,1.43459e-10\n",
      "Iteration 16795: loss = 2.5524676e-08,1.4347212e-10\n",
      "Iteration 16800: loss = 2.5464296e-08,1.4348513e-10\n",
      "Iteration 16805: loss = 2.5368239e-08,1.4349794e-10\n",
      "Iteration 16810: loss = 2.5308589e-08,1.4351123e-10\n",
      "Iteration 16815: loss = 2.5248506e-08,1.4352428e-10\n",
      "Iteration 16820: loss = 2.5171145e-08,1.4353733e-10\n",
      "Iteration 16825: loss = 2.5093676e-08,1.4355006e-10\n",
      "Iteration 16830: loss = 2.5016517e-08,1.4356245e-10\n",
      "Iteration 16835: loss = 2.4957664e-08,1.4357496e-10\n",
      "Iteration 16840: loss = 2.489849e-08,1.4358745e-10\n",
      "Iteration 16845: loss = 2.480396e-08,1.4360096e-10\n",
      "Iteration 16850: loss = 2.47446e-08,1.4361445e-10\n",
      "Iteration 16855: loss = 2.466809e-08,1.4362785e-10\n",
      "Iteration 16860: loss = 2.4608683e-08,1.4364207e-10\n",
      "Iteration 16865: loss = 2.4512708e-08,1.4365965e-10\n",
      "Iteration 16870: loss = 2.445362e-08,1.4367411e-10\n",
      "Iteration 16875: loss = 2.4394978e-08,1.4368798e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16880: loss = 2.4301395e-08,1.4370191e-10\n",
      "Iteration 16885: loss = 2.424261e-08,1.4371573e-10\n",
      "Iteration 16890: loss = 2.4166994e-08,1.437294e-10\n",
      "Iteration 16895: loss = 2.4108994e-08,1.437427e-10\n",
      "Iteration 16900: loss = 2.4016229e-08,1.4375658e-10\n",
      "Iteration 16905: loss = 2.3957824e-08,1.4377041e-10\n",
      "Iteration 16910: loss = 2.3899993e-08,1.437844e-10\n",
      "Iteration 16915: loss = 2.3824816e-08,1.4379854e-10\n",
      "Iteration 16920: loss = 2.3749978e-08,1.4381221e-10\n",
      "Iteration 16925: loss = 2.3675163e-08,1.4382553e-10\n",
      "Iteration 16930: loss = 2.3618023e-08,1.4383883e-10\n",
      "Iteration 16935: loss = 2.3560917e-08,1.4385232e-10\n",
      "Iteration 16940: loss = 2.3469534e-08,1.438661e-10\n",
      "Iteration 16945: loss = 2.3412284e-08,1.4387969e-10\n",
      "Iteration 16950: loss = 2.3354994e-08,1.4389415e-10\n",
      "Iteration 16955: loss = 2.328103e-08,1.4390823e-10\n",
      "Iteration 16960: loss = 2.320848e-08,1.439188e-10\n",
      "Iteration 16965: loss = 2.3153467e-08,1.4392859e-10\n",
      "Iteration 16970: loss = 2.3079236e-08,1.4394412e-10\n",
      "Iteration 16975: loss = 2.3022531e-08,1.439587e-10\n",
      "Iteration 16980: loss = 2.2949349e-08,1.4397289e-10\n",
      "Iteration 16985: loss = 2.2875872e-08,1.439876e-10\n",
      "Iteration 16990: loss = 2.281966e-08,1.4400187e-10\n",
      "Iteration 16995: loss = 2.2746695e-08,1.4401588e-10\n",
      "Iteration 17000: loss = 2.2690472e-08,1.4403062e-10\n",
      "Iteration 17005: loss = 2.2600467e-08,1.4404558e-10\n",
      "Iteration 17010: loss = 2.254444e-08,1.4406056e-10\n",
      "Iteration 17015: loss = 2.2488464e-08,1.4407564e-10\n",
      "Iteration 17020: loss = 2.2416126e-08,1.4408977e-10\n",
      "Iteration 17025: loss = 2.2343785e-08,1.441043e-10\n",
      "Iteration 17030: loss = 2.2288349e-08,1.441187e-10\n",
      "Iteration 17035: loss = 2.2216325e-08,1.4413343e-10\n",
      "Iteration 17040: loss = 2.2160997e-08,1.4414801e-10\n",
      "Iteration 17045: loss = 2.2089296e-08,1.4416279e-10\n",
      "Iteration 17050: loss = 2.2019064e-08,1.4417417e-10\n",
      "Iteration 17055: loss = 2.1965645e-08,1.4418469e-10\n",
      "Iteration 17060: loss = 2.1912712e-08,1.4419527e-10\n",
      "Iteration 17065: loss = 2.1842899e-08,1.442058e-10\n",
      "Iteration 17070: loss = 2.1790237e-08,1.4421622e-10\n",
      "Iteration 17075: loss = 2.1737266e-08,1.4422676e-10\n",
      "Iteration 17080: loss = 2.1684366e-08,1.4423712e-10\n",
      "Iteration 17085: loss = 2.1597446e-08,1.4425135e-10\n",
      "Iteration 17090: loss = 2.1543054e-08,1.442665e-10\n",
      "Iteration 17095: loss = 2.1489072e-08,1.4428161e-10\n",
      "Iteration 17100: loss = 2.1418408e-08,1.4429646e-10\n",
      "Iteration 17105: loss = 2.1364277e-08,1.4431172e-10\n",
      "Iteration 17110: loss = 2.1294214e-08,1.4432662e-10\n",
      "Iteration 17115: loss = 2.1223718e-08,1.4434264e-10\n",
      "Iteration 17120: loss = 2.1169976e-08,1.4435862e-10\n",
      "Iteration 17125: loss = 2.1099796e-08,1.4437412e-10\n",
      "Iteration 17130: loss = 2.1029784e-08,1.4438956e-10\n",
      "Iteration 17135: loss = 2.0976595e-08,1.4440507e-10\n",
      "Iteration 17140: loss = 2.090695e-08,1.4442023e-10\n",
      "Iteration 17145: loss = 2.0854143e-08,1.4443541e-10\n",
      "Iteration 17150: loss = 2.0802242e-08,1.4444686e-10\n",
      "Iteration 17155: loss = 2.075091e-08,1.4445803e-10\n",
      "Iteration 17160: loss = 2.0683506e-08,1.4446928e-10\n",
      "Iteration 17165: loss = 2.0616113e-08,1.4448062e-10\n",
      "Iteration 17170: loss = 2.0564705e-08,1.4449213e-10\n",
      "Iteration 17175: loss = 2.0513717e-08,1.4450356e-10\n",
      "Iteration 17180: loss = 2.0446906e-08,1.4451408e-10\n",
      "Iteration 17185: loss = 2.0396405e-08,1.4452514e-10\n",
      "Iteration 17190: loss = 2.034542e-08,1.4453619e-10\n",
      "Iteration 17195: loss = 2.0294936e-08,1.4454718e-10\n",
      "Iteration 17200: loss = 2.0226848e-08,1.4456286e-10\n",
      "Iteration 17205: loss = 2.0158529e-08,1.445788e-10\n",
      "Iteration 17210: loss = 2.0106503e-08,1.4459481e-10\n",
      "Iteration 17215: loss = 2.0038764e-08,1.446109e-10\n",
      "Iteration 17220: loss = 1.9986999e-08,1.4462706e-10\n",
      "Iteration 17225: loss = 1.9934788e-08,1.4464355e-10\n",
      "Iteration 17230: loss = 1.9867189e-08,1.4465998e-10\n",
      "Iteration 17235: loss = 1.979978e-08,1.4467669e-10\n",
      "Iteration 17240: loss = 1.9748335e-08,1.4469306e-10\n",
      "Iteration 17245: loss = 1.9697415e-08,1.4470758e-10\n",
      "Iteration 17250: loss = 1.9631731e-08,1.4471964e-10\n",
      "Iteration 17255: loss = 1.958231e-08,1.4473081e-10\n",
      "Iteration 17260: loss = 1.9532916e-08,1.447425e-10\n",
      "Iteration 17265: loss = 1.9483432e-08,1.4475415e-10\n",
      "Iteration 17270: loss = 1.9418607e-08,1.4476655e-10\n",
      "Iteration 17275: loss = 1.935352e-08,1.4477868e-10\n",
      "Iteration 17280: loss = 1.9304432e-08,1.4479047e-10\n",
      "Iteration 17285: loss = 1.9255362e-08,1.4480239e-10\n",
      "Iteration 17290: loss = 1.9206391e-08,1.4481402e-10\n",
      "Iteration 17295: loss = 1.9141977e-08,1.448259e-10\n",
      "Iteration 17300: loss = 1.909318e-08,1.44838e-10\n",
      "Iteration 17305: loss = 1.9044512e-08,1.4484954e-10\n",
      "Iteration 17310: loss = 1.8995655e-08,1.4486218e-10\n",
      "Iteration 17315: loss = 1.8945473e-08,1.4487886e-10\n",
      "Iteration 17320: loss = 1.8879874e-08,1.4489558e-10\n",
      "Iteration 17325: loss = 1.8814635e-08,1.44912e-10\n",
      "Iteration 17330: loss = 1.8764855e-08,1.4492842e-10\n",
      "Iteration 17335: loss = 1.8699675e-08,1.4494538e-10\n",
      "Iteration 17340: loss = 1.8650566e-08,1.4496065e-10\n",
      "Iteration 17345: loss = 1.8601826e-08,1.449751e-10\n",
      "Iteration 17350: loss = 1.85537e-08,1.4498777e-10\n",
      "Iteration 17355: loss = 1.8490818e-08,1.4500003e-10\n",
      "Iteration 17360: loss = 1.8442856e-08,1.4501245e-10\n",
      "Iteration 17365: loss = 1.8395149e-08,1.4502471e-10\n",
      "Iteration 17370: loss = 1.8347736e-08,1.4503693e-10\n",
      "Iteration 17375: loss = 1.8284897e-08,1.4504949e-10\n",
      "Iteration 17380: loss = 1.8237563e-08,1.4506195e-10\n",
      "Iteration 17385: loss = 1.8175117e-08,1.4507406e-10\n",
      "Iteration 17390: loss = 1.8127926e-08,1.450858e-10\n",
      "Iteration 17395: loss = 1.808083e-08,1.4509884e-10\n",
      "Iteration 17400: loss = 1.8033417e-08,1.4511178e-10\n",
      "Iteration 17405: loss = 1.798644e-08,1.4512454e-10\n",
      "Iteration 17410: loss = 1.7924284e-08,1.4513764e-10\n",
      "Iteration 17415: loss = 1.7877184e-08,1.4515036e-10\n",
      "Iteration 17420: loss = 1.7830802e-08,1.4516251e-10\n",
      "Iteration 17425: loss = 1.7784089e-08,1.4517466e-10\n",
      "Iteration 17430: loss = 1.7736255e-08,1.4519104e-10\n",
      "Iteration 17435: loss = 1.7673448e-08,1.4520818e-10\n",
      "Iteration 17440: loss = 1.7626062e-08,1.4522374e-10\n",
      "Iteration 17445: loss = 1.7564583e-08,1.4523759e-10\n",
      "Iteration 17450: loss = 1.7518122e-08,1.4525117e-10\n",
      "Iteration 17455: loss = 1.7471548e-08,1.4526455e-10\n",
      "Iteration 17460: loss = 1.7410683e-08,1.4527765e-10\n",
      "Iteration 17465: loss = 1.7364654e-08,1.4529095e-10\n",
      "Iteration 17470: loss = 1.731841e-08,1.4530405e-10\n",
      "Iteration 17475: loss = 1.7272416e-08,1.4531798e-10\n",
      "Iteration 17480: loss = 1.7226496e-08,1.4533152e-10\n",
      "Iteration 17485: loss = 1.7180676e-08,1.4534493e-10\n",
      "Iteration 17490: loss = 1.7120014e-08,1.4535846e-10\n",
      "Iteration 17495: loss = 1.7074663e-08,1.4537128e-10\n",
      "Iteration 17500: loss = 1.7029377e-08,1.453841e-10\n",
      "Iteration 17505: loss = 1.6983696e-08,1.453971e-10\n",
      "Iteration 17510: loss = 1.6938408e-08,1.4541071e-10\n",
      "Iteration 17515: loss = 1.689306e-08,1.4542435e-10\n",
      "Iteration 17520: loss = 1.6847851e-08,1.4543805e-10\n",
      "Iteration 17525: loss = 1.6788148e-08,1.4545107e-10\n",
      "Iteration 17530: loss = 1.6743313e-08,1.4546356e-10\n",
      "Iteration 17535: loss = 1.6698644e-08,1.4547676e-10\n",
      "Iteration 17540: loss = 1.6653905e-08,1.4549023e-10\n",
      "Iteration 17545: loss = 1.6608613e-08,1.4550437e-10\n",
      "Iteration 17550: loss = 1.6563527e-08,1.4551918e-10\n",
      "Iteration 17555: loss = 1.648995e-08,1.45533e-10\n",
      "Iteration 17560: loss = 1.6445068e-08,1.4554746e-10\n",
      "Iteration 17565: loss = 1.6400415e-08,1.4556165e-10\n",
      "Iteration 17570: loss = 1.6355836e-08,1.4557604e-10\n",
      "Iteration 17575: loss = 1.631135e-08,1.4559032e-10\n",
      "Iteration 17580: loss = 1.626699e-08,1.4560451e-10\n",
      "Iteration 17585: loss = 1.6222986e-08,1.4561846e-10\n",
      "Iteration 17590: loss = 1.6164515e-08,1.4563283e-10\n",
      "Iteration 17595: loss = 1.6120557e-08,1.456463e-10\n",
      "Iteration 17600: loss = 1.6076767e-08,1.4565966e-10\n",
      "Iteration 17605: loss = 1.6032889e-08,1.4567365e-10\n",
      "Iteration 17610: loss = 1.5989011e-08,1.4568781e-10\n",
      "Iteration 17615: loss = 1.5945242e-08,1.4570198e-10\n",
      "Iteration 17620: loss = 1.5901456e-08,1.4571627e-10\n",
      "Iteration 17625: loss = 1.5843652e-08,1.4573064e-10\n",
      "Iteration 17630: loss = 1.5800213e-08,1.4574444e-10\n",
      "Iteration 17635: loss = 1.5756857e-08,1.4575817e-10\n",
      "Iteration 17640: loss = 1.5713907e-08,1.457715e-10\n",
      "Iteration 17645: loss = 1.5671851e-08,1.4578116e-10\n",
      "Iteration 17650: loss = 1.563004e-08,1.457912e-10\n",
      "Iteration 17655: loss = 1.558781e-08,1.4580183e-10\n",
      "Iteration 17660: loss = 1.5544565e-08,1.4581653e-10\n",
      "Iteration 17665: loss = 1.5501756e-08,1.4583103e-10\n",
      "Iteration 17670: loss = 1.5458637e-08,1.4584559e-10\n",
      "Iteration 17675: loss = 1.5401875e-08,1.4586082e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17680: loss = 1.5358829e-08,1.4587584e-10\n",
      "Iteration 17685: loss = 1.531586e-08,1.4589116e-10\n",
      "Iteration 17690: loss = 1.527325e-08,1.4590607e-10\n",
      "Iteration 17695: loss = 1.5230503e-08,1.4592111e-10\n",
      "Iteration 17700: loss = 1.5187952e-08,1.4593562e-10\n",
      "Iteration 17705: loss = 1.5145853e-08,1.4595e-10\n",
      "Iteration 17710: loss = 1.5103621e-08,1.4596406e-10\n",
      "Iteration 17715: loss = 1.5047767e-08,1.4597884e-10\n",
      "Iteration 17720: loss = 1.5005346e-08,1.459941e-10\n",
      "Iteration 17725: loss = 1.4963021e-08,1.460093e-10\n",
      "Iteration 17730: loss = 1.4921254e-08,1.4602373e-10\n",
      "Iteration 17735: loss = 1.488629e-08,1.4603806e-10\n",
      "Iteration 17740: loss = 1.4844716e-08,1.4605239e-10\n",
      "Iteration 17745: loss = 1.4802981e-08,1.4606646e-10\n",
      "Iteration 17750: loss = 1.4762705e-08,1.4607655e-10\n",
      "Iteration 17755: loss = 1.47222385e-08,1.4608648e-10\n",
      "Iteration 17760: loss = 1.4682061e-08,1.4609675e-10\n",
      "Iteration 17765: loss = 1.4655474e-08,1.4610686e-10\n",
      "Iteration 17770: loss = 1.4615235e-08,1.4611683e-10\n",
      "Iteration 17775: loss = 1.4575339e-08,1.4612647e-10\n",
      "Iteration 17780: loss = 1.4535456e-08,1.461364e-10\n",
      "Iteration 17785: loss = 1.4495647e-08,1.4614635e-10\n",
      "Iteration 17790: loss = 1.445565e-08,1.4615645e-10\n",
      "Iteration 17795: loss = 1.44146455e-08,1.4617157e-10\n",
      "Iteration 17800: loss = 1.4373523e-08,1.4618731e-10\n",
      "Iteration 17805: loss = 1.4318698e-08,1.462033e-10\n",
      "Iteration 17810: loss = 1.42777035e-08,1.4621926e-10\n",
      "Iteration 17815: loss = 1.4236807e-08,1.4623526e-10\n",
      "Iteration 17820: loss = 1.4196087e-08,1.462508e-10\n",
      "Iteration 17825: loss = 1.415529e-08,1.4626583e-10\n",
      "Iteration 17830: loss = 1.41148755e-08,1.4628104e-10\n",
      "Iteration 17835: loss = 1.4074445e-08,1.462962e-10\n",
      "Iteration 17840: loss = 1.4033645e-08,1.4631221e-10\n",
      "Iteration 17845: loss = 1.39942244e-08,1.4632379e-10\n",
      "Iteration 17850: loss = 1.3954981e-08,1.463353e-10\n",
      "Iteration 17855: loss = 1.3915908e-08,1.463463e-10\n",
      "Iteration 17860: loss = 1.38903244e-08,1.4635763e-10\n",
      "Iteration 17865: loss = 1.3851339e-08,1.4636865e-10\n",
      "Iteration 17870: loss = 1.3812467e-08,1.4637955e-10\n",
      "Iteration 17875: loss = 1.3773595e-08,1.463907e-10\n",
      "Iteration 17880: loss = 1.3734791e-08,1.4640245e-10\n",
      "Iteration 17885: loss = 1.3696023e-08,1.4641346e-10\n",
      "Iteration 17890: loss = 1.3657331e-08,1.4642493e-10\n",
      "Iteration 17895: loss = 1.3618823e-08,1.4643609e-10\n",
      "Iteration 17900: loss = 1.3580404e-08,1.4644692e-10\n",
      "Iteration 17905: loss = 1.3555133e-08,1.4645757e-10\n",
      "Iteration 17910: loss = 1.3516832e-08,1.4646866e-10\n",
      "Iteration 17915: loss = 1.347858e-08,1.4647963e-10\n",
      "Iteration 17920: loss = 1.3440361e-08,1.4649071e-10\n",
      "Iteration 17925: loss = 1.3402315e-08,1.4650152e-10\n",
      "Iteration 17930: loss = 1.3363771e-08,1.4651441e-10\n",
      "Iteration 17935: loss = 1.3324445e-08,1.4653084e-10\n",
      "Iteration 17940: loss = 1.3285134e-08,1.4654786e-10\n",
      "Iteration 17945: loss = 1.32458675e-08,1.4656444e-10\n",
      "Iteration 17950: loss = 1.3206905e-08,1.4658029e-10\n",
      "Iteration 17955: loss = 1.316803e-08,1.4659658e-10\n",
      "Iteration 17960: loss = 1.3129647e-08,1.4661101e-10\n",
      "Iteration 17965: loss = 1.3091534e-08,1.4662381e-10\n",
      "Iteration 17970: loss = 1.3066846e-08,1.4663581e-10\n",
      "Iteration 17975: loss = 1.3029012e-08,1.466485e-10\n",
      "Iteration 17980: loss = 1.2991482e-08,1.466609e-10\n",
      "Iteration 17985: loss = 1.2953905e-08,1.4667281e-10\n",
      "Iteration 17990: loss = 1.2916439e-08,1.4668479e-10\n",
      "Iteration 17995: loss = 1.2879205e-08,1.4669682e-10\n",
      "Iteration 18000: loss = 1.28418876e-08,1.4670871e-10\n",
      "Iteration 18005: loss = 1.2804574e-08,1.4672093e-10\n",
      "Iteration 18010: loss = 1.27802755e-08,1.4673297e-10\n",
      "Iteration 18015: loss = 1.27430795e-08,1.4674507e-10\n",
      "Iteration 18020: loss = 1.2706408e-08,1.4675629e-10\n",
      "Iteration 18025: loss = 1.2669571e-08,1.4676775e-10\n",
      "Iteration 18030: loss = 1.2632751e-08,1.4677902e-10\n",
      "Iteration 18035: loss = 1.2596196e-08,1.4679073e-10\n",
      "Iteration 18040: loss = 1.2559435e-08,1.4680247e-10\n",
      "Iteration 18045: loss = 1.2535328e-08,1.4681428e-10\n",
      "Iteration 18050: loss = 1.2498909e-08,1.4682618e-10\n",
      "Iteration 18055: loss = 1.24622765e-08,1.4683832e-10\n",
      "Iteration 18060: loss = 1.2425499e-08,1.4685103e-10\n",
      "Iteration 18065: loss = 1.23884005e-08,1.4686652e-10\n",
      "Iteration 18070: loss = 1.23508315e-08,1.4688363e-10\n",
      "Iteration 18075: loss = 1.2313607e-08,1.4690033e-10\n",
      "Iteration 18080: loss = 1.2276406e-08,1.4691635e-10\n",
      "Iteration 18085: loss = 1.2240002e-08,1.4692948e-10\n",
      "Iteration 18090: loss = 1.2203819e-08,1.4694236e-10\n",
      "Iteration 18095: loss = 1.218016e-08,1.469552e-10\n",
      "Iteration 18100: loss = 1.2143977e-08,1.469684e-10\n",
      "Iteration 18105: loss = 1.2107745e-08,1.4698155e-10\n",
      "Iteration 18110: loss = 1.2071825e-08,1.469946e-10\n",
      "Iteration 18115: loss = 1.2035929e-08,1.4700771e-10\n",
      "Iteration 18120: loss = 1.199994e-08,1.4702053e-10\n",
      "Iteration 18125: loss = 1.1964171e-08,1.4703369e-10\n",
      "Iteration 18130: loss = 1.19408154e-08,1.4704679e-10\n",
      "Iteration 18135: loss = 1.1904928e-08,1.4705999e-10\n",
      "Iteration 18140: loss = 1.1881757e-08,1.4707252e-10\n",
      "Iteration 18145: loss = 1.1846434e-08,1.4708489e-10\n",
      "Iteration 18150: loss = 1.1811153e-08,1.470973e-10\n",
      "Iteration 18155: loss = 1.1775704e-08,1.4710992e-10\n",
      "Iteration 18160: loss = 1.1740437e-08,1.4712281e-10\n",
      "Iteration 18165: loss = 1.1717326e-08,1.471363e-10\n",
      "Iteration 18170: loss = 1.1681887e-08,1.4714974e-10\n",
      "Iteration 18175: loss = 1.1646701e-08,1.4716273e-10\n",
      "Iteration 18180: loss = 1.1611775e-08,1.471755e-10\n",
      "Iteration 18185: loss = 1.1575845e-08,1.4719206e-10\n",
      "Iteration 18190: loss = 1.1539973e-08,1.4720969e-10\n",
      "Iteration 18195: loss = 1.1504977e-08,1.4722312e-10\n",
      "Iteration 18200: loss = 1.1482121e-08,1.472363e-10\n",
      "Iteration 18205: loss = 1.1447365e-08,1.472495e-10\n",
      "Iteration 18210: loss = 1.1412513e-08,1.4726338e-10\n",
      "Iteration 18215: loss = 1.1377693e-08,1.4727748e-10\n",
      "Iteration 18220: loss = 1.1342977e-08,1.4729129e-10\n",
      "Iteration 18225: loss = 1.130836e-08,1.4730508e-10\n",
      "Iteration 18230: loss = 1.1273809e-08,1.4731893e-10\n",
      "Iteration 18235: loss = 1.1251173e-08,1.4733272e-10\n",
      "Iteration 18240: loss = 1.1228662e-08,1.4734639e-10\n",
      "Iteration 18245: loss = 1.1194317e-08,1.4735996e-10\n",
      "Iteration 18250: loss = 1.11601475e-08,1.4737289e-10\n",
      "Iteration 18255: loss = 1.1126065e-08,1.4738583e-10\n",
      "Iteration 18260: loss = 1.1092038e-08,1.4739875e-10\n",
      "Iteration 18265: loss = 1.1069914e-08,1.4741251e-10\n",
      "Iteration 18270: loss = 1.1035809e-08,1.4742652e-10\n",
      "Iteration 18275: loss = 1.1001776e-08,1.4744019e-10\n",
      "Iteration 18280: loss = 1.0967859e-08,1.4745413e-10\n",
      "Iteration 18285: loss = 1.0934144e-08,1.4746707e-10\n",
      "Iteration 18290: loss = 1.09129745e-08,1.4747625e-10\n",
      "Iteration 18295: loss = 1.0891842e-08,1.4748536e-10\n",
      "Iteration 18300: loss = 1.0859195e-08,1.4749453e-10\n",
      "Iteration 18305: loss = 1.0826419e-08,1.4750391e-10\n",
      "Iteration 18310: loss = 1.07937055e-08,1.4751311e-10\n",
      "Iteration 18315: loss = 1.0772855e-08,1.4752269e-10\n",
      "Iteration 18320: loss = 1.0740238e-08,1.4753179e-10\n",
      "Iteration 18325: loss = 1.0706667e-08,1.4754593e-10\n",
      "Iteration 18330: loss = 1.0684912e-08,1.4756041e-10\n",
      "Iteration 18335: loss = 1.0651427e-08,1.4757504e-10\n",
      "Iteration 18340: loss = 1.0629807e-08,1.475891e-10\n",
      "Iteration 18345: loss = 1.0596438e-08,1.4760385e-10\n",
      "Iteration 18350: loss = 1.05630775e-08,1.4761851e-10\n",
      "Iteration 18355: loss = 1.0530033e-08,1.4763317e-10\n",
      "Iteration 18360: loss = 1.0496869e-08,1.4764738e-10\n",
      "Iteration 18365: loss = 1.0475223e-08,1.4766188e-10\n",
      "Iteration 18370: loss = 1.0442311e-08,1.476767e-10\n",
      "Iteration 18375: loss = 1.0409354e-08,1.4769114e-10\n",
      "Iteration 18380: loss = 1.0376582e-08,1.4770497e-10\n",
      "Iteration 18385: loss = 1.0344067e-08,1.4771855e-10\n",
      "Iteration 18390: loss = 1.032264e-08,1.4773305e-10\n",
      "Iteration 18395: loss = 1.0301256e-08,1.4774756e-10\n",
      "Iteration 18400: loss = 1.0269145e-08,1.4775983e-10\n",
      "Iteration 18405: loss = 1.0237426e-08,1.477704e-10\n",
      "Iteration 18410: loss = 1.0205792e-08,1.4778052e-10\n",
      "Iteration 18415: loss = 1.0185498e-08,1.4779049e-10\n",
      "Iteration 18420: loss = 1.0165361e-08,1.4780038e-10\n",
      "Iteration 18425: loss = 1.0133772e-08,1.4781124e-10\n",
      "Iteration 18430: loss = 1.0102411e-08,1.4782112e-10\n",
      "Iteration 18435: loss = 1.0082207e-08,1.4783116e-10\n",
      "Iteration 18440: loss = 1.0051007e-08,1.4784127e-10\n",
      "Iteration 18445: loss = 1.0019804e-08,1.4785113e-10\n",
      "Iteration 18450: loss = 9.999698e-09,1.478613e-10\n",
      "Iteration 18455: loss = 9.9797886e-09,1.4787128e-10\n",
      "Iteration 18460: loss = 9.9487e-09,1.4788157e-10\n",
      "Iteration 18465: loss = 9.917511e-09,1.478917e-10\n",
      "Iteration 18470: loss = 9.886496e-09,1.4790216e-10\n",
      "Iteration 18475: loss = 9.877923e-09,1.4791217e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18480: loss = 9.846869e-09,1.4792224e-10\n",
      "Iteration 18485: loss = 9.815465e-09,1.479356e-10\n",
      "Iteration 18490: loss = 9.783959e-09,1.4794997e-10\n",
      "Iteration 18495: loss = 9.76363e-09,1.4796397e-10\n",
      "Iteration 18500: loss = 9.731839e-09,1.479793e-10\n",
      "Iteration 18505: loss = 9.700236e-09,1.4799505e-10\n",
      "Iteration 18510: loss = 9.668672e-09,1.4801067e-10\n",
      "Iteration 18515: loss = 9.637037e-09,1.4802647e-10\n",
      "Iteration 18520: loss = 9.627668e-09,1.4804209e-10\n",
      "Iteration 18525: loss = 9.59639e-09,1.4805737e-10\n",
      "Iteration 18530: loss = 9.565189e-09,1.4807135e-10\n",
      "Iteration 18535: loss = 9.534356e-09,1.480844e-10\n",
      "Iteration 18540: loss = 9.503907e-09,1.4809567e-10\n",
      "Iteration 18545: loss = 9.48451e-09,1.481069e-10\n",
      "Iteration 18550: loss = 9.465114e-09,1.4811817e-10\n",
      "Iteration 18555: loss = 9.4348325e-09,1.4812937e-10\n",
      "Iteration 18560: loss = 9.415785e-09,1.4814006e-10\n",
      "Iteration 18565: loss = 9.3856904e-09,1.4815084e-10\n",
      "Iteration 18570: loss = 9.35569e-09,1.4816134e-10\n",
      "Iteration 18575: loss = 9.336579e-09,1.4817211e-10\n",
      "Iteration 18580: loss = 9.317518e-09,1.4818273e-10\n",
      "Iteration 18585: loss = 9.287606e-09,1.4819389e-10\n",
      "Iteration 18590: loss = 9.257758e-09,1.4820448e-10\n",
      "Iteration 18595: loss = 9.238765e-09,1.4821534e-10\n",
      "Iteration 18600: loss = 9.219774e-09,1.4822615e-10\n",
      "Iteration 18605: loss = 9.190027e-09,1.4823727e-10\n",
      "Iteration 18610: loss = 9.160338e-09,1.482485e-10\n",
      "Iteration 18615: loss = 9.141418e-09,1.4825993e-10\n",
      "Iteration 18620: loss = 9.111834e-09,1.4827091e-10\n",
      "Iteration 18625: loss = 9.0931485e-09,1.4828122e-10\n",
      "Iteration 18630: loss = 9.074534e-09,1.482913e-10\n",
      "Iteration 18635: loss = 9.045275e-09,1.483011e-10\n",
      "Iteration 18640: loss = 9.015874e-09,1.4831252e-10\n",
      "Iteration 18645: loss = 8.98571e-09,1.4832854e-10\n",
      "Iteration 18650: loss = 8.976879e-09,1.4834438e-10\n",
      "Iteration 18655: loss = 8.94681e-09,1.4836046e-10\n",
      "Iteration 18660: loss = 8.916756e-09,1.4837646e-10\n",
      "Iteration 18665: loss = 8.887068e-09,1.4839138e-10\n",
      "Iteration 18670: loss = 8.857322e-09,1.484058e-10\n",
      "Iteration 18675: loss = 8.8389465e-09,1.484171e-10\n",
      "Iteration 18680: loss = 8.820462e-09,1.4842852e-10\n",
      "Iteration 18685: loss = 8.791317e-09,1.4844079e-10\n",
      "Iteration 18690: loss = 8.772877e-09,1.484532e-10\n",
      "Iteration 18695: loss = 8.743792e-09,1.4846564e-10\n",
      "Iteration 18700: loss = 8.714794e-09,1.4847794e-10\n",
      "Iteration 18705: loss = 8.6965555e-09,1.4849e-10\n",
      "Iteration 18710: loss = 8.678275e-09,1.4850118e-10\n",
      "Iteration 18715: loss = 8.649571e-09,1.485125e-10\n",
      "Iteration 18720: loss = 8.6210585e-09,1.4852423e-10\n",
      "Iteration 18725: loss = 8.602805e-09,1.4853595e-10\n",
      "Iteration 18730: loss = 8.584656e-09,1.4854754e-10\n",
      "Iteration 18735: loss = 8.556281e-09,1.4855905e-10\n",
      "Iteration 18740: loss = 8.538169e-09,1.4857046e-10\n",
      "Iteration 18745: loss = 8.509674e-09,1.4858233e-10\n",
      "Iteration 18750: loss = 8.491776e-09,1.4859391e-10\n",
      "Iteration 18755: loss = 8.463425e-09,1.486056e-10\n",
      "Iteration 18760: loss = 8.445496e-09,1.4861692e-10\n",
      "Iteration 18765: loss = 8.417398e-09,1.4862882e-10\n",
      "Iteration 18770: loss = 8.399399e-09,1.4864074e-10\n",
      "Iteration 18775: loss = 8.3814955e-09,1.486527e-10\n",
      "Iteration 18780: loss = 8.353479e-09,1.4866469e-10\n",
      "Iteration 18785: loss = 8.3252525e-09,1.4867732e-10\n",
      "Iteration 18790: loss = 8.296436e-09,1.48694e-10\n",
      "Iteration 18795: loss = 8.288263e-09,1.4871104e-10\n",
      "Iteration 18800: loss = 8.259942e-09,1.4872545e-10\n",
      "Iteration 18805: loss = 8.231874e-09,1.4873852e-10\n",
      "Iteration 18810: loss = 8.214321e-09,1.487503e-10\n",
      "Iteration 18815: loss = 8.186516e-09,1.4876295e-10\n",
      "Iteration 18820: loss = 8.168728e-09,1.4877591e-10\n",
      "Iteration 18825: loss = 8.1511065e-09,1.4878876e-10\n",
      "Iteration 18830: loss = 8.123432e-09,1.4880167e-10\n",
      "Iteration 18835: loss = 8.095678e-09,1.4881424e-10\n",
      "Iteration 18840: loss = 8.07817e-09,1.4882724e-10\n",
      "Iteration 18845: loss = 8.06081e-09,1.4883944e-10\n",
      "Iteration 18850: loss = 8.033273e-09,1.4885151e-10\n",
      "Iteration 18855: loss = 8.0059035e-09,1.4886387e-10\n",
      "Iteration 18860: loss = 7.998694e-09,1.4887594e-10\n",
      "Iteration 18865: loss = 7.97143e-09,1.4888842e-10\n",
      "Iteration 18870: loss = 7.944081e-09,1.4890056e-10\n",
      "Iteration 18875: loss = 7.926949e-09,1.4891248e-10\n",
      "Iteration 18880: loss = 7.899712e-09,1.489257e-10\n",
      "Iteration 18885: loss = 7.882325e-09,1.4893868e-10\n",
      "Iteration 18890: loss = 7.865286e-09,1.4895096e-10\n",
      "Iteration 18895: loss = 7.838273e-09,1.4896345e-10\n",
      "Iteration 18900: loss = 7.811179e-09,1.4897572e-10\n",
      "Iteration 18905: loss = 7.804119e-09,1.489883e-10\n",
      "Iteration 18910: loss = 7.7772135e-09,1.4900012e-10\n",
      "Iteration 18915: loss = 7.7504145e-09,1.4901241e-10\n",
      "Iteration 18920: loss = 7.743922e-09,1.4902093e-10\n",
      "Iteration 18925: loss = 7.717038e-09,1.4903402e-10\n",
      "Iteration 18930: loss = 7.690059e-09,1.4904798e-10\n",
      "Iteration 18935: loss = 7.66332e-09,1.4906107e-10\n",
      "Iteration 18940: loss = 7.656221e-09,1.4907436e-10\n",
      "Iteration 18945: loss = 7.629495e-09,1.4908783e-10\n",
      "Iteration 18950: loss = 7.602828e-09,1.4910084e-10\n",
      "Iteration 18955: loss = 7.586002e-09,1.491146e-10\n",
      "Iteration 18960: loss = 7.559376e-09,1.491283e-10\n",
      "Iteration 18965: loss = 7.5425275e-09,1.491419e-10\n",
      "Iteration 18970: loss = 7.525775e-09,1.4915562e-10\n",
      "Iteration 18975: loss = 7.499421e-09,1.4916858e-10\n",
      "Iteration 18980: loss = 7.473131e-09,1.491814e-10\n",
      "Iteration 18985: loss = 7.466269e-09,1.4919417e-10\n",
      "Iteration 18990: loss = 7.4399904e-09,1.492074e-10\n",
      "Iteration 18995: loss = 7.4137305e-09,1.4922058e-10\n",
      "Iteration 19000: loss = 7.4068005e-09,1.4923444e-10\n",
      "Iteration 19005: loss = 7.38062e-09,1.4924803e-10\n",
      "Iteration 19010: loss = 7.354507e-09,1.4926164e-10\n",
      "Iteration 19015: loss = 7.3380826e-09,1.4927475e-10\n",
      "Iteration 19020: loss = 7.322105e-09,1.492857e-10\n",
      "Iteration 19025: loss = 7.2966646e-09,1.4929481e-10\n",
      "Iteration 19030: loss = 7.280915e-09,1.4930486e-10\n",
      "Iteration 19035: loss = 7.2649216e-09,1.4931595e-10\n",
      "Iteration 19040: loss = 7.249247e-09,1.493248e-10\n",
      "Iteration 19045: loss = 7.2240005e-09,1.4933398e-10\n",
      "Iteration 19050: loss = 7.2083286e-09,1.493432e-10\n",
      "Iteration 19055: loss = 7.1928192e-09,1.493524e-10\n",
      "Iteration 19060: loss = 7.167602e-09,1.4936213e-10\n",
      "Iteration 19065: loss = 7.1615625e-09,1.4937163e-10\n",
      "Iteration 19070: loss = 7.1363977e-09,1.4938135e-10\n",
      "Iteration 19075: loss = 7.111529e-09,1.4939042e-10\n",
      "Iteration 19080: loss = 7.1054393e-09,1.4940003e-10\n",
      "Iteration 19085: loss = 7.0803723e-09,1.4941008e-10\n",
      "Iteration 19090: loss = 7.0550077e-09,1.4942378e-10\n",
      "Iteration 19095: loss = 7.0390147e-09,1.4943705e-10\n",
      "Iteration 19100: loss = 7.0229222e-09,1.4945127e-10\n",
      "Iteration 19105: loss = 6.997611e-09,1.4946538e-10\n",
      "Iteration 19110: loss = 6.9815775e-09,1.4947996e-10\n",
      "Iteration 19115: loss = 6.9561756e-09,1.4949428e-10\n",
      "Iteration 19120: loss = 6.940363e-09,1.4950853e-10\n",
      "Iteration 19125: loss = 6.9244415e-09,1.4952264e-10\n",
      "Iteration 19130: loss = 6.8992114e-09,1.495369e-10\n",
      "Iteration 19135: loss = 6.8834995e-09,1.4955079e-10\n",
      "Iteration 19140: loss = 6.8676465e-09,1.4956526e-10\n",
      "Iteration 19145: loss = 6.842513e-09,1.4957946e-10\n",
      "Iteration 19150: loss = 6.826871e-09,1.4959395e-10\n",
      "Iteration 19155: loss = 6.811265e-09,1.4960755e-10\n",
      "Iteration 19160: loss = 6.786422e-09,1.4962087e-10\n",
      "Iteration 19165: loss = 6.771311e-09,1.4963161e-10\n",
      "Iteration 19170: loss = 6.7562382e-09,1.4964127e-10\n",
      "Iteration 19175: loss = 6.732128e-09,1.4965075e-10\n",
      "Iteration 19180: loss = 6.7172437e-09,1.4966035e-10\n",
      "Iteration 19185: loss = 6.702313e-09,1.4966979e-10\n",
      "Iteration 19190: loss = 6.687465e-09,1.4967941e-10\n",
      "Iteration 19195: loss = 6.663503e-09,1.4968894e-10\n",
      "Iteration 19200: loss = 6.6485915e-09,1.4969884e-10\n",
      "Iteration 19205: loss = 6.633808e-09,1.4970825e-10\n",
      "Iteration 19210: loss = 6.609937e-09,1.4971815e-10\n",
      "Iteration 19215: loss = 6.6041275e-09,1.4972862e-10\n",
      "Iteration 19220: loss = 6.5802848e-09,1.4973855e-10\n",
      "Iteration 19225: loss = 6.5654775e-09,1.4974949e-10\n",
      "Iteration 19230: loss = 6.5505916e-09,1.4976018e-10\n",
      "Iteration 19235: loss = 6.526856e-09,1.4977027e-10\n",
      "Iteration 19240: loss = 6.5213457e-09,1.4978002e-10\n",
      "Iteration 19245: loss = 6.4975763e-09,1.4978982e-10\n",
      "Iteration 19250: loss = 6.482448e-09,1.4980417e-10\n",
      "Iteration 19255: loss = 6.4582797e-09,1.4981887e-10\n",
      "Iteration 19260: loss = 6.443076e-09,1.4983366e-10\n",
      "Iteration 19265: loss = 6.42799e-09,1.4984833e-10\n",
      "Iteration 19270: loss = 6.4039227e-09,1.4986305e-10\n",
      "Iteration 19275: loss = 6.3887806e-09,1.4987778e-10\n",
      "Iteration 19280: loss = 6.373758e-09,1.4989289e-10\n",
      "Iteration 19285: loss = 6.3497008e-09,1.4990864e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19290: loss = 6.334952e-09,1.4992137e-10\n",
      "Iteration 19295: loss = 6.320376e-09,1.4993316e-10\n",
      "Iteration 19300: loss = 6.2967946e-09,1.4994575e-10\n",
      "Iteration 19305: loss = 6.2913035e-09,1.4995662e-10\n",
      "Iteration 19310: loss = 6.2680336e-09,1.4996754e-10\n",
      "Iteration 19315: loss = 6.244829e-09,1.4997838e-10\n",
      "Iteration 19320: loss = 6.2393646e-09,1.4998941e-10\n",
      "Iteration 19325: loss = 6.2162813e-09,1.4999967e-10\n",
      "Iteration 19330: loss = 6.2021233e-09,1.500097e-10\n",
      "Iteration 19335: loss = 6.187945e-09,1.5002011e-10\n",
      "Iteration 19340: loss = 6.1737633e-09,1.50031e-10\n",
      "Iteration 19345: loss = 6.1507723e-09,1.5004162e-10\n",
      "Iteration 19350: loss = 6.1365415e-09,1.5005289e-10\n",
      "Iteration 19355: loss = 6.122375e-09,1.5006413e-10\n",
      "Iteration 19360: loss = 6.1082113e-09,1.5007535e-10\n",
      "Iteration 19365: loss = 6.094107e-09,1.5008651e-10\n",
      "Iteration 19370: loss = 6.0712586e-09,1.5009738e-10\n",
      "Iteration 19375: loss = 6.0659673e-09,1.5010859e-10\n",
      "Iteration 19380: loss = 6.04328e-09,1.5011875e-10\n",
      "Iteration 19385: loss = 6.020663e-09,1.5012919e-10\n",
      "Iteration 19390: loss = 6.015488e-09,1.5013907e-10\n",
      "Iteration 19395: loss = 5.9929217e-09,1.5014981e-10\n",
      "Iteration 19400: loss = 5.9877467e-09,1.5016027e-10\n",
      "Iteration 19405: loss = 5.9652066e-09,1.5017088e-10\n",
      "Iteration 19410: loss = 5.9508616e-09,1.5018593e-10\n",
      "Iteration 19415: loss = 5.936503e-09,1.5020099e-10\n",
      "Iteration 19420: loss = 5.913533e-09,1.5021656e-10\n",
      "Iteration 19425: loss = 5.89957e-09,1.5022877e-10\n",
      "Iteration 19430: loss = 5.885704e-09,1.5024115e-10\n",
      "Iteration 19435: loss = 5.871801e-09,1.5025353e-10\n",
      "Iteration 19440: loss = 5.849373e-09,1.5026508e-10\n",
      "Iteration 19445: loss = 5.835593e-09,1.502767e-10\n",
      "Iteration 19450: loss = 5.8219687e-09,1.5028803e-10\n",
      "Iteration 19455: loss = 5.799672e-09,1.5029979e-10\n",
      "Iteration 19460: loss = 5.7945324e-09,1.5031124e-10\n",
      "Iteration 19465: loss = 5.7724403e-09,1.50323e-10\n",
      "Iteration 19470: loss = 5.767309e-09,1.503342e-10\n",
      "Iteration 19475: loss = 5.7451452e-09,1.5034578e-10\n",
      "Iteration 19480: loss = 5.723112e-09,1.503576e-10\n",
      "Iteration 19485: loss = 5.7179963e-09,1.5036938e-10\n",
      "Iteration 19490: loss = 5.6960254e-09,1.5038021e-10\n",
      "Iteration 19495: loss = 5.682582e-09,1.5039223e-10\n",
      "Iteration 19500: loss = 5.66907e-09,1.5040383e-10\n",
      "Iteration 19505: loss = 5.6555494e-09,1.5041579e-10\n",
      "Iteration 19510: loss = 5.642171e-09,1.5042734e-10\n",
      "Iteration 19515: loss = 5.6203056e-09,1.5043901e-10\n",
      "Iteration 19520: loss = 5.6068905e-09,1.5045071e-10\n",
      "Iteration 19525: loss = 5.5936042e-09,1.5046246e-10\n",
      "Iteration 19530: loss = 5.580246e-09,1.5047401e-10\n",
      "Iteration 19535: loss = 5.5669944e-09,1.5048487e-10\n",
      "Iteration 19540: loss = 5.5538485e-09,1.5049582e-10\n",
      "Iteration 19545: loss = 5.5322738e-09,1.5050677e-10\n",
      "Iteration 19550: loss = 5.52744e-09,1.5051776e-10\n",
      "Iteration 19555: loss = 5.506123e-09,1.5052742e-10\n",
      "Iteration 19560: loss = 5.501729e-09,1.5053528e-10\n",
      "Iteration 19565: loss = 5.480659e-09,1.5054326e-10\n",
      "Iteration 19570: loss = 5.4672604e-09,1.5055664e-10\n",
      "Iteration 19575: loss = 5.454057e-09,1.5056936e-10\n",
      "Iteration 19580: loss = 5.4326024e-09,1.5058207e-10\n",
      "Iteration 19585: loss = 5.4276583e-09,1.5059456e-10\n",
      "Iteration 19590: loss = 5.4062834e-09,1.5060696e-10\n",
      "Iteration 19595: loss = 5.3932214e-09,1.5061938e-10\n",
      "Iteration 19600: loss = 5.3801337e-09,1.5063122e-10\n",
      "Iteration 19605: loss = 5.36718e-09,1.5064298e-10\n",
      "Iteration 19610: loss = 5.3459135e-09,1.5065527e-10\n",
      "Iteration 19615: loss = 5.341184e-09,1.5066719e-10\n",
      "Iteration 19620: loss = 5.320104e-09,1.5067901e-10\n",
      "Iteration 19625: loss = 5.307152e-09,1.5069111e-10\n",
      "Iteration 19630: loss = 5.294224e-09,1.5070344e-10\n",
      "Iteration 19635: loss = 5.273149e-09,1.5071636e-10\n",
      "Iteration 19640: loss = 5.268303e-09,1.5072923e-10\n",
      "Iteration 19645: loss = 5.247316e-09,1.5074184e-10\n",
      "Iteration 19650: loss = 5.2426565e-09,1.507544e-10\n",
      "Iteration 19655: loss = 5.2216733e-09,1.5076636e-10\n",
      "Iteration 19660: loss = 5.2089373e-09,1.5077878e-10\n",
      "Iteration 19665: loss = 5.196389e-09,1.5078902e-10\n",
      "Iteration 19670: loss = 5.1756146e-09,1.5080104e-10\n",
      "Iteration 19675: loss = 5.171314e-09,1.508103e-10\n",
      "Iteration 19680: loss = 5.1509557e-09,1.5081875e-10\n",
      "Iteration 19685: loss = 5.1467013e-09,1.5082774e-10\n",
      "Iteration 19690: loss = 5.1260725e-09,1.5083912e-10\n",
      "Iteration 19695: loss = 5.1135847e-09,1.5085043e-10\n",
      "Iteration 19700: loss = 5.101261e-09,1.5086019e-10\n",
      "Iteration 19705: loss = 5.0890847e-09,1.5086858e-10\n",
      "Iteration 19710: loss = 5.076917e-09,1.508769e-10\n",
      "Iteration 19715: loss = 5.064802e-09,1.5088537e-10\n",
      "Iteration 19720: loss = 5.0526245e-09,1.5089449e-10\n",
      "Iteration 19725: loss = 5.040462e-09,1.5090355e-10\n",
      "Iteration 19730: loss = 5.028356e-09,1.5091224e-10\n",
      "Iteration 19735: loss = 5.0163016e-09,1.509207e-10\n",
      "Iteration 19740: loss = 5.0042583e-09,1.5092934e-10\n",
      "Iteration 19745: loss = 4.9922186e-09,1.5093798e-10\n",
      "Iteration 19750: loss = 4.972271e-09,1.5094671e-10\n",
      "Iteration 19755: loss = 4.9679265e-09,1.5095815e-10\n",
      "Iteration 19760: loss = 4.947595e-09,1.509711e-10\n",
      "Iteration 19765: loss = 4.9431037e-09,1.5098436e-10\n",
      "Iteration 19770: loss = 4.9228914e-09,1.5099692e-10\n",
      "Iteration 19775: loss = 4.91849e-09,1.5100939e-10\n",
      "Iteration 19780: loss = 4.898324e-09,1.510223e-10\n",
      "Iteration 19785: loss = 4.8860795e-09,1.5103478e-10\n",
      "Iteration 19790: loss = 4.8738618e-09,1.5104723e-10\n",
      "Iteration 19795: loss = 4.861652e-09,1.5106026e-10\n",
      "Iteration 19800: loss = 4.8494115e-09,1.5107338e-10\n",
      "Iteration 19805: loss = 4.837155e-09,1.5108687e-10\n",
      "Iteration 19810: loss = 4.817124e-09,1.5110034e-10\n",
      "Iteration 19815: loss = 4.8127915e-09,1.5111314e-10\n",
      "Iteration 19820: loss = 4.792839e-09,1.5112694e-10\n",
      "Iteration 19825: loss = 4.780733e-09,1.5113961e-10\n",
      "Iteration 19830: loss = 4.7688573e-09,1.5115063e-10\n",
      "Iteration 19835: loss = 4.7648903e-09,1.5116007e-10\n",
      "Iteration 19840: loss = 4.7454147e-09,1.511694e-10\n",
      "Iteration 19845: loss = 4.741398e-09,1.5117904e-10\n",
      "Iteration 19850: loss = 4.722096e-09,1.5118853e-10\n",
      "Iteration 19855: loss = 4.71035e-09,1.5119841e-10\n",
      "Iteration 19860: loss = 4.6987494e-09,1.5120846e-10\n",
      "Iteration 19865: loss = 4.687021e-09,1.5121862e-10\n",
      "Iteration 19870: loss = 4.6754445e-09,1.512284e-10\n",
      "Iteration 19875: loss = 4.6638404e-09,1.5123813e-10\n",
      "Iteration 19880: loss = 4.6522506e-09,1.5124765e-10\n",
      "Iteration 19885: loss = 4.640811e-09,1.5125703e-10\n",
      "Iteration 19890: loss = 4.6293334e-09,1.512658e-10\n",
      "Iteration 19895: loss = 4.6178634e-09,1.5127494e-10\n",
      "Iteration 19900: loss = 4.606532e-09,1.5128376e-10\n",
      "Iteration 19905: loss = 4.595107e-09,1.5129228e-10\n",
      "Iteration 19910: loss = 4.5837405e-09,1.5130106e-10\n",
      "Iteration 19915: loss = 4.5723487e-09,1.5131005e-10\n",
      "Iteration 19920: loss = 4.561094e-09,1.5131858e-10\n",
      "Iteration 19925: loss = 4.5497255e-09,1.5132774e-10\n",
      "Iteration 19930: loss = 4.5383395e-09,1.5133722e-10\n",
      "Iteration 19935: loss = 4.527075e-09,1.5134699e-10\n",
      "Iteration 19940: loss = 4.515735e-09,1.5135604e-10\n",
      "Iteration 19945: loss = 4.5044106e-09,1.5136575e-10\n",
      "Iteration 19950: loss = 4.4928625e-09,1.5137873e-10\n",
      "Iteration 19955: loss = 4.4811883e-09,1.5139243e-10\n",
      "Iteration 19960: loss = 4.462051e-09,1.5140628e-10\n",
      "Iteration 19965: loss = 4.457943e-09,1.5142049e-10\n",
      "Iteration 19970: loss = 4.4388364e-09,1.5143442e-10\n",
      "Iteration 19975: loss = 4.43472e-09,1.514482e-10\n",
      "Iteration 19980: loss = 4.415702e-09,1.5146184e-10\n",
      "Iteration 19985: loss = 4.411707e-09,1.5147571e-10\n",
      "Iteration 19990: loss = 4.392825e-09,1.5148863e-10\n",
      "Iteration 19995: loss = 4.3817088e-09,1.5149859e-10\n",
      "Iteration 20000: loss = 4.3705346e-09,1.5150889e-10\n",
      "Iteration 20005: loss = 4.359372e-09,1.5151996e-10\n",
      "Iteration 20010: loss = 4.348155e-09,1.5153104e-10\n",
      "Iteration 20015: loss = 4.337081e-09,1.5154156e-10\n",
      "Iteration 20020: loss = 4.326063e-09,1.5155176e-10\n",
      "Iteration 20025: loss = 4.3149777e-09,1.5156223e-10\n",
      "Iteration 20030: loss = 4.311343e-09,1.5157256e-10\n",
      "Iteration 20035: loss = 4.293033e-09,1.515825e-10\n",
      "Iteration 20040: loss = 4.289355e-09,1.5159259e-10\n",
      "Iteration 20045: loss = 4.27109e-09,1.516027e-10\n",
      "Iteration 20050: loss = 4.2675206e-09,1.5161278e-10\n",
      "Iteration 20055: loss = 4.2492636e-09,1.5162252e-10\n",
      "Iteration 20060: loss = 4.2457704e-09,1.5163211e-10\n",
      "Iteration 20065: loss = 4.227592e-09,1.5164123e-10\n",
      "Iteration 20070: loss = 4.224128e-09,1.5165079e-10\n",
      "Iteration 20075: loss = 4.206097e-09,1.5166007e-10\n",
      "Iteration 20080: loss = 4.202568e-09,1.5166948e-10\n",
      "Iteration 20085: loss = 4.184572e-09,1.5167889e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20090: loss = 4.1811283e-09,1.5168841e-10\n",
      "Iteration 20095: loss = 4.170366e-09,1.5169774e-10\n",
      "Iteration 20100: loss = 4.1596597e-09,1.5170748e-10\n",
      "Iteration 20105: loss = 4.148984e-09,1.5171739e-10\n",
      "Iteration 20110: loss = 4.1382293e-09,1.5172746e-10\n",
      "Iteration 20115: loss = 4.127531e-09,1.5173766e-10\n",
      "Iteration 20120: loss = 4.1167603e-09,1.5174823e-10\n",
      "Iteration 20125: loss = 4.1057966e-09,1.5176249e-10\n",
      "Iteration 20130: loss = 4.0947667e-09,1.5177747e-10\n",
      "Iteration 20135: loss = 4.0839043e-09,1.5179016e-10\n",
      "Iteration 20140: loss = 4.073229e-09,1.5180107e-10\n",
      "Iteration 20145: loss = 4.0626023e-09,1.5181198e-10\n",
      "Iteration 20150: loss = 4.0519943e-09,1.5182278e-10\n",
      "Iteration 20155: loss = 4.041405e-09,1.518333e-10\n",
      "Iteration 20160: loss = 4.030877e-09,1.5184362e-10\n",
      "Iteration 20165: loss = 4.020357e-09,1.5185386e-10\n",
      "Iteration 20170: loss = 4.0169796e-09,1.5186401e-10\n",
      "Iteration 20175: loss = 3.999343e-09,1.5187479e-10\n",
      "Iteration 20180: loss = 3.9958885e-09,1.5188614e-10\n",
      "Iteration 20185: loss = 3.978277e-09,1.5189729e-10\n",
      "Iteration 20190: loss = 3.974829e-09,1.5190875e-10\n",
      "Iteration 20195: loss = 3.9573016e-09,1.5191937e-10\n",
      "Iteration 20200: loss = 3.953869e-09,1.5193019e-10\n",
      "Iteration 20205: loss = 3.9364183e-09,1.519409e-10\n",
      "Iteration 20210: loss = 3.9330517e-09,1.519514e-10\n",
      "Iteration 20215: loss = 3.9156287e-09,1.5196219e-10\n",
      "Iteration 20220: loss = 3.912313e-09,1.5197296e-10\n",
      "Iteration 20225: loss = 3.9019663e-09,1.5198383e-10\n",
      "Iteration 20230: loss = 3.8916435e-09,1.5199444e-10\n",
      "Iteration 20235: loss = 3.8813326e-09,1.5200527e-10\n",
      "Iteration 20240: loss = 3.8710373e-09,1.5201591e-10\n",
      "Iteration 20245: loss = 3.8607406e-09,1.5202697e-10\n",
      "Iteration 20250: loss = 3.8504875e-09,1.5203762e-10\n",
      "Iteration 20255: loss = 3.840269e-09,1.5204837e-10\n",
      "Iteration 20260: loss = 3.8300447e-09,1.5205902e-10\n",
      "Iteration 20265: loss = 3.820078e-09,1.520673e-10\n",
      "Iteration 20270: loss = 3.817128e-09,1.5207402e-10\n",
      "Iteration 20275: loss = 3.800324e-09,1.5208096e-10\n",
      "Iteration 20280: loss = 3.7973735e-09,1.5208758e-10\n",
      "Iteration 20285: loss = 3.780408e-09,1.5209754e-10\n",
      "Iteration 20290: loss = 3.777482e-09,1.5210509e-10\n",
      "Iteration 20295: loss = 3.767497e-09,1.521134e-10\n",
      "Iteration 20300: loss = 3.7576884e-09,1.5212133e-10\n",
      "Iteration 20305: loss = 3.754751e-09,1.5212849e-10\n",
      "Iteration 20310: loss = 3.738014e-09,1.5213625e-10\n",
      "Iteration 20315: loss = 3.7348045e-09,1.5214818e-10\n",
      "Iteration 20320: loss = 3.7178405e-09,1.5215978e-10\n",
      "Iteration 20325: loss = 3.714576e-09,1.5217169e-10\n",
      "Iteration 20330: loss = 3.6976482e-09,1.5218335e-10\n",
      "Iteration 20335: loss = 3.694481e-09,1.5219509e-10\n",
      "Iteration 20340: loss = 3.6844423e-09,1.5220661e-10\n",
      "Iteration 20345: loss = 3.674468e-09,1.5221757e-10\n",
      "Iteration 20350: loss = 3.6645258e-09,1.5222938e-10\n",
      "Iteration 20355: loss = 3.6545262e-09,1.5224096e-10\n",
      "Iteration 20360: loss = 3.644562e-09,1.5225249e-10\n",
      "Iteration 20365: loss = 3.6346675e-09,1.5226423e-10\n",
      "Iteration 20370: loss = 3.6247265e-09,1.5227573e-10\n",
      "Iteration 20375: loss = 3.614795e-09,1.5228781e-10\n",
      "Iteration 20380: loss = 3.6048993e-09,1.522995e-10\n",
      "Iteration 20385: loss = 3.601803e-09,1.5231141e-10\n",
      "Iteration 20390: loss = 3.5852088e-09,1.523229e-10\n",
      "Iteration 20395: loss = 3.5821868e-09,1.5233327e-10\n",
      "Iteration 20400: loss = 3.5656786e-09,1.5234422e-10\n",
      "Iteration 20405: loss = 3.5628869e-09,1.5235233e-10\n",
      "Iteration 20410: loss = 3.5601024e-09,1.5235994e-10\n",
      "Iteration 20415: loss = 3.5438068e-09,1.523687e-10\n",
      "Iteration 20420: loss = 3.5410341e-09,1.5237688e-10\n",
      "Iteration 20425: loss = 3.5247922e-09,1.5238594e-10\n",
      "Iteration 20430: loss = 3.5220478e-09,1.5239403e-10\n",
      "Iteration 20435: loss = 3.5058578e-09,1.5240328e-10\n",
      "Iteration 20440: loss = 3.5030467e-09,1.5241128e-10\n",
      "Iteration 20445: loss = 3.4936722e-09,1.5241916e-10\n",
      "Iteration 20450: loss = 3.4842447e-09,1.5242693e-10\n",
      "Iteration 20455: loss = 3.4815757e-09,1.5243426e-10\n",
      "Iteration 20460: loss = 3.4723004e-09,1.5244139e-10\n",
      "Iteration 20465: loss = 3.462947e-09,1.5244875e-10\n",
      "Iteration 20470: loss = 3.453687e-09,1.5245602e-10\n",
      "Iteration 20475: loss = 3.4443806e-09,1.5246328e-10\n",
      "Iteration 20480: loss = 3.4351546e-09,1.5247041e-10\n",
      "Iteration 20485: loss = 3.4325158e-09,1.5247777e-10\n",
      "Iteration 20490: loss = 3.4232268e-09,1.5248519e-10\n",
      "Iteration 20495: loss = 3.4140106e-09,1.5249307e-10\n",
      "Iteration 20500: loss = 3.4048355e-09,1.5250046e-10\n",
      "Iteration 20505: loss = 3.395608e-09,1.5250765e-10\n",
      "Iteration 20510: loss = 3.3929728e-09,1.5251529e-10\n",
      "Iteration 20515: loss = 3.3837437e-09,1.5252283e-10\n",
      "Iteration 20520: loss = 3.3745922e-09,1.5253068e-10\n",
      "Iteration 20525: loss = 3.3719927e-09,1.5253819e-10\n",
      "Iteration 20530: loss = 3.3562901e-09,1.5254571e-10\n",
      "Iteration 20535: loss = 3.3536922e-09,1.5255328e-10\n",
      "Iteration 20540: loss = 3.3381238e-09,1.5256063e-10\n",
      "Iteration 20545: loss = 3.3354868e-09,1.5256803e-10\n",
      "Iteration 20550: loss = 3.3329168e-09,1.5257542e-10\n",
      "Iteration 20555: loss = 3.3173417e-09,1.5258293e-10\n",
      "Iteration 20560: loss = 3.3147707e-09,1.525907e-10\n",
      "Iteration 20565: loss = 3.2992904e-09,1.5259818e-10\n",
      "Iteration 20570: loss = 3.2966732e-09,1.5260573e-10\n",
      "Iteration 20575: loss = 3.2941392e-09,1.526133e-10\n",
      "Iteration 20580: loss = 3.2785274e-09,1.5262315e-10\n",
      "Iteration 20585: loss = 3.2756244e-09,1.526352e-10\n",
      "Iteration 20590: loss = 3.2599523e-09,1.5264755e-10\n",
      "Iteration 20595: loss = 3.2570586e-09,1.5265955e-10\n",
      "Iteration 20600: loss = 3.2477987e-09,1.5267217e-10\n",
      "Iteration 20605: loss = 3.238523e-09,1.5268548e-10\n",
      "Iteration 20610: loss = 3.2292036e-09,1.5269849e-10\n",
      "Iteration 20615: loss = 3.2200174e-09,1.5271075e-10\n",
      "Iteration 20620: loss = 3.2110214e-09,1.5272023e-10\n",
      "Iteration 20625: loss = 3.2084815e-09,1.5272858e-10\n",
      "Iteration 20630: loss = 3.1995917e-09,1.5273657e-10\n",
      "Iteration 20635: loss = 3.1907075e-09,1.5274512e-10\n",
      "Iteration 20640: loss = 3.181838e-09,1.5275346e-10\n",
      "Iteration 20645: loss = 3.1729872e-09,1.5276169e-10\n",
      "Iteration 20650: loss = 3.1704694e-09,1.527699e-10\n",
      "Iteration 20655: loss = 3.1553287e-09,1.5277832e-10\n",
      "Iteration 20660: loss = 3.1528193e-09,1.527868e-10\n",
      "Iteration 20665: loss = 3.1439502e-09,1.5279536e-10\n",
      "Iteration 20670: loss = 3.135136e-09,1.528043e-10\n",
      "Iteration 20675: loss = 3.1325718e-09,1.5281364e-10\n",
      "Iteration 20680: loss = 3.1175007e-09,1.528225e-10\n",
      "Iteration 20685: loss = 3.1149856e-09,1.5283141e-10\n",
      "Iteration 20690: loss = 3.1124827e-09,1.5284025e-10\n",
      "Iteration 20695: loss = 3.0974716e-09,1.528492e-10\n",
      "Iteration 20700: loss = 3.0949845e-09,1.5285796e-10\n",
      "Iteration 20705: loss = 3.0800331e-09,1.5286651e-10\n",
      "Iteration 20710: loss = 3.0775797e-09,1.5287496e-10\n",
      "Iteration 20715: loss = 3.0689191e-09,1.5288378e-10\n",
      "Iteration 20720: loss = 3.0602694e-09,1.5289178e-10\n",
      "Iteration 20725: loss = 3.0578438e-09,1.5290039e-10\n",
      "Iteration 20730: loss = 3.0492233e-09,1.5290853e-10\n",
      "Iteration 20735: loss = 3.0405995e-09,1.5291722e-10\n",
      "Iteration 20740: loss = 3.0319904e-09,1.5292556e-10\n",
      "Iteration 20745: loss = 3.0234002e-09,1.5293408e-10\n",
      "Iteration 20750: loss = 3.0209975e-09,1.5294288e-10\n",
      "Iteration 20755: loss = 3.0124088e-09,1.5295142e-10\n",
      "Iteration 20760: loss = 3.0038574e-09,1.5295987e-10\n",
      "Iteration 20765: loss = 2.9953184e-09,1.5296851e-10\n",
      "Iteration 20770: loss = 2.9928584e-09,1.5297724e-10\n",
      "Iteration 20775: loss = 2.9843836e-09,1.5298525e-10\n",
      "Iteration 20780: loss = 2.9759102e-09,1.5299342e-10\n",
      "Iteration 20785: loss = 2.967515e-09,1.5300056e-10\n",
      "Iteration 20790: loss = 2.9588056e-09,1.5301264e-10\n",
      "Iteration 20795: loss = 2.9563652e-09,1.530227e-10\n",
      "Iteration 20800: loss = 2.9478313e-09,1.5303231e-10\n",
      "Iteration 20805: loss = 2.9393836e-09,1.5304188e-10\n",
      "Iteration 20810: loss = 2.9369824e-09,1.5305106e-10\n",
      "Iteration 20815: loss = 2.9224196e-09,1.5306029e-10\n",
      "Iteration 20820: loss = 2.9200757e-09,1.5307014e-10\n",
      "Iteration 20825: loss = 2.9116254e-09,1.530793e-10\n",
      "Iteration 20830: loss = 2.9031835e-09,1.5308897e-10\n",
      "Iteration 20835: loss = 2.900771e-09,1.5309876e-10\n",
      "Iteration 20840: loss = 2.8863765e-09,1.5310833e-10\n",
      "Iteration 20845: loss = 2.8840048e-09,1.5311803e-10\n",
      "Iteration 20850: loss = 2.87564e-09,1.5312684e-10\n",
      "Iteration 20855: loss = 2.867342e-09,1.5313582e-10\n",
      "Iteration 20860: loss = 2.8649885e-09,1.5314552e-10\n",
      "Iteration 20865: loss = 2.8506413e-09,1.5315488e-10\n",
      "Iteration 20870: loss = 2.8482983e-09,1.5316448e-10\n",
      "Iteration 20875: loss = 2.84001e-09,1.5317438e-10\n",
      "Iteration 20880: loss = 2.8316933e-09,1.5318409e-10\n",
      "Iteration 20885: loss = 2.8293579e-09,1.5319379e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20890: loss = 2.821074e-09,1.5320323e-10\n",
      "Iteration 20895: loss = 2.8128673e-09,1.5321253e-10\n",
      "Iteration 20900: loss = 2.804608e-09,1.5322213e-10\n",
      "Iteration 20905: loss = 2.8023086e-09,1.5323168e-10\n",
      "Iteration 20910: loss = 2.794138e-09,1.5324086e-10\n",
      "Iteration 20915: loss = 2.7859564e-09,1.5324984e-10\n",
      "Iteration 20920: loss = 2.777801e-09,1.5325856e-10\n",
      "Iteration 20925: loss = 2.769659e-09,1.5326719e-10\n",
      "Iteration 20930: loss = 2.7676463e-09,1.5327306e-10\n",
      "Iteration 20935: loss = 2.7596878e-09,1.5327953e-10\n",
      "Iteration 20940: loss = 2.7516498e-09,1.5328727e-10\n",
      "Iteration 20945: loss = 2.7496052e-09,1.5329356e-10\n",
      "Iteration 20950: loss = 2.7415197e-09,1.5330232e-10\n",
      "Iteration 20955: loss = 2.7335647e-09,1.5330935e-10\n",
      "Iteration 20960: loss = 2.7315228e-09,1.5331511e-10\n",
      "Iteration 20965: loss = 2.7236462e-09,1.5332169e-10\n",
      "Iteration 20970: loss = 2.7158082e-09,1.5332734e-10\n",
      "Iteration 20975: loss = 2.7137814e-09,1.5333333e-10\n",
      "Iteration 20980: loss = 2.6999984e-09,1.5334091e-10\n",
      "Iteration 20985: loss = 2.6980231e-09,1.5334695e-10\n",
      "Iteration 20990: loss = 2.6960443e-09,1.53353e-10\n",
      "Iteration 20995: loss = 2.6823856e-09,1.5335896e-10\n",
      "Iteration 21000: loss = 2.680423e-09,1.5336493e-10\n",
      "Iteration 21005: loss = 2.6784157e-09,1.533709e-10\n",
      "Iteration 21010: loss = 2.6647218e-09,1.5337884e-10\n",
      "Iteration 21015: loss = 2.6626747e-09,1.5338676e-10\n",
      "Iteration 21020: loss = 2.6604645e-09,1.5339738e-10\n",
      "Iteration 21025: loss = 2.6466624e-09,1.5340754e-10\n",
      "Iteration 21030: loss = 2.6444917e-09,1.5341796e-10\n",
      "Iteration 21035: loss = 2.6365183e-09,1.5342787e-10\n",
      "Iteration 21040: loss = 2.6286218e-09,1.5343771e-10\n",
      "Iteration 21045: loss = 2.6264928e-09,1.5344724e-10\n",
      "Iteration 21050: loss = 2.6185807e-09,1.5345684e-10\n",
      "Iteration 21055: loss = 2.610711e-09,1.5346671e-10\n",
      "Iteration 21060: loss = 2.6028397e-09,1.5347704e-10\n",
      "Iteration 21065: loss = 2.6006346e-09,1.5348775e-10\n",
      "Iteration 21070: loss = 2.5927587e-09,1.5349805e-10\n",
      "Iteration 21075: loss = 2.5848441e-09,1.5350915e-10\n",
      "Iteration 21080: loss = 2.5770062e-09,1.5351977e-10\n",
      "Iteration 21085: loss = 2.5691742e-09,1.535304e-10\n",
      "Iteration 21090: loss = 2.5670988e-09,1.5353965e-10\n",
      "Iteration 21095: loss = 2.5593099e-09,1.5354973e-10\n",
      "Iteration 21100: loss = 2.551575e-09,1.5355883e-10\n",
      "Iteration 21105: loss = 2.549698e-09,1.5356465e-10\n",
      "Iteration 21110: loss = 2.5421547e-09,1.5357088e-10\n",
      "Iteration 21115: loss = 2.5346087e-09,1.5357725e-10\n",
      "Iteration 21120: loss = 2.532694e-09,1.5358406e-10\n",
      "Iteration 21125: loss = 2.525174e-09,1.5359067e-10\n",
      "Iteration 21130: loss = 2.517659e-09,1.5359719e-10\n",
      "Iteration 21135: loss = 2.5157483e-09,1.5360441e-10\n",
      "Iteration 21140: loss = 2.5082791e-09,1.5361033e-10\n",
      "Iteration 21145: loss = 2.5007552e-09,1.5361654e-10\n",
      "Iteration 21150: loss = 2.4988698e-09,1.5362335e-10\n",
      "Iteration 21155: loss = 2.4970053e-09,1.5363023e-10\n",
      "Iteration 21160: loss = 2.483965e-09,1.5363696e-10\n",
      "Iteration 21165: loss = 2.4820535e-09,1.5364385e-10\n",
      "Iteration 21170: loss = 2.4746125e-09,1.5365095e-10\n",
      "Iteration 21175: loss = 2.467146e-09,1.5365811e-10\n",
      "Iteration 21180: loss = 2.4652784e-09,1.5366516e-10\n",
      "Iteration 21185: loss = 2.4578526e-09,1.5367224e-10\n",
      "Iteration 21190: loss = 2.4504205e-09,1.5367953e-10\n",
      "Iteration 21195: loss = 2.4485596e-09,1.5368698e-10\n",
      "Iteration 21200: loss = 2.4411146e-09,1.5369372e-10\n",
      "Iteration 21205: loss = 2.4337738e-09,1.5370012e-10\n",
      "Iteration 21210: loss = 2.4319564e-09,1.5370663e-10\n",
      "Iteration 21215: loss = 2.4246234e-09,1.5371314e-10\n",
      "Iteration 21220: loss = 2.4173064e-09,1.5371948e-10\n",
      "Iteration 21225: loss = 2.4155131e-09,1.5372612e-10\n",
      "Iteration 21230: loss = 2.4082045e-09,1.5373237e-10\n",
      "Iteration 21235: loss = 2.4009246e-09,1.5373872e-10\n",
      "Iteration 21240: loss = 2.3991424e-09,1.5374496e-10\n",
      "Iteration 21245: loss = 2.3918711e-09,1.5375155e-10\n",
      "Iteration 21250: loss = 2.3901034e-09,1.5375787e-10\n",
      "Iteration 21255: loss = 2.382846e-09,1.5376428e-10\n",
      "Iteration 21260: loss = 2.3756044e-09,1.5377044e-10\n",
      "Iteration 21265: loss = 2.373853e-09,1.5377719e-10\n",
      "Iteration 21270: loss = 2.366635e-09,1.5378306e-10\n",
      "Iteration 21275: loss = 2.3593327e-09,1.5379138e-10\n",
      "Iteration 21280: loss = 2.3573647e-09,1.5380211e-10\n",
      "Iteration 21285: loss = 2.3498987e-09,1.5381307e-10\n",
      "Iteration 21290: loss = 2.3425608e-09,1.5382293e-10\n",
      "Iteration 21295: loss = 2.340738e-09,1.5383095e-10\n",
      "Iteration 21300: loss = 2.3279798e-09,1.5384102e-10\n",
      "Iteration 21305: loss = 2.3261524e-09,1.5384923e-10\n",
      "Iteration 21310: loss = 2.3243873e-09,1.5385714e-10\n",
      "Iteration 21315: loss = 2.3117812e-09,1.5386514e-10\n",
      "Iteration 21320: loss = 2.309978e-09,1.5387296e-10\n",
      "Iteration 21325: loss = 2.308241e-09,1.5388081e-10\n",
      "Iteration 21330: loss = 2.2956825e-09,1.5388842e-10\n",
      "Iteration 21335: loss = 2.2939208e-09,1.538955e-10\n",
      "Iteration 21340: loss = 2.2921665e-09,1.5390302e-10\n",
      "Iteration 21345: loss = 2.279748e-09,1.5390988e-10\n",
      "Iteration 21350: loss = 2.2780078e-09,1.5391716e-10\n",
      "Iteration 21355: loss = 2.2762547e-09,1.539247e-10\n",
      "Iteration 21360: loss = 2.2692122e-09,1.5393198e-10\n",
      "Iteration 21365: loss = 2.2621294e-09,1.5393942e-10\n",
      "Iteration 21370: loss = 2.2603501e-09,1.5394774e-10\n",
      "Iteration 21375: loss = 2.2532773e-09,1.5395518e-10\n",
      "Iteration 21380: loss = 2.2462736e-09,1.5396272e-10\n",
      "Iteration 21385: loss = 2.2445352e-09,1.5397042e-10\n",
      "Iteration 21390: loss = 2.2375455e-09,1.5397694e-10\n",
      "Iteration 21395: loss = 2.2305677e-09,1.5398353e-10\n",
      "Iteration 21400: loss = 2.2289066e-09,1.539906e-10\n",
      "Iteration 21405: loss = 2.221914e-09,1.5399815e-10\n",
      "Iteration 21410: loss = 2.2149327e-09,1.5400531e-10\n",
      "Iteration 21415: loss = 2.2133018e-09,1.5401232e-10\n",
      "Iteration 21420: loss = 2.206346e-09,1.5401938e-10\n",
      "Iteration 21425: loss = 2.2046809e-09,1.5402636e-10\n",
      "Iteration 21430: loss = 2.1977384e-09,1.5403369e-10\n",
      "Iteration 21435: loss = 2.1908486e-09,1.5404059e-10\n",
      "Iteration 21440: loss = 2.1892212e-09,1.5404708e-10\n",
      "Iteration 21445: loss = 2.182387e-09,1.5405303e-10\n",
      "Iteration 21450: loss = 2.1754765e-09,1.5406043e-10\n",
      "Iteration 21455: loss = 2.174013e-09,1.540642e-10\n",
      "Iteration 21460: loss = 2.1725102e-09,1.540681e-10\n",
      "Iteration 21465: loss = 2.1656634e-09,1.5407457e-10\n",
      "Iteration 21470: loss = 2.1588382e-09,1.5408166e-10\n",
      "Iteration 21475: loss = 2.1572053e-09,1.5408895e-10\n",
      "Iteration 21480: loss = 2.1503623e-09,1.5409604e-10\n",
      "Iteration 21485: loss = 2.1436135e-09,1.5410145e-10\n",
      "Iteration 21490: loss = 2.1421915e-09,1.5410509e-10\n",
      "Iteration 21495: loss = 2.1407243e-09,1.5410845e-10\n",
      "Iteration 21500: loss = 2.1287707e-09,1.541153e-10\n",
      "Iteration 21505: loss = 2.1273043e-09,1.5411919e-10\n",
      "Iteration 21510: loss = 2.1258766e-09,1.5412281e-10\n",
      "Iteration 21515: loss = 2.1191626e-09,1.541286e-10\n",
      "Iteration 21520: loss = 2.1125444e-09,1.5413301e-10\n",
      "Iteration 21525: loss = 2.1110937e-09,1.5413634e-10\n",
      "Iteration 21530: loss = 2.1094957e-09,1.5414364e-10\n",
      "Iteration 21535: loss = 2.0976614e-09,1.5415033e-10\n",
      "Iteration 21540: loss = 2.0961923e-09,1.541543e-10\n",
      "Iteration 21545: loss = 2.0947948e-09,1.5415809e-10\n",
      "Iteration 21550: loss = 2.0881534e-09,1.5416347e-10\n",
      "Iteration 21555: loss = 2.0816364e-09,1.5416715e-10\n",
      "Iteration 21560: loss = 2.0801831e-09,1.5417106e-10\n",
      "Iteration 21565: loss = 2.0787823e-09,1.5417505e-10\n",
      "Iteration 21570: loss = 2.0671425e-09,1.5417856e-10\n",
      "Iteration 21575: loss = 2.0657505e-09,1.5418235e-10\n",
      "Iteration 21580: loss = 2.0643738e-09,1.5418593e-10\n",
      "Iteration 21585: loss = 2.0577982e-09,1.5419158e-10\n",
      "Iteration 21590: loss = 2.0513047e-09,1.5419471e-10\n",
      "Iteration 21595: loss = 2.0499387e-09,1.5419829e-10\n",
      "Iteration 21600: loss = 2.0485402e-09,1.5420178e-10\n",
      "Iteration 21605: loss = 2.042088e-09,1.5420562e-10\n",
      "Iteration 21610: loss = 2.0356572e-09,1.5420908e-10\n",
      "Iteration 21615: loss = 2.0342437e-09,1.54213e-10\n",
      "Iteration 21620: loss = 2.0278e-09,1.5421714e-10\n",
      "Iteration 21625: loss = 2.0264086e-09,1.5422086e-10\n",
      "Iteration 21630: loss = 2.0200046e-09,1.5422465e-10\n",
      "Iteration 21635: loss = 2.0186548e-09,1.5422798e-10\n",
      "Iteration 21640: loss = 2.0122242e-09,1.542319e-10\n",
      "Iteration 21645: loss = 2.010881e-09,1.5423543e-10\n",
      "Iteration 21650: loss = 2.0044542e-09,1.5423934e-10\n",
      "Iteration 21655: loss = 1.998078e-09,1.5424331e-10\n",
      "Iteration 21660: loss = 1.9967337e-09,1.5424723e-10\n",
      "Iteration 21665: loss = 1.9953676e-09,1.5425067e-10\n",
      "Iteration 21670: loss = 1.9890414e-09,1.5425401e-10\n",
      "Iteration 21675: loss = 1.9826831e-09,1.5425763e-10\n",
      "Iteration 21680: loss = 1.9813562e-09,1.5426094e-10\n",
      "Iteration 21685: loss = 1.9750446e-09,1.5426452e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21690: loss = 1.973673e-09,1.542686e-10\n",
      "Iteration 21695: loss = 1.9673627e-09,1.5427239e-10\n",
      "Iteration 21700: loss = 1.9660156e-09,1.542761e-10\n",
      "Iteration 21705: loss = 1.9597346e-09,1.5427959e-10\n",
      "Iteration 21710: loss = 1.9534625e-09,1.54283e-10\n",
      "Iteration 21715: loss = 1.952113e-09,1.542871e-10\n",
      "Iteration 21720: loss = 1.9507809e-09,1.5429133e-10\n",
      "Iteration 21725: loss = 1.9444448e-09,1.5429591e-10\n",
      "Iteration 21730: loss = 1.938171e-09,1.5430003e-10\n",
      "Iteration 21735: loss = 1.9368611e-09,1.5430399e-10\n",
      "Iteration 21740: loss = 1.9355315e-09,1.5430793e-10\n",
      "Iteration 21745: loss = 1.9244009e-09,1.5431079e-10\n",
      "Iteration 21750: loss = 1.923057e-09,1.5431491e-10\n",
      "Iteration 21755: loss = 1.9217223e-09,1.5431964e-10\n",
      "Iteration 21760: loss = 1.9204167e-09,1.5432405e-10\n",
      "Iteration 21765: loss = 1.909232e-09,1.5432874e-10\n",
      "Iteration 21770: loss = 1.9077389e-09,1.5433747e-10\n",
      "Iteration 21775: loss = 1.9062247e-09,1.543462e-10\n",
      "Iteration 21780: loss = 1.899868e-09,1.5435442e-10\n",
      "Iteration 21785: loss = 1.8935153e-09,1.543627e-10\n",
      "Iteration 21790: loss = 1.8921253e-09,1.5436907e-10\n",
      "Iteration 21795: loss = 1.8906894e-09,1.5437637e-10\n",
      "Iteration 21800: loss = 1.8794752e-09,1.5438423e-10\n",
      "Iteration 21805: loss = 1.87813e-09,1.5438974e-10\n",
      "Iteration 21810: loss = 1.8767903e-09,1.5439536e-10\n",
      "Iteration 21815: loss = 1.8704724e-09,1.5440427e-10\n",
      "Iteration 21820: loss = 1.8641813e-09,1.544119e-10\n",
      "Iteration 21825: loss = 1.8628203e-09,1.5441792e-10\n",
      "Iteration 21830: loss = 1.8565337e-09,1.5442676e-10\n",
      "Iteration 21835: loss = 1.8502865e-09,1.5443424e-10\n",
      "Iteration 21840: loss = 1.8489758e-09,1.5443949e-10\n",
      "Iteration 21845: loss = 1.8476776e-09,1.5444468e-10\n",
      "Iteration 21850: loss = 1.8415562e-09,1.5444977e-10\n",
      "Iteration 21855: loss = 1.8354841e-09,1.5445399e-10\n",
      "Iteration 21860: loss = 1.8342097e-09,1.544585e-10\n",
      "Iteration 21865: loss = 1.8281333e-09,1.5446333e-10\n",
      "Iteration 21870: loss = 1.8268717e-09,1.544679e-10\n",
      "Iteration 21875: loss = 1.8207996e-09,1.5447292e-10\n",
      "Iteration 21880: loss = 1.8195393e-09,1.5447746e-10\n",
      "Iteration 21885: loss = 1.8134815e-09,1.5448168e-10\n",
      "Iteration 21890: loss = 1.8122311e-09,1.544861e-10\n",
      "Iteration 21895: loss = 1.8062066e-09,1.5449074e-10\n",
      "Iteration 21900: loss = 1.804971e-09,1.5449514e-10\n",
      "Iteration 21905: loss = 1.798964e-09,1.5449955e-10\n",
      "Iteration 21910: loss = 1.7977212e-09,1.5450392e-10\n",
      "Iteration 21915: loss = 1.7917312e-09,1.5450838e-10\n",
      "Iteration 21920: loss = 1.7857471e-09,1.5451306e-10\n",
      "Iteration 21925: loss = 1.784476e-09,1.5451836e-10\n",
      "Iteration 21930: loss = 1.7832139e-09,1.5452398e-10\n",
      "Iteration 21935: loss = 1.7772592e-09,1.5452802e-10\n",
      "Iteration 21940: loss = 1.7712914e-09,1.5453283e-10\n",
      "Iteration 21945: loss = 1.7700232e-09,1.5453873e-10\n",
      "Iteration 21950: loss = 1.7687589e-09,1.5454454e-10\n",
      "Iteration 21955: loss = 1.7627394e-09,1.5455012e-10\n",
      "Iteration 21960: loss = 1.7567698e-09,1.5455587e-10\n",
      "Iteration 21965: loss = 1.7555098e-09,1.5456174e-10\n",
      "Iteration 21970: loss = 1.7542581e-09,1.5456722e-10\n",
      "Iteration 21975: loss = 1.7436341e-09,1.5457263e-10\n",
      "Iteration 21980: loss = 1.7423956e-09,1.545779e-10\n",
      "Iteration 21985: loss = 1.7411833e-09,1.545829e-10\n",
      "Iteration 21990: loss = 1.7399744e-09,1.5458787e-10\n",
      "Iteration 21995: loss = 1.7294223e-09,1.5459238e-10\n",
      "Iteration 22000: loss = 1.7282206e-09,1.5459736e-10\n",
      "Iteration 22005: loss = 1.727052e-09,1.5460119e-10\n",
      "Iteration 22010: loss = 1.725859e-09,1.5460591e-10\n",
      "Iteration 22015: loss = 1.7153278e-09,1.5461038e-10\n",
      "Iteration 22020: loss = 1.7142433e-09,1.5461282e-10\n",
      "Iteration 22025: loss = 1.7131861e-09,1.5461384e-10\n",
      "Iteration 22030: loss = 1.7120718e-09,1.5461682e-10\n",
      "Iteration 22035: loss = 1.7016323e-09,1.5462101e-10\n",
      "Iteration 22040: loss = 1.7005483e-09,1.5462291e-10\n",
      "Iteration 22045: loss = 1.6993158e-09,1.5462866e-10\n",
      "Iteration 22050: loss = 1.6933952e-09,1.5463669e-10\n",
      "Iteration 22055: loss = 1.6875749e-09,1.5464303e-10\n",
      "Iteration 22060: loss = 1.6863558e-09,1.5464865e-10\n",
      "Iteration 22065: loss = 1.6851657e-09,1.5465407e-10\n",
      "Iteration 22070: loss = 1.6793728e-09,1.5465898e-10\n",
      "Iteration 22075: loss = 1.67361e-09,1.5466436e-10\n",
      "Iteration 22080: loss = 1.6723848e-09,1.5467082e-10\n",
      "Iteration 22085: loss = 1.6711733e-09,1.5467684e-10\n",
      "Iteration 22090: loss = 1.6653757e-09,1.5468292e-10\n",
      "Iteration 22095: loss = 1.659615e-09,1.5468898e-10\n",
      "Iteration 22100: loss = 1.6584077e-09,1.5469537e-10\n",
      "Iteration 22105: loss = 1.6572047e-09,1.5470107e-10\n",
      "Iteration 22110: loss = 1.6514408e-09,1.5470737e-10\n",
      "Iteration 22115: loss = 1.6457191e-09,1.547133e-10\n",
      "Iteration 22120: loss = 1.6445186e-09,1.547191e-10\n",
      "Iteration 22125: loss = 1.6387781e-09,1.5472555e-10\n",
      "Iteration 22130: loss = 1.6376234e-09,1.5473142e-10\n",
      "Iteration 22135: loss = 1.631916e-09,1.54737e-10\n",
      "Iteration 22140: loss = 1.6307332e-09,1.54743e-10\n",
      "Iteration 22145: loss = 1.6250455e-09,1.5474834e-10\n",
      "Iteration 22150: loss = 1.6239204e-09,1.5475365e-10\n",
      "Iteration 22155: loss = 1.6227321e-09,1.5476004e-10\n",
      "Iteration 22160: loss = 1.6170412e-09,1.5476606e-10\n",
      "Iteration 22165: loss = 1.6113669e-09,1.5477176e-10\n",
      "Iteration 22170: loss = 1.6103109e-09,1.5477503e-10\n",
      "Iteration 22175: loss = 1.6092608e-09,1.5477862e-10\n",
      "Iteration 22180: loss = 1.6035936e-09,1.547848e-10\n",
      "Iteration 22185: loss = 1.5979671e-09,1.5479028e-10\n",
      "Iteration 22190: loss = 1.5969466e-09,1.5479283e-10\n",
      "Iteration 22195: loss = 1.5959061e-09,1.5479554e-10\n",
      "Iteration 22200: loss = 1.5903208e-09,1.5480037e-10\n",
      "Iteration 22205: loss = 1.5847582e-09,1.5480492e-10\n",
      "Iteration 22210: loss = 1.583765e-09,1.5480739e-10\n",
      "Iteration 22215: loss = 1.5827287e-09,1.5481005e-10\n",
      "Iteration 22220: loss = 1.5816971e-09,1.5481337e-10\n",
      "Iteration 22225: loss = 1.5716881e-09,1.5481794e-10\n",
      "Iteration 22230: loss = 1.5707005e-09,1.5482017e-10\n",
      "Iteration 22235: loss = 1.5697043e-09,1.5482289e-10\n",
      "Iteration 22240: loss = 1.5687047e-09,1.5482468e-10\n",
      "Iteration 22245: loss = 1.5587451e-09,1.5482968e-10\n",
      "Iteration 22250: loss = 1.5577694e-09,1.5483193e-10\n",
      "Iteration 22255: loss = 1.5567426e-09,1.5483453e-10\n",
      "Iteration 22260: loss = 1.5557794e-09,1.5483657e-10\n",
      "Iteration 22265: loss = 1.5503957e-09,1.5483856e-10\n",
      "Iteration 22270: loss = 1.5449789e-09,1.5484064e-10\n",
      "Iteration 22275: loss = 1.5440283e-09,1.5484258e-10\n",
      "Iteration 22280: loss = 1.5430482e-09,1.548443e-10\n",
      "Iteration 22285: loss = 1.5420335e-09,1.5484843e-10\n",
      "Iteration 22290: loss = 1.532177e-09,1.548527e-10\n",
      "Iteration 22295: loss = 1.5312129e-09,1.5485442e-10\n",
      "Iteration 22300: loss = 1.5302701e-09,1.548564e-10\n",
      "Iteration 22305: loss = 1.5292924e-09,1.5485827e-10\n",
      "Iteration 22310: loss = 1.5239551e-09,1.5486049e-10\n",
      "Iteration 22315: loss = 1.5186177e-09,1.548629e-10\n",
      "Iteration 22320: loss = 1.5176206e-09,1.548655e-10\n",
      "Iteration 22325: loss = 1.5166814e-09,1.548677e-10\n",
      "Iteration 22330: loss = 1.5157166e-09,1.5486959e-10\n",
      "Iteration 22335: loss = 1.5103883e-09,1.548722e-10\n",
      "Iteration 22340: loss = 1.5050663e-09,1.5487538e-10\n",
      "Iteration 22345: loss = 1.5040968e-09,1.5487728e-10\n",
      "Iteration 22350: loss = 1.503146e-09,1.5488e-10\n",
      "Iteration 22355: loss = 1.497832e-09,1.5488229e-10\n",
      "Iteration 22360: loss = 1.4969097e-09,1.5488422e-10\n",
      "Iteration 22365: loss = 1.4916316e-09,1.5488553e-10\n",
      "Iteration 22370: loss = 1.4906888e-09,1.5488827e-10\n",
      "Iteration 22375: loss = 1.485398e-09,1.5489135e-10\n",
      "Iteration 22380: loss = 1.4844427e-09,1.5489354e-10\n",
      "Iteration 22385: loss = 1.479214e-09,1.5489521e-10\n",
      "Iteration 22390: loss = 1.478275e-09,1.5489696e-10\n",
      "Iteration 22395: loss = 1.4773699e-09,1.548989e-10\n",
      "Iteration 22400: loss = 1.4721664e-09,1.5490006e-10\n",
      "Iteration 22405: loss = 1.4712317e-09,1.5490206e-10\n",
      "Iteration 22410: loss = 1.465993e-09,1.5490476e-10\n",
      "Iteration 22415: loss = 1.4650264e-09,1.5490782e-10\n",
      "Iteration 22420: loss = 1.4598158e-09,1.5491025e-10\n",
      "Iteration 22425: loss = 1.4587944e-09,1.5491496e-10\n",
      "Iteration 22430: loss = 1.4577458e-09,1.5492149e-10\n",
      "Iteration 22435: loss = 1.4524467e-09,1.5492717e-10\n",
      "Iteration 22440: loss = 1.4470892e-09,1.549338e-10\n",
      "Iteration 22445: loss = 1.4460461e-09,1.5494035e-10\n",
      "Iteration 22450: loss = 1.4449926e-09,1.5494661e-10\n",
      "Iteration 22455: loss = 1.4397251e-09,1.5495236e-10\n",
      "Iteration 22460: loss = 1.4345142e-09,1.5495585e-10\n",
      "Iteration 22465: loss = 1.4335719e-09,1.5495964e-10\n",
      "Iteration 22470: loss = 1.432625e-09,1.549631e-10\n",
      "Iteration 22475: loss = 1.4316774e-09,1.5496718e-10\n",
      "Iteration 22480: loss = 1.4264753e-09,1.5497043e-10\n",
      "Iteration 22485: loss = 1.4212551e-09,1.5497567e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22490: loss = 1.4203123e-09,1.5497958e-10\n",
      "Iteration 22495: loss = 1.4193712e-09,1.5498361e-10\n",
      "Iteration 22500: loss = 1.4142044e-09,1.549871e-10\n",
      "Iteration 22505: loss = 1.4090425e-09,1.5499035e-10\n",
      "Iteration 22510: loss = 1.4081275e-09,1.549935e-10\n",
      "Iteration 22515: loss = 1.4072216e-09,1.5499672e-10\n",
      "Iteration 22520: loss = 1.4063325e-09,1.5499914e-10\n",
      "Iteration 22525: loss = 1.3969693e-09,1.5500408e-10\n",
      "Iteration 22530: loss = 1.3960699e-09,1.5500692e-10\n",
      "Iteration 22535: loss = 1.3951001e-09,1.550116e-10\n",
      "Iteration 22540: loss = 1.3941817e-09,1.5501511e-10\n",
      "Iteration 22545: loss = 1.3932969e-09,1.550176e-10\n",
      "Iteration 22550: loss = 1.3840499e-09,1.5502011e-10\n",
      "Iteration 22555: loss = 1.3831795e-09,1.550228e-10\n",
      "Iteration 22560: loss = 1.3823566e-09,1.5502342e-10\n",
      "Iteration 22565: loss = 1.3815143e-09,1.5502533e-10\n",
      "Iteration 22570: loss = 1.3806103e-09,1.5502905e-10\n",
      "Iteration 22575: loss = 1.3713649e-09,1.5503195e-10\n",
      "Iteration 22580: loss = 1.3704561e-09,1.5503591e-10\n",
      "Iteration 22585: loss = 1.3695436e-09,1.5503979e-10\n",
      "Iteration 22590: loss = 1.3686517e-09,1.5504295e-10\n",
      "Iteration 22595: loss = 1.3636581e-09,1.5504545e-10\n",
      "Iteration 22600: loss = 1.3586545e-09,1.5504772e-10\n",
      "Iteration 22605: loss = 1.3578264e-09,1.5504942e-10\n",
      "Iteration 22610: loss = 1.3570242e-09,1.5505e-10\n",
      "Iteration 22615: loss = 1.3561655e-09,1.5505304e-10\n",
      "Iteration 22620: loss = 1.3511247e-09,1.5505609e-10\n",
      "Iteration 22625: loss = 1.3461677e-09,1.5505844e-10\n",
      "Iteration 22630: loss = 1.3453826e-09,1.5505847e-10\n",
      "Iteration 22635: loss = 1.3445284e-09,1.5506109e-10\n",
      "Iteration 22640: loss = 1.3437019e-09,1.55063e-10\n",
      "Iteration 22645: loss = 1.338747e-09,1.5506532e-10\n",
      "Iteration 22650: loss = 1.3337725e-09,1.550685e-10\n",
      "Iteration 22655: loss = 1.3329751e-09,1.5506951e-10\n",
      "Iteration 22660: loss = 1.3321707e-09,1.5507098e-10\n",
      "Iteration 22665: loss = 1.3272045e-09,1.5507472e-10\n",
      "Iteration 22670: loss = 1.3264335e-09,1.5507479e-10\n",
      "Iteration 22675: loss = 1.3215696e-09,1.5507513e-10\n",
      "Iteration 22680: loss = 1.3207951e-09,1.5507615e-10\n",
      "Iteration 22685: loss = 1.3199531e-09,1.5507876e-10\n",
      "Iteration 22690: loss = 1.3150462e-09,1.5508132e-10\n",
      "Iteration 22695: loss = 1.3141759e-09,1.5508425e-10\n",
      "Iteration 22700: loss = 1.3092686e-09,1.5508705e-10\n",
      "Iteration 22705: loss = 1.3085185e-09,1.5508779e-10\n",
      "Iteration 22710: loss = 1.3077619e-09,1.5508776e-10\n",
      "Iteration 22715: loss = 1.3029507e-09,1.5508761e-10\n",
      "Iteration 22720: loss = 1.3022e-09,1.550876e-10\n",
      "Iteration 22725: loss = 1.3014785e-09,1.5508751e-10\n",
      "Iteration 22730: loss = 1.296672e-09,1.5508735e-10\n",
      "Iteration 22735: loss = 1.2959295e-09,1.5508726e-10\n",
      "Iteration 22740: loss = 1.2911174e-09,1.5508861e-10\n",
      "Iteration 22745: loss = 1.2902763e-09,1.550921e-10\n",
      "Iteration 22750: loss = 1.2895435e-09,1.5509179e-10\n",
      "Iteration 22755: loss = 1.2847483e-09,1.5509286e-10\n",
      "Iteration 22760: loss = 1.2840103e-09,1.5509276e-10\n",
      "Iteration 22765: loss = 1.279246e-09,1.5509277e-10\n",
      "Iteration 22770: loss = 1.2784991e-09,1.5509288e-10\n",
      "Iteration 22775: loss = 1.2777694e-09,1.550934e-10\n",
      "Iteration 22780: loss = 1.277015e-09,1.5509405e-10\n",
      "Iteration 22785: loss = 1.2722666e-09,1.550939e-10\n",
      "Iteration 22790: loss = 1.2714491e-09,1.5509682e-10\n",
      "Iteration 22795: loss = 1.2666083e-09,1.5510113e-10\n",
      "Iteration 22800: loss = 1.2657364e-09,1.5510596e-10\n",
      "Iteration 22805: loss = 1.2648851e-09,1.5511016e-10\n",
      "Iteration 22810: loss = 1.2600956e-09,1.5511338e-10\n",
      "Iteration 22815: loss = 1.2552981e-09,1.5511666e-10\n",
      "Iteration 22820: loss = 1.2544207e-09,1.5512173e-10\n",
      "Iteration 22825: loss = 1.2536585e-09,1.5512387e-10\n",
      "Iteration 22830: loss = 1.2527942e-09,1.5512897e-10\n",
      "Iteration 22835: loss = 1.2479765e-09,1.5513327e-10\n",
      "Iteration 22840: loss = 1.2431642e-09,1.5513787e-10\n",
      "Iteration 22845: loss = 1.2424263e-09,1.5513936e-10\n",
      "Iteration 22850: loss = 1.2416507e-09,1.5514098e-10\n",
      "Iteration 22855: loss = 1.2408342e-09,1.551455e-10\n",
      "Iteration 22860: loss = 1.236054e-09,1.5514962e-10\n",
      "Iteration 22865: loss = 1.2312822e-09,1.5515353e-10\n",
      "Iteration 22870: loss = 1.2304449e-09,1.5515836e-10\n",
      "Iteration 22875: loss = 1.2296365e-09,1.5516263e-10\n",
      "Iteration 22880: loss = 1.2288134e-09,1.5516677e-10\n",
      "Iteration 22885: loss = 1.2201476e-09,1.551706e-10\n",
      "Iteration 22890: loss = 1.2193944e-09,1.5517264e-10\n",
      "Iteration 22895: loss = 1.2186558e-09,1.5517455e-10\n",
      "Iteration 22900: loss = 1.2178826e-09,1.5517686e-10\n",
      "Iteration 22905: loss = 1.2171403e-09,1.5517908e-10\n",
      "Iteration 22910: loss = 1.2125189e-09,1.5517891e-10\n",
      "Iteration 22915: loss = 1.2079137e-09,1.5517929e-10\n",
      "Iteration 22920: loss = 1.2072066e-09,1.551807e-10\n",
      "Iteration 22925: loss = 1.2064614e-09,1.551822e-10\n",
      "Iteration 22930: loss = 1.2057529e-09,1.5518317e-10\n",
      "Iteration 22935: loss = 1.2050386e-09,1.5518448e-10\n",
      "Iteration 22940: loss = 1.1964825e-09,1.5518753e-10\n",
      "Iteration 22945: loss = 1.1957746e-09,1.5518842e-10\n",
      "Iteration 22950: loss = 1.1950486e-09,1.551897e-10\n",
      "Iteration 22955: loss = 1.1943592e-09,1.5519071e-10\n",
      "Iteration 22960: loss = 1.1936354e-09,1.5519157e-10\n",
      "Iteration 22965: loss = 1.189019e-09,1.5519425e-10\n",
      "Iteration 22970: loss = 1.1844291e-09,1.5519618e-10\n",
      "Iteration 22975: loss = 1.1837175e-09,1.551971e-10\n",
      "Iteration 22980: loss = 1.183028e-09,1.5519824e-10\n",
      "Iteration 22985: loss = 1.1823348e-09,1.5519812e-10\n",
      "Iteration 22990: loss = 1.1777826e-09,1.5519931e-10\n",
      "Iteration 22995: loss = 1.1771047e-09,1.5519985e-10\n",
      "Iteration 23000: loss = 1.172564e-09,1.5520067e-10\n",
      "Iteration 23005: loss = 1.1719438e-09,1.5519891e-10\n",
      "Iteration 23010: loss = 1.1713109e-09,1.5519779e-10\n",
      "Iteration 23015: loss = 1.1706157e-09,1.5519927e-10\n",
      "Iteration 23020: loss = 1.1660619e-09,1.5520046e-10\n",
      "Iteration 23025: loss = 1.1653791e-09,1.5520198e-10\n",
      "Iteration 23030: loss = 1.1608404e-09,1.5520296e-10\n",
      "Iteration 23035: loss = 1.1601579e-09,1.5520452e-10\n",
      "Iteration 23040: loss = 1.1594846e-09,1.5520493e-10\n",
      "Iteration 23045: loss = 1.1549691e-09,1.5520607e-10\n",
      "Iteration 23050: loss = 1.1543049e-09,1.5520712e-10\n",
      "Iteration 23055: loss = 1.149815e-09,1.5520722e-10\n",
      "Iteration 23060: loss = 1.1491607e-09,1.5520765e-10\n",
      "Iteration 23065: loss = 1.148479e-09,1.5520815e-10\n",
      "Iteration 23070: loss = 1.1478193e-09,1.5520907e-10\n",
      "Iteration 23075: loss = 1.1433511e-09,1.5520991e-10\n",
      "Iteration 23080: loss = 1.14256e-09,1.5521485e-10\n",
      "Iteration 23085: loss = 1.1380317e-09,1.552187e-10\n",
      "Iteration 23090: loss = 1.1372531e-09,1.5522332e-10\n",
      "Iteration 23095: loss = 1.1326899e-09,1.5522879e-10\n",
      "Iteration 23100: loss = 1.1319946e-09,1.5523094e-10\n",
      "Iteration 23105: loss = 1.1312675e-09,1.5523419e-10\n",
      "Iteration 23110: loss = 1.1267753e-09,1.5523774e-10\n",
      "Iteration 23115: loss = 1.1260474e-09,1.5524089e-10\n",
      "Iteration 23120: loss = 1.1253563e-09,1.55243e-10\n",
      "Iteration 23125: loss = 1.1208922e-09,1.5524573e-10\n",
      "Iteration 23130: loss = 1.1202085e-09,1.5524784e-10\n",
      "Iteration 23135: loss = 1.1195062e-09,1.5524991e-10\n",
      "Iteration 23140: loss = 1.1150717e-09,1.5525159e-10\n",
      "Iteration 23145: loss = 1.1143727e-09,1.552546e-10\n",
      "Iteration 23150: loss = 1.1099642e-09,1.5525645e-10\n",
      "Iteration 23155: loss = 1.1093154e-09,1.552567e-10\n",
      "Iteration 23160: loss = 1.1086402e-09,1.5525882e-10\n",
      "Iteration 23165: loss = 1.1079935e-09,1.5526008e-10\n",
      "Iteration 23170: loss = 1.1036149e-09,1.552608e-10\n",
      "Iteration 23175: loss = 1.0992194e-09,1.5526257e-10\n",
      "Iteration 23180: loss = 1.0985844e-09,1.5526291e-10\n",
      "Iteration 23185: loss = 1.0979323e-09,1.5526425e-10\n",
      "Iteration 23190: loss = 1.0972658e-09,1.5526637e-10\n",
      "Iteration 23195: loss = 1.0966142e-09,1.5526758e-10\n",
      "Iteration 23200: loss = 1.09227e-09,1.5526824e-10\n",
      "Iteration 23205: loss = 1.0879065e-09,1.5527006e-10\n",
      "Iteration 23210: loss = 1.0873286e-09,1.5526824e-10\n",
      "Iteration 23215: loss = 1.0867501e-09,1.5526705e-10\n",
      "Iteration 23220: loss = 1.0861724e-09,1.55266e-10\n",
      "Iteration 23225: loss = 1.085528e-09,1.5526752e-10\n",
      "Iteration 23230: loss = 1.0811915e-09,1.5526785e-10\n",
      "Iteration 23235: loss = 1.076872e-09,1.5526896e-10\n",
      "Iteration 23240: loss = 1.0763075e-09,1.5526776e-10\n",
      "Iteration 23245: loss = 1.0757325e-09,1.5526636e-10\n",
      "Iteration 23250: loss = 1.0751591e-09,1.5526527e-10\n",
      "Iteration 23255: loss = 1.0745532e-09,1.5526519e-10\n",
      "Iteration 23260: loss = 1.0739439e-09,1.5526555e-10\n",
      "Iteration 23265: loss = 1.0659945e-09,1.552656e-10\n",
      "Iteration 23270: loss = 1.0653797e-09,1.5526568e-10\n",
      "Iteration 23275: loss = 1.0648081e-09,1.5526441e-10\n",
      "Iteration 23280: loss = 1.0642569e-09,1.5526305e-10\n",
      "Iteration 23285: loss = 1.0637013e-09,1.552614e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23290: loss = 1.063086e-09,1.5526219e-10\n",
      "Iteration 23295: loss = 1.0551856e-09,1.55262e-10\n",
      "Iteration 23300: loss = 1.0546038e-09,1.5526125e-10\n",
      "Iteration 23305: loss = 1.0540354e-09,1.5525983e-10\n",
      "Iteration 23310: loss = 1.0534863e-09,1.5525867e-10\n",
      "Iteration 23315: loss = 1.052927e-09,1.5525725e-10\n",
      "Iteration 23320: loss = 1.052344e-09,1.552567e-10\n",
      "Iteration 23325: loss = 1.0481064e-09,1.5525678e-10\n",
      "Iteration 23330: loss = 1.0439145e-09,1.5525625e-10\n",
      "Iteration 23335: loss = 1.0433612e-09,1.5525478e-10\n",
      "Iteration 23340: loss = 1.0428058e-09,1.5525323e-10\n",
      "Iteration 23345: loss = 1.0422758e-09,1.5525155e-10\n",
      "Iteration 23350: loss = 1.0417341e-09,1.552495e-10\n",
      "Iteration 23355: loss = 1.0374986e-09,1.5525095e-10\n",
      "Iteration 23360: loss = 1.0333073e-09,1.5525027e-10\n",
      "Iteration 23365: loss = 1.0327649e-09,1.5524856e-10\n",
      "Iteration 23370: loss = 1.0322326e-09,1.5524744e-10\n",
      "Iteration 23375: loss = 1.0316829e-09,1.5524573e-10\n",
      "Iteration 23380: loss = 1.0311437e-09,1.5524387e-10\n",
      "Iteration 23385: loss = 1.0306104e-09,1.5524193e-10\n",
      "Iteration 23390: loss = 1.026485e-09,1.5524002e-10\n",
      "Iteration 23395: loss = 1.0223663e-09,1.5523781e-10\n",
      "Iteration 23400: loss = 1.0217648e-09,1.5523904e-10\n",
      "Iteration 23405: loss = 1.0211079e-09,1.5524203e-10\n",
      "Iteration 23410: loss = 1.0204887e-09,1.5524501e-10\n",
      "Iteration 23415: loss = 1.0198508e-09,1.5524752e-10\n",
      "Iteration 23420: loss = 1.0156351e-09,1.5525e-10\n",
      "Iteration 23425: loss = 1.0114296e-09,1.5525245e-10\n",
      "Iteration 23430: loss = 1.0107993e-09,1.5525488e-10\n",
      "Iteration 23435: loss = 1.0102247e-09,1.5525557e-10\n",
      "Iteration 23440: loss = 1.0095718e-09,1.5525918e-10\n",
      "Iteration 23445: loss = 1.0053892e-09,1.5526229e-10\n",
      "Iteration 23450: loss = 1.0047528e-09,1.5526527e-10\n",
      "Iteration 23455: loss = 1.0005664e-09,1.5526821e-10\n",
      "Iteration 23460: loss = 1.0000274e-09,1.5526777e-10\n",
      "Iteration 23465: loss = 9.99464e-10,1.5526738e-10\n",
      "Iteration 23470: loss = 9.989306e-10,1.5526658e-10\n",
      "Iteration 23475: loss = 9.94782e-10,1.5526865e-10\n",
      "Iteration 23480: loss = 9.941624e-10,1.552712e-10\n",
      "Iteration 23485: loss = 9.935479e-10,1.5527354e-10\n",
      "Iteration 23490: loss = 9.894247e-10,1.5527574e-10\n",
      "Iteration 23495: loss = 9.888725e-10,1.5527518e-10\n",
      "Iteration 23500: loss = 9.883638e-10,1.552739e-10\n",
      "Iteration 23505: loss = 9.842879e-10,1.5527302e-10\n",
      "Iteration 23510: loss = 9.837854e-10,1.5527146e-10\n",
      "Iteration 23515: loss = 9.832799e-10,1.5526994e-10\n",
      "Iteration 23520: loss = 9.792321e-10,1.5526848e-10\n",
      "Iteration 23525: loss = 9.787304e-10,1.5526727e-10\n",
      "Iteration 23530: loss = 9.782058e-10,1.5526566e-10\n",
      "Iteration 23535: loss = 9.776866e-10,1.5526544e-10\n",
      "Iteration 23540: loss = 9.736348e-10,1.5526463e-10\n",
      "Iteration 23545: loss = 9.731255e-10,1.5526355e-10\n",
      "Iteration 23550: loss = 9.726162e-10,1.5526265e-10\n",
      "Iteration 23555: loss = 9.685983e-10,1.5526133e-10\n",
      "Iteration 23560: loss = 9.680882e-10,1.5526014e-10\n",
      "Iteration 23565: loss = 9.675617e-10,1.5525944e-10\n",
      "Iteration 23570: loss = 9.635616e-10,1.5525853e-10\n",
      "Iteration 23575: loss = 9.630277e-10,1.5525799e-10\n",
      "Iteration 23580: loss = 9.6251e-10,1.5525745e-10\n",
      "Iteration 23585: loss = 9.619823e-10,1.5525667e-10\n",
      "Iteration 23590: loss = 9.579949e-10,1.5525595e-10\n",
      "Iteration 23595: loss = 9.574939e-10,1.5525495e-10\n",
      "Iteration 23600: loss = 9.569772e-10,1.5525356e-10\n",
      "Iteration 23605: loss = 9.530127e-10,1.5525244e-10\n",
      "Iteration 23610: loss = 9.524931e-10,1.5525156e-10\n",
      "Iteration 23615: loss = 9.519885e-10,1.5525083e-10\n",
      "Iteration 23620: loss = 9.514749e-10,1.5524958e-10\n",
      "Iteration 23625: loss = 9.475342e-10,1.5524769e-10\n",
      "Iteration 23630: loss = 9.470469e-10,1.5524657e-10\n",
      "Iteration 23635: loss = 9.430815e-10,1.5524552e-10\n",
      "Iteration 23640: loss = 9.425788e-10,1.5524472e-10\n",
      "Iteration 23645: loss = 9.420948e-10,1.5524254e-10\n",
      "Iteration 23650: loss = 9.416649e-10,1.5523874e-10\n",
      "Iteration 23655: loss = 9.411973e-10,1.5523594e-10\n",
      "Iteration 23660: loss = 9.407114e-10,1.552348e-10\n",
      "Iteration 23665: loss = 9.367721e-10,1.5523335e-10\n",
      "Iteration 23670: loss = 9.328253e-10,1.5523328e-10\n",
      "Iteration 23675: loss = 9.323096e-10,1.552335e-10\n",
      "Iteration 23680: loss = 9.317733e-10,1.5523338e-10\n",
      "Iteration 23685: loss = 9.31267e-10,1.5523333e-10\n",
      "Iteration 23690: loss = 9.30709e-10,1.5523591e-10\n",
      "Iteration 23695: loss = 9.301259e-10,1.5523845e-10\n",
      "Iteration 23700: loss = 9.2272606e-10,1.5524129e-10\n",
      "Iteration 23705: loss = 9.2219415e-10,1.5524215e-10\n",
      "Iteration 23710: loss = 9.2166497e-10,1.5524304e-10\n",
      "Iteration 23715: loss = 9.2113783e-10,1.5524404e-10\n",
      "Iteration 23720: loss = 9.2062863e-10,1.5524426e-10\n",
      "Iteration 23725: loss = 9.2012903e-10,1.5524397e-10\n",
      "Iteration 23730: loss = 9.162117e-10,1.5524418e-10\n",
      "Iteration 23735: loss = 9.1229774e-10,1.5524382e-10\n",
      "Iteration 23740: loss = 9.1179303e-10,1.5524423e-10\n",
      "Iteration 23745: loss = 9.1127905e-10,1.5524472e-10\n",
      "Iteration 23750: loss = 9.107694e-10,1.5524512e-10\n",
      "Iteration 23755: loss = 9.102778e-10,1.5524479e-10\n",
      "Iteration 23760: loss = 9.0979624e-10,1.5524397e-10\n",
      "Iteration 23765: loss = 9.0254176e-10,1.5524353e-10\n",
      "Iteration 23770: loss = 9.020498e-10,1.552434e-10\n",
      "Iteration 23775: loss = 9.0154995e-10,1.5524342e-10\n",
      "Iteration 23780: loss = 9.0108604e-10,1.5524165e-10\n",
      "Iteration 23785: loss = 9.006386e-10,1.552394e-10\n",
      "Iteration 23790: loss = 9.00154e-10,1.5523896e-10\n",
      "Iteration 23795: loss = 8.996785e-10,1.5523813e-10\n",
      "Iteration 23800: loss = 8.958499e-10,1.552366e-10\n",
      "Iteration 23805: loss = 8.920087e-10,1.5523542e-10\n",
      "Iteration 23810: loss = 8.916012e-10,1.5523208e-10\n",
      "Iteration 23815: loss = 8.9117375e-10,1.5522886e-10\n",
      "Iteration 23820: loss = 8.9075086e-10,1.5522533e-10\n",
      "Iteration 23825: loss = 8.9026325e-10,1.552256e-10\n",
      "Iteration 23830: loss = 8.897873e-10,1.5522479e-10\n",
      "Iteration 23835: loss = 8.8598484e-10,1.5522325e-10\n",
      "Iteration 23840: loss = 8.821868e-10,1.5522203e-10\n",
      "Iteration 23845: loss = 8.817353e-10,1.5522009e-10\n",
      "Iteration 23850: loss = 8.8130503e-10,1.5521742e-10\n",
      "Iteration 23855: loss = 8.808822e-10,1.5521502e-10\n",
      "Iteration 23860: loss = 8.804557e-10,1.5521197e-10\n",
      "Iteration 23865: loss = 8.8004065e-10,1.5520868e-10\n",
      "Iteration 23870: loss = 8.7959734e-10,1.5520696e-10\n",
      "Iteration 23875: loss = 8.7579083e-10,1.5520693e-10\n",
      "Iteration 23880: loss = 8.7199686e-10,1.5520593e-10\n",
      "Iteration 23885: loss = 8.7160545e-10,1.5520256e-10\n",
      "Iteration 23890: loss = 8.7110424e-10,1.5520338e-10\n",
      "Iteration 23895: loss = 8.7058843e-10,1.552049e-10\n",
      "Iteration 23900: loss = 8.7008906e-10,1.5520592e-10\n",
      "Iteration 23905: loss = 8.696148e-10,1.5520661e-10\n",
      "Iteration 23910: loss = 8.657901e-10,1.5520792e-10\n",
      "Iteration 23915: loss = 8.619791e-10,1.5520933e-10\n",
      "Iteration 23920: loss = 8.614731e-10,1.5521102e-10\n",
      "Iteration 23925: loss = 8.6099067e-10,1.5521207e-10\n",
      "Iteration 23930: loss = 8.605007e-10,1.5521293e-10\n",
      "Iteration 23935: loss = 8.6000956e-10,1.5521391e-10\n",
      "Iteration 23940: loss = 8.5623686e-10,1.5521416e-10\n",
      "Iteration 23945: loss = 8.557675e-10,1.5521484e-10\n",
      "Iteration 23950: loss = 8.519964e-10,1.5521551e-10\n",
      "Iteration 23955: loss = 8.5155144e-10,1.5521404e-10\n",
      "Iteration 23960: loss = 8.5113544e-10,1.55212e-10\n",
      "Iteration 23965: loss = 8.506555e-10,1.5521248e-10\n",
      "Iteration 23970: loss = 8.5018076e-10,1.5521304e-10\n",
      "Iteration 23975: loss = 8.464495e-10,1.5521319e-10\n",
      "Iteration 23980: loss = 8.459917e-10,1.5521316e-10\n",
      "Iteration 23985: loss = 8.455681e-10,1.5521083e-10\n",
      "Iteration 23990: loss = 8.418743e-10,1.5520943e-10\n",
      "Iteration 23995: loss = 8.414458e-10,1.5520778e-10\n",
      "Iteration 24000: loss = 8.4104834e-10,1.5520484e-10\n",
      "Iteration 24005: loss = 8.406454e-10,1.5520174e-10\n",
      "Iteration 24010: loss = 8.370096e-10,1.5519845e-10\n",
      "Iteration 24015: loss = 8.3661145e-10,1.5519475e-10\n",
      "Iteration 24020: loss = 8.3623064e-10,1.5519125e-10\n",
      "Iteration 24025: loss = 8.358562e-10,1.5518781e-10\n",
      "Iteration 24030: loss = 8.3221613e-10,1.5518428e-10\n",
      "Iteration 24035: loss = 8.3183743e-10,1.5518102e-10\n",
      "Iteration 24040: loss = 8.314435e-10,1.5517751e-10\n",
      "Iteration 24045: loss = 8.310608e-10,1.5517437e-10\n",
      "Iteration 24050: loss = 8.2741597e-10,1.5517199e-10\n",
      "Iteration 24055: loss = 8.270309e-10,1.5516899e-10\n",
      "Iteration 24060: loss = 8.266327e-10,1.5516571e-10\n",
      "Iteration 24065: loss = 8.262619e-10,1.5516192e-10\n",
      "Iteration 24070: loss = 8.258942e-10,1.5515841e-10\n",
      "Iteration 24075: loss = 8.222781e-10,1.5515503e-10\n",
      "Iteration 24080: loss = 8.218966e-10,1.5515202e-10\n",
      "Iteration 24085: loss = 8.2150287e-10,1.5514878e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24090: loss = 8.2112467e-10,1.5514548e-10\n",
      "Iteration 24095: loss = 8.175111e-10,1.5514261e-10\n",
      "Iteration 24100: loss = 8.171253e-10,1.5513979e-10\n",
      "Iteration 24105: loss = 8.1673335e-10,1.5513693e-10\n",
      "Iteration 24110: loss = 8.163617e-10,1.551337e-10\n",
      "Iteration 24115: loss = 8.12775e-10,1.5512983e-10\n",
      "Iteration 24120: loss = 8.1240475e-10,1.5512626e-10\n",
      "Iteration 24125: loss = 8.1202933e-10,1.5512358e-10\n",
      "Iteration 24130: loss = 8.116337e-10,1.5512061e-10\n",
      "Iteration 24135: loss = 8.0806156e-10,1.5511759e-10\n",
      "Iteration 24140: loss = 8.076575e-10,1.5511512e-10\n",
      "Iteration 24145: loss = 8.071899e-10,1.5511728e-10\n",
      "Iteration 24150: loss = 8.0672297e-10,1.5511833e-10\n",
      "Iteration 24155: loss = 8.0309154e-10,1.5511886e-10\n",
      "Iteration 24160: loss = 8.0262336e-10,1.5512025e-10\n",
      "Iteration 24165: loss = 8.021594e-10,1.5512196e-10\n",
      "Iteration 24170: loss = 7.9856327e-10,1.5512142e-10\n",
      "Iteration 24175: loss = 7.981536e-10,1.5512007e-10\n",
      "Iteration 24180: loss = 7.977534e-10,1.5511861e-10\n",
      "Iteration 24185: loss = 7.973597e-10,1.5511673e-10\n",
      "Iteration 24190: loss = 7.9689605e-10,1.5511782e-10\n",
      "Iteration 24195: loss = 7.9330165e-10,1.5511831e-10\n",
      "Iteration 24200: loss = 7.92892e-10,1.5511634e-10\n",
      "Iteration 24205: loss = 7.893426e-10,1.5511431e-10\n",
      "Iteration 24210: loss = 7.889372e-10,1.551135e-10\n",
      "Iteration 24215: loss = 7.885253e-10,1.551128e-10\n",
      "Iteration 24220: loss = 7.8811463e-10,1.5511208e-10\n",
      "Iteration 24225: loss = 7.877115e-10,1.5511087e-10\n",
      "Iteration 24230: loss = 7.8733625e-10,1.551082e-10\n",
      "Iteration 24235: loss = 7.8376566e-10,1.5510741e-10\n",
      "Iteration 24240: loss = 7.833772e-10,1.5510586e-10\n",
      "Iteration 24245: loss = 7.798316e-10,1.5510494e-10\n",
      "Iteration 24250: loss = 7.7943785e-10,1.551037e-10\n",
      "Iteration 24255: loss = 7.790548e-10,1.5510171e-10\n",
      "Iteration 24260: loss = 7.7867535e-10,1.5509947e-10\n",
      "Iteration 24265: loss = 7.782983e-10,1.5509725e-10\n",
      "Iteration 24270: loss = 7.779306e-10,1.5509476e-10\n",
      "Iteration 24275: loss = 7.7755935e-10,1.5509141e-10\n",
      "Iteration 24280: loss = 7.7406886e-10,1.5508854e-10\n",
      "Iteration 24285: loss = 7.705907e-10,1.5508543e-10\n",
      "Iteration 24290: loss = 7.70256e-10,1.5508088e-10\n",
      "Iteration 24295: loss = 7.699372e-10,1.5507634e-10\n",
      "Iteration 24300: loss = 7.696019e-10,1.5507158e-10\n",
      "Iteration 24305: loss = 7.6927265e-10,1.5506711e-10\n",
      "Iteration 24310: loss = 7.6892287e-10,1.5506366e-10\n",
      "Iteration 24315: loss = 7.6854595e-10,1.5506096e-10\n",
      "Iteration 24320: loss = 7.6819506e-10,1.5505768e-10\n",
      "Iteration 24325: loss = 7.647318e-10,1.5505439e-10\n",
      "Iteration 24330: loss = 7.6129947e-10,1.5505056e-10\n",
      "Iteration 24335: loss = 7.609035e-10,1.5504939e-10\n",
      "Iteration 24340: loss = 7.604917e-10,1.5504951e-10\n",
      "Iteration 24345: loss = 7.600844e-10,1.5504933e-10\n",
      "Iteration 24350: loss = 7.59681e-10,1.55049e-10\n",
      "Iteration 24355: loss = 7.5930195e-10,1.550482e-10\n",
      "Iteration 24360: loss = 7.5886986e-10,1.5504893e-10\n",
      "Iteration 24365: loss = 7.5227063e-10,1.5505015e-10\n",
      "Iteration 24370: loss = 7.5187484e-10,1.5504925e-10\n",
      "Iteration 24375: loss = 7.5150036e-10,1.5504789e-10\n",
      "Iteration 24380: loss = 7.51154e-10,1.5504519e-10\n",
      "Iteration 24385: loss = 7.5076095e-10,1.5504449e-10\n",
      "Iteration 24390: loss = 7.5038326e-10,1.5504306e-10\n",
      "Iteration 24395: loss = 7.5000317e-10,1.5504156e-10\n",
      "Iteration 24400: loss = 7.4964407e-10,1.5503923e-10\n",
      "Iteration 24405: loss = 7.4623446e-10,1.5503603e-10\n",
      "Iteration 24410: loss = 7.428051e-10,1.5503418e-10\n",
      "Iteration 24415: loss = 7.4247847e-10,1.550306e-10\n",
      "Iteration 24420: loss = 7.4213263e-10,1.5502699e-10\n",
      "Iteration 24425: loss = 7.41802e-10,1.5502369e-10\n",
      "Iteration 24430: loss = 7.4145295e-10,1.5502034e-10\n",
      "Iteration 24435: loss = 7.4113377e-10,1.5501672e-10\n",
      "Iteration 24440: loss = 7.4076173e-10,1.5501506e-10\n",
      "Iteration 24445: loss = 7.403993e-10,1.5501295e-10\n",
      "Iteration 24450: loss = 7.36985e-10,1.5501078e-10\n",
      "Iteration 24455: loss = 7.33595e-10,1.5500815e-10\n",
      "Iteration 24460: loss = 7.332832e-10,1.5500395e-10\n",
      "Iteration 24465: loss = 7.329552e-10,1.5499999e-10\n",
      "Iteration 24470: loss = 7.32642e-10,1.5499582e-10\n",
      "Iteration 24475: loss = 7.323407e-10,1.5499096e-10\n",
      "Iteration 24480: loss = 7.3197887e-10,1.5498874e-10\n",
      "Iteration 24485: loss = 7.316259e-10,1.5498665e-10\n",
      "Iteration 24490: loss = 7.312775e-10,1.5498422e-10\n",
      "Iteration 24495: loss = 7.278918e-10,1.5498165e-10\n",
      "Iteration 24500: loss = 7.24572e-10,1.5497695e-10\n",
      "Iteration 24505: loss = 7.242454e-10,1.5497302e-10\n",
      "Iteration 24510: loss = 7.2393314e-10,1.54969e-10\n",
      "Iteration 24515: loss = 7.235444e-10,1.5496898e-10\n",
      "Iteration 24520: loss = 7.231589e-10,1.5496929e-10\n",
      "Iteration 24525: loss = 7.2277867e-10,1.5496954e-10\n",
      "Iteration 24530: loss = 7.223921e-10,1.5496926e-10\n",
      "Iteration 24535: loss = 7.1900214e-10,1.5496891e-10\n",
      "Iteration 24540: loss = 7.1861894e-10,1.5496851e-10\n",
      "Iteration 24545: loss = 7.152324e-10,1.5496868e-10\n",
      "Iteration 24550: loss = 7.1490197e-10,1.54966e-10\n",
      "Iteration 24555: loss = 7.145812e-10,1.5496249e-10\n",
      "Iteration 24560: loss = 7.142427e-10,1.5496035e-10\n",
      "Iteration 24565: loss = 7.1393447e-10,1.5495635e-10\n",
      "Iteration 24570: loss = 7.136078e-10,1.5495247e-10\n",
      "Iteration 24575: loss = 7.1024564e-10,1.5495202e-10\n",
      "Iteration 24580: loss = 7.098831e-10,1.5495147e-10\n",
      "Iteration 24585: loss = 7.095194e-10,1.549503e-10\n",
      "Iteration 24590: loss = 7.0617573e-10,1.5494867e-10\n",
      "Iteration 24595: loss = 7.058616e-10,1.5494525e-10\n",
      "Iteration 24600: loss = 7.055379e-10,1.5494245e-10\n",
      "Iteration 24605: loss = 7.052268e-10,1.5493912e-10\n",
      "Iteration 24610: loss = 7.0490397e-10,1.5493563e-10\n",
      "Iteration 24615: loss = 7.045606e-10,1.5493422e-10\n",
      "Iteration 24620: loss = 7.0122663e-10,1.5493283e-10\n",
      "Iteration 24625: loss = 7.009308e-10,1.5492854e-10\n",
      "Iteration 24630: loss = 7.006273e-10,1.5492456e-10\n",
      "Iteration 24635: loss = 6.973622e-10,1.5492033e-10\n",
      "Iteration 24640: loss = 6.970548e-10,1.5491662e-10\n",
      "Iteration 24645: loss = 6.9673817e-10,1.549138e-10\n",
      "Iteration 24650: loss = 6.9640427e-10,1.549108e-10\n",
      "Iteration 24655: loss = 6.960854e-10,1.5490796e-10\n",
      "Iteration 24660: loss = 6.957781e-10,1.5490445e-10\n",
      "Iteration 24665: loss = 6.925356e-10,1.5489954e-10\n",
      "Iteration 24670: loss = 6.922492e-10,1.5489494e-10\n",
      "Iteration 24675: loss = 6.9195893e-10,1.5489071e-10\n",
      "Iteration 24680: loss = 6.9167333e-10,1.5488592e-10\n",
      "Iteration 24685: loss = 6.884431e-10,1.5488091e-10\n",
      "Iteration 24690: loss = 6.880887e-10,1.5487968e-10\n",
      "Iteration 24695: loss = 6.8772305e-10,1.5487964e-10\n",
      "Iteration 24700: loss = 6.873509e-10,1.5488041e-10\n",
      "Iteration 24705: loss = 6.869882e-10,1.548807e-10\n",
      "Iteration 24710: loss = 6.837038e-10,1.5487961e-10\n",
      "Iteration 24715: loss = 6.833433e-10,1.5487968e-10\n",
      "Iteration 24720: loss = 6.8302186e-10,1.5487718e-10\n",
      "Iteration 24725: loss = 6.827163e-10,1.5487389e-10\n",
      "Iteration 24730: loss = 6.7946754e-10,1.548717e-10\n",
      "Iteration 24735: loss = 6.791588e-10,1.5486862e-10\n",
      "Iteration 24740: loss = 6.788477e-10,1.5486568e-10\n",
      "Iteration 24745: loss = 6.7852496e-10,1.5486354e-10\n",
      "Iteration 24750: loss = 6.7820477e-10,1.5486172e-10\n",
      "Iteration 24755: loss = 6.749547e-10,1.5486004e-10\n",
      "Iteration 24760: loss = 6.746231e-10,1.5485856e-10\n",
      "Iteration 24765: loss = 6.7430245e-10,1.5485607e-10\n",
      "Iteration 24770: loss = 6.7399925e-10,1.5485295e-10\n",
      "Iteration 24775: loss = 6.737131e-10,1.5484927e-10\n",
      "Iteration 24780: loss = 6.704919e-10,1.5484662e-10\n",
      "Iteration 24785: loss = 6.7019373e-10,1.5484307e-10\n",
      "Iteration 24790: loss = 6.6989797e-10,1.5483938e-10\n",
      "Iteration 24795: loss = 6.6960887e-10,1.548354e-10\n",
      "Iteration 24800: loss = 6.664278e-10,1.5483091e-10\n",
      "Iteration 24805: loss = 6.6615263e-10,1.5482671e-10\n",
      "Iteration 24810: loss = 6.658533e-10,1.5482356e-10\n",
      "Iteration 24815: loss = 6.6554384e-10,1.5482074e-10\n",
      "Iteration 24820: loss = 6.6525047e-10,1.5481715e-10\n",
      "Iteration 24825: loss = 6.649708e-10,1.5481291e-10\n",
      "Iteration 24830: loss = 6.618195e-10,1.5480789e-10\n",
      "Iteration 24835: loss = 6.615423e-10,1.5480353e-10\n",
      "Iteration 24840: loss = 6.61255e-10,1.5479946e-10\n",
      "Iteration 24845: loss = 6.609571e-10,1.5479647e-10\n",
      "Iteration 24850: loss = 6.57771e-10,1.547936e-10\n",
      "Iteration 24855: loss = 6.57467e-10,1.5479165e-10\n",
      "Iteration 24860: loss = 6.571406e-10,1.547896e-10\n",
      "Iteration 24865: loss = 6.56829e-10,1.5478821e-10\n",
      "Iteration 24870: loss = 6.565212e-10,1.547863e-10\n",
      "Iteration 24875: loss = 6.562011e-10,1.5478481e-10\n",
      "Iteration 24880: loss = 6.5302347e-10,1.5478291e-10\n",
      "Iteration 24885: loss = 6.527064e-10,1.5478084e-10\n",
      "Iteration 24890: loss = 6.5240435e-10,1.5477894e-10\n",
      "Iteration 24895: loss = 6.492184e-10,1.5477701e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24900: loss = 6.489171e-10,1.5477473e-10\n",
      "Iteration 24905: loss = 6.4860944e-10,1.547727e-10\n",
      "Iteration 24910: loss = 6.4831296e-10,1.547702e-10\n",
      "Iteration 24915: loss = 6.4801103e-10,1.5476773e-10\n",
      "Iteration 24920: loss = 6.477219e-10,1.5476513e-10\n",
      "Iteration 24925: loss = 6.4742767e-10,1.5476209e-10\n",
      "Iteration 24930: loss = 6.442941e-10,1.5475854e-10\n",
      "Iteration 24935: loss = 6.4400746e-10,1.5475506e-10\n",
      "Iteration 24940: loss = 6.437302e-10,1.5475198e-10\n",
      "Iteration 24945: loss = 6.405974e-10,1.547484e-10\n",
      "Iteration 24950: loss = 6.403435e-10,1.5474329e-10\n",
      "Iteration 24955: loss = 6.400893e-10,1.5473842e-10\n",
      "Iteration 24960: loss = 6.398427e-10,1.5473324e-10\n",
      "Iteration 24965: loss = 6.3954997e-10,1.5473034e-10\n",
      "Iteration 24970: loss = 6.3927924e-10,1.547268e-10\n",
      "Iteration 24975: loss = 6.3900335e-10,1.5472258e-10\n",
      "Iteration 24980: loss = 6.387413e-10,1.5471845e-10\n",
      "Iteration 24985: loss = 6.356147e-10,1.5471524e-10\n",
      "Iteration 24990: loss = 6.3532773e-10,1.5471313e-10\n",
      "Iteration 24995: loss = 6.3220956e-10,1.5471041e-10\n",
      "Iteration 25000: loss = 6.319029e-10,1.5470901e-10\n",
      "Iteration 25005: loss = 6.3160366e-10,1.5470748e-10\n",
      "Iteration 25010: loss = 6.312782e-10,1.5470669e-10\n",
      "Iteration 25015: loss = 6.309606e-10,1.5470623e-10\n",
      "Iteration 25020: loss = 6.3067196e-10,1.5470393e-10\n",
      "Iteration 25025: loss = 6.303775e-10,1.5470186e-10\n",
      "Iteration 25030: loss = 6.272463e-10,1.5470118e-10\n",
      "Iteration 25035: loss = 6.2695876e-10,1.5469911e-10\n",
      "Iteration 25040: loss = 6.238347e-10,1.5469796e-10\n",
      "Iteration 25045: loss = 6.235648e-10,1.546951e-10\n",
      "Iteration 25050: loss = 6.2329125e-10,1.546919e-10\n",
      "Iteration 25055: loss = 6.230185e-10,1.546885e-10\n",
      "Iteration 25060: loss = 6.227599e-10,1.5468428e-10\n",
      "Iteration 25065: loss = 6.224692e-10,1.5468256e-10\n",
      "Iteration 25070: loss = 6.221737e-10,1.5468081e-10\n",
      "Iteration 25075: loss = 6.218947e-10,1.5467838e-10\n",
      "Iteration 25080: loss = 6.216161e-10,1.546757e-10\n",
      "Iteration 25085: loss = 6.157343e-10,1.5467341e-10\n",
      "Iteration 25090: loss = 6.154843e-10,1.5466926e-10\n",
      "Iteration 25095: loss = 6.152249e-10,1.5466531e-10\n",
      "Iteration 25100: loss = 6.149733e-10,1.5466091e-10\n",
      "Iteration 25105: loss = 6.146834e-10,1.546588e-10\n",
      "Iteration 25110: loss = 6.1437494e-10,1.5465829e-10\n",
      "Iteration 25115: loss = 6.1408306e-10,1.5465727e-10\n",
      "Iteration 25120: loss = 6.137932e-10,1.5465546e-10\n",
      "Iteration 25125: loss = 6.1351657e-10,1.5465276e-10\n",
      "Iteration 25130: loss = 6.1321487e-10,1.5465164e-10\n",
      "Iteration 25135: loss = 6.07362e-10,1.5465115e-10\n",
      "Iteration 25140: loss = 6.0705907e-10,1.5465008e-10\n",
      "Iteration 25145: loss = 6.0679417e-10,1.5464748e-10\n",
      "Iteration 25150: loss = 6.065233e-10,1.5464446e-10\n",
      "Iteration 25155: loss = 6.0626665e-10,1.5464094e-10\n",
      "Iteration 25160: loss = 6.0599875e-10,1.5463786e-10\n",
      "Iteration 25165: loss = 6.057126e-10,1.5463605e-10\n",
      "Iteration 25170: loss = 6.054346e-10,1.546338e-10\n",
      "Iteration 25175: loss = 6.051906e-10,1.5463014e-10\n",
      "Iteration 25180: loss = 6.049415e-10,1.5462581e-10\n",
      "Iteration 25185: loss = 5.9916566e-10,1.5462254e-10\n",
      "Iteration 25190: loss = 5.989093e-10,1.546195e-10\n",
      "Iteration 25195: loss = 5.98631e-10,1.5461708e-10\n",
      "Iteration 25200: loss = 5.983827e-10,1.5461356e-10\n",
      "Iteration 25205: loss = 5.9812494e-10,1.5460974e-10\n",
      "Iteration 25210: loss = 5.9788857e-10,1.5460572e-10\n",
      "Iteration 25215: loss = 5.9765326e-10,1.5460035e-10\n",
      "Iteration 25220: loss = 5.9739674e-10,1.5459697e-10\n",
      "Iteration 25225: loss = 5.971837e-10,1.5459115e-10\n",
      "Iteration 25230: loss = 5.96955e-10,1.5458552e-10\n",
      "Iteration 25235: loss = 5.9672e-10,1.545809e-10\n",
      "Iteration 25240: loss = 5.9373356e-10,1.5457696e-10\n",
      "Iteration 25245: loss = 5.907828e-10,1.5457172e-10\n",
      "Iteration 25250: loss = 5.9049743e-10,1.545702e-10\n",
      "Iteration 25255: loss = 5.901965e-10,1.5457058e-10\n",
      "Iteration 25260: loss = 5.8992183e-10,1.5456884e-10\n",
      "Iteration 25265: loss = 5.8964245e-10,1.5456655e-10\n",
      "Iteration 25270: loss = 5.893717e-10,1.545647e-10\n",
      "Iteration 25275: loss = 5.8909166e-10,1.5456372e-10\n",
      "Iteration 25280: loss = 5.888166e-10,1.5456167e-10\n",
      "Iteration 25285: loss = 5.8856475e-10,1.5455884e-10\n",
      "Iteration 25290: loss = 5.8557714e-10,1.5455585e-10\n",
      "Iteration 25295: loss = 5.8260735e-10,1.545537e-10\n",
      "Iteration 25300: loss = 5.823463e-10,1.5455143e-10\n",
      "Iteration 25305: loss = 5.8207733e-10,1.545489e-10\n",
      "Iteration 25310: loss = 5.818194e-10,1.5454624e-10\n",
      "Iteration 25315: loss = 5.8157185e-10,1.5454299e-10\n",
      "Iteration 25320: loss = 5.813288e-10,1.5453952e-10\n",
      "Iteration 25325: loss = 5.810891e-10,1.5453601e-10\n",
      "Iteration 25330: loss = 5.8086275e-10,1.5453122e-10\n",
      "Iteration 25335: loss = 5.805854e-10,1.5452972e-10\n",
      "Iteration 25340: loss = 5.7762917e-10,1.5452736e-10\n",
      "Iteration 25345: loss = 5.773871e-10,1.545232e-10\n",
      "Iteration 25350: loss = 5.7445865e-10,1.5451929e-10\n",
      "Iteration 25355: loss = 5.7421196e-10,1.5451587e-10\n",
      "Iteration 25360: loss = 5.7399824e-10,1.5451028e-10\n",
      "Iteration 25365: loss = 5.737464e-10,1.5450768e-10\n",
      "Iteration 25370: loss = 5.7349453e-10,1.545055e-10\n",
      "Iteration 25375: loss = 5.732059e-10,1.5450558e-10\n",
      "Iteration 25380: loss = 5.7290256e-10,1.545057e-10\n",
      "Iteration 25385: loss = 5.7262256e-10,1.5450512e-10\n",
      "Iteration 25390: loss = 5.696577e-10,1.5450496e-10\n",
      "Iteration 25395: loss = 5.693999e-10,1.5450356e-10\n",
      "Iteration 25400: loss = 5.6915966e-10,1.5449969e-10\n",
      "Iteration 25405: loss = 5.6624966e-10,1.5449604e-10\n",
      "Iteration 25410: loss = 5.6599514e-10,1.5449339e-10\n",
      "Iteration 25415: loss = 5.657404e-10,1.5449123e-10\n",
      "Iteration 25420: loss = 5.6549326e-10,1.5448906e-10\n",
      "Iteration 25425: loss = 5.6523547e-10,1.5448721e-10\n",
      "Iteration 25430: loss = 5.6499344e-10,1.5448413e-10\n",
      "Iteration 25435: loss = 5.6476296e-10,1.5448015e-10\n",
      "Iteration 25440: loss = 5.645454e-10,1.5447547e-10\n",
      "Iteration 25445: loss = 5.6165245e-10,1.5447163e-10\n",
      "Iteration 25450: loss = 5.6144195e-10,1.5446652e-10\n",
      "Iteration 25455: loss = 5.612176e-10,1.5446225e-10\n",
      "Iteration 25460: loss = 5.609757e-10,1.5445907e-10\n",
      "Iteration 25465: loss = 5.581176e-10,1.5445388e-10\n",
      "Iteration 25470: loss = 5.5790794e-10,1.5444807e-10\n",
      "Iteration 25475: loss = 5.5771165e-10,1.5444249e-10\n",
      "Iteration 25480: loss = 5.574507e-10,1.5444068e-10\n",
      "Iteration 25485: loss = 5.5719945e-10,1.5443904e-10\n",
      "Iteration 25490: loss = 5.569127e-10,1.5443967e-10\n",
      "Iteration 25495: loss = 5.5662597e-10,1.5444e-10\n",
      "Iteration 25500: loss = 5.5372135e-10,1.5443874e-10\n",
      "Iteration 25505: loss = 5.534682e-10,1.5443741e-10\n",
      "Iteration 25510: loss = 5.5320964e-10,1.5443562e-10\n",
      "Iteration 25515: loss = 5.529838e-10,1.5443234e-10\n",
      "Iteration 25520: loss = 5.501147e-10,1.5442869e-10\n",
      "Iteration 25525: loss = 5.4989985e-10,1.5442483e-10\n",
      "Iteration 25530: loss = 5.4967236e-10,1.5442092e-10\n",
      "Iteration 25535: loss = 5.4945265e-10,1.5441692e-10\n",
      "Iteration 25540: loss = 5.492204e-10,1.5441376e-10\n",
      "Iteration 25545: loss = 5.49004e-10,1.5441007e-10\n",
      "Iteration 25550: loss = 5.487812e-10,1.5440571e-10\n",
      "Iteration 25555: loss = 5.459455e-10,1.5440124e-10\n",
      "Iteration 25560: loss = 5.457212e-10,1.5439747e-10\n",
      "Iteration 25565: loss = 5.454968e-10,1.5439414e-10\n",
      "Iteration 25570: loss = 5.452636e-10,1.5439079e-10\n",
      "Iteration 25575: loss = 5.450417e-10,1.5438749e-10\n",
      "Iteration 25580: loss = 5.421872e-10,1.5438481e-10\n",
      "Iteration 25585: loss = 5.419947e-10,1.5437933e-10\n",
      "Iteration 25590: loss = 5.417931e-10,1.5437429e-10\n",
      "Iteration 25595: loss = 5.415954e-10,1.5436902e-10\n",
      "Iteration 25600: loss = 5.413762e-10,1.5436534e-10\n",
      "Iteration 25605: loss = 5.4110477e-10,1.5436517e-10\n",
      "Iteration 25610: loss = 5.382332e-10,1.5436499e-10\n",
      "Iteration 25615: loss = 5.3799665e-10,1.5436293e-10\n",
      "Iteration 25620: loss = 5.3775157e-10,1.5436041e-10\n",
      "Iteration 25625: loss = 5.375162e-10,1.5435805e-10\n",
      "Iteration 25630: loss = 5.372796e-10,1.5435626e-10\n",
      "Iteration 25635: loss = 5.370505e-10,1.5435382e-10\n",
      "Iteration 25640: loss = 5.342133e-10,1.5435174e-10\n",
      "Iteration 25645: loss = 5.339751e-10,1.5434934e-10\n",
      "Iteration 25650: loss = 5.337795e-10,1.543447e-10\n",
      "Iteration 25655: loss = 5.335332e-10,1.5434286e-10\n",
      "Iteration 25660: loss = 5.3329513e-10,1.5434092e-10\n",
      "Iteration 25665: loss = 5.3046906e-10,1.543392e-10\n",
      "Iteration 25670: loss = 5.302551e-10,1.5433504e-10\n",
      "Iteration 25675: loss = 5.300738e-10,1.5432981e-10\n",
      "Iteration 25680: loss = 5.298778e-10,1.5432441e-10\n",
      "Iteration 25685: loss = 5.296789e-10,1.543197e-10\n",
      "Iteration 25690: loss = 5.2948657e-10,1.5431428e-10\n",
      "Iteration 25695: loss = 5.2927857e-10,1.5430984e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25700: loss = 5.2648347e-10,1.5430665e-10\n",
      "Iteration 25705: loss = 5.262907e-10,1.5430185e-10\n",
      "Iteration 25710: loss = 5.2610666e-10,1.5429596e-10\n",
      "Iteration 25715: loss = 5.2586274e-10,1.5429447e-10\n",
      "Iteration 25720: loss = 5.2561827e-10,1.5429358e-10\n",
      "Iteration 25725: loss = 5.2279897e-10,1.5429268e-10\n",
      "Iteration 25730: loss = 5.225516e-10,1.5429244e-10\n",
      "Iteration 25735: loss = 5.223048e-10,1.5429194e-10\n",
      "Iteration 25740: loss = 5.220773e-10,1.5428929e-10\n",
      "Iteration 25745: loss = 5.2185734e-10,1.5428613e-10\n",
      "Iteration 25750: loss = 5.216335e-10,1.5428335e-10\n",
      "Iteration 25755: loss = 5.2141785e-10,1.5428087e-10\n",
      "Iteration 25760: loss = 5.1863364e-10,1.5427813e-10\n",
      "Iteration 25765: loss = 5.1843e-10,1.5427429e-10\n",
      "Iteration 25770: loss = 5.182305e-10,1.5427025e-10\n",
      "Iteration 25775: loss = 5.1804055e-10,1.5426535e-10\n",
      "Iteration 25780: loss = 5.178254e-10,1.5426242e-10\n",
      "Iteration 25785: loss = 5.150615e-10,1.542593e-10\n",
      "Iteration 25790: loss = 5.148687e-10,1.5425428e-10\n",
      "Iteration 25795: loss = 5.1468724e-10,1.5424922e-10\n",
      "Iteration 25800: loss = 5.144935e-10,1.5424424e-10\n",
      "Iteration 25805: loss = 5.1430077e-10,1.542402e-10\n",
      "Iteration 25810: loss = 5.1409915e-10,1.5423565e-10\n",
      "Iteration 25815: loss = 5.139015e-10,1.542314e-10\n",
      "Iteration 25820: loss = 5.137313e-10,1.542257e-10\n",
      "Iteration 25825: loss = 5.109852e-10,1.5422139e-10\n",
      "Iteration 25830: loss = 5.1081367e-10,1.5421511e-10\n",
      "Iteration 25835: loss = 5.1058796e-10,1.5421364e-10\n",
      "Iteration 25840: loss = 5.103535e-10,1.5421209e-10\n",
      "Iteration 25845: loss = 5.075951e-10,1.542108e-10\n",
      "Iteration 25850: loss = 5.07376e-10,1.5420844e-10\n",
      "Iteration 25855: loss = 5.071607e-10,1.542062e-10\n",
      "Iteration 25860: loss = 5.0694476e-10,1.542039e-10\n",
      "Iteration 25865: loss = 5.0673576e-10,1.5420118e-10\n",
      "Iteration 25870: loss = 5.06526e-10,1.5419757e-10\n",
      "Iteration 25875: loss = 5.0628973e-10,1.5419714e-10\n",
      "Iteration 25880: loss = 5.0607313e-10,1.5419452e-10\n",
      "Iteration 25885: loss = 5.0333926e-10,1.5419223e-10\n",
      "Iteration 25890: loss = 5.0312127e-10,1.5418938e-10\n",
      "Iteration 25895: loss = 5.029221e-10,1.5418594e-10\n",
      "Iteration 25900: loss = 5.027509e-10,1.5418022e-10\n",
      "Iteration 25905: loss = 5.0005755e-10,1.5417512e-10\n",
      "Iteration 25910: loss = 4.998692e-10,1.541709e-10\n",
      "Iteration 25915: loss = 4.99682e-10,1.5416698e-10\n",
      "Iteration 25920: loss = 4.9949694e-10,1.5416249e-10\n",
      "Iteration 25925: loss = 4.992995e-10,1.541582e-10\n",
      "Iteration 25930: loss = 4.9911697e-10,1.541537e-10\n",
      "Iteration 25935: loss = 4.989275e-10,1.5414943e-10\n",
      "Iteration 25940: loss = 4.987006e-10,1.5414851e-10\n",
      "Iteration 25945: loss = 4.984788e-10,1.5414695e-10\n",
      "Iteration 25950: loss = 4.957416e-10,1.5414572e-10\n",
      "Iteration 25955: loss = 4.955178e-10,1.5414466e-10\n",
      "Iteration 25960: loss = 4.9527926e-10,1.5414499e-10\n",
      "Iteration 25965: loss = 4.925618e-10,1.5414309e-10\n",
      "Iteration 25970: loss = 4.9234267e-10,1.5414227e-10\n",
      "Iteration 25975: loss = 4.9212884e-10,1.5414037e-10\n",
      "Iteration 25980: loss = 4.919188e-10,1.541381e-10\n",
      "Iteration 25985: loss = 4.9171706e-10,1.5413526e-10\n",
      "Iteration 25990: loss = 4.915199e-10,1.5413143e-10\n",
      "Iteration 25995: loss = 4.913271e-10,1.541283e-10\n",
      "Iteration 26000: loss = 4.911383e-10,1.5412407e-10\n",
      "Iteration 26005: loss = 4.909628e-10,1.5412027e-10\n",
      "Iteration 26010: loss = 4.907756e-10,1.5411608e-10\n",
      "Iteration 26015: loss = 4.8810617e-10,1.5411175e-10\n",
      "Iteration 26020: loss = 4.879353e-10,1.5410632e-10\n",
      "Iteration 26025: loss = 4.8529336e-10,1.541005e-10\n",
      "Iteration 26030: loss = 4.85102e-10,1.5409712e-10\n",
      "Iteration 26035: loss = 4.848663e-10,1.5409732e-10\n",
      "Iteration 26040: loss = 4.846384e-10,1.5409765e-10\n",
      "Iteration 26045: loss = 4.84403e-10,1.540976e-10\n",
      "Iteration 26050: loss = 4.8420074e-10,1.5409492e-10\n",
      "Iteration 26055: loss = 4.840046e-10,1.5409249e-10\n",
      "Iteration 26060: loss = 4.8380366e-10,1.5408964e-10\n",
      "Iteration 26065: loss = 4.8357257e-10,1.5408942e-10\n",
      "Iteration 26070: loss = 4.833549e-10,1.5408819e-10\n",
      "Iteration 26075: loss = 4.80682e-10,1.5408688e-10\n",
      "Iteration 26080: loss = 4.7800286e-10,1.54086e-10\n",
      "Iteration 26085: loss = 4.7779736e-10,1.5408375e-10\n",
      "Iteration 26090: loss = 4.7760884e-10,1.5408022e-10\n",
      "Iteration 26095: loss = 4.7742e-10,1.5407708e-10\n",
      "Iteration 26100: loss = 4.772413e-10,1.5407325e-10\n",
      "Iteration 26105: loss = 4.77072e-10,1.5406865e-10\n",
      "Iteration 26110: loss = 4.7691034e-10,1.5406329e-10\n",
      "Iteration 26115: loss = 4.7673193e-10,1.5405917e-10\n",
      "Iteration 26120: loss = 4.7653553e-10,1.5405621e-10\n",
      "Iteration 26125: loss = 4.763296e-10,1.5405507e-10\n",
      "Iteration 26130: loss = 4.761188e-10,1.5405346e-10\n",
      "Iteration 26135: loss = 4.7591703e-10,1.540518e-10\n",
      "Iteration 26140: loss = 4.732416e-10,1.540515e-10\n",
      "Iteration 26145: loss = 4.705808e-10,1.5405115e-10\n",
      "Iteration 26150: loss = 4.7036863e-10,1.540504e-10\n",
      "Iteration 26155: loss = 4.70163e-10,1.5404891e-10\n",
      "Iteration 26160: loss = 4.699636e-10,1.5404689e-10\n",
      "Iteration 26165: loss = 4.697687e-10,1.5404494e-10\n",
      "Iteration 26170: loss = 4.6958026e-10,1.540418e-10\n",
      "Iteration 26175: loss = 4.6938536e-10,1.540393e-10\n",
      "Iteration 26180: loss = 4.692036e-10,1.5403633e-10\n",
      "Iteration 26185: loss = 4.6902554e-10,1.5403256e-10\n",
      "Iteration 26190: loss = 4.6884635e-10,1.5402898e-10\n",
      "Iteration 26195: loss = 4.6866855e-10,1.5402546e-10\n",
      "Iteration 26200: loss = 4.684841e-10,1.5402199e-10\n",
      "Iteration 26205: loss = 4.6346474e-10,1.5401777e-10\n",
      "Iteration 26210: loss = 4.6328794e-10,1.5401466e-10\n",
      "Iteration 26215: loss = 4.6312706e-10,1.5400886e-10\n",
      "Iteration 26220: loss = 4.6293536e-10,1.5400672e-10\n",
      "Iteration 26225: loss = 4.6274276e-10,1.5400539e-10\n",
      "Iteration 26230: loss = 4.6254034e-10,1.5400364e-10\n",
      "Iteration 26235: loss = 4.6234438e-10,1.5400184e-10\n",
      "Iteration 26240: loss = 4.6215046e-10,1.5400002e-10\n",
      "Iteration 26245: loss = 4.619577e-10,1.5399762e-10\n",
      "Iteration 26250: loss = 4.617681e-10,1.5399527e-10\n",
      "Iteration 26255: loss = 4.6155427e-10,1.5399546e-10\n",
      "Iteration 26260: loss = 4.6134607e-10,1.5399462e-10\n",
      "Iteration 26265: loss = 4.5872417e-10,1.5399373e-10\n",
      "Iteration 26270: loss = 4.5612505e-10,1.539929e-10\n",
      "Iteration 26275: loss = 4.5594142e-10,1.5398932e-10\n",
      "Iteration 26280: loss = 4.557644e-10,1.5398702e-10\n",
      "Iteration 26285: loss = 4.5558832e-10,1.5398326e-10\n",
      "Iteration 26290: loss = 4.554257e-10,1.5397902e-10\n",
      "Iteration 26295: loss = 4.5525406e-10,1.5397536e-10\n",
      "Iteration 26300: loss = 4.550936e-10,1.5397121e-10\n",
      "Iteration 26305: loss = 4.549292e-10,1.5396678e-10\n",
      "Iteration 26310: loss = 4.547367e-10,1.539653e-10\n",
      "Iteration 26315: loss = 4.545446e-10,1.5396323e-10\n",
      "Iteration 26320: loss = 4.54365e-10,1.5396091e-10\n",
      "Iteration 26325: loss = 4.5414095e-10,1.5396213e-10\n",
      "Iteration 26330: loss = 4.5153128e-10,1.5396254e-10\n",
      "Iteration 26335: loss = 4.4893142e-10,1.5396301e-10\n",
      "Iteration 26340: loss = 4.4875295e-10,1.5396073e-10\n",
      "Iteration 26345: loss = 4.485636e-10,1.5395843e-10\n",
      "Iteration 26350: loss = 4.484002e-10,1.5395496e-10\n",
      "Iteration 26355: loss = 4.4823398e-10,1.5395146e-10\n",
      "Iteration 26360: loss = 4.4805626e-10,1.5394813e-10\n",
      "Iteration 26365: loss = 4.4788784e-10,1.53945e-10\n",
      "Iteration 26370: loss = 4.4774026e-10,1.5393989e-10\n",
      "Iteration 26375: loss = 4.4755777e-10,1.5393715e-10\n",
      "Iteration 26380: loss = 4.473916e-10,1.5393378e-10\n",
      "Iteration 26385: loss = 4.47223e-10,1.5393001e-10\n",
      "Iteration 26390: loss = 4.470692e-10,1.5392572e-10\n",
      "Iteration 26395: loss = 4.445401e-10,1.5392106e-10\n",
      "Iteration 26400: loss = 4.44392e-10,1.5391607e-10\n",
      "Iteration 26405: loss = 4.4422552e-10,1.5391226e-10\n",
      "Iteration 26410: loss = 4.416998e-10,1.539085e-10\n",
      "Iteration 26415: loss = 4.4153167e-10,1.5390557e-10\n",
      "Iteration 26420: loss = 4.4135087e-10,1.5390343e-10\n",
      "Iteration 26425: loss = 4.4116777e-10,1.5390156e-10\n",
      "Iteration 26430: loss = 4.409908e-10,1.5389898e-10\n",
      "Iteration 26435: loss = 4.4080062e-10,1.5389819e-10\n",
      "Iteration 26440: loss = 4.4062168e-10,1.5389573e-10\n",
      "Iteration 26445: loss = 4.40441e-10,1.5389347e-10\n",
      "Iteration 26450: loss = 4.402796e-10,1.5389003e-10\n",
      "Iteration 26455: loss = 4.400941e-10,1.5388828e-10\n",
      "Iteration 26460: loss = 4.3993328e-10,1.5388481e-10\n",
      "Iteration 26465: loss = 4.3740644e-10,1.5388234e-10\n",
      "Iteration 26470: loss = 4.372401e-10,1.5387905e-10\n",
      "Iteration 26475: loss = 4.370703e-10,1.5387645e-10\n",
      "Iteration 26480: loss = 4.3455697e-10,1.5387332e-10\n",
      "Iteration 26485: loss = 4.343891e-10,1.538703e-10\n",
      "Iteration 26490: loss = 4.3422443e-10,1.5386706e-10\n",
      "Iteration 26495: loss = 4.340688e-10,1.5386378e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26500: loss = 4.3388404e-10,1.5386241e-10\n",
      "Iteration 26505: loss = 4.3370604e-10,1.5386051e-10\n",
      "Iteration 26510: loss = 4.3352663e-10,1.5385855e-10\n",
      "Iteration 26515: loss = 4.333486e-10,1.538567e-10\n",
      "Iteration 26520: loss = 4.331692e-10,1.5385479e-10\n",
      "Iteration 26525: loss = 4.3298912e-10,1.538534e-10\n",
      "Iteration 26530: loss = 4.3046622e-10,1.538524e-10\n",
      "Iteration 26535: loss = 4.3028917e-10,1.5385072e-10\n",
      "Iteration 26540: loss = 4.3013637e-10,1.5384718e-10\n",
      "Iteration 26545: loss = 4.299792e-10,1.5384323e-10\n",
      "Iteration 26550: loss = 4.274774e-10,1.5384127e-10\n",
      "Iteration 26555: loss = 4.2731596e-10,1.5383793e-10\n",
      "Iteration 26560: loss = 4.2716772e-10,1.5383403e-10\n",
      "Iteration 26565: loss = 4.2701373e-10,1.5382984e-10\n",
      "Iteration 26570: loss = 4.268753e-10,1.5382572e-10\n",
      "Iteration 26575: loss = 4.267291e-10,1.5382046e-10\n",
      "Iteration 26580: loss = 4.2658985e-10,1.5381602e-10\n",
      "Iteration 26585: loss = 4.2644419e-10,1.5381144e-10\n",
      "Iteration 26590: loss = 4.263091e-10,1.5380602e-10\n",
      "Iteration 26595: loss = 4.2616977e-10,1.5380075e-10\n",
      "Iteration 26600: loss = 4.2600903e-10,1.5379795e-10\n",
      "Iteration 26605: loss = 4.2354567e-10,1.5379378e-10\n",
      "Iteration 26610: loss = 4.2337797e-10,1.5379127e-10\n",
      "Iteration 26615: loss = 4.2318618e-10,1.5379167e-10\n",
      "Iteration 26620: loss = 4.2298107e-10,1.5379235e-10\n",
      "Iteration 26625: loss = 4.2047943e-10,1.5379263e-10\n",
      "Iteration 26630: loss = 4.203163e-10,1.537902e-10\n",
      "Iteration 26635: loss = 4.2016765e-10,1.5378657e-10\n",
      "Iteration 26640: loss = 4.2001702e-10,1.5378301e-10\n",
      "Iteration 26645: loss = 4.1986611e-10,1.5377961e-10\n",
      "Iteration 26650: loss = 4.1970907e-10,1.5377591e-10\n",
      "Iteration 26655: loss = 4.195562e-10,1.5377252e-10\n",
      "Iteration 26660: loss = 4.1938222e-10,1.5377156e-10\n",
      "Iteration 26665: loss = 4.191988e-10,1.5377076e-10\n",
      "Iteration 26670: loss = 4.1673318e-10,1.5376875e-10\n",
      "Iteration 26675: loss = 4.1657724e-10,1.5376597e-10\n",
      "Iteration 26680: loss = 4.1641904e-10,1.5376353e-10\n",
      "Iteration 26685: loss = 4.1626078e-10,1.5376064e-10\n",
      "Iteration 26690: loss = 4.161201e-10,1.5375665e-10\n",
      "Iteration 26695: loss = 4.15973e-10,1.5375216e-10\n",
      "Iteration 26700: loss = 4.1353185e-10,1.5374907e-10\n",
      "Iteration 26705: loss = 4.1334e-10,1.537501e-10\n",
      "Iteration 26710: loss = 4.1318005e-10,1.5374732e-10\n",
      "Iteration 26715: loss = 4.1302448e-10,1.5374468e-10\n",
      "Iteration 26720: loss = 4.1286405e-10,1.5374219e-10\n",
      "Iteration 26725: loss = 4.1270753e-10,1.5373952e-10\n",
      "Iteration 26730: loss = 4.1255088e-10,1.5373658e-10\n",
      "Iteration 26735: loss = 4.1236592e-10,1.5373681e-10\n",
      "Iteration 26740: loss = 4.099325e-10,1.5373476e-10\n",
      "Iteration 26745: loss = 4.0978243e-10,1.5373118e-10\n",
      "Iteration 26750: loss = 4.0963297e-10,1.537283e-10\n",
      "Iteration 26755: loss = 4.0948644e-10,1.5372455e-10\n",
      "Iteration 26760: loss = 4.0933976e-10,1.5372094e-10\n",
      "Iteration 26765: loss = 4.0918754e-10,1.5371765e-10\n",
      "Iteration 26770: loss = 4.0903156e-10,1.5371508e-10\n",
      "Iteration 26775: loss = 4.066106e-10,1.5371232e-10\n",
      "Iteration 26780: loss = 4.0648684e-10,1.5370707e-10\n",
      "Iteration 26785: loss = 4.0635542e-10,1.5370216e-10\n",
      "Iteration 26790: loss = 4.0620382e-10,1.536996e-10\n",
      "Iteration 26795: loss = 4.0602519e-10,1.536994e-10\n",
      "Iteration 26800: loss = 4.0584558e-10,1.5369928e-10\n",
      "Iteration 26805: loss = 4.056714e-10,1.5369853e-10\n",
      "Iteration 26810: loss = 4.0324208e-10,1.5369747e-10\n",
      "Iteration 26815: loss = 4.0306383e-10,1.536972e-10\n",
      "Iteration 26820: loss = 4.0292178e-10,1.5369392e-10\n",
      "Iteration 26825: loss = 4.0275883e-10,1.5369228e-10\n",
      "Iteration 26830: loss = 4.026021e-10,1.5368996e-10\n",
      "Iteration 26835: loss = 4.0246836e-10,1.5368615e-10\n",
      "Iteration 26840: loss = 4.0231027e-10,1.5368389e-10\n",
      "Iteration 26845: loss = 4.021494e-10,1.5368254e-10\n",
      "Iteration 26850: loss = 3.9973833e-10,1.5368068e-10\n",
      "Iteration 26855: loss = 3.9958833e-10,1.5367765e-10\n",
      "Iteration 26860: loss = 3.994491e-10,1.536745e-10\n",
      "Iteration 26865: loss = 3.9930523e-10,1.5367072e-10\n",
      "Iteration 26870: loss = 3.991775e-10,1.5366666e-10\n",
      "Iteration 26875: loss = 3.9904238e-10,1.5366214e-10\n",
      "Iteration 26880: loss = 3.988797e-10,1.536615e-10\n",
      "Iteration 26885: loss = 3.9646122e-10,1.5366136e-10\n",
      "Iteration 26890: loss = 3.9629075e-10,1.536609e-10\n",
      "Iteration 26895: loss = 3.9612122e-10,1.5366033e-10\n",
      "Iteration 26900: loss = 3.9597756e-10,1.5365725e-10\n",
      "Iteration 26905: loss = 3.9580814e-10,1.5365714e-10\n",
      "Iteration 26910: loss = 3.9564654e-10,1.5365592e-10\n",
      "Iteration 26915: loss = 3.9548617e-10,1.5365396e-10\n",
      "Iteration 26920: loss = 3.9534442e-10,1.5365126e-10\n",
      "Iteration 26925: loss = 3.92971e-10,1.536484e-10\n",
      "Iteration 26930: loss = 3.9283118e-10,1.5364525e-10\n",
      "Iteration 26935: loss = 3.9268985e-10,1.536425e-10\n",
      "Iteration 26940: loss = 3.925339e-10,1.5364032e-10\n",
      "Iteration 26945: loss = 3.923857e-10,1.5363795e-10\n",
      "Iteration 26950: loss = 3.9223946e-10,1.5363565e-10\n",
      "Iteration 26955: loss = 3.8987605e-10,1.536327e-10\n",
      "Iteration 26960: loss = 3.8973494e-10,1.5362953e-10\n",
      "Iteration 26965: loss = 3.8958112e-10,1.5362776e-10\n",
      "Iteration 26970: loss = 3.8941148e-10,1.536278e-10\n",
      "Iteration 26975: loss = 3.892536e-10,1.5362718e-10\n",
      "Iteration 26980: loss = 3.8909265e-10,1.5362617e-10\n",
      "Iteration 26985: loss = 3.8893996e-10,1.5362464e-10\n",
      "Iteration 26990: loss = 3.8879108e-10,1.536225e-10\n",
      "Iteration 26995: loss = 3.8864076e-10,1.5362064e-10\n",
      "Iteration 27000: loss = 3.8627435e-10,1.5361908e-10\n",
      "Iteration 27005: loss = 3.8611925e-10,1.5361752e-10\n",
      "Iteration 27010: loss = 3.859747e-10,1.5361534e-10\n",
      "Iteration 27015: loss = 3.8583114e-10,1.536126e-10\n",
      "Iteration 27020: loss = 3.8568562e-10,1.5361057e-10\n",
      "Iteration 27025: loss = 3.8334969e-10,1.536071e-10\n",
      "Iteration 27030: loss = 3.8322726e-10,1.5360328e-10\n",
      "Iteration 27035: loss = 3.8308498e-10,1.5360041e-10\n",
      "Iteration 27040: loss = 3.8295336e-10,1.5359745e-10\n",
      "Iteration 27045: loss = 3.8280612e-10,1.5359518e-10\n",
      "Iteration 27050: loss = 3.826296e-10,1.5359593e-10\n",
      "Iteration 27055: loss = 3.8246353e-10,1.5359644e-10\n",
      "Iteration 27060: loss = 3.823034e-10,1.5359548e-10\n",
      "Iteration 27065: loss = 3.8215642e-10,1.535942e-10\n",
      "Iteration 27070: loss = 3.8200546e-10,1.5359264e-10\n",
      "Iteration 27075: loss = 3.7966538e-10,1.53591e-10\n",
      "Iteration 27080: loss = 3.795205e-10,1.5358925e-10\n",
      "Iteration 27085: loss = 3.7937467e-10,1.5358759e-10\n",
      "Iteration 27090: loss = 3.7921485e-10,1.5358673e-10\n",
      "Iteration 27095: loss = 3.7687065e-10,1.5358594e-10\n",
      "Iteration 27100: loss = 3.767359e-10,1.5358408e-10\n",
      "Iteration 27105: loss = 3.7660183e-10,1.5358052e-10\n",
      "Iteration 27110: loss = 3.7647419e-10,1.5357708e-10\n",
      "Iteration 27115: loss = 3.7635414e-10,1.5357318e-10\n",
      "Iteration 27120: loss = 3.7623674e-10,1.535688e-10\n",
      "Iteration 27125: loss = 3.76113e-10,1.535641e-10\n",
      "Iteration 27130: loss = 3.7599554e-10,1.5355997e-10\n",
      "Iteration 27135: loss = 3.758173e-10,1.5356234e-10\n",
      "Iteration 27140: loss = 3.7563827e-10,1.5356386e-10\n",
      "Iteration 27145: loss = 3.7546333e-10,1.5356517e-10\n",
      "Iteration 27150: loss = 3.7530376e-10,1.5356544e-10\n",
      "Iteration 27155: loss = 3.7296713e-10,1.5356548e-10\n",
      "Iteration 27160: loss = 3.728106e-10,1.5356544e-10\n",
      "Iteration 27165: loss = 3.7265802e-10,1.5356456e-10\n",
      "Iteration 27170: loss = 3.70346e-10,1.5356347e-10\n",
      "Iteration 27175: loss = 3.701939e-10,1.5356272e-10\n",
      "Iteration 27180: loss = 3.7004647e-10,1.5356143e-10\n",
      "Iteration 27185: loss = 3.6990597e-10,1.5355969e-10\n",
      "Iteration 27190: loss = 3.6978084e-10,1.5355696e-10\n",
      "Iteration 27195: loss = 3.6965941e-10,1.5355325e-10\n",
      "Iteration 27200: loss = 3.695456e-10,1.5354834e-10\n",
      "Iteration 27205: loss = 3.6941272e-10,1.5354618e-10\n",
      "Iteration 27210: loss = 3.6927586e-10,1.5354341e-10\n",
      "Iteration 27215: loss = 3.6915584e-10,1.5353992e-10\n",
      "Iteration 27220: loss = 3.6902703e-10,1.5353707e-10\n",
      "Iteration 27225: loss = 3.6890102e-10,1.5353428e-10\n",
      "Iteration 27230: loss = 3.6878187e-10,1.5353004e-10\n",
      "Iteration 27235: loss = 3.6650097e-10,1.535268e-10\n",
      "Iteration 27240: loss = 3.663443e-10,1.5352655e-10\n",
      "Iteration 27245: loss = 3.6620165e-10,1.5352541e-10\n",
      "Iteration 27250: loss = 3.6390976e-10,1.5352461e-10\n",
      "Iteration 27255: loss = 3.6376444e-10,1.535239e-10\n",
      "Iteration 27260: loss = 3.6361134e-10,1.535234e-10\n",
      "Iteration 27265: loss = 3.6348435e-10,1.5352111e-10\n",
      "Iteration 27270: loss = 3.6335576e-10,1.535182e-10\n",
      "Iteration 27275: loss = 3.6323355e-10,1.5351473e-10\n",
      "Iteration 27280: loss = 3.6310058e-10,1.535128e-10\n",
      "Iteration 27285: loss = 3.629603e-10,1.5351104e-10\n",
      "Iteration 27290: loss = 3.628167e-10,1.5351018e-10\n",
      "Iteration 27295: loss = 3.627062e-10,1.5350604e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27300: loss = 3.625843e-10,1.5350238e-10\n",
      "Iteration 27305: loss = 3.6246892e-10,1.534988e-10\n",
      "Iteration 27310: loss = 3.6234882e-10,1.5349524e-10\n",
      "Iteration 27315: loss = 3.622299e-10,1.5349128e-10\n",
      "Iteration 27320: loss = 3.5997436e-10,1.5348842e-10\n",
      "Iteration 27325: loss = 3.5768197e-10,1.5349e-10\n",
      "Iteration 27330: loss = 3.5754164e-10,1.5348856e-10\n",
      "Iteration 27335: loss = 3.574073e-10,1.5348701e-10\n",
      "Iteration 27340: loss = 3.5727174e-10,1.5348575e-10\n",
      "Iteration 27345: loss = 3.5713807e-10,1.5348424e-10\n",
      "Iteration 27350: loss = 3.570035e-10,1.5348214e-10\n",
      "Iteration 27355: loss = 3.56874e-10,1.5347992e-10\n",
      "Iteration 27360: loss = 3.5673908e-10,1.534788e-10\n",
      "Iteration 27365: loss = 3.566146e-10,1.5347629e-10\n",
      "Iteration 27370: loss = 3.564872e-10,1.5347336e-10\n",
      "Iteration 27375: loss = 3.5636405e-10,1.5347104e-10\n",
      "Iteration 27380: loss = 3.562415e-10,1.5346838e-10\n",
      "Iteration 27385: loss = 3.5611739e-10,1.5346512e-10\n",
      "Iteration 27390: loss = 3.560046e-10,1.5346162e-10\n",
      "Iteration 27395: loss = 3.5589556e-10,1.5345779e-10\n",
      "Iteration 27400: loss = 3.5578482e-10,1.5345353e-10\n",
      "Iteration 27405: loss = 3.5355122e-10,1.5345039e-10\n",
      "Iteration 27410: loss = 3.5129702e-10,1.5345086e-10\n",
      "Iteration 27415: loss = 3.5113148e-10,1.5345239e-10\n",
      "Iteration 27420: loss = 3.5099482e-10,1.5345158e-10\n",
      "Iteration 27425: loss = 3.508539e-10,1.5345103e-10\n",
      "Iteration 27430: loss = 3.5072598e-10,1.5344907e-10\n",
      "Iteration 27435: loss = 3.506007e-10,1.534468e-10\n",
      "Iteration 27440: loss = 3.5047887e-10,1.5344437e-10\n",
      "Iteration 27445: loss = 3.5036138e-10,1.5344129e-10\n",
      "Iteration 27450: loss = 3.5024036e-10,1.5343876e-10\n",
      "Iteration 27455: loss = 3.5012107e-10,1.5343603e-10\n",
      "Iteration 27460: loss = 3.5000133e-10,1.5343316e-10\n",
      "Iteration 27465: loss = 3.49887e-10,1.5342985e-10\n",
      "Iteration 27470: loss = 3.4976153e-10,1.534283e-10\n",
      "Iteration 27475: loss = 3.496254e-10,1.5342752e-10\n",
      "Iteration 27480: loss = 3.4948322e-10,1.5342665e-10\n",
      "Iteration 27485: loss = 3.4516603e-10,1.534254e-10\n",
      "Iteration 27490: loss = 3.4505412e-10,1.5342216e-10\n",
      "Iteration 27495: loss = 3.4492506e-10,1.5342043e-10\n",
      "Iteration 27500: loss = 3.4476924e-10,1.5342255e-10\n",
      "Iteration 27505: loss = 3.446149e-10,1.5342357e-10\n",
      "Iteration 27510: loss = 3.4447203e-10,1.5342404e-10\n",
      "Iteration 27515: loss = 3.4432868e-10,1.5342402e-10\n",
      "Iteration 27520: loss = 3.4419045e-10,1.534235e-10\n",
      "Iteration 27525: loss = 3.4405792e-10,1.5342283e-10\n",
      "Iteration 27530: loss = 3.4393166e-10,1.5342087e-10\n",
      "Iteration 27535: loss = 3.4381653e-10,1.534188e-10\n",
      "Iteration 27540: loss = 3.4369677e-10,1.5341617e-10\n",
      "Iteration 27545: loss = 3.4358016e-10,1.5341313e-10\n",
      "Iteration 27550: loss = 3.4347392e-10,1.5340988e-10\n",
      "Iteration 27555: loss = 3.433569e-10,1.5340679e-10\n",
      "Iteration 27560: loss = 3.4324707e-10,1.534041e-10\n",
      "Iteration 27565: loss = 3.4105163e-10,1.5340129e-10\n",
      "Iteration 27570: loss = 3.388636e-10,1.533985e-10\n",
      "Iteration 27575: loss = 3.3874867e-10,1.533963e-10\n",
      "Iteration 27580: loss = 3.3860625e-10,1.5339657e-10\n",
      "Iteration 27585: loss = 3.3846934e-10,1.5339667e-10\n",
      "Iteration 27590: loss = 3.383449e-10,1.53395e-10\n",
      "Iteration 27595: loss = 3.3822553e-10,1.533929e-10\n",
      "Iteration 27600: loss = 3.3810144e-10,1.5339123e-10\n",
      "Iteration 27605: loss = 3.379861e-10,1.5338893e-10\n",
      "Iteration 27610: loss = 3.378684e-10,1.5338691e-10\n",
      "Iteration 27615: loss = 3.3774525e-10,1.533847e-10\n",
      "Iteration 27620: loss = 3.3762915e-10,1.5338264e-10\n",
      "Iteration 27625: loss = 3.3751094e-10,1.5338064e-10\n",
      "Iteration 27630: loss = 3.3738964e-10,1.5337924e-10\n",
      "Iteration 27635: loss = 3.37264e-10,1.5337753e-10\n",
      "Iteration 27640: loss = 3.371504e-10,1.5337512e-10\n",
      "Iteration 27645: loss = 3.3703942e-10,1.5337256e-10\n",
      "Iteration 27650: loss = 3.3486358e-10,1.5337012e-10\n",
      "Iteration 27655: loss = 3.3473183e-10,1.5336933e-10\n",
      "Iteration 27660: loss = 3.3256795e-10,1.5336743e-10\n",
      "Iteration 27665: loss = 3.3245007e-10,1.5336551e-10\n",
      "Iteration 27670: loss = 3.3230632e-10,1.533664e-10\n",
      "Iteration 27675: loss = 3.321655e-10,1.5336721e-10\n",
      "Iteration 27680: loss = 3.320263e-10,1.5336792e-10\n",
      "Iteration 27685: loss = 3.3188827e-10,1.5336823e-10\n",
      "Iteration 27690: loss = 3.3174696e-10,1.5336932e-10\n",
      "Iteration 27695: loss = 3.3160608e-10,1.5337011e-10\n",
      "Iteration 27700: loss = 3.314754e-10,1.5336998e-10\n",
      "Iteration 27705: loss = 3.3134687e-10,1.533694e-10\n",
      "Iteration 27710: loss = 3.312222e-10,1.5336858e-10\n",
      "Iteration 27715: loss = 3.3110167e-10,1.533671e-10\n",
      "Iteration 27720: loss = 3.309848e-10,1.533651e-10\n",
      "Iteration 27725: loss = 3.2883166e-10,1.5336285e-10\n",
      "Iteration 27730: loss = 3.2871939e-10,1.5336121e-10\n",
      "Iteration 27735: loss = 3.2859918e-10,1.5336017e-10\n",
      "Iteration 27740: loss = 3.2848013e-10,1.533584e-10\n",
      "Iteration 27745: loss = 3.2631886e-10,1.5335838e-10\n",
      "Iteration 27750: loss = 3.2618852e-10,1.5335777e-10\n",
      "Iteration 27755: loss = 3.2607184e-10,1.5335685e-10\n",
      "Iteration 27760: loss = 3.2594297e-10,1.5335655e-10\n",
      "Iteration 27765: loss = 3.258019e-10,1.5335745e-10\n",
      "Iteration 27770: loss = 3.256915e-10,1.5335552e-10\n",
      "Iteration 27775: loss = 3.2557604e-10,1.5335397e-10\n",
      "Iteration 27780: loss = 3.2543765e-10,1.5335455e-10\n",
      "Iteration 27785: loss = 3.2531014e-10,1.5335505e-10\n",
      "Iteration 27790: loss = 3.2518496e-10,1.533542e-10\n",
      "Iteration 27795: loss = 3.2507838e-10,1.5335225e-10\n",
      "Iteration 27800: loss = 3.2496905e-10,1.5334976e-10\n",
      "Iteration 27805: loss = 3.248621e-10,1.5334674e-10\n",
      "Iteration 27810: loss = 3.2273703e-10,1.5334417e-10\n",
      "Iteration 27815: loss = 3.2262062e-10,1.533428e-10\n",
      "Iteration 27820: loss = 3.2252737e-10,1.5333877e-10\n",
      "Iteration 27825: loss = 3.224292e-10,1.5333493e-10\n",
      "Iteration 27830: loss = 3.223374e-10,1.5333082e-10\n",
      "Iteration 27835: loss = 3.2022066e-10,1.5332843e-10\n",
      "Iteration 27840: loss = 3.2008826e-10,1.533288e-10\n",
      "Iteration 27845: loss = 3.1996647e-10,1.5332835e-10\n",
      "Iteration 27850: loss = 3.198395e-10,1.5332843e-10\n",
      "Iteration 27855: loss = 3.197191e-10,1.5332775e-10\n",
      "Iteration 27860: loss = 3.19605e-10,1.5332613e-10\n",
      "Iteration 27865: loss = 3.194964e-10,1.533244e-10\n",
      "Iteration 27870: loss = 3.1937522e-10,1.5332409e-10\n",
      "Iteration 27875: loss = 3.1924677e-10,1.5332478e-10\n",
      "Iteration 27880: loss = 3.1911876e-10,1.5332455e-10\n",
      "Iteration 27885: loss = 3.1900982e-10,1.5332279e-10\n",
      "Iteration 27890: loss = 3.189049e-10,1.533206e-10\n",
      "Iteration 27895: loss = 3.1679195e-10,1.5331852e-10\n",
      "Iteration 27900: loss = 3.166872e-10,1.5331654e-10\n",
      "Iteration 27905: loss = 3.165909e-10,1.5331371e-10\n",
      "Iteration 27910: loss = 3.1649816e-10,1.533098e-10\n",
      "Iteration 27915: loss = 3.164041e-10,1.5330612e-10\n",
      "Iteration 27920: loss = 3.1631195e-10,1.5330275e-10\n",
      "Iteration 27925: loss = 3.142195e-10,1.5329968e-10\n",
      "Iteration 27930: loss = 3.1408776e-10,1.5330019e-10\n",
      "Iteration 27935: loss = 3.13953e-10,1.533018e-10\n",
      "Iteration 27940: loss = 3.1383166e-10,1.5330154e-10\n",
      "Iteration 27945: loss = 3.137218e-10,1.5330094e-10\n",
      "Iteration 27950: loss = 3.136045e-10,1.5329976e-10\n",
      "Iteration 27955: loss = 3.134832e-10,1.5329973e-10\n",
      "Iteration 27960: loss = 3.1336256e-10,1.5329943e-10\n",
      "Iteration 27965: loss = 3.1324696e-10,1.5329885e-10\n",
      "Iteration 27970: loss = 3.1313308e-10,1.5329779e-10\n",
      "Iteration 27975: loss = 3.1302597e-10,1.53296e-10\n",
      "Iteration 27980: loss = 3.109305e-10,1.5329496e-10\n",
      "Iteration 27985: loss = 3.108239e-10,1.5329328e-10\n",
      "Iteration 27990: loss = 3.10721e-10,1.5329091e-10\n",
      "Iteration 27995: loss = 3.10624e-10,1.5328855e-10\n",
      "Iteration 28000: loss = 3.1051986e-10,1.5328645e-10\n",
      "Iteration 28005: loss = 3.1042582e-10,1.5328384e-10\n",
      "Iteration 28010: loss = 3.103264e-10,1.5328093e-10\n",
      "Iteration 28015: loss = 3.1022065e-10,1.5327883e-10\n",
      "Iteration 28020: loss = 3.0813146e-10,1.5327893e-10\n",
      "Iteration 28025: loss = 3.0800915e-10,1.532793e-10\n",
      "Iteration 28030: loss = 3.078804e-10,1.5327986e-10\n",
      "Iteration 28035: loss = 3.0776007e-10,1.5328036e-10\n",
      "Iteration 28040: loss = 3.0764294e-10,1.5328064e-10\n",
      "Iteration 28045: loss = 3.0752137e-10,1.5328075e-10\n",
      "Iteration 28050: loss = 3.0741054e-10,1.5328006e-10\n",
      "Iteration 28055: loss = 3.0730196e-10,1.5327885e-10\n",
      "Iteration 28060: loss = 3.072019e-10,1.5327725e-10\n",
      "Iteration 28065: loss = 3.0709776e-10,1.5327505e-10\n",
      "Iteration 28070: loss = 3.0503514e-10,1.5327294e-10\n",
      "Iteration 28075: loss = 3.0493497e-10,1.5327062e-10\n",
      "Iteration 28080: loss = 3.0482147e-10,1.5326981e-10\n",
      "Iteration 28085: loss = 3.0471795e-10,1.5326843e-10\n",
      "Iteration 28090: loss = 3.0461625e-10,1.5326687e-10\n",
      "Iteration 28095: loss = 3.0448902e-10,1.5326843e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28100: loss = 3.0436742e-10,1.5326887e-10\n",
      "Iteration 28105: loss = 3.0425426e-10,1.5326852e-10\n",
      "Iteration 28110: loss = 3.021844e-10,1.532688e-10\n",
      "Iteration 28115: loss = 3.020586e-10,1.5326912e-10\n",
      "Iteration 28120: loss = 3.0195604e-10,1.5326805e-10\n",
      "Iteration 28125: loss = 3.0185307e-10,1.5326648e-10\n",
      "Iteration 28130: loss = 3.0174926e-10,1.5326493e-10\n",
      "Iteration 28135: loss = 3.0164174e-10,1.5326404e-10\n",
      "Iteration 28140: loss = 3.015286e-10,1.5326385e-10\n",
      "Iteration 28145: loss = 3.0141767e-10,1.5326368e-10\n",
      "Iteration 28150: loss = 3.013151e-10,1.5326285e-10\n",
      "Iteration 28155: loss = 2.992643e-10,1.5326151e-10\n",
      "Iteration 28160: loss = 2.991657e-10,1.5325947e-10\n",
      "Iteration 28165: loss = 2.9906805e-10,1.5325746e-10\n",
      "Iteration 28170: loss = 2.989713e-10,1.5325537e-10\n",
      "Iteration 28175: loss = 2.9886435e-10,1.5325463e-10\n",
      "Iteration 28180: loss = 2.9873634e-10,1.532569e-10\n",
      "Iteration 28185: loss = 2.9861208e-10,1.5325834e-10\n",
      "Iteration 28190: loss = 2.9849442e-10,1.5325868e-10\n",
      "Iteration 28195: loss = 2.9838146e-10,1.5325867e-10\n",
      "Iteration 28200: loss = 2.96331e-10,1.5325903e-10\n",
      "Iteration 28205: loss = 2.9620592e-10,1.5326049e-10\n",
      "Iteration 28210: loss = 2.960934e-10,1.5326003e-10\n",
      "Iteration 28215: loss = 2.9597896e-10,1.5326115e-10\n",
      "Iteration 28220: loss = 2.9585373e-10,1.5326283e-10\n",
      "Iteration 28225: loss = 2.95736e-10,1.5326308e-10\n",
      "Iteration 28230: loss = 2.9563094e-10,1.5326335e-10\n",
      "Iteration 28235: loss = 2.9552338e-10,1.532624e-10\n",
      "Iteration 28240: loss = 2.9349267e-10,1.5326124e-10\n",
      "Iteration 28245: loss = 2.9339445e-10,1.5326039e-10\n",
      "Iteration 28250: loss = 2.9329206e-10,1.5325904e-10\n",
      "Iteration 28255: loss = 2.932047e-10,1.532569e-10\n",
      "Iteration 28260: loss = 2.9307953e-10,1.532581e-10\n",
      "Iteration 28265: loss = 2.9295935e-10,1.5325949e-10\n",
      "Iteration 28270: loss = 2.9284733e-10,1.5326036e-10\n",
      "Iteration 28275: loss = 2.927335e-10,1.5326088e-10\n",
      "Iteration 28280: loss = 2.9262306e-10,1.5326061e-10\n",
      "Iteration 28285: loss = 2.9251845e-10,1.5326053e-10\n",
      "Iteration 28290: loss = 2.9047997e-10,1.5326188e-10\n",
      "Iteration 28295: loss = 2.9037608e-10,1.5326179e-10\n",
      "Iteration 28300: loss = 2.9026223e-10,1.5326276e-10\n",
      "Iteration 28305: loss = 2.9016614e-10,1.532611e-10\n",
      "Iteration 28310: loss = 2.90068e-10,1.5325989e-10\n",
      "Iteration 28315: loss = 2.8994596e-10,1.532612e-10\n",
      "Iteration 28320: loss = 2.898407e-10,1.5326135e-10\n",
      "Iteration 28325: loss = 2.8783867e-10,1.5325968e-10\n",
      "Iteration 28330: loss = 2.877418e-10,1.5325782e-10\n",
      "Iteration 28335: loss = 2.8765457e-10,1.5325544e-10\n",
      "Iteration 28340: loss = 2.8756272e-10,1.5325324e-10\n",
      "Iteration 28345: loss = 2.8745967e-10,1.5325294e-10\n",
      "Iteration 28350: loss = 2.8734568e-10,1.5325413e-10\n",
      "Iteration 28355: loss = 2.8724137e-10,1.5325427e-10\n",
      "Iteration 28360: loss = 2.8714017e-10,1.5325352e-10\n",
      "Iteration 28365: loss = 2.870359e-10,1.5325384e-10\n",
      "Iteration 28370: loss = 2.8693684e-10,1.5325316e-10\n",
      "Iteration 28375: loss = 2.868368e-10,1.5325266e-10\n",
      "Iteration 28380: loss = 2.8673722e-10,1.5325166e-10\n",
      "Iteration 28385: loss = 2.8473934e-10,1.5325093e-10\n",
      "Iteration 28390: loss = 2.8463112e-10,1.5325144e-10\n",
      "Iteration 28395: loss = 2.8453437e-10,1.5324994e-10\n",
      "Iteration 28400: loss = 2.84443e-10,1.5324902e-10\n",
      "Iteration 28405: loss = 2.8434557e-10,1.5324744e-10\n",
      "Iteration 28410: loss = 2.8425837e-10,1.5324506e-10\n",
      "Iteration 28415: loss = 2.841761e-10,1.5324277e-10\n",
      "Iteration 28420: loss = 2.8218314e-10,1.5324282e-10\n",
      "Iteration 28425: loss = 2.8210226e-10,1.5323932e-10\n",
      "Iteration 28430: loss = 2.8200878e-10,1.5323869e-10\n",
      "Iteration 28435: loss = 2.8189398e-10,1.5324018e-10\n",
      "Iteration 28440: loss = 2.8178396e-10,1.5324093e-10\n",
      "Iteration 28445: loss = 2.8168923e-10,1.5324028e-10\n",
      "Iteration 28450: loss = 2.8159153e-10,1.5323941e-10\n",
      "Iteration 28455: loss = 2.814942e-10,1.532384e-10\n",
      "Iteration 28460: loss = 2.8140001e-10,1.5323803e-10\n",
      "Iteration 28465: loss = 2.812993e-10,1.5323745e-10\n",
      "Iteration 28470: loss = 2.8119948e-10,1.5323683e-10\n",
      "Iteration 28475: loss = 2.8110925e-10,1.5323584e-10\n",
      "Iteration 28480: loss = 2.8101527e-10,1.5323474e-10\n",
      "Iteration 28485: loss = 2.8092192e-10,1.5323331e-10\n",
      "Iteration 28490: loss = 2.7895178e-10,1.5323214e-10\n",
      "Iteration 28495: loss = 2.7885547e-10,1.5323166e-10\n",
      "Iteration 28500: loss = 2.787693e-10,1.5322965e-10\n",
      "Iteration 28505: loss = 2.7867772e-10,1.5322812e-10\n",
      "Iteration 28510: loss = 2.7859284e-10,1.5322615e-10\n",
      "Iteration 28515: loss = 2.785107e-10,1.5322428e-10\n",
      "Iteration 28520: loss = 2.7655733e-10,1.5322107e-10\n",
      "Iteration 28525: loss = 2.764766e-10,1.5321883e-10\n",
      "Iteration 28530: loss = 2.7636818e-10,1.5322027e-10\n",
      "Iteration 28535: loss = 2.7625188e-10,1.5322199e-10\n",
      "Iteration 28540: loss = 2.7614502e-10,1.5322327e-10\n",
      "Iteration 28545: loss = 2.7603833e-10,1.5322454e-10\n",
      "Iteration 28550: loss = 2.7592392e-10,1.5322621e-10\n",
      "Iteration 28555: loss = 2.7581537e-10,1.5322754e-10\n",
      "Iteration 28560: loss = 2.7571176e-10,1.5322864e-10\n",
      "Iteration 28565: loss = 2.7561095e-10,1.5322846e-10\n",
      "Iteration 28570: loss = 2.755218e-10,1.5322758e-10\n",
      "Iteration 28575: loss = 2.7543398e-10,1.5322638e-10\n",
      "Iteration 28580: loss = 2.7534977e-10,1.5322471e-10\n",
      "Iteration 28585: loss = 2.7526129e-10,1.5322295e-10\n",
      "Iteration 28590: loss = 2.7331926e-10,1.5322091e-10\n",
      "Iteration 28595: loss = 2.7322783e-10,1.5322027e-10\n",
      "Iteration 28600: loss = 2.731506e-10,1.532173e-10\n",
      "Iteration 28605: loss = 2.73067e-10,1.5321543e-10\n",
      "Iteration 28610: loss = 2.7298264e-10,1.532137e-10\n",
      "Iteration 28615: loss = 2.710461e-10,1.532121e-10\n",
      "Iteration 28620: loss = 2.7095656e-10,1.5321105e-10\n",
      "Iteration 28625: loss = 2.7083363e-10,1.532146e-10\n",
      "Iteration 28630: loss = 2.7072097e-10,1.5321694e-10\n",
      "Iteration 28635: loss = 2.7062516e-10,1.5321658e-10\n",
      "Iteration 28640: loss = 2.705416e-10,1.5321522e-10\n",
      "Iteration 28645: loss = 2.7045224e-10,1.5321423e-10\n",
      "Iteration 28650: loss = 2.703584e-10,1.5321358e-10\n",
      "Iteration 28655: loss = 2.7025115e-10,1.5321536e-10\n",
      "Iteration 28660: loss = 2.701462e-10,1.532165e-10\n",
      "Iteration 28665: loss = 2.7004501e-10,1.5321733e-10\n",
      "Iteration 28670: loss = 2.699568e-10,1.5321638e-10\n",
      "Iteration 28675: loss = 2.6986982e-10,1.532153e-10\n",
      "Iteration 28680: loss = 2.6978517e-10,1.5321369e-10\n",
      "Iteration 28685: loss = 2.6970365e-10,1.5321189e-10\n",
      "Iteration 28690: loss = 2.6962407e-10,1.5320961e-10\n",
      "Iteration 28695: loss = 2.677011e-10,1.5320759e-10\n",
      "Iteration 28700: loss = 2.6761166e-10,1.5320709e-10\n",
      "Iteration 28705: loss = 2.675301e-10,1.5320556e-10\n",
      "Iteration 28710: loss = 2.6560518e-10,1.5320503e-10\n",
      "Iteration 28715: loss = 2.655326e-10,1.5320212e-10\n",
      "Iteration 28720: loss = 2.6544825e-10,1.5320092e-10\n",
      "Iteration 28725: loss = 2.6535202e-10,1.5320167e-10\n",
      "Iteration 28730: loss = 2.6524716e-10,1.5320278e-10\n",
      "Iteration 28735: loss = 2.6514838e-10,1.5320409e-10\n",
      "Iteration 28740: loss = 2.6505426e-10,1.532044e-10\n",
      "Iteration 28745: loss = 2.6496136e-10,1.532039e-10\n",
      "Iteration 28750: loss = 2.6487837e-10,1.5320288e-10\n",
      "Iteration 28755: loss = 2.6479774e-10,1.5320134e-10\n",
      "Iteration 28760: loss = 2.647039e-10,1.5320152e-10\n",
      "Iteration 28765: loss = 2.646027e-10,1.5320259e-10\n",
      "Iteration 28770: loss = 2.645028e-10,1.5320319e-10\n",
      "Iteration 28775: loss = 2.644102e-10,1.5320381e-10\n",
      "Iteration 28780: loss = 2.6431538e-10,1.5320387e-10\n",
      "Iteration 28785: loss = 2.642323e-10,1.5320238e-10\n",
      "Iteration 28790: loss = 2.641533e-10,1.5320092e-10\n",
      "Iteration 28795: loss = 2.6406674e-10,1.5320024e-10\n",
      "Iteration 28800: loss = 2.6215316e-10,1.5319951e-10\n",
      "Iteration 28805: loss = 2.6207345e-10,1.5319798e-10\n",
      "Iteration 28810: loss = 2.6017596e-10,1.5319734e-10\n",
      "Iteration 28815: loss = 2.600978e-10,1.5319543e-10\n",
      "Iteration 28820: loss = 2.600011e-10,1.5319623e-10\n",
      "Iteration 28825: loss = 2.5990468e-10,1.5319737e-10\n",
      "Iteration 28830: loss = 2.5980953e-10,1.5319791e-10\n",
      "Iteration 28835: loss = 2.5971708e-10,1.5319841e-10\n",
      "Iteration 28840: loss = 2.596351e-10,1.5319716e-10\n",
      "Iteration 28845: loss = 2.5955196e-10,1.531965e-10\n",
      "Iteration 28850: loss = 2.5946764e-10,1.5319562e-10\n",
      "Iteration 28855: loss = 2.5938302e-10,1.5319451e-10\n",
      "Iteration 28860: loss = 2.5930327e-10,1.5319367e-10\n",
      "Iteration 28865: loss = 2.592259e-10,1.5319167e-10\n",
      "Iteration 28870: loss = 2.5914992e-10,1.5318957e-10\n",
      "Iteration 28875: loss = 2.5907534e-10,1.5318766e-10\n",
      "Iteration 28880: loss = 2.5898092e-10,1.5318805e-10\n",
      "Iteration 28885: loss = 2.5888872e-10,1.5318873e-10\n",
      "Iteration 28890: loss = 2.588003e-10,1.5318896e-10\n",
      "Iteration 28895: loss = 2.5870603e-10,1.5318888e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28900: loss = 2.5862354e-10,1.5318853e-10\n",
      "Iteration 28905: loss = 2.5854965e-10,1.531865e-10\n",
      "Iteration 28910: loss = 2.5487426e-10,1.5318487e-10\n",
      "Iteration 28915: loss = 2.5479777e-10,1.5318355e-10\n",
      "Iteration 28920: loss = 2.546933e-10,1.5318558e-10\n",
      "Iteration 28925: loss = 2.545932e-10,1.5318731e-10\n",
      "Iteration 28930: loss = 2.5449995e-10,1.5318882e-10\n",
      "Iteration 28935: loss = 2.5440322e-10,1.5318982e-10\n",
      "Iteration 28940: loss = 2.5430982e-10,1.5319063e-10\n",
      "Iteration 28945: loss = 2.5422192e-10,1.5319135e-10\n",
      "Iteration 28950: loss = 2.5413702e-10,1.5319068e-10\n",
      "Iteration 28955: loss = 2.540566e-10,1.5318971e-10\n",
      "Iteration 28960: loss = 2.5398086e-10,1.5318868e-10\n",
      "Iteration 28965: loss = 2.5390012e-10,1.5318767e-10\n",
      "Iteration 28970: loss = 2.5382063e-10,1.5318652e-10\n",
      "Iteration 28975: loss = 2.5374966e-10,1.5318459e-10\n",
      "Iteration 28980: loss = 2.5367503e-10,1.531827e-10\n",
      "Iteration 28985: loss = 2.5360333e-10,1.5318029e-10\n",
      "Iteration 28990: loss = 2.5353777e-10,1.5317796e-10\n",
      "Iteration 28995: loss = 2.534638e-10,1.5317603e-10\n",
      "Iteration 29000: loss = 2.5338845e-10,1.5317436e-10\n",
      "Iteration 29005: loss = 2.5331293e-10,1.5317257e-10\n",
      "Iteration 29010: loss = 2.532443e-10,1.5317057e-10\n",
      "Iteration 29015: loss = 2.5317184e-10,1.5316833e-10\n",
      "Iteration 29020: loss = 2.513129e-10,1.5316631e-10\n",
      "Iteration 29025: loss = 2.4945493e-10,1.5316674e-10\n",
      "Iteration 29030: loss = 2.4935384e-10,1.5316873e-10\n",
      "Iteration 29035: loss = 2.4925237e-10,1.5317136e-10\n",
      "Iteration 29040: loss = 2.491585e-10,1.5317331e-10\n",
      "Iteration 29045: loss = 2.4906396e-10,1.5317442e-10\n",
      "Iteration 29050: loss = 2.4899047e-10,1.5317288e-10\n",
      "Iteration 29055: loss = 2.4891023e-10,1.531724e-10\n",
      "Iteration 29060: loss = 2.4881658e-10,1.5317438e-10\n",
      "Iteration 29065: loss = 2.487344e-10,1.5317404e-10\n",
      "Iteration 29070: loss = 2.4865573e-10,1.5317321e-10\n",
      "Iteration 29075: loss = 2.4858268e-10,1.531716e-10\n",
      "Iteration 29080: loss = 2.4851024e-10,1.5317028e-10\n",
      "Iteration 29085: loss = 2.484298e-10,1.5316956e-10\n",
      "Iteration 29090: loss = 2.483537e-10,1.5316859e-10\n",
      "Iteration 29095: loss = 2.482808e-10,1.5316755e-10\n",
      "Iteration 29100: loss = 2.4820337e-10,1.5316626e-10\n",
      "Iteration 29105: loss = 2.4813007e-10,1.5316529e-10\n",
      "Iteration 29110: loss = 2.4805621e-10,1.5316388e-10\n",
      "Iteration 29115: loss = 2.4797883e-10,1.5316272e-10\n",
      "Iteration 29120: loss = 2.479075e-10,1.5316146e-10\n",
      "Iteration 29125: loss = 2.4605926e-10,1.531617e-10\n",
      "Iteration 29130: loss = 2.4595548e-10,1.5316431e-10\n",
      "Iteration 29135: loss = 2.4411276e-10,1.5316501e-10\n",
      "Iteration 29140: loss = 2.4402366e-10,1.5316612e-10\n",
      "Iteration 29145: loss = 2.439352e-10,1.5316692e-10\n",
      "Iteration 29150: loss = 2.4385138e-10,1.5316759e-10\n",
      "Iteration 29155: loss = 2.437804e-10,1.5316595e-10\n",
      "Iteration 29160: loss = 2.437137e-10,1.5316423e-10\n",
      "Iteration 29165: loss = 2.4364338e-10,1.5316262e-10\n",
      "Iteration 29170: loss = 2.435642e-10,1.531619e-10\n",
      "Iteration 29175: loss = 2.434755e-10,1.5316287e-10\n",
      "Iteration 29180: loss = 2.4339739e-10,1.5316262e-10\n",
      "Iteration 29185: loss = 2.433282e-10,1.531609e-10\n",
      "Iteration 29190: loss = 2.4326016e-10,1.531591e-10\n",
      "Iteration 29195: loss = 2.4319388e-10,1.531567e-10\n",
      "Iteration 29200: loss = 2.431303e-10,1.5315432e-10\n",
      "Iteration 29205: loss = 2.4306737e-10,1.5315155e-10\n",
      "Iteration 29210: loss = 2.4299776e-10,1.5314991e-10\n",
      "Iteration 29215: loss = 2.429214e-10,1.5314942e-10\n",
      "Iteration 29220: loss = 2.4284366e-10,1.5314923e-10\n",
      "Iteration 29225: loss = 2.4276994e-10,1.5314817e-10\n",
      "Iteration 29230: loss = 2.4269706e-10,1.5314712e-10\n",
      "Iteration 29235: loss = 2.4085647e-10,1.5314935e-10\n",
      "Iteration 29240: loss = 2.407742e-10,1.5315021e-10\n",
      "Iteration 29245: loss = 2.4068839e-10,1.5315116e-10\n",
      "Iteration 29250: loss = 2.4060273e-10,1.531517e-10\n",
      "Iteration 29255: loss = 2.3878063e-10,1.5315296e-10\n",
      "Iteration 29260: loss = 2.386979e-10,1.5315357e-10\n",
      "Iteration 29265: loss = 2.3862004e-10,1.5315316e-10\n",
      "Iteration 29270: loss = 2.3854238e-10,1.5315331e-10\n",
      "Iteration 29275: loss = 2.3847016e-10,1.5315266e-10\n",
      "Iteration 29280: loss = 2.3839386e-10,1.5315207e-10\n",
      "Iteration 29285: loss = 2.3832503e-10,1.531511e-10\n",
      "Iteration 29290: loss = 2.3825958e-10,1.5314969e-10\n",
      "Iteration 29295: loss = 2.3819077e-10,1.5314751e-10\n",
      "Iteration 29300: loss = 2.381284e-10,1.5314497e-10\n",
      "Iteration 29305: loss = 2.3806132e-10,1.531434e-10\n",
      "Iteration 29310: loss = 2.3798125e-10,1.5314365e-10\n",
      "Iteration 29315: loss = 2.3788813e-10,1.5314659e-10\n",
      "Iteration 29320: loss = 2.3778893e-10,1.5315008e-10\n",
      "Iteration 29325: loss = 2.3769078e-10,1.5315327e-10\n",
      "Iteration 29330: loss = 2.375983e-10,1.5315622e-10\n",
      "Iteration 29335: loss = 2.3751043e-10,1.5315724e-10\n",
      "Iteration 29340: loss = 2.3570615e-10,1.5315735e-10\n",
      "Iteration 29345: loss = 2.356239e-10,1.531587e-10\n",
      "Iteration 29350: loss = 2.3553356e-10,1.5316039e-10\n",
      "Iteration 29355: loss = 2.3544586e-10,1.5316257e-10\n",
      "Iteration 29360: loss = 2.3535865e-10,1.5316456e-10\n",
      "Iteration 29365: loss = 2.3355787e-10,1.531652e-10\n",
      "Iteration 29370: loss = 2.334831e-10,1.5316491e-10\n",
      "Iteration 29375: loss = 2.334035e-10,1.531658e-10\n",
      "Iteration 29380: loss = 2.33329e-10,1.5316559e-10\n",
      "Iteration 29385: loss = 2.3325838e-10,1.5316469e-10\n",
      "Iteration 29390: loss = 2.3319144e-10,1.531639e-10\n",
      "Iteration 29395: loss = 2.331268e-10,1.5316229e-10\n",
      "Iteration 29400: loss = 2.3305988e-10,1.5316096e-10\n",
      "Iteration 29405: loss = 2.3296973e-10,1.5316341e-10\n",
      "Iteration 29410: loss = 2.3287972e-10,1.5316567e-10\n",
      "Iteration 29415: loss = 2.3280133e-10,1.5316648e-10\n",
      "Iteration 29420: loss = 2.327253e-10,1.5316665e-10\n",
      "Iteration 29425: loss = 2.326484e-10,1.531672e-10\n",
      "Iteration 29430: loss = 2.3257248e-10,1.5316744e-10\n",
      "Iteration 29435: loss = 2.3249719e-10,1.5316742e-10\n",
      "Iteration 29440: loss = 2.3241954e-10,1.531684e-10\n",
      "Iteration 29445: loss = 2.306334e-10,1.5316882e-10\n",
      "Iteration 29450: loss = 2.3055509e-10,1.5316937e-10\n",
      "Iteration 29455: loss = 2.304783e-10,1.5317024e-10\n",
      "Iteration 29460: loss = 2.3040032e-10,1.5317068e-10\n",
      "Iteration 29465: loss = 2.3031703e-10,1.5317246e-10\n",
      "Iteration 29470: loss = 2.3023472e-10,1.5317388e-10\n",
      "Iteration 29475: loss = 2.301563e-10,1.5317422e-10\n",
      "Iteration 29480: loss = 2.2839586e-10,1.5317263e-10\n",
      "Iteration 29485: loss = 2.283261e-10,1.53172e-10\n",
      "Iteration 29490: loss = 2.2825193e-10,1.5317177e-10\n",
      "Iteration 29495: loss = 2.2817714e-10,1.5317192e-10\n",
      "Iteration 29500: loss = 2.2811586e-10,1.531709e-10\n",
      "Iteration 29505: loss = 2.2803663e-10,1.5317168e-10\n",
      "Iteration 29510: loss = 2.2794848e-10,1.5317386e-10\n",
      "Iteration 29515: loss = 2.2786821e-10,1.5317589e-10\n",
      "Iteration 29520: loss = 2.27786e-10,1.5317732e-10\n",
      "Iteration 29525: loss = 2.2770465e-10,1.5317864e-10\n",
      "Iteration 29530: loss = 2.2762524e-10,1.5318033e-10\n",
      "Iteration 29535: loss = 2.2753688e-10,1.5318263e-10\n",
      "Iteration 29540: loss = 2.2744863e-10,1.5318513e-10\n",
      "Iteration 29545: loss = 2.2736645e-10,1.5318757e-10\n",
      "Iteration 29550: loss = 2.2558884e-10,1.5318985e-10\n",
      "Iteration 29555: loss = 2.2551659e-10,1.531904e-10\n",
      "Iteration 29560: loss = 2.2544831e-10,1.5318916e-10\n",
      "Iteration 29565: loss = 2.2538725e-10,1.5318777e-10\n",
      "Iteration 29570: loss = 2.2532538e-10,1.5318666e-10\n",
      "Iteration 29575: loss = 2.2526044e-10,1.5318531e-10\n",
      "Iteration 29580: loss = 2.2520201e-10,1.5318348e-10\n",
      "Iteration 29585: loss = 2.2514214e-10,1.531821e-10\n",
      "Iteration 29590: loss = 2.2508173e-10,1.5317968e-10\n",
      "Iteration 29595: loss = 2.2502221e-10,1.5317862e-10\n",
      "Iteration 29600: loss = 2.2496273e-10,1.5317704e-10\n",
      "Iteration 29605: loss = 2.2321027e-10,1.5317678e-10\n",
      "Iteration 29610: loss = 2.2314922e-10,1.5317494e-10\n",
      "Iteration 29615: loss = 2.2307052e-10,1.5317647e-10\n",
      "Iteration 29620: loss = 2.2299712e-10,1.5317703e-10\n",
      "Iteration 29625: loss = 2.2292529e-10,1.5317746e-10\n",
      "Iteration 29630: loss = 2.2285025e-10,1.5317905e-10\n",
      "Iteration 29635: loss = 2.2275816e-10,1.531823e-10\n",
      "Iteration 29640: loss = 2.2267221e-10,1.5318541e-10\n",
      "Iteration 29645: loss = 2.2258645e-10,1.5318771e-10\n",
      "Iteration 29650: loss = 2.2250361e-10,1.5319047e-10\n",
      "Iteration 29655: loss = 2.2242141e-10,1.5319282e-10\n",
      "Iteration 29660: loss = 2.2233626e-10,1.5319507e-10\n",
      "Iteration 29665: loss = 2.2059637e-10,1.531957e-10\n",
      "Iteration 29670: loss = 2.2053365e-10,1.5319479e-10\n",
      "Iteration 29675: loss = 2.2047082e-10,1.5319412e-10\n",
      "Iteration 29680: loss = 2.2040668e-10,1.531932e-10\n",
      "Iteration 29685: loss = 2.2034524e-10,1.53192e-10\n",
      "Iteration 29690: loss = 2.2028591e-10,1.5319089e-10\n",
      "Iteration 29695: loss = 2.2022854e-10,1.5318888e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29700: loss = 2.2017231e-10,1.5318713e-10\n",
      "Iteration 29705: loss = 2.2011225e-10,1.5318513e-10\n",
      "Iteration 29710: loss = 2.2005715e-10,1.5318313e-10\n",
      "Iteration 29715: loss = 2.2000128e-10,1.5318108e-10\n",
      "Iteration 29720: loss = 2.199456e-10,1.5317927e-10\n",
      "Iteration 29725: loss = 2.1988032e-10,1.531788e-10\n",
      "Iteration 29730: loss = 2.1813214e-10,1.5318186e-10\n",
      "Iteration 29735: loss = 2.1804354e-10,1.5318558e-10\n",
      "Iteration 29740: loss = 2.1797264e-10,1.5318691e-10\n",
      "Iteration 29745: loss = 2.1790253e-10,1.531873e-10\n",
      "Iteration 29750: loss = 2.1783274e-10,1.5318755e-10\n",
      "Iteration 29755: loss = 2.177647e-10,1.5318774e-10\n",
      "Iteration 29760: loss = 2.177027e-10,1.5318766e-10\n",
      "Iteration 29765: loss = 2.1763735e-10,1.5318716e-10\n",
      "Iteration 29770: loss = 2.175734e-10,1.5318652e-10\n",
      "Iteration 29775: loss = 2.1751323e-10,1.5318638e-10\n",
      "Iteration 29780: loss = 2.1744871e-10,1.5318567e-10\n",
      "Iteration 29785: loss = 2.1738512e-10,1.5318463e-10\n",
      "Iteration 29790: loss = 2.1566758e-10,1.5318458e-10\n",
      "Iteration 29795: loss = 2.156002e-10,1.5318523e-10\n",
      "Iteration 29800: loss = 2.1553252e-10,1.5318538e-10\n",
      "Iteration 29805: loss = 2.1546687e-10,1.531853e-10\n",
      "Iteration 29810: loss = 2.1540658e-10,1.5318509e-10\n",
      "Iteration 29815: loss = 2.1533643e-10,1.5318558e-10\n",
      "Iteration 29820: loss = 2.1526664e-10,1.5318602e-10\n",
      "Iteration 29825: loss = 2.1520323e-10,1.531862e-10\n",
      "Iteration 29830: loss = 2.151168e-10,1.5318982e-10\n",
      "Iteration 29835: loss = 2.150261e-10,1.5319364e-10\n",
      "Iteration 29840: loss = 2.1493518e-10,1.5319807e-10\n",
      "Iteration 29845: loss = 2.1485147e-10,1.532018e-10\n",
      "Iteration 29850: loss = 2.1311995e-10,1.5320577e-10\n",
      "Iteration 29855: loss = 2.1305957e-10,1.5320561e-10\n",
      "Iteration 29860: loss = 2.1299312e-10,1.5320542e-10\n",
      "Iteration 29865: loss = 2.129326e-10,1.5320513e-10\n",
      "Iteration 29870: loss = 2.1286878e-10,1.5320455e-10\n",
      "Iteration 29875: loss = 2.1280978e-10,1.5320407e-10\n",
      "Iteration 29880: loss = 2.127522e-10,1.53203e-10\n",
      "Iteration 29885: loss = 2.1268916e-10,1.532027e-10\n",
      "Iteration 29890: loss = 2.1263098e-10,1.5320223e-10\n",
      "Iteration 29895: loss = 2.1257231e-10,1.5320131e-10\n",
      "Iteration 29900: loss = 2.125035e-10,1.532017e-10\n",
      "Iteration 29905: loss = 2.1080467e-10,1.5320233e-10\n",
      "Iteration 29910: loss = 2.1074546e-10,1.532014e-10\n",
      "Iteration 29915: loss = 2.1068809e-10,1.5320048e-10\n",
      "Iteration 29920: loss = 2.1063273e-10,1.5319923e-10\n",
      "Iteration 29925: loss = 2.1058442e-10,1.5319639e-10\n",
      "Iteration 29930: loss = 2.1052397e-10,1.5319648e-10\n",
      "Iteration 29935: loss = 2.1045561e-10,1.5319707e-10\n",
      "Iteration 29940: loss = 2.1036771e-10,1.5320131e-10\n",
      "Iteration 29945: loss = 2.1028464e-10,1.5320502e-10\n",
      "Iteration 29950: loss = 2.1020463e-10,1.532085e-10\n",
      "Iteration 29955: loss = 2.1012157e-10,1.5321146e-10\n",
      "Iteration 29960: loss = 2.1005347e-10,1.532132e-10\n",
      "Iteration 29965: loss = 2.099844e-10,1.5321348e-10\n",
      "Iteration 29970: loss = 2.0991998e-10,1.5321422e-10\n",
      "Iteration 29975: loss = 2.0985715e-10,1.5321454e-10\n",
      "Iteration 29980: loss = 2.0816433e-10,1.5321522e-10\n",
      "Iteration 29985: loss = 2.0809637e-10,1.5321669e-10\n",
      "Iteration 29990: loss = 2.0803383e-10,1.5321691e-10\n",
      "Iteration 29995: loss = 2.0797253e-10,1.5321669e-10\n",
      "Iteration 30000: loss = 2.0791312e-10,1.5321633e-10\n",
      "Iteration 30005: loss = 2.0785566e-10,1.5321566e-10\n",
      "Iteration 30010: loss = 2.0780018e-10,1.5321469e-10\n",
      "Iteration 30015: loss = 2.0774538e-10,1.532135e-10\n",
      "Iteration 30020: loss = 2.0769074e-10,1.5321217e-10\n",
      "Iteration 30025: loss = 2.0601792e-10,1.5321173e-10\n",
      "Iteration 30030: loss = 2.059587e-10,1.5321175e-10\n",
      "Iteration 30035: loss = 2.059059e-10,1.532098e-10\n",
      "Iteration 30040: loss = 2.0584782e-10,1.5320928e-10\n",
      "Iteration 30045: loss = 2.0579403e-10,1.5320874e-10\n",
      "Iteration 30050: loss = 2.0571315e-10,1.5321187e-10\n",
      "Iteration 30055: loss = 2.0563888e-10,1.5321422e-10\n",
      "Iteration 30060: loss = 2.0556823e-10,1.5321595e-10\n",
      "Iteration 30065: loss = 2.0550454e-10,1.532172e-10\n",
      "Iteration 30070: loss = 2.0543811e-10,1.5321791e-10\n",
      "Iteration 30075: loss = 2.053745e-10,1.532185e-10\n",
      "Iteration 30080: loss = 2.0531676e-10,1.5321863e-10\n",
      "Iteration 30085: loss = 2.0525583e-10,1.5321847e-10\n",
      "Iteration 30090: loss = 2.0519414e-10,1.5321852e-10\n",
      "Iteration 30095: loss = 2.0513259e-10,1.532186e-10\n",
      "Iteration 30100: loss = 2.0507634e-10,1.5321867e-10\n",
      "Iteration 30105: loss = 2.0501621e-10,1.5321827e-10\n",
      "Iteration 30110: loss = 2.0495383e-10,1.5321869e-10\n",
      "Iteration 30115: loss = 2.0328454e-10,1.5321916e-10\n",
      "Iteration 30120: loss = 2.0322237e-10,1.5322021e-10\n",
      "Iteration 30125: loss = 2.0314894e-10,1.5322246e-10\n",
      "Iteration 30130: loss = 2.0308073e-10,1.5322378e-10\n",
      "Iteration 30135: loss = 2.0302071e-10,1.5322481e-10\n",
      "Iteration 30140: loss = 2.02957e-10,1.5322547e-10\n",
      "Iteration 30145: loss = 2.0289471e-10,1.5322585e-10\n",
      "Iteration 30150: loss = 2.0124082e-10,1.5322543e-10\n",
      "Iteration 30155: loss = 2.0119593e-10,1.5322328e-10\n",
      "Iteration 30160: loss = 2.011361e-10,1.5322386e-10\n",
      "Iteration 30165: loss = 2.0105623e-10,1.5322749e-10\n",
      "Iteration 30170: loss = 2.0098163e-10,1.5323058e-10\n",
      "Iteration 30175: loss = 2.0090356e-10,1.5323386e-10\n",
      "Iteration 30180: loss = 2.0083463e-10,1.5323602e-10\n",
      "Iteration 30185: loss = 2.0077147e-10,1.5323726e-10\n",
      "Iteration 30190: loss = 2.0070574e-10,1.5323824e-10\n",
      "Iteration 30195: loss = 2.0064174e-10,1.5323977e-10\n",
      "Iteration 30200: loss = 2.0057132e-10,1.5324171e-10\n",
      "Iteration 30205: loss = 2.0050694e-10,1.5324302e-10\n",
      "Iteration 30210: loss = 2.0044466e-10,1.5324421e-10\n",
      "Iteration 30215: loss = 2.0037945e-10,1.5324511e-10\n",
      "Iteration 30220: loss = 2.0031815e-10,1.5324625e-10\n",
      "Iteration 30225: loss = 2.0025308e-10,1.5324747e-10\n",
      "Iteration 30230: loss = 2.0019149e-10,1.5324847e-10\n",
      "Iteration 30235: loss = 2.0013095e-10,1.5324941e-10\n",
      "Iteration 30240: loss = 1.9848022e-10,1.5325015e-10\n",
      "Iteration 30245: loss = 1.9842002e-10,1.5325097e-10\n",
      "Iteration 30250: loss = 1.9835528e-10,1.532526e-10\n",
      "Iteration 30255: loss = 1.9828955e-10,1.5325392e-10\n",
      "Iteration 30260: loss = 1.9823344e-10,1.5325399e-10\n",
      "Iteration 30265: loss = 1.981746e-10,1.5325403e-10\n",
      "Iteration 30270: loss = 1.9655066e-10,1.5325219e-10\n",
      "Iteration 30275: loss = 1.9647799e-10,1.532551e-10\n",
      "Iteration 30280: loss = 1.9641684e-10,1.5325648e-10\n",
      "Iteration 30285: loss = 1.9635533e-10,1.5325732e-10\n",
      "Iteration 30290: loss = 1.96293e-10,1.5325834e-10\n",
      "Iteration 30295: loss = 1.9623465e-10,1.5325942e-10\n",
      "Iteration 30300: loss = 1.961733e-10,1.5326045e-10\n",
      "Iteration 30305: loss = 1.9611417e-10,1.5326058e-10\n",
      "Iteration 30310: loss = 1.9606271e-10,1.532606e-10\n",
      "Iteration 30315: loss = 1.9599473e-10,1.5326282e-10\n",
      "Iteration 30320: loss = 1.9593059e-10,1.5326465e-10\n",
      "Iteration 30325: loss = 1.9586173e-10,1.5326633e-10\n",
      "Iteration 30330: loss = 1.9580049e-10,1.5326736e-10\n",
      "Iteration 30335: loss = 1.9573985e-10,1.5326845e-10\n",
      "Iteration 30340: loss = 1.9568158e-10,1.5326918e-10\n",
      "Iteration 30345: loss = 1.9562311e-10,1.5326963e-10\n",
      "Iteration 30350: loss = 1.955678e-10,1.5327004e-10\n",
      "Iteration 30355: loss = 1.9551039e-10,1.5327019e-10\n",
      "Iteration 30360: loss = 1.9544992e-10,1.5327127e-10\n",
      "Iteration 30365: loss = 1.9538841e-10,1.5327262e-10\n",
      "Iteration 30370: loss = 1.9532716e-10,1.5327381e-10\n",
      "Iteration 30375: loss = 1.9369761e-10,1.5327527e-10\n",
      "Iteration 30380: loss = 1.9364914e-10,1.532737e-10\n",
      "Iteration 30385: loss = 1.9359948e-10,1.5327255e-10\n",
      "Iteration 30390: loss = 1.935541e-10,1.5327169e-10\n",
      "Iteration 30395: loss = 1.9350481e-10,1.532703e-10\n",
      "Iteration 30400: loss = 1.9187864e-10,1.5327274e-10\n",
      "Iteration 30405: loss = 1.9181777e-10,1.5327423e-10\n",
      "Iteration 30410: loss = 1.917631e-10,1.5327503e-10\n",
      "Iteration 30415: loss = 1.9170242e-10,1.5327581e-10\n",
      "Iteration 30420: loss = 1.9164674e-10,1.5327652e-10\n",
      "Iteration 30425: loss = 1.9159002e-10,1.5327666e-10\n",
      "Iteration 30430: loss = 1.9153844e-10,1.532766e-10\n",
      "Iteration 30435: loss = 1.9148604e-10,1.5327666e-10\n",
      "Iteration 30440: loss = 1.9143405e-10,1.5327574e-10\n",
      "Iteration 30445: loss = 1.9138602e-10,1.5327505e-10\n",
      "Iteration 30450: loss = 1.9133346e-10,1.5327446e-10\n",
      "Iteration 30455: loss = 1.9128588e-10,1.5327375e-10\n",
      "Iteration 30460: loss = 1.9123907e-10,1.5327305e-10\n",
      "Iteration 30465: loss = 1.9118873e-10,1.5327217e-10\n",
      "Iteration 30470: loss = 1.9114454e-10,1.5327059e-10\n",
      "Iteration 30475: loss = 1.910968e-10,1.5326904e-10\n",
      "Iteration 30480: loss = 1.9105545e-10,1.5326723e-10\n",
      "Iteration 30485: loss = 1.9100667e-10,1.5326657e-10\n",
      "Iteration 30490: loss = 1.909453e-10,1.5326775e-10\n",
      "Iteration 30495: loss = 1.908858e-10,1.5326898e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30500: loss = 1.908322e-10,1.5326966e-10\n",
      "Iteration 30505: loss = 1.9077527e-10,1.532704e-10\n",
      "Iteration 30510: loss = 1.9072037e-10,1.5327073e-10\n",
      "Iteration 30515: loss = 1.9066686e-10,1.5327137e-10\n",
      "Iteration 30520: loss = 1.9061495e-10,1.5327128e-10\n",
      "Iteration 30525: loss = 1.9057093e-10,1.5326912e-10\n",
      "Iteration 30530: loss = 1.8898312e-10,1.5326694e-10\n",
      "Iteration 30535: loss = 1.8893771e-10,1.5326515e-10\n",
      "Iteration 30540: loss = 1.8734164e-10,1.5326651e-10\n",
      "Iteration 30545: loss = 1.8727474e-10,1.5326888e-10\n",
      "Iteration 30550: loss = 1.8720847e-10,1.5327195e-10\n",
      "Iteration 30555: loss = 1.8714243e-10,1.5327434e-10\n",
      "Iteration 30560: loss = 1.8708123e-10,1.532765e-10\n",
      "Iteration 30565: loss = 1.8702088e-10,1.5327842e-10\n",
      "Iteration 30570: loss = 1.8696261e-10,1.5327982e-10\n",
      "Iteration 30575: loss = 1.8690556e-10,1.5328108e-10\n",
      "Iteration 30580: loss = 1.8685027e-10,1.5328189e-10\n",
      "Iteration 30585: loss = 1.8680439e-10,1.5328092e-10\n",
      "Iteration 30590: loss = 1.8675306e-10,1.5328035e-10\n",
      "Iteration 30595: loss = 1.8670622e-10,1.5327946e-10\n",
      "Iteration 30600: loss = 1.8665951e-10,1.5327854e-10\n",
      "Iteration 30605: loss = 1.8661399e-10,1.5327772e-10\n",
      "Iteration 30610: loss = 1.8656827e-10,1.5327682e-10\n",
      "Iteration 30615: loss = 1.8652603e-10,1.5327535e-10\n",
      "Iteration 30620: loss = 1.8648295e-10,1.532738e-10\n",
      "Iteration 30625: loss = 1.8644086e-10,1.5327234e-10\n",
      "Iteration 30630: loss = 1.8640044e-10,1.5327051e-10\n",
      "Iteration 30635: loss = 1.8635875e-10,1.532686e-10\n",
      "Iteration 30640: loss = 1.8631285e-10,1.5326772e-10\n",
      "Iteration 30645: loss = 1.8626618e-10,1.5326729e-10\n",
      "Iteration 30650: loss = 1.8621964e-10,1.5326637e-10\n",
      "Iteration 30655: loss = 1.8617502e-10,1.5326573e-10\n",
      "Iteration 30660: loss = 1.8612663e-10,1.5326433e-10\n",
      "Iteration 30665: loss = 1.8608358e-10,1.5326297e-10\n",
      "Iteration 30670: loss = 1.8604157e-10,1.5326136e-10\n",
      "Iteration 30675: loss = 1.8600042e-10,1.5325977e-10\n",
      "Iteration 30680: loss = 1.8594294e-10,1.5326114e-10\n",
      "Iteration 30685: loss = 1.8588041e-10,1.5326358e-10\n",
      "Iteration 30690: loss = 1.827647e-10,1.5326712e-10\n",
      "Iteration 30695: loss = 1.827037e-10,1.5326887e-10\n",
      "Iteration 30700: loss = 1.8264483e-10,1.5327162e-10\n",
      "Iteration 30705: loss = 1.8258699e-10,1.5327271e-10\n",
      "Iteration 30710: loss = 1.8253073e-10,1.5327392e-10\n",
      "Iteration 30715: loss = 1.8247476e-10,1.5327481e-10\n",
      "Iteration 30720: loss = 1.8242503e-10,1.532757e-10\n",
      "Iteration 30725: loss = 1.8237274e-10,1.5327617e-10\n",
      "Iteration 30730: loss = 1.8232187e-10,1.5327624e-10\n",
      "Iteration 30735: loss = 1.8226959e-10,1.5327686e-10\n",
      "Iteration 30740: loss = 1.822223e-10,1.5327706e-10\n",
      "Iteration 30745: loss = 1.8217113e-10,1.5327728e-10\n",
      "Iteration 30750: loss = 1.8212225e-10,1.5327697e-10\n",
      "Iteration 30755: loss = 1.8207852e-10,1.5327592e-10\n",
      "Iteration 30760: loss = 1.82035e-10,1.5327542e-10\n",
      "Iteration 30765: loss = 1.8198705e-10,1.5327492e-10\n",
      "Iteration 30770: loss = 1.8193712e-10,1.5327509e-10\n",
      "Iteration 30775: loss = 1.81892e-10,1.532749e-10\n",
      "Iteration 30780: loss = 1.8184594e-10,1.532743e-10\n",
      "Iteration 30785: loss = 1.8180034e-10,1.5327357e-10\n",
      "Iteration 30790: loss = 1.8175635e-10,1.5327252e-10\n",
      "Iteration 30795: loss = 1.8171692e-10,1.5327135e-10\n",
      "Iteration 30800: loss = 1.8167455e-10,1.5327005e-10\n",
      "Iteration 30805: loss = 1.8163133e-10,1.5326855e-10\n",
      "Iteration 30810: loss = 1.8157374e-10,1.5327024e-10\n",
      "Iteration 30815: loss = 1.8151693e-10,1.5327259e-10\n",
      "Iteration 30820: loss = 1.8145625e-10,1.5327459e-10\n",
      "Iteration 30825: loss = 1.8139822e-10,1.5327675e-10\n",
      "Iteration 30830: loss = 1.7982942e-10,1.5327895e-10\n",
      "Iteration 30835: loss = 1.7976974e-10,1.5328155e-10\n",
      "Iteration 30840: loss = 1.7971476e-10,1.5328294e-10\n",
      "Iteration 30845: loss = 1.7815865e-10,1.5328447e-10\n",
      "Iteration 30850: loss = 1.781093e-10,1.5328494e-10\n",
      "Iteration 30855: loss = 1.7805268e-10,1.5328608e-10\n",
      "Iteration 30860: loss = 1.7800013e-10,1.5328733e-10\n",
      "Iteration 30865: loss = 1.7795214e-10,1.532875e-10\n",
      "Iteration 30870: loss = 1.7790724e-10,1.5328702e-10\n",
      "Iteration 30875: loss = 1.7786504e-10,1.5328618e-10\n",
      "Iteration 30880: loss = 1.7782463e-10,1.5328522e-10\n",
      "Iteration 30885: loss = 1.777758e-10,1.5328522e-10\n",
      "Iteration 30890: loss = 1.7772182e-10,1.5328729e-10\n",
      "Iteration 30895: loss = 1.7767297e-10,1.5328716e-10\n",
      "Iteration 30900: loss = 1.7762786e-10,1.5328716e-10\n",
      "Iteration 30905: loss = 1.7758532e-10,1.5328655e-10\n",
      "Iteration 30910: loss = 1.7753911e-10,1.5328593e-10\n",
      "Iteration 30915: loss = 1.7749706e-10,1.5328555e-10\n",
      "Iteration 30920: loss = 1.774446e-10,1.5328594e-10\n",
      "Iteration 30925: loss = 1.7738533e-10,1.5328865e-10\n",
      "Iteration 30930: loss = 1.7732159e-10,1.5329199e-10\n",
      "Iteration 30935: loss = 1.7725948e-10,1.5329552e-10\n",
      "Iteration 30940: loss = 1.7719852e-10,1.5329814e-10\n",
      "Iteration 30945: loss = 1.7715035e-10,1.5329876e-10\n",
      "Iteration 30950: loss = 1.771024e-10,1.5329915e-10\n",
      "Iteration 30955: loss = 1.7705522e-10,1.5329961e-10\n",
      "Iteration 30960: loss = 1.7700356e-10,1.533004e-10\n",
      "Iteration 30965: loss = 1.7694257e-10,1.533039e-10\n",
      "Iteration 30970: loss = 1.7538652e-10,1.5330727e-10\n",
      "Iteration 30975: loss = 1.7533767e-10,1.5330807e-10\n",
      "Iteration 30980: loss = 1.7528769e-10,1.5330863e-10\n",
      "Iteration 30985: loss = 1.7523737e-10,1.5330982e-10\n",
      "Iteration 30990: loss = 1.7518746e-10,1.5331098e-10\n",
      "Iteration 30995: loss = 1.7365291e-10,1.5331253e-10\n",
      "Iteration 31000: loss = 1.7359963e-10,1.5331414e-10\n",
      "Iteration 31005: loss = 1.7354788e-10,1.5331533e-10\n",
      "Iteration 31010: loss = 1.7350821e-10,1.5331406e-10\n",
      "Iteration 31015: loss = 1.7346857e-10,1.5331275e-10\n",
      "Iteration 31020: loss = 1.734305e-10,1.5331134e-10\n",
      "Iteration 31025: loss = 1.7339534e-10,1.533097e-10\n",
      "Iteration 31030: loss = 1.7335888e-10,1.5330771e-10\n",
      "Iteration 31035: loss = 1.7332082e-10,1.5330603e-10\n",
      "Iteration 31040: loss = 1.7328168e-10,1.533047e-10\n",
      "Iteration 31045: loss = 1.732479e-10,1.5330304e-10\n",
      "Iteration 31050: loss = 1.7320949e-10,1.5330148e-10\n",
      "Iteration 31055: loss = 1.7317321e-10,1.5329954e-10\n",
      "Iteration 31060: loss = 1.7312328e-10,1.5330043e-10\n",
      "Iteration 31065: loss = 1.730715e-10,1.5330233e-10\n",
      "Iteration 31070: loss = 1.7300926e-10,1.5330609e-10\n",
      "Iteration 31075: loss = 1.7294792e-10,1.5330937e-10\n",
      "Iteration 31080: loss = 1.7288763e-10,1.5331295e-10\n",
      "Iteration 31085: loss = 1.7282493e-10,1.5331614e-10\n",
      "Iteration 31090: loss = 1.7276643e-10,1.5331914e-10\n",
      "Iteration 31095: loss = 1.7270813e-10,1.5332188e-10\n",
      "Iteration 31100: loss = 1.7265067e-10,1.5332526e-10\n",
      "Iteration 31105: loss = 1.7260655e-10,1.5332553e-10\n",
      "Iteration 31110: loss = 1.7256148e-10,1.533256e-10\n",
      "Iteration 31115: loss = 1.7251518e-10,1.5332592e-10\n",
      "Iteration 31120: loss = 1.7099609e-10,1.5332607e-10\n",
      "Iteration 31125: loss = 1.7094948e-10,1.5332738e-10\n",
      "Iteration 31130: loss = 1.7089292e-10,1.5332974e-10\n",
      "Iteration 31135: loss = 1.7084718e-10,1.5333004e-10\n",
      "Iteration 31140: loss = 1.7080393e-10,1.5332952e-10\n",
      "Iteration 31145: loss = 1.7076386e-10,1.5332918e-10\n",
      "Iteration 31150: loss = 1.7072094e-10,1.533286e-10\n",
      "Iteration 31155: loss = 1.7068247e-10,1.5332796e-10\n",
      "Iteration 31160: loss = 1.6917993e-10,1.5332738e-10\n",
      "Iteration 31165: loss = 1.6913267e-10,1.5332768e-10\n",
      "Iteration 31170: loss = 1.6909389e-10,1.5332723e-10\n",
      "Iteration 31175: loss = 1.690525e-10,1.5332638e-10\n",
      "Iteration 31180: loss = 1.6901587e-10,1.5332557e-10\n",
      "Iteration 31185: loss = 1.6897639e-10,1.5332456e-10\n",
      "Iteration 31190: loss = 1.6893993e-10,1.5332326e-10\n",
      "Iteration 31195: loss = 1.6889995e-10,1.5332276e-10\n",
      "Iteration 31200: loss = 1.6884068e-10,1.5332582e-10\n",
      "Iteration 31205: loss = 1.6878594e-10,1.5332889e-10\n",
      "Iteration 31210: loss = 1.6872832e-10,1.5333158e-10\n",
      "Iteration 31215: loss = 1.6867635e-10,1.5333389e-10\n",
      "Iteration 31220: loss = 1.6862307e-10,1.5333597e-10\n",
      "Iteration 31225: loss = 1.6857898e-10,1.5333645e-10\n",
      "Iteration 31230: loss = 1.6853687e-10,1.533367e-10\n",
      "Iteration 31235: loss = 1.6849112e-10,1.5333709e-10\n",
      "Iteration 31240: loss = 1.684503e-10,1.5333694e-10\n",
      "Iteration 31245: loss = 1.6840673e-10,1.5333712e-10\n",
      "Iteration 31250: loss = 1.6835873e-10,1.5333873e-10\n",
      "Iteration 31255: loss = 1.6830375e-10,1.5334092e-10\n",
      "Iteration 31260: loss = 1.682577e-10,1.5334184e-10\n",
      "Iteration 31265: loss = 1.682131e-10,1.5334298e-10\n",
      "Iteration 31270: loss = 1.6816698e-10,1.5334316e-10\n",
      "Iteration 31275: loss = 1.6666912e-10,1.5334396e-10\n",
      "Iteration 31280: loss = 1.6661598e-10,1.533461e-10\n",
      "Iteration 31285: loss = 1.6657763e-10,1.5334534e-10\n",
      "Iteration 31290: loss = 1.6653805e-10,1.5334492e-10\n",
      "Iteration 31295: loss = 1.6649994e-10,1.5334421e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31300: loss = 1.6646166e-10,1.5334367e-10\n",
      "Iteration 31305: loss = 1.66424e-10,1.5334264e-10\n",
      "Iteration 31310: loss = 1.6638807e-10,1.5334164e-10\n",
      "Iteration 31315: loss = 1.6635233e-10,1.5334052e-10\n",
      "Iteration 31320: loss = 1.6630357e-10,1.5334176e-10\n",
      "Iteration 31325: loss = 1.6625717e-10,1.5334302e-10\n",
      "Iteration 31330: loss = 1.647678e-10,1.5334323e-10\n",
      "Iteration 31335: loss = 1.6473833e-10,1.5334096e-10\n",
      "Iteration 31340: loss = 1.6470524e-10,1.5333906e-10\n",
      "Iteration 31345: loss = 1.646726e-10,1.5333734e-10\n",
      "Iteration 31350: loss = 1.6462524e-10,1.5333912e-10\n",
      "Iteration 31355: loss = 1.6457458e-10,1.5334109e-10\n",
      "Iteration 31360: loss = 1.6452495e-10,1.5334298e-10\n",
      "Iteration 31365: loss = 1.6447521e-10,1.5334506e-10\n",
      "Iteration 31370: loss = 1.6442014e-10,1.5334743e-10\n",
      "Iteration 31375: loss = 1.6436476e-10,1.5335085e-10\n",
      "Iteration 31380: loss = 1.64308e-10,1.5335429e-10\n",
      "Iteration 31385: loss = 1.642515e-10,1.533577e-10\n",
      "Iteration 31390: loss = 1.6419609e-10,1.5336135e-10\n",
      "Iteration 31395: loss = 1.6414177e-10,1.533643e-10\n",
      "Iteration 31400: loss = 1.6408784e-10,1.5336732e-10\n",
      "Iteration 31405: loss = 1.6403678e-10,1.5336976e-10\n",
      "Iteration 31410: loss = 1.6398731e-10,1.5337188e-10\n",
      "Iteration 31415: loss = 1.6394386e-10,1.5337248e-10\n",
      "Iteration 31420: loss = 1.6389702e-10,1.5337304e-10\n",
      "Iteration 31425: loss = 1.6242017e-10,1.5337345e-10\n",
      "Iteration 31430: loss = 1.623775e-10,1.5337412e-10\n",
      "Iteration 31435: loss = 1.623303e-10,1.53376e-10\n",
      "Iteration 31440: loss = 1.6229301e-10,1.5337552e-10\n",
      "Iteration 31445: loss = 1.622596e-10,1.5337374e-10\n",
      "Iteration 31450: loss = 1.6222701e-10,1.5337243e-10\n",
      "Iteration 31455: loss = 1.6219422e-10,1.5337044e-10\n",
      "Iteration 31460: loss = 1.6215579e-10,1.5337033e-10\n",
      "Iteration 31465: loss = 1.6212039e-10,1.5336969e-10\n",
      "Iteration 31470: loss = 1.6208719e-10,1.5336808e-10\n",
      "Iteration 31475: loss = 1.6204847e-10,1.5336764e-10\n",
      "Iteration 31480: loss = 1.6200802e-10,1.5336779e-10\n",
      "Iteration 31485: loss = 1.6196693e-10,1.5336826e-10\n",
      "Iteration 31490: loss = 1.6193046e-10,1.5336803e-10\n",
      "Iteration 31495: loss = 1.6189079e-10,1.5336776e-10\n",
      "Iteration 31500: loss = 1.6185252e-10,1.533671e-10\n",
      "Iteration 31505: loss = 1.6038744e-10,1.533673e-10\n",
      "Iteration 31510: loss = 1.6034644e-10,1.533678e-10\n",
      "Iteration 31515: loss = 1.602996e-10,1.5336979e-10\n",
      "Iteration 31520: loss = 1.6025321e-10,1.5337087e-10\n",
      "Iteration 31525: loss = 1.6021134e-10,1.5337187e-10\n",
      "Iteration 31530: loss = 1.6017103e-10,1.5337248e-10\n",
      "Iteration 31535: loss = 1.6012741e-10,1.53373e-10\n",
      "Iteration 31540: loss = 1.6007912e-10,1.5337531e-10\n",
      "Iteration 31545: loss = 1.6002331e-10,1.5337871e-10\n",
      "Iteration 31550: loss = 1.5997277e-10,1.53382e-10\n",
      "Iteration 31555: loss = 1.5992184e-10,1.5338497e-10\n",
      "Iteration 31560: loss = 1.598709e-10,1.5338726e-10\n",
      "Iteration 31565: loss = 1.5981903e-10,1.5338986e-10\n",
      "Iteration 31570: loss = 1.5976752e-10,1.5339269e-10\n",
      "Iteration 31575: loss = 1.597193e-10,1.5339549e-10\n",
      "Iteration 31580: loss = 1.5966839e-10,1.5339811e-10\n",
      "Iteration 31585: loss = 1.5962388e-10,1.5339963e-10\n",
      "Iteration 31590: loss = 1.581645e-10,1.5340028e-10\n",
      "Iteration 31595: loss = 1.581269e-10,1.5340036e-10\n",
      "Iteration 31600: loss = 1.580888e-10,1.5340053e-10\n",
      "Iteration 31605: loss = 1.5804362e-10,1.5340164e-10\n",
      "Iteration 31610: loss = 1.5800289e-10,1.5340267e-10\n",
      "Iteration 31615: loss = 1.5795985e-10,1.5340335e-10\n",
      "Iteration 31620: loss = 1.579218e-10,1.5340373e-10\n",
      "Iteration 31625: loss = 1.5788139e-10,1.5340373e-10\n",
      "Iteration 31630: loss = 1.5784567e-10,1.5340379e-10\n",
      "Iteration 31635: loss = 1.5781132e-10,1.5340335e-10\n",
      "Iteration 31640: loss = 1.5777386e-10,1.5340293e-10\n",
      "Iteration 31645: loss = 1.577413e-10,1.534021e-10\n",
      "Iteration 31650: loss = 1.5769559e-10,1.5340329e-10\n",
      "Iteration 31655: loss = 1.5765195e-10,1.5340523e-10\n",
      "Iteration 31660: loss = 1.5760514e-10,1.5340695e-10\n",
      "Iteration 31665: loss = 1.5755552e-10,1.5340992e-10\n",
      "Iteration 31670: loss = 1.560947e-10,1.5341317e-10\n",
      "Iteration 31675: loss = 1.5604196e-10,1.53417e-10\n",
      "Iteration 31680: loss = 1.560048e-10,1.5341727e-10\n",
      "Iteration 31685: loss = 1.559662e-10,1.534177e-10\n",
      "Iteration 31690: loss = 1.5592821e-10,1.5341783e-10\n",
      "Iteration 31695: loss = 1.5589113e-10,1.5341824e-10\n",
      "Iteration 31700: loss = 1.5585133e-10,1.5341801e-10\n",
      "Iteration 31705: loss = 1.5581408e-10,1.5341801e-10\n",
      "Iteration 31710: loss = 1.5577682e-10,1.5341801e-10\n",
      "Iteration 31715: loss = 1.5573955e-10,1.5341828e-10\n",
      "Iteration 31720: loss = 1.5570224e-10,1.5341829e-10\n",
      "Iteration 31725: loss = 1.5566627e-10,1.5341824e-10\n",
      "Iteration 31730: loss = 1.556303e-10,1.5341804e-10\n",
      "Iteration 31735: loss = 1.5559594e-10,1.5341779e-10\n",
      "Iteration 31740: loss = 1.5556122e-10,1.5341714e-10\n",
      "Iteration 31745: loss = 1.5552352e-10,1.5341675e-10\n",
      "Iteration 31750: loss = 1.5549122e-10,1.5341595e-10\n",
      "Iteration 31755: loss = 1.554455e-10,1.534181e-10\n",
      "Iteration 31760: loss = 1.5401032e-10,1.5341854e-10\n",
      "Iteration 31765: loss = 1.5397765e-10,1.5341746e-10\n",
      "Iteration 31770: loss = 1.5394214e-10,1.5341733e-10\n",
      "Iteration 31775: loss = 1.5390268e-10,1.5341828e-10\n",
      "Iteration 31780: loss = 1.5386341e-10,1.5341903e-10\n",
      "Iteration 31785: loss = 1.5382122e-10,1.5341962e-10\n",
      "Iteration 31790: loss = 1.5378254e-10,1.534201e-10\n",
      "Iteration 31795: loss = 1.5374474e-10,1.5342079e-10\n",
      "Iteration 31800: loss = 1.5370917e-10,1.5342067e-10\n",
      "Iteration 31805: loss = 1.5367291e-10,1.5342058e-10\n",
      "Iteration 31810: loss = 1.5363787e-10,1.5342067e-10\n",
      "Iteration 31815: loss = 1.5358667e-10,1.5342441e-10\n",
      "Iteration 31820: loss = 1.53536e-10,1.5342777e-10\n",
      "Iteration 31825: loss = 1.5348653e-10,1.5343077e-10\n",
      "Iteration 31830: loss = 1.534348e-10,1.5343413e-10\n",
      "Iteration 31835: loss = 1.5338564e-10,1.5343696e-10\n",
      "Iteration 31840: loss = 1.5333636e-10,1.5344047e-10\n",
      "Iteration 31845: loss = 1.5328772e-10,1.5344345e-10\n",
      "Iteration 31850: loss = 1.5185017e-10,1.53447e-10\n",
      "Iteration 31855: loss = 1.5180544e-10,1.5344845e-10\n",
      "Iteration 31860: loss = 1.5176742e-10,1.5345006e-10\n",
      "Iteration 31865: loss = 1.5172406e-10,1.5345165e-10\n",
      "Iteration 31870: loss = 1.5168165e-10,1.5345294e-10\n",
      "Iteration 31875: loss = 1.516401e-10,1.5345458e-10\n",
      "Iteration 31880: loss = 1.515982e-10,1.5345587e-10\n",
      "Iteration 31885: loss = 1.5156072e-10,1.5345683e-10\n",
      "Iteration 31890: loss = 1.5152074e-10,1.5345822e-10\n",
      "Iteration 31895: loss = 1.5148079e-10,1.5345888e-10\n",
      "Iteration 31900: loss = 1.5144312e-10,1.534592e-10\n",
      "Iteration 31905: loss = 1.5140905e-10,1.5345976e-10\n",
      "Iteration 31910: loss = 1.5137296e-10,1.5346001e-10\n",
      "Iteration 31915: loss = 1.513372e-10,1.5345988e-10\n",
      "Iteration 31920: loss = 1.4992378e-10,1.5345973e-10\n",
      "Iteration 31925: loss = 1.4988727e-10,1.5346005e-10\n",
      "Iteration 31930: loss = 1.498527e-10,1.5346058e-10\n",
      "Iteration 31935: loss = 1.4981384e-10,1.5346124e-10\n",
      "Iteration 31940: loss = 1.4977754e-10,1.5346156e-10\n",
      "Iteration 31945: loss = 1.4974204e-10,1.5346173e-10\n",
      "Iteration 31950: loss = 1.4971054e-10,1.5346151e-10\n",
      "Iteration 31955: loss = 1.496759e-10,1.5346126e-10\n",
      "Iteration 31960: loss = 1.4964047e-10,1.5346141e-10\n",
      "Iteration 31965: loss = 1.4960487e-10,1.5346133e-10\n",
      "Iteration 31970: loss = 1.4956593e-10,1.534624e-10\n",
      "Iteration 31975: loss = 1.4952575e-10,1.5346402e-10\n",
      "Iteration 31980: loss = 1.4948788e-10,1.5346528e-10\n",
      "Iteration 31985: loss = 1.4944611e-10,1.5346664e-10\n",
      "Iteration 31990: loss = 1.4940779e-10,1.5346782e-10\n",
      "Iteration 31995: loss = 1.4936764e-10,1.5346903e-10\n",
      "Iteration 32000: loss = 1.493314e-10,1.5346999e-10\n",
      "Iteration 32005: loss = 1.4929226e-10,1.5347054e-10\n",
      "Iteration 32010: loss = 1.4925798e-10,1.5347096e-10\n",
      "Iteration 32015: loss = 1.4922062e-10,1.5347203e-10\n",
      "Iteration 32020: loss = 1.4916975e-10,1.534756e-10\n",
      "Iteration 32025: loss = 1.4912123e-10,1.5347912e-10\n",
      "Iteration 32030: loss = 1.4907331e-10,1.5348228e-10\n",
      "Iteration 32035: loss = 1.4766137e-10,1.5348466e-10\n",
      "Iteration 32040: loss = 1.4762502e-10,1.5348584e-10\n",
      "Iteration 32045: loss = 1.4758418e-10,1.5348675e-10\n",
      "Iteration 32050: loss = 1.4754696e-10,1.5348828e-10\n",
      "Iteration 32055: loss = 1.4751099e-10,1.5348818e-10\n",
      "Iteration 32060: loss = 1.4748074e-10,1.5348799e-10\n",
      "Iteration 32065: loss = 1.4744748e-10,1.5348833e-10\n",
      "Iteration 32070: loss = 1.4741114e-10,1.5348839e-10\n",
      "Iteration 32075: loss = 1.4737818e-10,1.5348849e-10\n",
      "Iteration 32080: loss = 1.4734285e-10,1.5348868e-10\n",
      "Iteration 32085: loss = 1.4730937e-10,1.5348875e-10\n",
      "Iteration 32090: loss = 1.4727099e-10,1.5348965e-10\n",
      "Iteration 32095: loss = 1.4587713e-10,1.5349041e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32100: loss = 1.458479e-10,1.5348953e-10\n",
      "Iteration 32105: loss = 1.4581315e-10,1.5348929e-10\n",
      "Iteration 32110: loss = 1.4578135e-10,1.5348925e-10\n",
      "Iteration 32115: loss = 1.457489e-10,1.5348899e-10\n",
      "Iteration 32120: loss = 1.4571859e-10,1.5348883e-10\n",
      "Iteration 32125: loss = 1.4568767e-10,1.534881e-10\n",
      "Iteration 32130: loss = 1.4566053e-10,1.534868e-10\n",
      "Iteration 32135: loss = 1.4562748e-10,1.534869e-10\n",
      "Iteration 32140: loss = 1.455887e-10,1.5348847e-10\n",
      "Iteration 32145: loss = 1.4554698e-10,1.5348973e-10\n",
      "Iteration 32150: loss = 1.4550933e-10,1.5349128e-10\n",
      "Iteration 32155: loss = 1.4547304e-10,1.5349237e-10\n",
      "Iteration 32160: loss = 1.4543841e-10,1.5349302e-10\n",
      "Iteration 32165: loss = 1.4540381e-10,1.5349333e-10\n",
      "Iteration 32170: loss = 1.4536965e-10,1.534941e-10\n",
      "Iteration 32175: loss = 1.4531863e-10,1.5349781e-10\n",
      "Iteration 32180: loss = 1.4527286e-10,1.5350146e-10\n",
      "Iteration 32185: loss = 1.4522741e-10,1.5350471e-10\n",
      "Iteration 32190: loss = 1.4517933e-10,1.5350804e-10\n",
      "Iteration 32195: loss = 1.4513507e-10,1.5351151e-10\n",
      "Iteration 32200: loss = 1.4509997e-10,1.535116e-10\n",
      "Iteration 32205: loss = 1.450648e-10,1.5351234e-10\n",
      "Iteration 32210: loss = 1.450297e-10,1.5351309e-10\n",
      "Iteration 32215: loss = 1.4499452e-10,1.5351362e-10\n",
      "Iteration 32220: loss = 1.4495992e-10,1.5351448e-10\n",
      "Iteration 32225: loss = 1.4492517e-10,1.535152e-10\n",
      "Iteration 32230: loss = 1.4353783e-10,1.535165e-10\n",
      "Iteration 32235: loss = 1.4349616e-10,1.535188e-10\n",
      "Iteration 32240: loss = 1.4346148e-10,1.5351911e-10\n",
      "Iteration 32245: loss = 1.4342938e-10,1.5351932e-10\n",
      "Iteration 32250: loss = 1.4339742e-10,1.5351934e-10\n",
      "Iteration 32255: loss = 1.4337102e-10,1.5351848e-10\n",
      "Iteration 32260: loss = 1.4334288e-10,1.5351714e-10\n",
      "Iteration 32265: loss = 1.4330571e-10,1.5351856e-10\n",
      "Iteration 32270: loss = 1.4326966e-10,1.5351982e-10\n",
      "Iteration 32275: loss = 1.4189291e-10,1.5352095e-10\n",
      "Iteration 32280: loss = 1.4186793e-10,1.5351904e-10\n",
      "Iteration 32285: loss = 1.4183961e-10,1.5351828e-10\n",
      "Iteration 32290: loss = 1.4181045e-10,1.5351721e-10\n",
      "Iteration 32295: loss = 1.4178221e-10,1.5351619e-10\n",
      "Iteration 32300: loss = 1.4175787e-10,1.5351517e-10\n",
      "Iteration 32305: loss = 1.4172706e-10,1.5351488e-10\n",
      "Iteration 32310: loss = 1.416841e-10,1.5351777e-10\n",
      "Iteration 32315: loss = 1.4164338e-10,1.5351967e-10\n",
      "Iteration 32320: loss = 1.4160557e-10,1.5352106e-10\n",
      "Iteration 32325: loss = 1.415686e-10,1.5352325e-10\n",
      "Iteration 32330: loss = 1.4153033e-10,1.5352467e-10\n",
      "Iteration 32335: loss = 1.4149272e-10,1.5352633e-10\n",
      "Iteration 32340: loss = 1.4145478e-10,1.5352761e-10\n",
      "Iteration 32345: loss = 1.4141759e-10,1.5352888e-10\n",
      "Iteration 32350: loss = 1.413844e-10,1.5352991e-10\n",
      "Iteration 32355: loss = 1.413487e-10,1.5353138e-10\n",
      "Iteration 32360: loss = 1.4131335e-10,1.5353217e-10\n",
      "Iteration 32365: loss = 1.4127897e-10,1.5353291e-10\n",
      "Iteration 32370: loss = 1.4124722e-10,1.5353382e-10\n",
      "Iteration 32375: loss = 1.4120934e-10,1.5353521e-10\n",
      "Iteration 32380: loss = 1.4117081e-10,1.5353666e-10\n",
      "Iteration 32385: loss = 1.4113215e-10,1.5353875e-10\n",
      "Iteration 32390: loss = 1.4109412e-10,1.535406e-10\n",
      "Iteration 32395: loss = 1.4105969e-10,1.5354196e-10\n",
      "Iteration 32400: loss = 1.4102229e-10,1.5354304e-10\n",
      "Iteration 32405: loss = 1.4098685e-10,1.535443e-10\n",
      "Iteration 32410: loss = 1.4095106e-10,1.5354547e-10\n",
      "Iteration 32415: loss = 1.4091583e-10,1.5354626e-10\n",
      "Iteration 32420: loss = 1.4088522e-10,1.5354698e-10\n",
      "Iteration 32425: loss = 1.4085137e-10,1.5354762e-10\n",
      "Iteration 32430: loss = 1.3948674e-10,1.5354815e-10\n",
      "Iteration 32435: loss = 1.3945035e-10,1.5354969e-10\n",
      "Iteration 32440: loss = 1.3941803e-10,1.5355027e-10\n",
      "Iteration 32445: loss = 1.3939012e-10,1.535499e-10\n",
      "Iteration 32450: loss = 1.3935882e-10,1.5354962e-10\n",
      "Iteration 32455: loss = 1.3933125e-10,1.5354923e-10\n",
      "Iteration 32460: loss = 1.3930039e-10,1.5354892e-10\n",
      "Iteration 32465: loss = 1.3795229e-10,1.5354835e-10\n",
      "Iteration 32470: loss = 1.3792169e-10,1.5354806e-10\n",
      "Iteration 32475: loss = 1.3789327e-10,1.5354781e-10\n",
      "Iteration 32480: loss = 1.3786251e-10,1.5354781e-10\n",
      "Iteration 32485: loss = 1.378348e-10,1.5354759e-10\n",
      "Iteration 32490: loss = 1.3780108e-10,1.5354809e-10\n",
      "Iteration 32495: loss = 1.3775879e-10,1.5355171e-10\n",
      "Iteration 32500: loss = 1.3771435e-10,1.5355504e-10\n",
      "Iteration 32505: loss = 1.376738e-10,1.53558e-10\n",
      "Iteration 32510: loss = 1.3763402e-10,1.5356089e-10\n",
      "Iteration 32515: loss = 1.3759187e-10,1.5356383e-10\n",
      "Iteration 32520: loss = 1.375519e-10,1.5356635e-10\n",
      "Iteration 32525: loss = 1.3750878e-10,1.5356949e-10\n",
      "Iteration 32530: loss = 1.3746927e-10,1.5357232e-10\n",
      "Iteration 32535: loss = 1.374305e-10,1.5357449e-10\n",
      "Iteration 32540: loss = 1.3739411e-10,1.5357646e-10\n",
      "Iteration 32545: loss = 1.373549e-10,1.535784e-10\n",
      "Iteration 32550: loss = 1.3731953e-10,1.5358026e-10\n",
      "Iteration 32555: loss = 1.3728056e-10,1.5358238e-10\n",
      "Iteration 32560: loss = 1.3724374e-10,1.5358448e-10\n",
      "Iteration 32565: loss = 1.3720401e-10,1.5358703e-10\n",
      "Iteration 32570: loss = 1.3716728e-10,1.5358903e-10\n",
      "Iteration 32575: loss = 1.3713121e-10,1.5359114e-10\n",
      "Iteration 32580: loss = 1.370935e-10,1.5359301e-10\n",
      "Iteration 32585: loss = 1.3705931e-10,1.5359447e-10\n",
      "Iteration 32590: loss = 1.3702295e-10,1.5359562e-10\n",
      "Iteration 32595: loss = 1.3699042e-10,1.535969e-10\n",
      "Iteration 32600: loss = 1.3695454e-10,1.5359844e-10\n",
      "Iteration 32605: loss = 1.3692258e-10,1.535993e-10\n",
      "Iteration 32610: loss = 1.3688732e-10,1.5360055e-10\n",
      "Iteration 32615: loss = 1.3685696e-10,1.536011e-10\n",
      "Iteration 32620: loss = 1.3682422e-10,1.5360192e-10\n",
      "Iteration 32625: loss = 1.3679448e-10,1.5360212e-10\n",
      "Iteration 32630: loss = 1.3544904e-10,1.5360295e-10\n",
      "Iteration 32635: loss = 1.3541678e-10,1.5360438e-10\n",
      "Iteration 32640: loss = 1.3539213e-10,1.5360307e-10\n",
      "Iteration 32645: loss = 1.3406469e-10,1.5360228e-10\n",
      "Iteration 32650: loss = 1.3403335e-10,1.5360205e-10\n",
      "Iteration 32655: loss = 1.3400558e-10,1.5360191e-10\n",
      "Iteration 32660: loss = 1.3398187e-10,1.5360102e-10\n",
      "Iteration 32665: loss = 1.339568e-10,1.535999e-10\n",
      "Iteration 32670: loss = 1.3393213e-10,1.5359933e-10\n",
      "Iteration 32675: loss = 1.3389416e-10,1.5360184e-10\n",
      "Iteration 32680: loss = 1.338534e-10,1.5360539e-10\n",
      "Iteration 32685: loss = 1.338148e-10,1.5360756e-10\n",
      "Iteration 32690: loss = 1.3378275e-10,1.5360826e-10\n",
      "Iteration 32695: loss = 1.3375283e-10,1.536095e-10\n",
      "Iteration 32700: loss = 1.3372041e-10,1.536106e-10\n",
      "Iteration 32705: loss = 1.3367728e-10,1.5361443e-10\n",
      "Iteration 32710: loss = 1.3363453e-10,1.5361862e-10\n",
      "Iteration 32715: loss = 1.3358997e-10,1.5362214e-10\n",
      "Iteration 32720: loss = 1.3355134e-10,1.5362463e-10\n",
      "Iteration 32725: loss = 1.3351452e-10,1.5362746e-10\n",
      "Iteration 32730: loss = 1.3347702e-10,1.5362996e-10\n",
      "Iteration 32735: loss = 1.3343938e-10,1.5363245e-10\n",
      "Iteration 32740: loss = 1.3340236e-10,1.5363491e-10\n",
      "Iteration 32745: loss = 1.3336592e-10,1.5363733e-10\n",
      "Iteration 32750: loss = 1.3333078e-10,1.5363848e-10\n",
      "Iteration 32755: loss = 1.3329914e-10,1.5363956e-10\n",
      "Iteration 32760: loss = 1.3326763e-10,1.5364059e-10\n",
      "Iteration 32765: loss = 1.3323678e-10,1.5364149e-10\n",
      "Iteration 32770: loss = 1.3320674e-10,1.5364209e-10\n",
      "Iteration 32775: loss = 1.3317762e-10,1.5364225e-10\n",
      "Iteration 32780: loss = 1.3314898e-10,1.5364286e-10\n",
      "Iteration 32785: loss = 1.331205e-10,1.5364311e-10\n",
      "Iteration 32790: loss = 1.3308916e-10,1.5364364e-10\n",
      "Iteration 32795: loss = 1.3306117e-10,1.5364371e-10\n",
      "Iteration 32800: loss = 1.330308e-10,1.5364422e-10\n",
      "Iteration 32805: loss = 1.3299718e-10,1.5364598e-10\n",
      "Iteration 32810: loss = 1.3296188e-10,1.5364852e-10\n",
      "Iteration 32815: loss = 1.329273e-10,1.5365036e-10\n",
      "Iteration 32820: loss = 1.3290082e-10,1.5364995e-10\n",
      "Iteration 32825: loss = 1.3287314e-10,1.5364979e-10\n",
      "Iteration 32830: loss = 1.3284385e-10,1.5365012e-10\n",
      "Iteration 32835: loss = 1.302375e-10,1.5365051e-10\n",
      "Iteration 32840: loss = 1.3020922e-10,1.5365076e-10\n",
      "Iteration 32845: loss = 1.3018951e-10,1.5364915e-10\n",
      "Iteration 32850: loss = 1.3016489e-10,1.5364822e-10\n",
      "Iteration 32855: loss = 1.3014194e-10,1.5364729e-10\n",
      "Iteration 32860: loss = 1.3011785e-10,1.5364703e-10\n",
      "Iteration 32865: loss = 1.30092e-10,1.5364629e-10\n",
      "Iteration 32870: loss = 1.3006121e-10,1.5364744e-10\n",
      "Iteration 32875: loss = 1.3002883e-10,1.5364877e-10\n",
      "Iteration 32880: loss = 1.3000062e-10,1.5364929e-10\n",
      "Iteration 32885: loss = 1.299703e-10,1.5364993e-10\n",
      "Iteration 32890: loss = 1.2994096e-10,1.5365068e-10\n",
      "Iteration 32895: loss = 1.2990095e-10,1.5365417e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32900: loss = 1.2986169e-10,1.536577e-10\n",
      "Iteration 32905: loss = 1.2982289e-10,1.5366061e-10\n",
      "Iteration 32910: loss = 1.297882e-10,1.5366333e-10\n",
      "Iteration 32915: loss = 1.2975059e-10,1.5366616e-10\n",
      "Iteration 32920: loss = 1.2971428e-10,1.5366862e-10\n",
      "Iteration 32925: loss = 1.2967817e-10,1.5367135e-10\n",
      "Iteration 32930: loss = 1.2964176e-10,1.5367356e-10\n",
      "Iteration 32935: loss = 1.2960823e-10,1.5367586e-10\n",
      "Iteration 32940: loss = 1.2956868e-10,1.5367929e-10\n",
      "Iteration 32945: loss = 1.2952985e-10,1.5368253e-10\n",
      "Iteration 32950: loss = 1.294897e-10,1.536862e-10\n",
      "Iteration 32955: loss = 1.2945094e-10,1.5368914e-10\n",
      "Iteration 32960: loss = 1.2941578e-10,1.5369227e-10\n",
      "Iteration 32965: loss = 1.2937772e-10,1.536953e-10\n",
      "Iteration 32970: loss = 1.2935027e-10,1.536952e-10\n",
      "Iteration 32975: loss = 1.2932487e-10,1.5369542e-10\n",
      "Iteration 32980: loss = 1.2929718e-10,1.5369525e-10\n",
      "Iteration 32985: loss = 1.292727e-10,1.5369528e-10\n",
      "Iteration 32990: loss = 1.2924549e-10,1.5369517e-10\n",
      "Iteration 32995: loss = 1.292222e-10,1.5369504e-10\n",
      "Iteration 33000: loss = 1.2919608e-10,1.5369424e-10\n",
      "Iteration 33005: loss = 1.2917238e-10,1.5369382e-10\n",
      "Iteration 33010: loss = 1.2914035e-10,1.5369517e-10\n",
      "Iteration 33015: loss = 1.2910845e-10,1.5369676e-10\n",
      "Iteration 33020: loss = 1.2907968e-10,1.5369775e-10\n",
      "Iteration 33025: loss = 1.2904817e-10,1.5369903e-10\n",
      "Iteration 33030: loss = 1.277461e-10,1.5369966e-10\n",
      "Iteration 33035: loss = 1.2772243e-10,1.5369855e-10\n",
      "Iteration 33040: loss = 1.2769935e-10,1.5369814e-10\n",
      "Iteration 33045: loss = 1.276751e-10,1.5369792e-10\n",
      "Iteration 33050: loss = 1.2764845e-10,1.5369754e-10\n",
      "Iteration 33055: loss = 1.2635853e-10,1.5369789e-10\n",
      "Iteration 33060: loss = 1.2633776e-10,1.5369651e-10\n",
      "Iteration 33065: loss = 1.263167e-10,1.5369471e-10\n",
      "Iteration 33070: loss = 1.2629825e-10,1.5369289e-10\n",
      "Iteration 33075: loss = 1.2627098e-10,1.5369331e-10\n",
      "Iteration 33080: loss = 1.2623692e-10,1.5369585e-10\n",
      "Iteration 33085: loss = 1.2619954e-10,1.5369911e-10\n",
      "Iteration 33090: loss = 1.2616307e-10,1.5370266e-10\n",
      "Iteration 33095: loss = 1.261261e-10,1.5370563e-10\n",
      "Iteration 33100: loss = 1.2609193e-10,1.5370757e-10\n",
      "Iteration 33105: loss = 1.2606173e-10,1.5370905e-10\n",
      "Iteration 33110: loss = 1.2603245e-10,1.5371031e-10\n",
      "Iteration 33115: loss = 1.2600332e-10,1.5371139e-10\n",
      "Iteration 33120: loss = 1.2597524e-10,1.537121e-10\n",
      "Iteration 33125: loss = 1.2594757e-10,1.5371279e-10\n",
      "Iteration 33130: loss = 1.2592066e-10,1.5371338e-10\n",
      "Iteration 33135: loss = 1.2589167e-10,1.5371383e-10\n",
      "Iteration 33140: loss = 1.258601e-10,1.5371607e-10\n",
      "Iteration 33145: loss = 1.258313e-10,1.5371639e-10\n",
      "Iteration 33150: loss = 1.2580452e-10,1.5371696e-10\n",
      "Iteration 33155: loss = 1.2577762e-10,1.5371758e-10\n",
      "Iteration 33160: loss = 1.2575067e-10,1.5371823e-10\n",
      "Iteration 33165: loss = 1.2572397e-10,1.5371882e-10\n",
      "Iteration 33170: loss = 1.2569777e-10,1.5371937e-10\n",
      "Iteration 33175: loss = 1.25672e-10,1.5371975e-10\n",
      "Iteration 33180: loss = 1.2564415e-10,1.5371959e-10\n",
      "Iteration 33185: loss = 1.2561922e-10,1.5371979e-10\n",
      "Iteration 33190: loss = 1.2559505e-10,1.5371965e-10\n",
      "Iteration 33195: loss = 1.2557158e-10,1.5371941e-10\n",
      "Iteration 33200: loss = 1.2554852e-10,1.5371888e-10\n",
      "Iteration 33205: loss = 1.2551714e-10,1.5372045e-10\n",
      "Iteration 33210: loss = 1.2548408e-10,1.53723e-10\n",
      "Iteration 33215: loss = 1.2544811e-10,1.5372534e-10\n",
      "Iteration 33220: loss = 1.2541808e-10,1.5372735e-10\n",
      "Iteration 33225: loss = 1.2539357e-10,1.537267e-10\n",
      "Iteration 33230: loss = 1.2536981e-10,1.5372663e-10\n",
      "Iteration 33235: loss = 1.2534625e-10,1.537262e-10\n",
      "Iteration 33240: loss = 1.2532243e-10,1.5372592e-10\n",
      "Iteration 33245: loss = 1.2529931e-10,1.5372559e-10\n",
      "Iteration 33250: loss = 1.2402167e-10,1.5372525e-10\n",
      "Iteration 33255: loss = 1.2399674e-10,1.5372521e-10\n",
      "Iteration 33260: loss = 1.2397176e-10,1.5372487e-10\n",
      "Iteration 33265: loss = 1.2394913e-10,1.5372437e-10\n",
      "Iteration 33270: loss = 1.2392716e-10,1.5372363e-10\n",
      "Iteration 33275: loss = 1.2390519e-10,1.5372292e-10\n",
      "Iteration 33280: loss = 1.2388425e-10,1.5372215e-10\n",
      "Iteration 33285: loss = 1.2386366e-10,1.5372112e-10\n",
      "Iteration 33290: loss = 1.2384266e-10,1.5372008e-10\n",
      "Iteration 33295: loss = 1.238197e-10,1.5371905e-10\n",
      "Iteration 33300: loss = 1.2379985e-10,1.5371796e-10\n",
      "Iteration 33305: loss = 1.2253117e-10,1.5371739e-10\n",
      "Iteration 33310: loss = 1.2250768e-10,1.5371751e-10\n",
      "Iteration 33315: loss = 1.2247832e-10,1.53719e-10\n",
      "Iteration 33320: loss = 1.2244798e-10,1.5372055e-10\n",
      "Iteration 33325: loss = 1.2241813e-10,1.537221e-10\n",
      "Iteration 33330: loss = 1.2238902e-10,1.5372331e-10\n",
      "Iteration 33335: loss = 1.2235915e-10,1.5372442e-10\n",
      "Iteration 33340: loss = 1.2233062e-10,1.5372656e-10\n",
      "Iteration 33345: loss = 1.2229796e-10,1.5372906e-10\n",
      "Iteration 33350: loss = 1.222662e-10,1.5373113e-10\n",
      "Iteration 33355: loss = 1.2223354e-10,1.5373328e-10\n",
      "Iteration 33360: loss = 1.2220179e-10,1.5373526e-10\n",
      "Iteration 33365: loss = 1.2217642e-10,1.5373632e-10\n",
      "Iteration 33370: loss = 1.2215091e-10,1.5373675e-10\n",
      "Iteration 33375: loss = 1.2212505e-10,1.5373713e-10\n",
      "Iteration 33380: loss = 1.2209962e-10,1.5373725e-10\n",
      "Iteration 33385: loss = 1.2207366e-10,1.537375e-10\n",
      "Iteration 33390: loss = 1.2204804e-10,1.537383e-10\n",
      "Iteration 33395: loss = 1.2202389e-10,1.5373885e-10\n",
      "Iteration 33400: loss = 1.2199676e-10,1.5373958e-10\n",
      "Iteration 33405: loss = 1.2196975e-10,1.5374049e-10\n",
      "Iteration 33410: loss = 1.2194333e-10,1.5374096e-10\n",
      "Iteration 33415: loss = 1.2191863e-10,1.5374103e-10\n",
      "Iteration 33420: loss = 1.2189615e-10,1.5374166e-10\n",
      "Iteration 33425: loss = 1.2187105e-10,1.537415e-10\n",
      "Iteration 33430: loss = 1.2184609e-10,1.5374162e-10\n",
      "Iteration 33435: loss = 1.2182223e-10,1.5374176e-10\n",
      "Iteration 33440: loss = 1.217985e-10,1.5374169e-10\n",
      "Iteration 33445: loss = 1.217748e-10,1.5374133e-10\n",
      "Iteration 33450: loss = 1.217539e-10,1.5374102e-10\n",
      "Iteration 33455: loss = 1.2173092e-10,1.5374071e-10\n",
      "Iteration 33460: loss = 1.217094e-10,1.5374009e-10\n",
      "Iteration 33465: loss = 1.2168765e-10,1.5373935e-10\n",
      "Iteration 33470: loss = 1.2166577e-10,1.5373884e-10\n",
      "Iteration 33475: loss = 1.2164704e-10,1.5373813e-10\n",
      "Iteration 33480: loss = 1.2162633e-10,1.5373704e-10\n",
      "Iteration 33485: loss = 1.2160493e-10,1.5373597e-10\n",
      "Iteration 33490: loss = 1.2158395e-10,1.5373501e-10\n",
      "Iteration 33495: loss = 1.203262e-10,1.5373455e-10\n",
      "Iteration 33500: loss = 1.203001e-10,1.5373501e-10\n",
      "Iteration 33505: loss = 1.2027557e-10,1.5373604e-10\n",
      "Iteration 33510: loss = 1.202481e-10,1.5373725e-10\n",
      "Iteration 33515: loss = 1.2021983e-10,1.5373805e-10\n",
      "Iteration 33520: loss = 1.2019252e-10,1.5373905e-10\n",
      "Iteration 33525: loss = 1.2016595e-10,1.5374e-10\n",
      "Iteration 33530: loss = 1.2014183e-10,1.5374081e-10\n",
      "Iteration 33535: loss = 1.201152e-10,1.5374155e-10\n",
      "Iteration 33540: loss = 1.2008704e-10,1.5374291e-10\n",
      "Iteration 33545: loss = 1.2006496e-10,1.5374274e-10\n",
      "Iteration 33550: loss = 1.2004582e-10,1.5374121e-10\n",
      "Iteration 33555: loss = 1.2003366e-10,1.5373836e-10\n",
      "Iteration 33560: loss = 1.1879098e-10,1.5373566e-10\n",
      "Iteration 33565: loss = 1.1877511e-10,1.5373418e-10\n",
      "Iteration 33570: loss = 1.187405e-10,1.537371e-10\n",
      "Iteration 33575: loss = 1.1871316e-10,1.5373838e-10\n",
      "Iteration 33580: loss = 1.1868813e-10,1.5373924e-10\n",
      "Iteration 33585: loss = 1.186637e-10,1.5374008e-10\n",
      "Iteration 33590: loss = 1.1863957e-10,1.5374052e-10\n",
      "Iteration 33595: loss = 1.1860297e-10,1.5374416e-10\n",
      "Iteration 33600: loss = 1.1856903e-10,1.5374754e-10\n",
      "Iteration 33605: loss = 1.1853447e-10,1.5375104e-10\n",
      "Iteration 33610: loss = 1.1850357e-10,1.537539e-10\n",
      "Iteration 33615: loss = 1.1847313e-10,1.5375565e-10\n",
      "Iteration 33620: loss = 1.1844802e-10,1.5375669e-10\n",
      "Iteration 33625: loss = 1.1842001e-10,1.5375783e-10\n",
      "Iteration 33630: loss = 1.1839559e-10,1.5375887e-10\n",
      "Iteration 33635: loss = 1.183685e-10,1.5375982e-10\n",
      "Iteration 33640: loss = 1.1834476e-10,1.5376027e-10\n",
      "Iteration 33645: loss = 1.1831884e-10,1.5376109e-10\n",
      "Iteration 33650: loss = 1.1829632e-10,1.537614e-10\n",
      "Iteration 33655: loss = 1.1827149e-10,1.5376146e-10\n",
      "Iteration 33660: loss = 1.1824956e-10,1.5376195e-10\n",
      "Iteration 33665: loss = 1.1822554e-10,1.5376196e-10\n",
      "Iteration 33670: loss = 1.1820417e-10,1.5376198e-10\n",
      "Iteration 33675: loss = 1.1818323e-10,1.537621e-10\n",
      "Iteration 33680: loss = 1.1815936e-10,1.5376198e-10\n",
      "Iteration 33685: loss = 1.1813889e-10,1.5376181e-10\n",
      "Iteration 33690: loss = 1.1811539e-10,1.537618e-10\n",
      "Iteration 33695: loss = 1.1809527e-10,1.5376145e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33700: loss = 1.1807218e-10,1.5376109e-10\n",
      "Iteration 33705: loss = 1.1805318e-10,1.537605e-10\n",
      "Iteration 33710: loss = 1.180313e-10,1.5375998e-10\n",
      "Iteration 33715: loss = 1.1801254e-10,1.5375941e-10\n",
      "Iteration 33720: loss = 1.1799169e-10,1.537586e-10\n",
      "Iteration 33725: loss = 1.1797337e-10,1.537577e-10\n",
      "Iteration 33730: loss = 1.179516e-10,1.5375731e-10\n",
      "Iteration 33735: loss = 1.1671288e-10,1.5375733e-10\n",
      "Iteration 33740: loss = 1.1668673e-10,1.5375842e-10\n",
      "Iteration 33745: loss = 1.1666369e-10,1.53758e-10\n",
      "Iteration 33750: loss = 1.1664669e-10,1.537568e-10\n",
      "Iteration 33755: loss = 1.1662638e-10,1.5375634e-10\n",
      "Iteration 33760: loss = 1.1660649e-10,1.5375579e-10\n",
      "Iteration 33765: loss = 1.165859e-10,1.5375543e-10\n",
      "Iteration 33770: loss = 1.1656691e-10,1.537552e-10\n",
      "Iteration 33775: loss = 1.1654387e-10,1.5375459e-10\n",
      "Iteration 33780: loss = 1.1652555e-10,1.5375352e-10\n",
      "Iteration 33785: loss = 1.1650858e-10,1.5375234e-10\n",
      "Iteration 33790: loss = 1.1649065e-10,1.5375103e-10\n",
      "Iteration 33795: loss = 1.1647464e-10,1.5374962e-10\n",
      "Iteration 33800: loss = 1.1646036e-10,1.5374757e-10\n",
      "Iteration 33805: loss = 1.1644789e-10,1.5374471e-10\n",
      "Iteration 33810: loss = 1.1643096e-10,1.5374307e-10\n",
      "Iteration 33815: loss = 1.16406385e-10,1.5374332e-10\n",
      "Iteration 33820: loss = 1.1638474e-10,1.537435e-10\n",
      "Iteration 33825: loss = 1.1635295e-10,1.5374618e-10\n",
      "Iteration 33830: loss = 1.163181e-10,1.5375025e-10\n",
      "Iteration 33835: loss = 1.1508067e-10,1.5375268e-10\n",
      "Iteration 33840: loss = 1.15055125e-10,1.5375418e-10\n",
      "Iteration 33845: loss = 1.1502529e-10,1.5375592e-10\n",
      "Iteration 33850: loss = 1.14995936e-10,1.5375837e-10\n",
      "Iteration 33855: loss = 1.1496618e-10,1.5376106e-10\n",
      "Iteration 33860: loss = 1.1493653e-10,1.5376364e-10\n",
      "Iteration 33865: loss = 1.149077e-10,1.5376575e-10\n",
      "Iteration 33870: loss = 1.1487894e-10,1.5376833e-10\n",
      "Iteration 33875: loss = 1.1485121e-10,1.5377029e-10\n",
      "Iteration 33880: loss = 1.1482165e-10,1.5377238e-10\n",
      "Iteration 33885: loss = 1.1479493e-10,1.5377384e-10\n",
      "Iteration 33890: loss = 1.1476872e-10,1.5377531e-10\n",
      "Iteration 33895: loss = 1.1474365e-10,1.5377666e-10\n",
      "Iteration 33900: loss = 1.1471881e-10,1.5377807e-10\n",
      "Iteration 33905: loss = 1.146939e-10,1.5377896e-10\n",
      "Iteration 33910: loss = 1.1466467e-10,1.5378088e-10\n",
      "Iteration 33915: loss = 1.14637584e-10,1.5378264e-10\n",
      "Iteration 33920: loss = 1.1461076e-10,1.5378447e-10\n",
      "Iteration 33925: loss = 1.145841e-10,1.5378628e-10\n",
      "Iteration 33930: loss = 1.1455791e-10,1.5378765e-10\n",
      "Iteration 33935: loss = 1.1453225e-10,1.537893e-10\n",
      "Iteration 33940: loss = 1.1450385e-10,1.537909e-10\n",
      "Iteration 33945: loss = 1.1447863e-10,1.537922e-10\n",
      "Iteration 33950: loss = 1.1445405e-10,1.5379314e-10\n",
      "Iteration 33955: loss = 1.1442886e-10,1.5379473e-10\n",
      "Iteration 33960: loss = 1.1440249e-10,1.5379628e-10\n",
      "Iteration 33965: loss = 1.14375474e-10,1.5379836e-10\n",
      "Iteration 33970: loss = 1.1314813e-10,1.5380064e-10\n",
      "Iteration 33975: loss = 1.1312539e-10,1.5380122e-10\n",
      "Iteration 33980: loss = 1.13104026e-10,1.5380107e-10\n",
      "Iteration 33985: loss = 1.1308521e-10,1.5380006e-10\n",
      "Iteration 33990: loss = 1.1306405e-10,1.5379983e-10\n",
      "Iteration 33995: loss = 1.1304594e-10,1.5379983e-10\n",
      "Iteration 34000: loss = 1.1302557e-10,1.537993e-10\n",
      "Iteration 34005: loss = 1.1300546e-10,1.5379945e-10\n",
      "Iteration 34010: loss = 1.12985905e-10,1.5379868e-10\n",
      "Iteration 34015: loss = 1.12967045e-10,1.5379799e-10\n",
      "Iteration 34020: loss = 1.12949185e-10,1.5379689e-10\n",
      "Iteration 34025: loss = 1.1293363e-10,1.5379595e-10\n",
      "Iteration 34030: loss = 1.12917536e-10,1.537944e-10\n",
      "Iteration 34035: loss = 1.1290326e-10,1.5379233e-10\n",
      "Iteration 34040: loss = 1.1288944e-10,1.5379012e-10\n",
      "Iteration 34045: loss = 1.1287621e-10,1.5378782e-10\n",
      "Iteration 34050: loss = 1.1286231e-10,1.5378532e-10\n",
      "Iteration 34055: loss = 1.1285046e-10,1.5378301e-10\n",
      "Iteration 34060: loss = 1.1283724e-10,1.5378085e-10\n",
      "Iteration 34065: loss = 1.128238e-10,1.5377859e-10\n",
      "Iteration 34070: loss = 1.12811004e-10,1.5377592e-10\n",
      "Iteration 34075: loss = 1.1278611e-10,1.5377732e-10\n",
      "Iteration 34080: loss = 1.1275906e-10,1.5377907e-10\n",
      "Iteration 34085: loss = 1.12734155e-10,1.5378096e-10\n",
      "Iteration 34090: loss = 1.12701536e-10,1.5378428e-10\n",
      "Iteration 34095: loss = 1.1266949e-10,1.5378815e-10\n",
      "Iteration 34100: loss = 1.12642035e-10,1.5379006e-10\n",
      "Iteration 34105: loss = 1.1142436e-10,1.5379287e-10\n",
      "Iteration 34110: loss = 1.11394415e-10,1.537955e-10\n",
      "Iteration 34115: loss = 1.1136688e-10,1.5379742e-10\n",
      "Iteration 34120: loss = 1.1133847e-10,1.5379996e-10\n",
      "Iteration 34125: loss = 1.1131233e-10,1.5380233e-10\n",
      "Iteration 34130: loss = 1.1128496e-10,1.538045e-10\n",
      "Iteration 34135: loss = 1.1125709e-10,1.538066e-10\n",
      "Iteration 34140: loss = 1.1123039e-10,1.5380855e-10\n",
      "Iteration 34145: loss = 1.1120382e-10,1.538101e-10\n",
      "Iteration 34150: loss = 1.1117787e-10,1.538123e-10\n",
      "Iteration 34155: loss = 1.1115452e-10,1.5381384e-10\n",
      "Iteration 34160: loss = 1.1112856e-10,1.5381532e-10\n",
      "Iteration 34165: loss = 1.1110376e-10,1.5381672e-10\n",
      "Iteration 34170: loss = 1.1107864e-10,1.53818e-10\n",
      "Iteration 34175: loss = 1.1105363e-10,1.538193e-10\n",
      "Iteration 34180: loss = 1.1102939e-10,1.538214e-10\n",
      "Iteration 34185: loss = 1.1100266e-10,1.5382363e-10\n",
      "Iteration 34190: loss = 1.10975305e-10,1.5382558e-10\n",
      "Iteration 34195: loss = 1.10948084e-10,1.5382753e-10\n",
      "Iteration 34200: loss = 1.1092157e-10,1.5382937e-10\n",
      "Iteration 34205: loss = 1.1089527e-10,1.538313e-10\n",
      "Iteration 34210: loss = 1.1087189e-10,1.5383302e-10\n",
      "Iteration 34215: loss = 1.1084672e-10,1.5383453e-10\n",
      "Iteration 34220: loss = 1.0964176e-10,1.5383606e-10\n",
      "Iteration 34225: loss = 1.0961932e-10,1.5383693e-10\n",
      "Iteration 34230: loss = 1.09598795e-10,1.5383651e-10\n",
      "Iteration 34235: loss = 1.0958079e-10,1.5383637e-10\n",
      "Iteration 34240: loss = 1.09559674e-10,1.5383665e-10\n",
      "Iteration 34245: loss = 1.095413e-10,1.5383655e-10\n",
      "Iteration 34250: loss = 1.0952028e-10,1.5383637e-10\n",
      "Iteration 34255: loss = 1.0950397e-10,1.5383561e-10\n",
      "Iteration 34260: loss = 1.09487426e-10,1.538341e-10\n",
      "Iteration 34265: loss = 1.0947371e-10,1.5383295e-10\n",
      "Iteration 34270: loss = 1.0945492e-10,1.5383211e-10\n",
      "Iteration 34275: loss = 1.09436536e-10,1.5383211e-10\n",
      "Iteration 34280: loss = 1.0941679e-10,1.5383166e-10\n",
      "Iteration 34285: loss = 1.0939987e-10,1.5383145e-10\n",
      "Iteration 34290: loss = 1.09379616e-10,1.5383118e-10\n",
      "Iteration 34295: loss = 1.09362325e-10,1.5383075e-10\n",
      "Iteration 34300: loss = 1.09343264e-10,1.5383016e-10\n",
      "Iteration 34305: loss = 1.0932766e-10,1.5382945e-10\n",
      "Iteration 34310: loss = 1.0930997e-10,1.5382845e-10\n",
      "Iteration 34315: loss = 1.09294e-10,1.5382744e-10\n",
      "Iteration 34320: loss = 1.09277136e-10,1.5382638e-10\n",
      "Iteration 34325: loss = 1.0926191e-10,1.5382512e-10\n",
      "Iteration 34330: loss = 1.0924576e-10,1.5382395e-10\n",
      "Iteration 34335: loss = 1.0923118e-10,1.5382255e-10\n",
      "Iteration 34340: loss = 1.09215074e-10,1.5382125e-10\n",
      "Iteration 34345: loss = 1.09196964e-10,1.538212e-10\n",
      "Iteration 34350: loss = 1.0916867e-10,1.5382332e-10\n",
      "Iteration 34355: loss = 1.0914364e-10,1.5382569e-10\n",
      "Iteration 34360: loss = 1.0911527e-10,1.5382799e-10\n",
      "Iteration 34365: loss = 1.09090924e-10,1.5383005e-10\n",
      "Iteration 34370: loss = 1.0906385e-10,1.5383213e-10\n",
      "Iteration 34375: loss = 1.09039285e-10,1.538341e-10\n",
      "Iteration 34380: loss = 1.0784279e-10,1.5383637e-10\n",
      "Iteration 34385: loss = 1.07817456e-10,1.5383848e-10\n",
      "Iteration 34390: loss = 1.0778983e-10,1.5384097e-10\n",
      "Iteration 34395: loss = 1.07764776e-10,1.5384335e-10\n",
      "Iteration 34400: loss = 1.07736764e-10,1.5384516e-10\n",
      "Iteration 34405: loss = 1.077155e-10,1.5384652e-10\n",
      "Iteration 34410: loss = 1.0768616e-10,1.5384943e-10\n",
      "Iteration 34415: loss = 1.0765829e-10,1.538526e-10\n",
      "Iteration 34420: loss = 1.0762783e-10,1.5385593e-10\n",
      "Iteration 34425: loss = 1.07600547e-10,1.5385901e-10\n",
      "Iteration 34430: loss = 1.07574116e-10,1.5386092e-10\n",
      "Iteration 34435: loss = 1.07550586e-10,1.538627e-10\n",
      "Iteration 34440: loss = 1.0752446e-10,1.538646e-10\n",
      "Iteration 34445: loss = 1.0750115e-10,1.5386661e-10\n",
      "Iteration 34450: loss = 1.0747545e-10,1.5386863e-10\n",
      "Iteration 34455: loss = 1.0744999e-10,1.5387054e-10\n",
      "Iteration 34460: loss = 1.0742976e-10,1.5387096e-10\n",
      "Iteration 34465: loss = 1.07414265e-10,1.5387008e-10\n",
      "Iteration 34470: loss = 1.07398236e-10,1.538693e-10\n",
      "Iteration 34475: loss = 1.0738057e-10,1.5386806e-10\n",
      "Iteration 34480: loss = 1.07365596e-10,1.53867e-10\n",
      "Iteration 34485: loss = 1.0618295e-10,1.5386808e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34490: loss = 1.0616513e-10,1.5386775e-10\n",
      "Iteration 34495: loss = 1.0614532e-10,1.5386807e-10\n",
      "Iteration 34500: loss = 1.0612463e-10,1.5386906e-10\n",
      "Iteration 34505: loss = 1.0610433e-10,1.5386972e-10\n",
      "Iteration 34510: loss = 1.0608342e-10,1.5387035e-10\n",
      "Iteration 34515: loss = 1.06059585e-10,1.5387085e-10\n",
      "Iteration 34520: loss = 1.06039934e-10,1.538716e-10\n",
      "Iteration 34525: loss = 1.0602008e-10,1.5387225e-10\n",
      "Iteration 34530: loss = 1.060008e-10,1.5387243e-10\n",
      "Iteration 34535: loss = 1.0598259e-10,1.5387241e-10\n",
      "Iteration 34540: loss = 1.0596474e-10,1.5387235e-10\n",
      "Iteration 34545: loss = 1.0594715e-10,1.5387135e-10\n",
      "Iteration 34550: loss = 1.0593343e-10,1.5386975e-10\n",
      "Iteration 34555: loss = 1.05919884e-10,1.5386838e-10\n",
      "Iteration 34560: loss = 1.0590681e-10,1.5386653e-10\n",
      "Iteration 34565: loss = 1.05893544e-10,1.5386518e-10\n",
      "Iteration 34570: loss = 1.0588103e-10,1.5386348e-10\n",
      "Iteration 34575: loss = 1.0586528e-10,1.538617e-10\n",
      "Iteration 34580: loss = 1.05852604e-10,1.5386008e-10\n",
      "Iteration 34585: loss = 1.05839365e-10,1.5385838e-10\n",
      "Iteration 34590: loss = 1.05827874e-10,1.5385651e-10\n",
      "Iteration 34595: loss = 1.0581538e-10,1.5385437e-10\n",
      "Iteration 34600: loss = 1.0580357e-10,1.5385217e-10\n",
      "Iteration 34605: loss = 1.057896e-10,1.538501e-10\n",
      "Iteration 34610: loss = 1.05778775e-10,1.538478e-10\n",
      "Iteration 34615: loss = 1.0576704e-10,1.5384549e-10\n",
      "Iteration 34620: loss = 1.05755515e-10,1.5384317e-10\n",
      "Iteration 34625: loss = 1.0574427e-10,1.5384097e-10\n",
      "Iteration 34630: loss = 1.05733644e-10,1.5383866e-10\n",
      "Iteration 34635: loss = 1.0572022e-10,1.538363e-10\n",
      "Iteration 34640: loss = 1.057087e-10,1.5383408e-10\n",
      "Iteration 34645: loss = 1.05697666e-10,1.5383166e-10\n",
      "Iteration 34650: loss = 1.05687716e-10,1.5382931e-10\n",
      "Iteration 34655: loss = 1.0567672e-10,1.5382692e-10\n",
      "Iteration 34660: loss = 1.0566357e-10,1.5382459e-10\n",
      "Iteration 34665: loss = 1.0565262e-10,1.5382212e-10\n",
      "Iteration 34670: loss = 1.0564244e-10,1.5381968e-10\n",
      "Iteration 34675: loss = 1.0563173e-10,1.5381729e-10\n",
      "Iteration 34680: loss = 1.05621185e-10,1.5381475e-10\n",
      "Iteration 34685: loss = 1.05610354e-10,1.5381213e-10\n",
      "Iteration 34690: loss = 1.0559815e-10,1.5380971e-10\n",
      "Iteration 34695: loss = 1.05587435e-10,1.5380716e-10\n",
      "Iteration 34700: loss = 1.0557672e-10,1.5380458e-10\n",
      "Iteration 34705: loss = 1.0556442e-10,1.5380286e-10\n",
      "Iteration 34710: loss = 1.0555248e-10,1.5380093e-10\n",
      "Iteration 34715: loss = 1.05538855e-10,1.5379953e-10\n",
      "Iteration 34720: loss = 1.05522306e-10,1.5379832e-10\n",
      "Iteration 34725: loss = 1.05507526e-10,1.5379727e-10\n",
      "Iteration 34730: loss = 1.0433951e-10,1.5379722e-10\n",
      "Iteration 34735: loss = 1.0431347e-10,1.5380013e-10\n",
      "Iteration 34740: loss = 1.04290604e-10,1.5380126e-10\n",
      "Iteration 34745: loss = 1.0426671e-10,1.5380311e-10\n",
      "Iteration 34750: loss = 1.0424358e-10,1.5380489e-10\n",
      "Iteration 34755: loss = 1.042196e-10,1.5380637e-10\n",
      "Iteration 34760: loss = 1.04198816e-10,1.5380824e-10\n",
      "Iteration 34765: loss = 1.0417583e-10,1.538094e-10\n",
      "Iteration 34770: loss = 1.0415455e-10,1.5381069e-10\n",
      "Iteration 34775: loss = 1.04133126e-10,1.5381148e-10\n",
      "Iteration 34780: loss = 1.0411152e-10,1.5381252e-10\n",
      "Iteration 34785: loss = 1.0409046e-10,1.5381335e-10\n",
      "Iteration 34790: loss = 1.0407226e-10,1.5381413e-10\n",
      "Iteration 34795: loss = 1.0405193e-10,1.5381521e-10\n",
      "Iteration 34800: loss = 1.0402625e-10,1.5381765e-10\n",
      "Iteration 34805: loss = 1.0400029e-10,1.538199e-10\n",
      "Iteration 34810: loss = 1.03975994e-10,1.5382155e-10\n",
      "Iteration 34815: loss = 1.0395282e-10,1.5382395e-10\n",
      "Iteration 34820: loss = 1.0392859e-10,1.5382581e-10\n",
      "Iteration 34825: loss = 1.02767704e-10,1.538262e-10\n",
      "Iteration 34830: loss = 1.02752036e-10,1.5382645e-10\n",
      "Iteration 34835: loss = 1.02731e-10,1.5382727e-10\n",
      "Iteration 34840: loss = 1.02711e-10,1.5382816e-10\n",
      "Iteration 34845: loss = 1.02690016e-10,1.5382884e-10\n",
      "Iteration 34850: loss = 1.0267037e-10,1.5382975e-10\n",
      "Iteration 34855: loss = 1.02649673e-10,1.5383048e-10\n",
      "Iteration 34860: loss = 1.0263284e-10,1.5383088e-10\n",
      "Iteration 34865: loss = 1.02613695e-10,1.5383143e-10\n",
      "Iteration 34870: loss = 1.025944e-10,1.5383161e-10\n",
      "Iteration 34875: loss = 1.0257664e-10,1.5383186e-10\n",
      "Iteration 34880: loss = 1.0255815e-10,1.5383184e-10\n",
      "Iteration 34885: loss = 1.02540886e-10,1.5383178e-10\n",
      "Iteration 34890: loss = 1.0252305e-10,1.5383177e-10\n",
      "Iteration 34895: loss = 1.0250902e-10,1.5383161e-10\n",
      "Iteration 34900: loss = 1.0249296e-10,1.5383109e-10\n",
      "Iteration 34905: loss = 1.0247639e-10,1.5383048e-10\n",
      "Iteration 34910: loss = 1.02460894e-10,1.5382992e-10\n",
      "Iteration 34915: loss = 1.0244433e-10,1.5382909e-10\n",
      "Iteration 34920: loss = 1.02428795e-10,1.5382856e-10\n",
      "Iteration 34925: loss = 1.0241603e-10,1.5382787e-10\n",
      "Iteration 34930: loss = 1.0240051e-10,1.538269e-10\n",
      "Iteration 34935: loss = 1.0238633e-10,1.538258e-10\n",
      "Iteration 34940: loss = 1.0237066e-10,1.5382493e-10\n",
      "Iteration 34945: loss = 1.0235616e-10,1.5382388e-10\n",
      "Iteration 34950: loss = 1.02339394e-10,1.538235e-10\n",
      "Iteration 34955: loss = 1.0232548e-10,1.538232e-10\n",
      "Iteration 34960: loss = 1.02308655e-10,1.5382304e-10\n",
      "Iteration 34965: loss = 1.02291176e-10,1.5382268e-10\n",
      "Iteration 34970: loss = 1.02274834e-10,1.5382248e-10\n",
      "Iteration 34975: loss = 1.022576e-10,1.5382194e-10\n",
      "Iteration 34980: loss = 1.0223954e-10,1.5382229e-10\n",
      "Iteration 34985: loss = 1.0221901e-10,1.5382298e-10\n",
      "Iteration 34990: loss = 1.02201025e-10,1.5382405e-10\n",
      "Iteration 34995: loss = 1.02180715e-10,1.5382512e-10\n",
      "Iteration 35000: loss = 1.0215961e-10,1.5382653e-10\n",
      "Iteration 35005: loss = 1.02140886e-10,1.5382673e-10\n",
      "Iteration 35010: loss = 1.0212489e-10,1.5382602e-10\n",
      "Iteration 35015: loss = 1.0210947e-10,1.5382533e-10\n",
      "Iteration 35020: loss = 1.0209646e-10,1.5382459e-10\n",
      "Iteration 35025: loss = 1.020814e-10,1.5382391e-10\n",
      "Iteration 35030: loss = 1.0206597e-10,1.5382322e-10\n",
      "Iteration 35035: loss = 1.0203938e-10,1.5382623e-10\n",
      "Iteration 35040: loss = 1.02013314e-10,1.5382914e-10\n",
      "Iteration 35045: loss = 1.0085514e-10,1.5383214e-10\n",
      "Iteration 35050: loss = 1.0083023e-10,1.5383596e-10\n",
      "Iteration 35055: loss = 1.0081017e-10,1.5383654e-10\n",
      "Iteration 35060: loss = 1.0079298e-10,1.538365e-10\n",
      "Iteration 35065: loss = 1.00772966e-10,1.5383723e-10\n",
      "Iteration 35070: loss = 1.0075579e-10,1.5383798e-10\n",
      "Iteration 35075: loss = 1.007359e-10,1.5383857e-10\n",
      "Iteration 35080: loss = 1.00719426e-10,1.5383896e-10\n",
      "Iteration 35085: loss = 1.00693814e-10,1.5384177e-10\n",
      "Iteration 35090: loss = 1.0066615e-10,1.5384537e-10\n",
      "Iteration 35095: loss = 1.00639046e-10,1.5384885e-10\n",
      "Iteration 35100: loss = 1.00612484e-10,1.5385213e-10\n",
      "Iteration 35105: loss = 1.0058599e-10,1.5385518e-10\n",
      "Iteration 35110: loss = 9.9437236e-11,1.5385862e-10\n",
      "Iteration 35115: loss = 9.941816e-11,1.5385954e-10\n",
      "Iteration 35120: loss = 9.940022e-11,1.5386015e-10\n",
      "Iteration 35125: loss = 9.937822e-11,1.538617e-10\n",
      "Iteration 35130: loss = 9.93576e-11,1.5386346e-10\n",
      "Iteration 35135: loss = 9.933476e-11,1.5386535e-10\n",
      "Iteration 35140: loss = 9.9312426e-11,1.5386696e-10\n",
      "Iteration 35145: loss = 9.929399e-11,1.5386839e-10\n",
      "Iteration 35150: loss = 9.927551e-11,1.5386856e-10\n",
      "Iteration 35155: loss = 9.9257914e-11,1.538694e-10\n",
      "Iteration 35160: loss = 9.9237674e-11,1.5386997e-10\n",
      "Iteration 35165: loss = 9.9219626e-11,1.5387104e-10\n",
      "Iteration 35170: loss = 9.9200204e-11,1.5387178e-10\n",
      "Iteration 35175: loss = 9.9182405e-11,1.5387258e-10\n",
      "Iteration 35180: loss = 9.916252e-11,1.5387325e-10\n",
      "Iteration 35185: loss = 9.914483e-11,1.5387423e-10\n",
      "Iteration 35190: loss = 9.912624e-11,1.5387462e-10\n",
      "Iteration 35195: loss = 9.910922e-11,1.5387511e-10\n",
      "Iteration 35200: loss = 9.9090236e-11,1.5387587e-10\n",
      "Iteration 35205: loss = 9.9074006e-11,1.5387588e-10\n",
      "Iteration 35210: loss = 9.9056374e-11,1.5387616e-10\n",
      "Iteration 35215: loss = 9.90405e-11,1.538761e-10\n",
      "Iteration 35220: loss = 9.9023345e-11,1.5387605e-10\n",
      "Iteration 35225: loss = 9.9008586e-11,1.5387541e-10\n",
      "Iteration 35230: loss = 9.8992516e-11,1.5387563e-10\n",
      "Iteration 35235: loss = 9.897685e-11,1.5387562e-10\n",
      "Iteration 35240: loss = 9.8957904e-11,1.5387608e-10\n",
      "Iteration 35245: loss = 9.894093e-11,1.5387654e-10\n",
      "Iteration 35250: loss = 9.892182e-11,1.5387726e-10\n",
      "Iteration 35255: loss = 9.890586e-11,1.5387752e-10\n",
      "Iteration 35260: loss = 9.888742e-11,1.538781e-10\n",
      "Iteration 35265: loss = 9.887097e-11,1.5387841e-10\n",
      "Iteration 35270: loss = 9.8852405e-11,1.5387866e-10\n",
      "Iteration 35275: loss = 9.883744e-11,1.5387899e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35280: loss = 9.882009e-11,1.5387883e-10\n",
      "Iteration 35285: loss = 9.880526e-11,1.5387863e-10\n",
      "Iteration 35290: loss = 9.878865e-11,1.5387831e-10\n",
      "Iteration 35295: loss = 9.877494e-11,1.538781e-10\n",
      "Iteration 35300: loss = 9.875833e-11,1.5387751e-10\n",
      "Iteration 35305: loss = 9.8744464e-11,1.5387733e-10\n",
      "Iteration 35310: loss = 9.872824e-11,1.5387683e-10\n",
      "Iteration 35315: loss = 9.8714335e-11,1.5387631e-10\n",
      "Iteration 35320: loss = 9.8699375e-11,1.538758e-10\n",
      "Iteration 35325: loss = 9.868626e-11,1.5387513e-10\n",
      "Iteration 35330: loss = 9.867002e-11,1.538749e-10\n",
      "Iteration 35335: loss = 9.8654564e-11,1.5387472e-10\n",
      "Iteration 35340: loss = 9.86382e-11,1.538747e-10\n",
      "Iteration 35345: loss = 9.862361e-11,1.538748e-10\n",
      "Iteration 35350: loss = 9.860657e-11,1.5387444e-10\n",
      "Iteration 35355: loss = 9.85854e-11,1.5387575e-10\n",
      "Iteration 35360: loss = 9.744767e-11,1.5387949e-10\n",
      "Iteration 35365: loss = 9.7419094e-11,1.5388357e-10\n",
      "Iteration 35370: loss = 9.7395654e-11,1.5388643e-10\n",
      "Iteration 35375: loss = 9.7370605e-11,1.5388911e-10\n",
      "Iteration 35380: loss = 9.734702e-11,1.538922e-10\n",
      "Iteration 35385: loss = 9.7327896e-11,1.538937e-10\n",
      "Iteration 35390: loss = 9.730897e-11,1.5389491e-10\n",
      "Iteration 35395: loss = 9.728788e-11,1.5389669e-10\n",
      "Iteration 35400: loss = 9.72681e-11,1.5389773e-10\n",
      "Iteration 35405: loss = 9.6146514e-11,1.5389841e-10\n",
      "Iteration 35410: loss = 9.612717e-11,1.5390009e-10\n",
      "Iteration 35415: loss = 9.61048e-11,1.5390277e-10\n",
      "Iteration 35420: loss = 9.608922e-11,1.5390238e-10\n",
      "Iteration 35425: loss = 9.6074565e-11,1.53902e-10\n",
      "Iteration 35430: loss = 9.605933e-11,1.5390159e-10\n",
      "Iteration 35435: loss = 9.604494e-11,1.539013e-10\n",
      "Iteration 35440: loss = 9.6032404e-11,1.5390063e-10\n",
      "Iteration 35445: loss = 9.600764e-11,1.5390333e-10\n",
      "Iteration 35450: loss = 9.598586e-11,1.5390567e-10\n",
      "Iteration 35455: loss = 9.5963425e-11,1.5390811e-10\n",
      "Iteration 35460: loss = 9.594265e-11,1.5391026e-10\n",
      "Iteration 35465: loss = 9.592185e-11,1.5391215e-10\n",
      "Iteration 35470: loss = 9.590186e-11,1.5391415e-10\n",
      "Iteration 35475: loss = 9.588209e-11,1.5391491e-10\n",
      "Iteration 35480: loss = 9.586639e-11,1.5391503e-10\n",
      "Iteration 35485: loss = 9.585097e-11,1.5391519e-10\n",
      "Iteration 35490: loss = 9.583542e-11,1.5391549e-10\n",
      "Iteration 35495: loss = 9.5819325e-11,1.5391544e-10\n",
      "Iteration 35500: loss = 9.580209e-11,1.5391546e-10\n",
      "Iteration 35505: loss = 9.578727e-11,1.5391562e-10\n",
      "Iteration 35510: loss = 9.577272e-11,1.5391532e-10\n",
      "Iteration 35515: loss = 9.575741e-11,1.5391555e-10\n",
      "Iteration 35520: loss = 9.574128e-11,1.5391581e-10\n",
      "Iteration 35525: loss = 9.572188e-11,1.5391757e-10\n",
      "Iteration 35530: loss = 9.570152e-11,1.5391863e-10\n",
      "Iteration 35535: loss = 9.5685106e-11,1.5391921e-10\n",
      "Iteration 35540: loss = 9.5667786e-11,1.5391968e-10\n",
      "Iteration 35545: loss = 9.5651646e-11,1.5392024e-10\n",
      "Iteration 35550: loss = 9.5635964e-11,1.5392071e-10\n",
      "Iteration 35555: loss = 9.5617486e-11,1.539209e-10\n",
      "Iteration 35560: loss = 9.560177e-11,1.5392126e-10\n",
      "Iteration 35565: loss = 9.5586206e-11,1.5392147e-10\n",
      "Iteration 35570: loss = 9.5571134e-11,1.5392174e-10\n",
      "Iteration 35575: loss = 9.5555876e-11,1.5392193e-10\n",
      "Iteration 35580: loss = 9.554115e-11,1.5392171e-10\n",
      "Iteration 35585: loss = 9.552404e-11,1.539216e-10\n",
      "Iteration 35590: loss = 9.5509885e-11,1.5392121e-10\n",
      "Iteration 35595: loss = 9.549623e-11,1.5392072e-10\n",
      "Iteration 35600: loss = 9.54827e-11,1.5392027e-10\n",
      "Iteration 35605: loss = 9.5469216e-11,1.5391967e-10\n",
      "Iteration 35610: loss = 9.545364e-11,1.5391907e-10\n",
      "Iteration 35615: loss = 9.544012e-11,1.5391875e-10\n",
      "Iteration 35620: loss = 9.5426646e-11,1.5391843e-10\n",
      "Iteration 35625: loss = 9.541373e-11,1.5391798e-10\n",
      "Iteration 35630: loss = 9.539966e-11,1.5391756e-10\n",
      "Iteration 35635: loss = 9.538496e-11,1.5391727e-10\n",
      "Iteration 35640: loss = 9.536566e-11,1.5391839e-10\n",
      "Iteration 35645: loss = 9.535276e-11,1.5391799e-10\n",
      "Iteration 35650: loss = 9.534406e-11,1.539156e-10\n",
      "Iteration 35655: loss = 9.533561e-11,1.5391305e-10\n",
      "Iteration 35660: loss = 9.5322035e-11,1.5391244e-10\n",
      "Iteration 35665: loss = 9.530504e-11,1.5391323e-10\n",
      "Iteration 35670: loss = 9.52864e-11,1.5391428e-10\n",
      "Iteration 35675: loss = 9.5270125e-11,1.5391455e-10\n",
      "Iteration 35680: loss = 9.525337e-11,1.5391519e-10\n",
      "Iteration 35685: loss = 9.5237374e-11,1.5391549e-10\n",
      "Iteration 35690: loss = 9.522164e-11,1.5391571e-10\n",
      "Iteration 35695: loss = 9.410942e-11,1.5391659e-10\n",
      "Iteration 35700: loss = 9.4093566e-11,1.5391735e-10\n",
      "Iteration 35705: loss = 9.4074866e-11,1.5391852e-10\n",
      "Iteration 35710: loss = 9.405606e-11,1.5391995e-10\n",
      "Iteration 35715: loss = 9.403708e-11,1.539214e-10\n",
      "Iteration 35720: loss = 9.401818e-11,1.539229e-10\n",
      "Iteration 35725: loss = 9.291656e-11,1.5392387e-10\n",
      "Iteration 35730: loss = 9.28948e-11,1.5392627e-10\n",
      "Iteration 35735: loss = 9.28718e-11,1.5392923e-10\n",
      "Iteration 35740: loss = 9.284994e-11,1.5393202e-10\n",
      "Iteration 35745: loss = 9.283404e-11,1.5393198e-10\n",
      "Iteration 35750: loss = 9.282112e-11,1.53932e-10\n",
      "Iteration 35755: loss = 9.2807116e-11,1.5393134e-10\n",
      "Iteration 35760: loss = 9.278913e-11,1.5393231e-10\n",
      "Iteration 35765: loss = 9.2768765e-11,1.5393437e-10\n",
      "Iteration 35770: loss = 9.275091e-11,1.5393666e-10\n",
      "Iteration 35775: loss = 9.2732384e-11,1.5393771e-10\n",
      "Iteration 35780: loss = 9.271289e-11,1.5393914e-10\n",
      "Iteration 35785: loss = 9.269376e-11,1.5394058e-10\n",
      "Iteration 35790: loss = 9.26752e-11,1.5394191e-10\n",
      "Iteration 35795: loss = 9.2658e-11,1.5394305e-10\n",
      "Iteration 35800: loss = 9.264023e-11,1.53944e-10\n",
      "Iteration 35805: loss = 9.262466e-11,1.5394493e-10\n",
      "Iteration 35810: loss = 9.26069e-11,1.5394586e-10\n",
      "Iteration 35815: loss = 9.2589936e-11,1.5394655e-10\n",
      "Iteration 35820: loss = 9.257208e-11,1.5394756e-10\n",
      "Iteration 35825: loss = 9.2552306e-11,1.5394946e-10\n",
      "Iteration 35830: loss = 9.253187e-11,1.5395138e-10\n",
      "Iteration 35835: loss = 9.251125e-11,1.5395353e-10\n",
      "Iteration 35840: loss = 9.249367e-11,1.5395552e-10\n",
      "Iteration 35845: loss = 9.2473355e-11,1.539577e-10\n",
      "Iteration 35850: loss = 9.245362e-11,1.5395925e-10\n",
      "Iteration 35855: loss = 9.243389e-11,1.539612e-10\n",
      "Iteration 35860: loss = 9.241476e-11,1.5396287e-10\n",
      "Iteration 35865: loss = 9.239618e-11,1.5396422e-10\n",
      "Iteration 35870: loss = 9.238025e-11,1.5396479e-10\n",
      "Iteration 35875: loss = 9.2369924e-11,1.5396354e-10\n",
      "Iteration 35880: loss = 9.235718e-11,1.5396236e-10\n",
      "Iteration 35885: loss = 9.23475e-11,1.5396148e-10\n",
      "Iteration 35890: loss = 9.2335e-11,1.5396032e-10\n",
      "Iteration 35895: loss = 9.232445e-11,1.5395937e-10\n",
      "Iteration 35900: loss = 9.23124e-11,1.5395805e-10\n",
      "Iteration 35905: loss = 9.230058e-11,1.5395668e-10\n",
      "Iteration 35910: loss = 9.2291876e-11,1.5395527e-10\n",
      "Iteration 35915: loss = 9.22754e-11,1.5395572e-10\n",
      "Iteration 35920: loss = 9.225904e-11,1.5395624e-10\n",
      "Iteration 35925: loss = 9.224325e-11,1.5395685e-10\n",
      "Iteration 35930: loss = 9.2226865e-11,1.5395728e-10\n",
      "Iteration 35935: loss = 9.2213376e-11,1.5395774e-10\n",
      "Iteration 35940: loss = 9.219803e-11,1.5395807e-10\n",
      "Iteration 35945: loss = 9.2183185e-11,1.5395794e-10\n",
      "Iteration 35950: loss = 9.216808e-11,1.5395824e-10\n",
      "Iteration 35955: loss = 9.2153424e-11,1.5395829e-10\n",
      "Iteration 35960: loss = 9.2138186e-11,1.5395835e-10\n",
      "Iteration 35965: loss = 9.212411e-11,1.5395826e-10\n",
      "Iteration 35970: loss = 9.211116e-11,1.5395842e-10\n",
      "Iteration 35975: loss = 9.209747e-11,1.5395829e-10\n",
      "Iteration 35980: loss = 9.2084125e-11,1.539579e-10\n",
      "Iteration 35985: loss = 9.207027e-11,1.5395725e-10\n",
      "Iteration 35990: loss = 9.2057355e-11,1.5395675e-10\n",
      "Iteration 35995: loss = 9.204371e-11,1.5395629e-10\n",
      "Iteration 36000: loss = 9.203075e-11,1.5395589e-10\n",
      "Iteration 36005: loss = 9.201945e-11,1.539554e-10\n",
      "Iteration 36010: loss = 9.200693e-11,1.5395459e-10\n",
      "Iteration 36015: loss = 9.1993364e-11,1.5395406e-10\n",
      "Iteration 36020: loss = 9.198084e-11,1.5395324e-10\n",
      "Iteration 36025: loss = 9.196848e-11,1.5395282e-10\n",
      "Iteration 36030: loss = 9.195506e-11,1.5395235e-10\n",
      "Iteration 36035: loss = 9.194447e-11,1.5395148e-10\n",
      "Iteration 36040: loss = 9.193144e-11,1.5395082e-10\n",
      "Iteration 36045: loss = 9.191925e-11,1.539501e-10\n",
      "Iteration 36050: loss = 9.1905934e-11,1.5394946e-10\n",
      "Iteration 36055: loss = 9.081796e-11,1.5394905e-10\n",
      "Iteration 36060: loss = 9.081072e-11,1.5394659e-10\n",
      "Iteration 36065: loss = 8.973757e-11,1.5394389e-10\n",
      "Iteration 36070: loss = 8.9719066e-11,1.5394577e-10\n",
      "Iteration 36075: loss = 8.9697395e-11,1.539484e-10\n",
      "Iteration 36080: loss = 8.9677786e-11,1.5395099e-10\n",
      "Iteration 36085: loss = 8.965479e-11,1.5395396e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36090: loss = 8.963513e-11,1.5395688e-10\n",
      "Iteration 36095: loss = 8.961284e-11,1.5395914e-10\n",
      "Iteration 36100: loss = 8.9594034e-11,1.5396177e-10\n",
      "Iteration 36105: loss = 8.957276e-11,1.5396405e-10\n",
      "Iteration 36110: loss = 8.955243e-11,1.5396606e-10\n",
      "Iteration 36115: loss = 8.953388e-11,1.539684e-10\n",
      "Iteration 36120: loss = 8.951429e-11,1.5397042e-10\n",
      "Iteration 36125: loss = 8.949708e-11,1.5397218e-10\n",
      "Iteration 36130: loss = 8.9477314e-11,1.5397386e-10\n",
      "Iteration 36135: loss = 8.946029e-11,1.5397572e-10\n",
      "Iteration 36140: loss = 8.944087e-11,1.5397719e-10\n",
      "Iteration 36145: loss = 8.942392e-11,1.5397927e-10\n",
      "Iteration 36150: loss = 8.940373e-11,1.5398094e-10\n",
      "Iteration 36155: loss = 8.9389655e-11,1.5398147e-10\n",
      "Iteration 36160: loss = 8.937459e-11,1.5398198e-10\n",
      "Iteration 36165: loss = 8.9360165e-11,1.5398254e-10\n",
      "Iteration 36170: loss = 8.934309e-11,1.5398323e-10\n",
      "Iteration 36175: loss = 8.932877e-11,1.5398367e-10\n",
      "Iteration 36180: loss = 8.9315534e-11,1.5398363e-10\n",
      "Iteration 36185: loss = 8.930486e-11,1.5398277e-10\n",
      "Iteration 36190: loss = 8.929422e-11,1.5398166e-10\n",
      "Iteration 36195: loss = 8.928161e-11,1.5398058e-10\n",
      "Iteration 36200: loss = 8.92675e-11,1.5398098e-10\n",
      "Iteration 36205: loss = 8.924941e-11,1.5398291e-10\n",
      "Iteration 36210: loss = 8.9229284e-11,1.5398519e-10\n",
      "Iteration 36215: loss = 8.921149e-11,1.5398718e-10\n",
      "Iteration 36220: loss = 8.9192355e-11,1.5398871e-10\n",
      "Iteration 36225: loss = 8.917613e-11,1.5399021e-10\n",
      "Iteration 36230: loss = 8.9157876e-11,1.539916e-10\n",
      "Iteration 36235: loss = 8.9141645e-11,1.5399304e-10\n",
      "Iteration 36240: loss = 8.912377e-11,1.539943e-10\n",
      "Iteration 36245: loss = 8.910805e-11,1.5399573e-10\n",
      "Iteration 36250: loss = 8.908995e-11,1.5399731e-10\n",
      "Iteration 36255: loss = 8.907439e-11,1.539984e-10\n",
      "Iteration 36260: loss = 8.905724e-11,1.5399952e-10\n",
      "Iteration 36265: loss = 8.904231e-11,1.5400019e-10\n",
      "Iteration 36270: loss = 8.902535e-11,1.5400126e-10\n",
      "Iteration 36275: loss = 8.901039e-11,1.5400206e-10\n",
      "Iteration 36280: loss = 8.899346e-11,1.5400287e-10\n",
      "Iteration 36285: loss = 8.897865e-11,1.5400392e-10\n",
      "Iteration 36290: loss = 8.8962455e-11,1.540046e-10\n",
      "Iteration 36295: loss = 8.89455e-11,1.5400543e-10\n",
      "Iteration 36300: loss = 8.893201e-11,1.5400595e-10\n",
      "Iteration 36305: loss = 8.891616e-11,1.5400667e-10\n",
      "Iteration 36310: loss = 8.890325e-11,1.5400703e-10\n",
      "Iteration 36315: loss = 8.8887904e-11,1.5400706e-10\n",
      "Iteration 36320: loss = 8.8875324e-11,1.5400721e-10\n",
      "Iteration 36325: loss = 8.886041e-11,1.5400753e-10\n",
      "Iteration 36330: loss = 8.884741e-11,1.5400768e-10\n",
      "Iteration 36335: loss = 8.8832275e-11,1.5400808e-10\n",
      "Iteration 36340: loss = 8.8818675e-11,1.5400828e-10\n",
      "Iteration 36345: loss = 8.880416e-11,1.5400828e-10\n",
      "Iteration 36350: loss = 8.87913e-11,1.5400839e-10\n",
      "Iteration 36355: loss = 8.87767e-11,1.5400856e-10\n",
      "Iteration 36360: loss = 8.876411e-11,1.540088e-10\n",
      "Iteration 36365: loss = 8.874949e-11,1.540087e-10\n",
      "Iteration 36370: loss = 8.87373e-11,1.540087e-10\n",
      "Iteration 36375: loss = 8.872275e-11,1.5400922e-10\n",
      "Iteration 36380: loss = 8.7652406e-11,1.5400947e-10\n",
      "Iteration 36385: loss = 8.764193e-11,1.5400785e-10\n",
      "Iteration 36390: loss = 8.763181e-11,1.5400685e-10\n",
      "Iteration 36395: loss = 8.762171e-11,1.5400586e-10\n",
      "Iteration 36400: loss = 8.761123e-11,1.5400488e-10\n",
      "Iteration 36405: loss = 8.760079e-11,1.54004e-10\n",
      "Iteration 36410: loss = 8.7587965e-11,1.5400314e-10\n",
      "Iteration 36415: loss = 8.652992e-11,1.5400256e-10\n",
      "Iteration 36420: loss = 8.651326e-11,1.5400398e-10\n",
      "Iteration 36425: loss = 8.648993e-11,1.5400829e-10\n",
      "Iteration 36430: loss = 8.647094e-11,1.540102e-10\n",
      "Iteration 36435: loss = 8.6454316e-11,1.5401184e-10\n",
      "Iteration 36440: loss = 8.643949e-11,1.5401269e-10\n",
      "Iteration 36445: loss = 8.642479e-11,1.5401327e-10\n",
      "Iteration 36450: loss = 8.641283e-11,1.5401361e-10\n",
      "Iteration 36455: loss = 8.6398465e-11,1.5401416e-10\n",
      "Iteration 36460: loss = 8.6384344e-11,1.5401436e-10\n",
      "Iteration 36465: loss = 8.6370765e-11,1.5401466e-10\n",
      "Iteration 36470: loss = 8.635745e-11,1.5401444e-10\n",
      "Iteration 36475: loss = 8.634279e-11,1.5401519e-10\n",
      "Iteration 36480: loss = 8.6322303e-11,1.5401846e-10\n",
      "Iteration 36485: loss = 8.630887e-11,1.5401888e-10\n",
      "Iteration 36490: loss = 8.6294825e-11,1.5401955e-10\n",
      "Iteration 36495: loss = 8.628059e-11,1.5401966e-10\n",
      "Iteration 36500: loss = 8.6267375e-11,1.5402014e-10\n",
      "Iteration 36505: loss = 8.625305e-11,1.5402041e-10\n",
      "Iteration 36510: loss = 8.6239065e-11,1.5402088e-10\n",
      "Iteration 36515: loss = 8.622536e-11,1.5402093e-10\n",
      "Iteration 36520: loss = 8.621215e-11,1.540211e-10\n",
      "Iteration 36525: loss = 8.620176e-11,1.5402127e-10\n",
      "Iteration 36530: loss = 8.618924e-11,1.5402127e-10\n",
      "Iteration 36535: loss = 8.617699e-11,1.5402057e-10\n",
      "Iteration 36540: loss = 8.6159274e-11,1.5402185e-10\n",
      "Iteration 36545: loss = 8.6140595e-11,1.5402495e-10\n",
      "Iteration 36550: loss = 8.6122436e-11,1.5402686e-10\n",
      "Iteration 36555: loss = 8.6106566e-11,1.540285e-10\n",
      "Iteration 36560: loss = 8.609036e-11,1.5402991e-10\n",
      "Iteration 36565: loss = 8.607304e-11,1.5403111e-10\n",
      "Iteration 36570: loss = 8.605725e-11,1.5403254e-10\n",
      "Iteration 36575: loss = 8.604212e-11,1.5403366e-10\n",
      "Iteration 36580: loss = 8.6027095e-11,1.5403506e-10\n",
      "Iteration 36585: loss = 8.601131e-11,1.5403603e-10\n",
      "Iteration 36590: loss = 8.599459e-11,1.540372e-10\n",
      "Iteration 36595: loss = 8.5979536e-11,1.5403788e-10\n",
      "Iteration 36600: loss = 8.59654e-11,1.5403898e-10\n",
      "Iteration 36605: loss = 8.595088e-11,1.5403986e-10\n",
      "Iteration 36610: loss = 8.593747e-11,1.5404031e-10\n",
      "Iteration 36615: loss = 8.5921326e-11,1.540412e-10\n",
      "Iteration 36620: loss = 8.590611e-11,1.5404227e-10\n",
      "Iteration 36625: loss = 8.5890045e-11,1.5404372e-10\n",
      "Iteration 36630: loss = 8.587698e-11,1.5404392e-10\n",
      "Iteration 36635: loss = 8.586334e-11,1.5404483e-10\n",
      "Iteration 36640: loss = 8.5845976e-11,1.5404578e-10\n",
      "Iteration 36645: loss = 8.583107e-11,1.5404686e-10\n",
      "Iteration 36650: loss = 8.5817416e-11,1.5404789e-10\n",
      "Iteration 36655: loss = 8.580265e-11,1.5404875e-10\n",
      "Iteration 36660: loss = 8.578907e-11,1.5404941e-10\n",
      "Iteration 36665: loss = 8.5774665e-11,1.5405024e-10\n",
      "Iteration 36670: loss = 8.575892e-11,1.5405084e-10\n",
      "Iteration 36675: loss = 8.574463e-11,1.5405205e-10\n",
      "Iteration 36680: loss = 8.572998e-11,1.5405274e-10\n",
      "Iteration 36685: loss = 8.57166e-11,1.5405346e-10\n",
      "Iteration 36690: loss = 8.570213e-11,1.5405417e-10\n",
      "Iteration 36695: loss = 8.5686645e-11,1.5405484e-10\n",
      "Iteration 36700: loss = 8.567206e-11,1.5405573e-10\n",
      "Iteration 36705: loss = 8.565542e-11,1.5405735e-10\n",
      "Iteration 36710: loss = 8.563916e-11,1.540585e-10\n",
      "Iteration 36715: loss = 8.563078e-11,1.5405763e-10\n",
      "Iteration 36720: loss = 8.4582626e-11,1.5405678e-10\n",
      "Iteration 36725: loss = 8.457182e-11,1.5405636e-10\n",
      "Iteration 36730: loss = 8.4560685e-11,1.5405556e-10\n",
      "Iteration 36735: loss = 8.454948e-11,1.5405494e-10\n",
      "Iteration 36740: loss = 8.45374e-11,1.5405466e-10\n",
      "Iteration 36745: loss = 8.452606e-11,1.5405417e-10\n",
      "Iteration 36750: loss = 8.45164e-11,1.5405367e-10\n",
      "Iteration 36755: loss = 8.450576e-11,1.5405316e-10\n",
      "Iteration 36760: loss = 8.449464e-11,1.5405241e-10\n",
      "Iteration 36765: loss = 8.448417e-11,1.5405147e-10\n",
      "Iteration 36770: loss = 8.447404e-11,1.5405044e-10\n",
      "Iteration 36775: loss = 8.446355e-11,1.540496e-10\n",
      "Iteration 36780: loss = 8.4453805e-11,1.5404869e-10\n",
      "Iteration 36785: loss = 8.444571e-11,1.5404758e-10\n",
      "Iteration 36790: loss = 8.443605e-11,1.5404647e-10\n",
      "Iteration 36795: loss = 8.442641e-11,1.5404523e-10\n",
      "Iteration 36800: loss = 8.4417466e-11,1.5404393e-10\n",
      "Iteration 36805: loss = 8.337986e-11,1.5404258e-10\n",
      "Iteration 36810: loss = 8.337062e-11,1.540415e-10\n",
      "Iteration 36815: loss = 8.335799e-11,1.5404125e-10\n",
      "Iteration 36820: loss = 8.333789e-11,1.540446e-10\n",
      "Iteration 36825: loss = 8.331872e-11,1.5404794e-10\n",
      "Iteration 36830: loss = 8.32986e-11,1.5405105e-10\n",
      "Iteration 36835: loss = 8.327863e-11,1.5405464e-10\n",
      "Iteration 36840: loss = 8.3260975e-11,1.5405673e-10\n",
      "Iteration 36845: loss = 8.32433e-11,1.5405839e-10\n",
      "Iteration 36850: loss = 8.3221575e-11,1.5406237e-10\n",
      "Iteration 36855: loss = 8.320027e-11,1.5406612e-10\n",
      "Iteration 36860: loss = 8.318716e-11,1.5406719e-10\n",
      "Iteration 36865: loss = 8.317325e-11,1.5406726e-10\n",
      "Iteration 36870: loss = 8.3160916e-11,1.54068e-10\n",
      "Iteration 36875: loss = 8.314116e-11,1.540711e-10\n",
      "Iteration 36880: loss = 8.3121544e-11,1.5407414e-10\n",
      "Iteration 36885: loss = 8.310266e-11,1.5407692e-10\n",
      "Iteration 36890: loss = 8.308328e-11,1.5407967e-10\n",
      "Iteration 36895: loss = 8.306474e-11,1.5408193e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36900: loss = 8.304834e-11,1.5408458e-10\n",
      "Iteration 36905: loss = 8.303064e-11,1.5408706e-10\n",
      "Iteration 36910: loss = 8.301296e-11,1.5408955e-10\n",
      "Iteration 36915: loss = 8.2995395e-11,1.5409174e-10\n",
      "Iteration 36920: loss = 8.297984e-11,1.5409315e-10\n",
      "Iteration 36925: loss = 8.296402e-11,1.5409443e-10\n",
      "Iteration 36930: loss = 8.2947905e-11,1.5409593e-10\n",
      "Iteration 36935: loss = 8.293454e-11,1.5409726e-10\n",
      "Iteration 36940: loss = 8.29198e-11,1.5409833e-10\n",
      "Iteration 36945: loss = 8.290502e-11,1.540994e-10\n",
      "Iteration 36950: loss = 8.28907e-11,1.541001e-10\n",
      "Iteration 36955: loss = 8.2875935e-11,1.5410138e-10\n",
      "Iteration 36960: loss = 8.286152e-11,1.5410243e-10\n",
      "Iteration 36965: loss = 8.284647e-11,1.5410342e-10\n",
      "Iteration 36970: loss = 8.2832026e-11,1.5410412e-10\n",
      "Iteration 36975: loss = 8.281964e-11,1.5410503e-10\n",
      "Iteration 36980: loss = 8.280531e-11,1.541061e-10\n",
      "Iteration 36985: loss = 8.279105e-11,1.5410712e-10\n",
      "Iteration 36990: loss = 8.2776765e-11,1.5410774e-10\n",
      "Iteration 36995: loss = 8.275927e-11,1.5411045e-10\n",
      "Iteration 37000: loss = 8.2742514e-11,1.5411235e-10\n",
      "Iteration 37005: loss = 8.273234e-11,1.5411175e-10\n",
      "Iteration 37010: loss = 8.272111e-11,1.5411143e-10\n",
      "Iteration 37015: loss = 8.271164e-11,1.5411099e-10\n",
      "Iteration 37020: loss = 8.270007e-11,1.5411057e-10\n",
      "Iteration 37025: loss = 8.2688884e-11,1.5410997e-10\n",
      "Iteration 37030: loss = 8.267927e-11,1.5410942e-10\n",
      "Iteration 37035: loss = 8.266671e-11,1.5411014e-10\n",
      "Iteration 37040: loss = 8.2648e-11,1.541129e-10\n",
      "Iteration 37045: loss = 8.262994e-11,1.5411525e-10\n",
      "Iteration 37050: loss = 8.261201e-11,1.5411793e-10\n",
      "Iteration 37055: loss = 8.2594126e-11,1.5412029e-10\n",
      "Iteration 37060: loss = 8.257702e-11,1.5412255e-10\n",
      "Iteration 37065: loss = 8.1544334e-11,1.541238e-10\n",
      "Iteration 37070: loss = 8.15339e-11,1.5412359e-10\n",
      "Iteration 37075: loss = 8.152016e-11,1.5412427e-10\n",
      "Iteration 37080: loss = 8.150799e-11,1.5412463e-10\n",
      "Iteration 37085: loss = 8.149417e-11,1.5412535e-10\n",
      "Iteration 37090: loss = 8.1482376e-11,1.54126e-10\n",
      "Iteration 37095: loss = 8.146848e-11,1.5412652e-10\n",
      "Iteration 37100: loss = 8.145507e-11,1.5412698e-10\n",
      "Iteration 37105: loss = 8.144371e-11,1.5412732e-10\n",
      "Iteration 37110: loss = 8.143077e-11,1.5412752e-10\n",
      "Iteration 37115: loss = 8.1420086e-11,1.5412724e-10\n",
      "Iteration 37120: loss = 8.141018e-11,1.5412621e-10\n",
      "Iteration 37125: loss = 8.140258e-11,1.5412525e-10\n",
      "Iteration 37130: loss = 8.139311e-11,1.5412394e-10\n",
      "Iteration 37135: loss = 8.138543e-11,1.5412259e-10\n",
      "Iteration 37140: loss = 8.137616e-11,1.5412131e-10\n",
      "Iteration 37145: loss = 8.13686e-11,1.5411993e-10\n",
      "Iteration 37150: loss = 8.1359953e-11,1.5411865e-10\n",
      "Iteration 37155: loss = 8.135289e-11,1.5411727e-10\n",
      "Iteration 37160: loss = 8.134362e-11,1.5411623e-10\n",
      "Iteration 37165: loss = 8.1334224e-11,1.5411479e-10\n",
      "Iteration 37170: loss = 8.132742e-11,1.5411329e-10\n",
      "Iteration 37175: loss = 8.131962e-11,1.5411164e-10\n",
      "Iteration 37180: loss = 8.1313095e-11,1.5410992e-10\n",
      "Iteration 37185: loss = 8.1304734e-11,1.5410812e-10\n",
      "Iteration 37190: loss = 8.129842e-11,1.5410606e-10\n",
      "Iteration 37195: loss = 8.129037e-11,1.5410428e-10\n",
      "Iteration 37200: loss = 8.027557e-11,1.5410265e-10\n",
      "Iteration 37205: loss = 8.0265905e-11,1.5410134e-10\n",
      "Iteration 37210: loss = 8.025817e-11,1.5410025e-10\n",
      "Iteration 37215: loss = 8.0244866e-11,1.5410058e-10\n",
      "Iteration 37220: loss = 8.023284e-11,1.5410115e-10\n",
      "Iteration 37225: loss = 8.022006e-11,1.5410179e-10\n",
      "Iteration 37230: loss = 8.02064e-11,1.5410236e-10\n",
      "Iteration 37235: loss = 8.0199715e-11,1.5410037e-10\n",
      "Iteration 37240: loss = 8.0194316e-11,1.5409791e-10\n",
      "Iteration 37245: loss = 8.0188904e-11,1.5409557e-10\n",
      "Iteration 37250: loss = 8.01843e-11,1.5409313e-10\n",
      "Iteration 37255: loss = 8.017892e-11,1.5409071e-10\n",
      "Iteration 37260: loss = 8.016435e-11,1.5409152e-10\n",
      "Iteration 37265: loss = 8.014999e-11,1.5409288e-10\n",
      "Iteration 37270: loss = 8.013571e-11,1.5409443e-10\n",
      "Iteration 37275: loss = 8.012215e-11,1.540955e-10\n",
      "Iteration 37280: loss = 8.0108475e-11,1.5409689e-10\n",
      "Iteration 37285: loss = 8.008546e-11,1.541017e-10\n",
      "Iteration 37290: loss = 8.006282e-11,1.541059e-10\n",
      "Iteration 37295: loss = 8.004233e-11,1.5411034e-10\n",
      "Iteration 37300: loss = 8.0021045e-11,1.5411415e-10\n",
      "Iteration 37305: loss = 8.0004364e-11,1.5411669e-10\n",
      "Iteration 37310: loss = 7.9985386e-11,1.5411962e-10\n",
      "Iteration 37315: loss = 7.996953e-11,1.5412237e-10\n",
      "Iteration 37320: loss = 7.995072e-11,1.5412488e-10\n",
      "Iteration 37325: loss = 7.993537e-11,1.5412749e-10\n",
      "Iteration 37330: loss = 7.9918246e-11,1.5412957e-10\n",
      "Iteration 37335: loss = 7.989954e-11,1.541334e-10\n",
      "Iteration 37340: loss = 7.988502e-11,1.541342e-10\n",
      "Iteration 37345: loss = 7.987183e-11,1.5413489e-10\n",
      "Iteration 37350: loss = 7.985859e-11,1.5413577e-10\n",
      "Iteration 37355: loss = 7.9845845e-11,1.5413676e-10\n",
      "Iteration 37360: loss = 7.983669e-11,1.5413634e-10\n",
      "Iteration 37365: loss = 7.982453e-11,1.5413594e-10\n",
      "Iteration 37370: loss = 7.981532e-11,1.5413551e-10\n",
      "Iteration 37375: loss = 7.9805475e-11,1.5413512e-10\n",
      "Iteration 37380: loss = 7.9796614e-11,1.5413452e-10\n",
      "Iteration 37385: loss = 7.978762e-11,1.5413382e-10\n",
      "Iteration 37390: loss = 7.9776685e-11,1.5413304e-10\n",
      "Iteration 37395: loss = 7.976622e-11,1.541331e-10\n",
      "Iteration 37400: loss = 7.9749755e-11,1.5413557e-10\n",
      "Iteration 37405: loss = 7.973231e-11,1.5413795e-10\n",
      "Iteration 37410: loss = 7.971635e-11,1.5414044e-10\n",
      "Iteration 37415: loss = 7.969909e-11,1.5414289e-10\n",
      "Iteration 37420: loss = 7.968302e-11,1.5414528e-10\n",
      "Iteration 37425: loss = 7.9665934e-11,1.541475e-10\n",
      "Iteration 37430: loss = 7.965042e-11,1.5415003e-10\n",
      "Iteration 37435: loss = 7.9633945e-11,1.5415198e-10\n",
      "Iteration 37440: loss = 7.961686e-11,1.5415408e-10\n",
      "Iteration 37445: loss = 7.960233e-11,1.541564e-10\n",
      "Iteration 37450: loss = 7.958568e-11,1.5415826e-10\n",
      "Iteration 37455: loss = 7.8575514e-11,1.5415902e-10\n",
      "Iteration 37460: loss = 7.85647e-11,1.5415816e-10\n",
      "Iteration 37465: loss = 7.855629e-11,1.541573e-10\n",
      "Iteration 37470: loss = 7.854667e-11,1.541566e-10\n",
      "Iteration 37475: loss = 7.853792e-11,1.541562e-10\n",
      "Iteration 37480: loss = 7.852862e-11,1.5415537e-10\n",
      "Iteration 37485: loss = 7.851798e-11,1.5415491e-10\n",
      "Iteration 37490: loss = 7.850944e-11,1.5415427e-10\n",
      "Iteration 37495: loss = 7.8500505e-11,1.5415325e-10\n",
      "Iteration 37500: loss = 7.8492025e-11,1.5415266e-10\n",
      "Iteration 37505: loss = 7.848322e-11,1.541518e-10\n",
      "Iteration 37510: loss = 7.847329e-11,1.5415103e-10\n",
      "Iteration 37515: loss = 7.8464575e-11,1.5415001e-10\n",
      "Iteration 37520: loss = 7.845637e-11,1.5414911e-10\n",
      "Iteration 37525: loss = 7.844792e-11,1.5414818e-10\n",
      "Iteration 37530: loss = 7.843812e-11,1.5414699e-10\n",
      "Iteration 37535: loss = 7.84299e-11,1.5414604e-10\n",
      "Iteration 37540: loss = 7.842241e-11,1.5414474e-10\n",
      "Iteration 37545: loss = 7.8414386e-11,1.5414361e-10\n",
      "Iteration 37550: loss = 7.840703e-11,1.5414228e-10\n",
      "Iteration 37555: loss = 7.839734e-11,1.5414114e-10\n",
      "Iteration 37560: loss = 7.838984e-11,1.541399e-10\n",
      "Iteration 37565: loss = 7.8382e-11,1.5413881e-10\n",
      "Iteration 37570: loss = 7.837506e-11,1.5413756e-10\n",
      "Iteration 37575: loss = 7.836751e-11,1.5413615e-10\n",
      "Iteration 37580: loss = 7.835913e-11,1.5413451e-10\n",
      "Iteration 37585: loss = 7.8352325e-11,1.5413316e-10\n",
      "Iteration 37590: loss = 7.834519e-11,1.5413162e-10\n",
      "Iteration 37595: loss = 7.833869e-11,1.5413022e-10\n",
      "Iteration 37600: loss = 7.833134e-11,1.5412871e-10\n",
      "Iteration 37605: loss = 7.832283e-11,1.5412716e-10\n",
      "Iteration 37610: loss = 7.8315444e-11,1.5412563e-10\n",
      "Iteration 37615: loss = 7.830888e-11,1.5412419e-10\n",
      "Iteration 37620: loss = 7.830175e-11,1.5412258e-10\n",
      "Iteration 37625: loss = 7.829512e-11,1.5412108e-10\n",
      "Iteration 37630: loss = 7.8286135e-11,1.5411955e-10\n",
      "Iteration 37635: loss = 7.827947e-11,1.5411797e-10\n",
      "Iteration 37640: loss = 7.827217e-11,1.5411644e-10\n",
      "Iteration 37645: loss = 7.826579e-11,1.5411483e-10\n",
      "Iteration 37650: loss = 7.825923e-11,1.5411313e-10\n",
      "Iteration 37655: loss = 7.8251065e-11,1.5411136e-10\n",
      "Iteration 37660: loss = 7.824419e-11,1.541096e-10\n",
      "Iteration 37665: loss = 7.8237965e-11,1.5410796e-10\n",
      "Iteration 37670: loss = 7.724097e-11,1.5410624e-10\n",
      "Iteration 37675: loss = 7.7230077e-11,1.5410632e-10\n",
      "Iteration 37680: loss = 7.7218676e-11,1.5410617e-10\n",
      "Iteration 37685: loss = 7.720707e-11,1.5410721e-10\n",
      "Iteration 37690: loss = 7.719811e-11,1.5410685e-10\n",
      "Iteration 37695: loss = 7.7191496e-11,1.5410481e-10\n",
      "Iteration 37700: loss = 7.7185036e-11,1.541031e-10\n",
      "Iteration 37705: loss = 7.7177785e-11,1.5410138e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37710: loss = 7.717153e-11,1.5409961e-10\n",
      "Iteration 37715: loss = 7.716507e-11,1.5409757e-10\n",
      "Iteration 37720: loss = 7.7160216e-11,1.540951e-10\n",
      "Iteration 37725: loss = 7.7156566e-11,1.5409193e-10\n",
      "Iteration 37730: loss = 7.714968e-11,1.5409043e-10\n",
      "Iteration 37735: loss = 7.713988e-11,1.5409048e-10\n",
      "Iteration 37740: loss = 7.7129976e-11,1.5409032e-10\n",
      "Iteration 37745: loss = 7.711861e-11,1.5409005e-10\n",
      "Iteration 37750: loss = 7.7108674e-11,1.5408994e-10\n",
      "Iteration 37755: loss = 7.710054e-11,1.540892e-10\n",
      "Iteration 37760: loss = 7.709644e-11,1.5408606e-10\n",
      "Iteration 37765: loss = 7.709215e-11,1.540832e-10\n",
      "Iteration 37770: loss = 7.708755e-11,1.5408058e-10\n",
      "Iteration 37775: loss = 7.708312e-11,1.5407797e-10\n",
      "Iteration 37780: loss = 7.70784e-11,1.5407521e-10\n",
      "Iteration 37785: loss = 7.707584e-11,1.5407267e-10\n",
      "Iteration 37790: loss = 7.707134e-11,1.5407009e-10\n",
      "Iteration 37795: loss = 7.70663e-11,1.5406743e-10\n",
      "Iteration 37800: loss = 7.7061975e-11,1.5406441e-10\n",
      "Iteration 37805: loss = 7.705777e-11,1.5406178e-10\n",
      "Iteration 37810: loss = 7.705399e-11,1.5405889e-10\n",
      "Iteration 37815: loss = 7.705004e-11,1.5405578e-10\n",
      "Iteration 37820: loss = 7.704631e-11,1.5405287e-10\n",
      "Iteration 37825: loss = 7.704384e-11,1.5404981e-10\n",
      "Iteration 37830: loss = 7.704062e-11,1.5404669e-10\n",
      "Iteration 37835: loss = 7.703744e-11,1.5404333e-10\n",
      "Iteration 37840: loss = 7.703428e-11,1.540401e-10\n",
      "Iteration 37845: loss = 7.702581e-11,1.540392e-10\n",
      "Iteration 37850: loss = 7.701042e-11,1.5404171e-10\n",
      "Iteration 37855: loss = 7.698992e-11,1.5404582e-10\n",
      "Iteration 37860: loss = 7.6970944e-11,1.5404933e-10\n",
      "Iteration 37865: loss = 7.6952986e-11,1.5405233e-10\n",
      "Iteration 37870: loss = 7.693511e-11,1.540562e-10\n",
      "Iteration 37875: loss = 7.691688e-11,1.5405983e-10\n",
      "Iteration 37880: loss = 7.6897294e-11,1.5406333e-10\n",
      "Iteration 37885: loss = 7.688003e-11,1.5406657e-10\n",
      "Iteration 37890: loss = 7.6862676e-11,1.5406997e-10\n",
      "Iteration 37895: loss = 7.6845114e-11,1.5407298e-10\n",
      "Iteration 37900: loss = 7.682619e-11,1.5407611e-10\n",
      "Iteration 37905: loss = 7.6809656e-11,1.5407904e-10\n",
      "Iteration 37910: loss = 7.679333e-11,1.54082e-10\n",
      "Iteration 37915: loss = 7.677682e-11,1.5408448e-10\n",
      "Iteration 37920: loss = 7.6760515e-11,1.5408767e-10\n",
      "Iteration 37925: loss = 7.674275e-11,1.5409063e-10\n",
      "Iteration 37930: loss = 7.672656e-11,1.5409324e-10\n",
      "Iteration 37935: loss = 7.671106e-11,1.5409563e-10\n",
      "Iteration 37940: loss = 7.669563e-11,1.5409805e-10\n",
      "Iteration 37945: loss = 7.668031e-11,1.5410044e-10\n",
      "Iteration 37950: loss = 7.6663155e-11,1.5410276e-10\n",
      "Iteration 37955: loss = 7.6647994e-11,1.5410505e-10\n",
      "Iteration 37960: loss = 7.5652255e-11,1.5410767e-10\n",
      "Iteration 37965: loss = 7.563939e-11,1.5410835e-10\n",
      "Iteration 37970: loss = 7.5629634e-11,1.5410784e-10\n",
      "Iteration 37975: loss = 7.5618865e-11,1.5410895e-10\n",
      "Iteration 37980: loss = 7.560637e-11,1.5410993e-10\n",
      "Iteration 37985: loss = 7.5593705e-11,1.541111e-10\n",
      "Iteration 37990: loss = 7.558136e-11,1.5411211e-10\n",
      "Iteration 37995: loss = 7.556913e-11,1.5411275e-10\n",
      "Iteration 38000: loss = 7.555621e-11,1.5411378e-10\n",
      "Iteration 38005: loss = 7.554423e-11,1.5411479e-10\n",
      "Iteration 38010: loss = 7.5532275e-11,1.5411539e-10\n",
      "Iteration 38015: loss = 7.552294e-11,1.5411586e-10\n",
      "Iteration 38020: loss = 7.551365e-11,1.5411525e-10\n",
      "Iteration 38025: loss = 7.550485e-11,1.541146e-10\n",
      "Iteration 38030: loss = 7.549573e-11,1.5411397e-10\n",
      "Iteration 38035: loss = 7.548731e-11,1.5411342e-10\n",
      "Iteration 38040: loss = 7.547862e-11,1.5411278e-10\n",
      "Iteration 38045: loss = 7.546972e-11,1.54112e-10\n",
      "Iteration 38050: loss = 7.546084e-11,1.5411133e-10\n",
      "Iteration 38055: loss = 7.545387e-11,1.541107e-10\n",
      "Iteration 38060: loss = 7.544484e-11,1.5411003e-10\n",
      "Iteration 38065: loss = 7.5436664e-11,1.5410934e-10\n",
      "Iteration 38070: loss = 7.542791e-11,1.5410866e-10\n",
      "Iteration 38075: loss = 7.541923e-11,1.54108e-10\n",
      "Iteration 38080: loss = 7.541051e-11,1.5410728e-10\n",
      "Iteration 38085: loss = 7.540171e-11,1.5410642e-10\n",
      "Iteration 38090: loss = 7.539375e-11,1.5410566e-10\n",
      "Iteration 38095: loss = 7.5387176e-11,1.5410466e-10\n",
      "Iteration 38100: loss = 7.537866e-11,1.541039e-10\n",
      "Iteration 38105: loss = 7.5370495e-11,1.5410284e-10\n",
      "Iteration 38110: loss = 7.5362376e-11,1.541019e-10\n",
      "Iteration 38115: loss = 7.535431e-11,1.5410104e-10\n",
      "Iteration 38120: loss = 7.5346916e-11,1.5410014e-10\n",
      "Iteration 38125: loss = 7.533868e-11,1.5409923e-10\n",
      "Iteration 38130: loss = 7.533072e-11,1.5409796e-10\n",
      "Iteration 38135: loss = 7.532474e-11,1.540969e-10\n",
      "Iteration 38140: loss = 7.5317065e-11,1.5409568e-10\n",
      "Iteration 38145: loss = 7.530992e-11,1.5409474e-10\n",
      "Iteration 38150: loss = 7.5301994e-11,1.5409354e-10\n",
      "Iteration 38155: loss = 7.529427e-11,1.5409239e-10\n",
      "Iteration 38160: loss = 7.528601e-11,1.5409163e-10\n",
      "Iteration 38165: loss = 7.5275994e-11,1.540915e-10\n",
      "Iteration 38170: loss = 7.526293e-11,1.5409271e-10\n",
      "Iteration 38175: loss = 7.525252e-11,1.5409343e-10\n",
      "Iteration 38180: loss = 7.5242444e-11,1.5409352e-10\n",
      "Iteration 38185: loss = 7.5232535e-11,1.5409353e-10\n",
      "Iteration 38190: loss = 7.425066e-11,1.5409383e-10\n",
      "Iteration 38195: loss = 7.424127e-11,1.5409339e-10\n",
      "Iteration 38200: loss = 7.423206e-11,1.5409342e-10\n",
      "Iteration 38205: loss = 7.421768e-11,1.5409536e-10\n",
      "Iteration 38210: loss = 7.420597e-11,1.540966e-10\n",
      "Iteration 38215: loss = 7.4193256e-11,1.5409728e-10\n",
      "Iteration 38220: loss = 7.418078e-11,1.5409807e-10\n",
      "Iteration 38225: loss = 7.4170635e-11,1.5409886e-10\n",
      "Iteration 38230: loss = 7.416066e-11,1.5409893e-10\n",
      "Iteration 38235: loss = 7.415377e-11,1.5409787e-10\n",
      "Iteration 38240: loss = 7.414573e-11,1.5409682e-10\n",
      "Iteration 38245: loss = 7.4139375e-11,1.540955e-10\n",
      "Iteration 38250: loss = 7.413179e-11,1.5409432e-10\n",
      "Iteration 38255: loss = 7.412599e-11,1.5409266e-10\n",
      "Iteration 38260: loss = 7.4119384e-11,1.5409117e-10\n",
      "Iteration 38265: loss = 7.411411e-11,1.5408931e-10\n",
      "Iteration 38270: loss = 7.410746e-11,1.5408747e-10\n",
      "Iteration 38275: loss = 7.410114e-11,1.5408556e-10\n",
      "Iteration 38280: loss = 7.409825e-11,1.5408305e-10\n",
      "Iteration 38285: loss = 7.409497e-11,1.5407986e-10\n",
      "Iteration 38290: loss = 7.409307e-11,1.5407679e-10\n",
      "Iteration 38295: loss = 7.409013e-11,1.5407339e-10\n",
      "Iteration 38300: loss = 7.408863e-11,1.540701e-10\n",
      "Iteration 38305: loss = 7.4085786e-11,1.5406663e-10\n",
      "Iteration 38310: loss = 7.408414e-11,1.5406336e-10\n",
      "Iteration 38315: loss = 7.4081304e-11,1.5406008e-10\n",
      "Iteration 38320: loss = 7.4077834e-11,1.5405666e-10\n",
      "Iteration 38325: loss = 7.4076835e-11,1.5405342e-10\n",
      "Iteration 38330: loss = 7.4073365e-11,1.540499e-10\n",
      "Iteration 38335: loss = 7.407253e-11,1.540465e-10\n",
      "Iteration 38340: loss = 7.4069716e-11,1.5404317e-10\n",
      "Iteration 38345: loss = 7.406919e-11,1.5403936e-10\n",
      "Iteration 38350: loss = 7.406615e-11,1.5403581e-10\n",
      "Iteration 38355: loss = 7.406572e-11,1.5403218e-10\n",
      "Iteration 38360: loss = 7.406294e-11,1.5402835e-10\n",
      "Iteration 38365: loss = 7.406217e-11,1.5402478e-10\n",
      "Iteration 38370: loss = 7.405947e-11,1.5402131e-10\n",
      "Iteration 38375: loss = 7.4057406e-11,1.5401765e-10\n",
      "Iteration 38380: loss = 7.405642e-11,1.5401398e-10\n",
      "Iteration 38385: loss = 7.405439e-11,1.5401022e-10\n",
      "Iteration 38390: loss = 7.405343e-11,1.5400652e-10\n",
      "Iteration 38395: loss = 7.405135e-11,1.540029e-10\n",
      "Iteration 38400: loss = 7.4050314e-11,1.5399919e-10\n",
      "Iteration 38405: loss = 7.40484e-11,1.5399546e-10\n",
      "Iteration 38410: loss = 7.404744e-11,1.5399176e-10\n",
      "Iteration 38415: loss = 7.4045416e-11,1.5398804e-10\n",
      "Iteration 38420: loss = 7.404507e-11,1.5398438e-10\n",
      "Iteration 38425: loss = 7.404313e-11,1.5398043e-10\n",
      "Iteration 38430: loss = 7.404076e-11,1.539766e-10\n",
      "Iteration 38435: loss = 7.404085e-11,1.5397256e-10\n",
      "Iteration 38440: loss = 7.4038546e-11,1.5396878e-10\n",
      "Iteration 38445: loss = 7.403847e-11,1.5396495e-10\n",
      "Iteration 38450: loss = 7.403637e-11,1.5396116e-10\n",
      "Iteration 38455: loss = 7.403622e-11,1.5395718e-10\n",
      "Iteration 38460: loss = 7.403394e-11,1.5395357e-10\n",
      "Iteration 38465: loss = 7.4033966e-11,1.5394981e-10\n",
      "Iteration 38470: loss = 7.403143e-11,1.5394573e-10\n",
      "Iteration 38475: loss = 7.402994e-11,1.5394198e-10\n",
      "Iteration 38480: loss = 7.402967e-11,1.5393818e-10\n",
      "Iteration 38485: loss = 7.4027486e-11,1.5393439e-10\n",
      "Iteration 38490: loss = 7.4027305e-11,1.5393065e-10\n",
      "Iteration 38495: loss = 7.402486e-11,1.5392658e-10\n",
      "Iteration 38500: loss = 7.4024835e-11,1.5392294e-10\n",
      "Iteration 38505: loss = 7.402256e-11,1.5391889e-10\n",
      "Iteration 38510: loss = 7.40227e-11,1.5391523e-10\n",
      "Iteration 38515: loss = 7.4020144e-11,1.5391136e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38520: loss = 7.4020075e-11,1.5390757e-10\n",
      "Iteration 38525: loss = 7.401782e-11,1.539038e-10\n",
      "Iteration 38530: loss = 7.401612e-11,1.5389998e-10\n",
      "Iteration 38535: loss = 7.401548e-11,1.5389592e-10\n",
      "Iteration 38540: loss = 7.401365e-11,1.5389226e-10\n",
      "Iteration 38545: loss = 7.4013344e-11,1.5388833e-10\n",
      "Iteration 38550: loss = 7.401161e-11,1.5388456e-10\n",
      "Iteration 38555: loss = 7.4011276e-11,1.5388076e-10\n",
      "Iteration 38560: loss = 7.400939e-11,1.5387663e-10\n",
      "Iteration 38565: loss = 7.400896e-11,1.5387289e-10\n",
      "Iteration 38570: loss = 7.400739e-11,1.5386904e-10\n",
      "Iteration 38575: loss = 7.4005434e-11,1.5386514e-10\n",
      "Iteration 38580: loss = 7.400546e-11,1.5386101e-10\n",
      "Iteration 38585: loss = 7.400344e-11,1.538574e-10\n",
      "Iteration 38590: loss = 7.4003276e-11,1.5385347e-10\n",
      "Iteration 38595: loss = 7.400122e-11,1.5384974e-10\n",
      "Iteration 38600: loss = 7.4001284e-11,1.5384566e-10\n",
      "Iteration 38605: loss = 7.3999175e-11,1.538417e-10\n",
      "Iteration 38610: loss = 7.399925e-11,1.5383808e-10\n",
      "Iteration 38615: loss = 7.399719e-11,1.5383399e-10\n",
      "Iteration 38620: loss = 7.399715e-11,1.5382998e-10\n",
      "Iteration 38625: loss = 7.399523e-11,1.5382609e-10\n",
      "Iteration 38630: loss = 7.399332e-11,1.5382232e-10\n",
      "Iteration 38635: loss = 7.399292e-11,1.5381833e-10\n",
      "Iteration 38640: loss = 7.39914e-11,1.5381454e-10\n",
      "Iteration 38645: loss = 7.3990945e-11,1.5381071e-10\n",
      "Iteration 38650: loss = 7.3989405e-11,1.5380662e-10\n",
      "Iteration 38655: loss = 7.398896e-11,1.5380283e-10\n",
      "Iteration 38660: loss = 7.398738e-11,1.537989e-10\n",
      "Iteration 38665: loss = 7.3987066e-11,1.5379499e-10\n",
      "Iteration 38670: loss = 7.3985325e-11,1.5379098e-10\n",
      "Iteration 38675: loss = 7.39849e-11,1.5378707e-10\n",
      "Iteration 38680: loss = 7.398324e-11,1.5378324e-10\n",
      "Iteration 38685: loss = 7.398173e-11,1.5377935e-10\n",
      "Iteration 38690: loss = 7.3981134e-11,1.5377535e-10\n",
      "Iteration 38695: loss = 7.397972e-11,1.5377145e-10\n",
      "Iteration 38700: loss = 7.3979274e-11,1.5376794e-10\n",
      "Iteration 38705: loss = 7.39778e-11,1.537638e-10\n",
      "Iteration 38710: loss = 7.39774e-11,1.5375991e-10\n",
      "Iteration 38715: loss = 7.3975874e-11,1.5375594e-10\n",
      "Iteration 38720: loss = 7.397546e-11,1.5375219e-10\n",
      "Iteration 38725: loss = 7.3973876e-11,1.5374818e-10\n",
      "Iteration 38730: loss = 7.397173e-11,1.5374424e-10\n",
      "Iteration 38735: loss = 7.397189e-11,1.5374055e-10\n",
      "Iteration 38740: loss = 7.39697e-11,1.5373668e-10\n",
      "Iteration 38745: loss = 7.396976e-11,1.5373278e-10\n",
      "Iteration 38750: loss = 7.396759e-11,1.5372878e-10\n",
      "Iteration 38755: loss = 7.396769e-11,1.5372491e-10\n",
      "Iteration 38760: loss = 7.396562e-11,1.5372095e-10\n",
      "Iteration 38765: loss = 7.396573e-11,1.5371701e-10\n",
      "Iteration 38770: loss = 7.396384e-11,1.5371315e-10\n",
      "Iteration 38775: loss = 7.396379e-11,1.5370928e-10\n",
      "Iteration 38780: loss = 7.396181e-11,1.5370538e-10\n",
      "Iteration 38785: loss = 7.396014e-11,1.5370155e-10\n",
      "Iteration 38790: loss = 7.3959804e-11,1.5369776e-10\n",
      "Iteration 38795: loss = 7.395831e-11,1.5369377e-10\n",
      "Iteration 38800: loss = 7.3957986e-11,1.5368984e-10\n",
      "Iteration 38805: loss = 7.395621e-11,1.5368588e-10\n",
      "Iteration 38810: loss = 7.3955904e-11,1.536819e-10\n",
      "Iteration 38815: loss = 7.395435e-11,1.5367813e-10\n",
      "Iteration 38820: loss = 7.395381e-11,1.5367418e-10\n",
      "Iteration 38825: loss = 7.395227e-11,1.5367024e-10\n",
      "Iteration 38830: loss = 7.395182e-11,1.5366622e-10\n",
      "Iteration 38835: loss = 7.3950214e-11,1.5366249e-10\n",
      "Iteration 38840: loss = 7.394818e-11,1.5365842e-10\n",
      "Iteration 38845: loss = 7.394828e-11,1.5365498e-10\n",
      "Iteration 38850: loss = 7.394604e-11,1.536508e-10\n",
      "Iteration 38855: loss = 7.3946155e-11,1.5364679e-10\n",
      "Iteration 38860: loss = 7.3943636e-11,1.5364322e-10\n",
      "Iteration 38865: loss = 7.394186e-11,1.536401e-10\n",
      "Iteration 38870: loss = 7.393979e-11,1.5363608e-10\n",
      "Iteration 38875: loss = 7.393788e-11,1.536333e-10\n",
      "Iteration 38880: loss = 7.39366e-11,1.5362932e-10\n",
      "Iteration 38885: loss = 7.393455e-11,1.5362538e-10\n",
      "Iteration 38890: loss = 7.393495e-11,1.5362144e-10\n",
      "Iteration 38895: loss = 7.3929814e-11,1.5361883e-10\n",
      "Iteration 38900: loss = 7.3929605e-11,1.5361522e-10\n",
      "Iteration 38905: loss = 7.392744e-11,1.5361118e-10\n",
      "Iteration 38910: loss = 7.3927836e-11,1.5360713e-10\n",
      "Iteration 38915: loss = 7.3923576e-11,1.5360413e-10\n",
      "Iteration 38920: loss = 7.392096e-11,1.5360174e-10\n",
      "Iteration 38925: loss = 7.391834e-11,1.5359782e-10\n",
      "Iteration 38930: loss = 7.39186e-11,1.5359392e-10\n",
      "Iteration 38935: loss = 7.391431e-11,1.5359103e-10\n",
      "Iteration 38940: loss = 7.391046e-11,1.5358824e-10\n",
      "Iteration 38945: loss = 7.391051e-11,1.5358424e-10\n",
      "Iteration 38950: loss = 7.3906464e-11,1.5358112e-10\n",
      "Iteration 38955: loss = 7.390525e-11,1.5357794e-10\n",
      "Iteration 38960: loss = 7.390382e-11,1.535738e-10\n",
      "Iteration 38965: loss = 7.3901225e-11,1.5357093e-10\n",
      "Iteration 38970: loss = 7.3898755e-11,1.5356735e-10\n",
      "Iteration 38975: loss = 7.3898714e-11,1.5356338e-10\n",
      "Iteration 38980: loss = 7.389542e-11,1.5356029e-10\n",
      "Iteration 38985: loss = 7.389347e-11,1.5355713e-10\n",
      "Iteration 38990: loss = 7.388996e-11,1.5355428e-10\n",
      "Iteration 38995: loss = 7.388778e-11,1.5355031e-10\n",
      "Iteration 39000: loss = 7.3881526e-11,1.5354935e-10\n",
      "Iteration 39005: loss = 7.386863e-11,1.535504e-10\n",
      "Iteration 39010: loss = 7.3858114e-11,1.5355157e-10\n",
      "Iteration 39015: loss = 7.3845464e-11,1.5355264e-10\n",
      "Iteration 39020: loss = 7.383486e-11,1.5355359e-10\n",
      "Iteration 39025: loss = 7.382217e-11,1.535546e-10\n",
      "Iteration 39030: loss = 7.38119e-11,1.5355554e-10\n",
      "Iteration 39035: loss = 7.37975e-11,1.5355728e-10\n",
      "Iteration 39040: loss = 7.377887e-11,1.535613e-10\n",
      "Iteration 39045: loss = 7.37597e-11,1.5356544e-10\n",
      "Iteration 39050: loss = 7.3743144e-11,1.5356962e-10\n",
      "Iteration 39055: loss = 7.276593e-11,1.5357153e-10\n",
      "Iteration 39060: loss = 7.2754906e-11,1.5357286e-10\n",
      "Iteration 39065: loss = 7.2741584e-11,1.5357418e-10\n",
      "Iteration 39070: loss = 7.273061e-11,1.535756e-10\n",
      "Iteration 39075: loss = 7.271726e-11,1.5357689e-10\n",
      "Iteration 39080: loss = 7.270621e-11,1.5357823e-10\n",
      "Iteration 39085: loss = 7.2693296e-11,1.535797e-10\n",
      "Iteration 39090: loss = 7.2680445e-11,1.5358084e-10\n",
      "Iteration 39095: loss = 7.2669856e-11,1.5358208e-10\n",
      "Iteration 39100: loss = 7.26568e-11,1.5358295e-10\n",
      "Iteration 39105: loss = 7.264634e-11,1.5358417e-10\n",
      "Iteration 39110: loss = 7.263349e-11,1.5358519e-10\n",
      "Iteration 39115: loss = 7.2623206e-11,1.5358609e-10\n",
      "Iteration 39120: loss = 7.261073e-11,1.5358703e-10\n",
      "Iteration 39125: loss = 7.260045e-11,1.5358786e-10\n",
      "Iteration 39130: loss = 7.258845e-11,1.5358889e-10\n",
      "Iteration 39135: loss = 7.257677e-11,1.5358978e-10\n",
      "Iteration 39140: loss = 7.2566585e-11,1.5359043e-10\n",
      "Iteration 39145: loss = 7.255518e-11,1.5359117e-10\n",
      "Iteration 39150: loss = 7.254514e-11,1.5359186e-10\n",
      "Iteration 39155: loss = 7.2533736e-11,1.5359262e-10\n",
      "Iteration 39160: loss = 7.252373e-11,1.5359322e-10\n",
      "Iteration 39165: loss = 7.251274e-11,1.5359371e-10\n",
      "Iteration 39170: loss = 7.250197e-11,1.5359443e-10\n",
      "Iteration 39175: loss = 7.248951e-11,1.5359564e-10\n",
      "Iteration 39180: loss = 7.2478266e-11,1.5359727e-10\n",
      "Iteration 39185: loss = 7.2465624e-11,1.535986e-10\n",
      "Iteration 39190: loss = 7.2452246e-11,1.5359977e-10\n",
      "Iteration 39195: loss = 7.244109e-11,1.5360144e-10\n",
      "Iteration 39200: loss = 7.242797e-11,1.5360255e-10\n",
      "Iteration 39205: loss = 7.241736e-11,1.5360387e-10\n",
      "Iteration 39210: loss = 7.240487e-11,1.5360485e-10\n",
      "Iteration 39215: loss = 7.2394826e-11,1.5360573e-10\n",
      "Iteration 39220: loss = 7.23828e-11,1.5360664e-10\n",
      "Iteration 39225: loss = 7.237295e-11,1.536075e-10\n",
      "Iteration 39230: loss = 7.2360874e-11,1.5360835e-10\n",
      "Iteration 39235: loss = 7.23494e-11,1.5360893e-10\n",
      "Iteration 39240: loss = 7.233914e-11,1.536098e-10\n",
      "Iteration 39245: loss = 7.232797e-11,1.5361032e-10\n",
      "Iteration 39250: loss = 7.231803e-11,1.5361118e-10\n",
      "Iteration 39255: loss = 7.135615e-11,1.5361115e-10\n",
      "Iteration 39260: loss = 7.134656e-11,1.5361169e-10\n",
      "Iteration 39265: loss = 7.133572e-11,1.5361264e-10\n",
      "Iteration 39270: loss = 7.132642e-11,1.5361276e-10\n",
      "Iteration 39275: loss = 7.131492e-11,1.5361307e-10\n",
      "Iteration 39280: loss = 7.1306995e-11,1.5361298e-10\n",
      "Iteration 39285: loss = 7.1299244e-11,1.5361247e-10\n",
      "Iteration 39290: loss = 7.1292215e-11,1.5361167e-10\n",
      "Iteration 39295: loss = 7.128348e-11,1.536111e-10\n",
      "Iteration 39300: loss = 7.127667e-11,1.536104e-10\n",
      "Iteration 39305: loss = 7.1269685e-11,1.5360954e-10\n",
      "Iteration 39310: loss = 7.126328e-11,1.5360849e-10\n",
      "Iteration 39315: loss = 7.125644e-11,1.5360746e-10\n",
      "Iteration 39320: loss = 7.124866e-11,1.5360638e-10\n",
      "Iteration 39325: loss = 7.124258e-11,1.5360538e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39330: loss = 7.12369e-11,1.53604e-10\n",
      "Iteration 39335: loss = 7.12313e-11,1.5360245e-10\n",
      "Iteration 39340: loss = 7.122621e-11,1.5360091e-10\n",
      "Iteration 39345: loss = 7.121944e-11,1.5359919e-10\n",
      "Iteration 39350: loss = 7.121367e-11,1.5359786e-10\n",
      "Iteration 39355: loss = 7.1208615e-11,1.5359616e-10\n",
      "Iteration 39360: loss = 7.119872e-11,1.5359654e-10\n",
      "Iteration 39365: loss = 7.118537e-11,1.5359819e-10\n",
      "Iteration 39370: loss = 7.117358e-11,1.5360013e-10\n",
      "Iteration 39375: loss = 7.1160786e-11,1.5360158e-10\n",
      "Iteration 39380: loss = 7.1149246e-11,1.5360288e-10\n",
      "Iteration 39385: loss = 7.113709e-11,1.5360418e-10\n",
      "Iteration 39390: loss = 7.1126195e-11,1.5360532e-10\n",
      "Iteration 39395: loss = 7.111436e-11,1.5360638e-10\n",
      "Iteration 39400: loss = 7.110399e-11,1.5360756e-10\n",
      "Iteration 39405: loss = 7.109208e-11,1.5360854e-10\n",
      "Iteration 39410: loss = 7.108126e-11,1.536097e-10\n",
      "Iteration 39415: loss = 7.106973e-11,1.5361068e-10\n",
      "Iteration 39420: loss = 7.1058354e-11,1.5361133e-10\n",
      "Iteration 39425: loss = 7.1048854e-11,1.5361214e-10\n",
      "Iteration 39430: loss = 7.103774e-11,1.5361289e-10\n",
      "Iteration 39435: loss = 7.1028315e-11,1.5361368e-10\n",
      "Iteration 39440: loss = 7.10172e-11,1.5361463e-10\n",
      "Iteration 39445: loss = 7.100742e-11,1.5361536e-10\n",
      "Iteration 39450: loss = 7.0996355e-11,1.536157e-10\n",
      "Iteration 39455: loss = 7.098698e-11,1.5361652e-10\n",
      "Iteration 39460: loss = 7.097605e-11,1.5361715e-10\n",
      "Iteration 39465: loss = 7.096721e-11,1.5361772e-10\n",
      "Iteration 39470: loss = 7.09566e-11,1.5361802e-10\n",
      "Iteration 39475: loss = 7.094556e-11,1.5361844e-10\n",
      "Iteration 39480: loss = 7.093717e-11,1.5361885e-10\n",
      "Iteration 39485: loss = 7.0926966e-11,1.5361908e-10\n",
      "Iteration 39490: loss = 7.0918486e-11,1.5361935e-10\n",
      "Iteration 39495: loss = 6.996549e-11,1.5361976e-10\n",
      "Iteration 39500: loss = 6.995895e-11,1.5361877e-10\n",
      "Iteration 39505: loss = 6.995483e-11,1.5361651e-10\n",
      "Iteration 39510: loss = 6.995048e-11,1.5361484e-10\n",
      "Iteration 39515: loss = 6.994033e-11,1.5361595e-10\n",
      "Iteration 39520: loss = 6.992825e-11,1.5361655e-10\n",
      "Iteration 39525: loss = 6.99184e-11,1.5361729e-10\n",
      "Iteration 39530: loss = 6.990837e-11,1.536182e-10\n",
      "Iteration 39535: loss = 6.989866e-11,1.5361877e-10\n",
      "Iteration 39540: loss = 6.988961e-11,1.5361926e-10\n",
      "Iteration 39545: loss = 6.987882e-11,1.5361959e-10\n",
      "Iteration 39550: loss = 6.9869534e-11,1.5362002e-10\n",
      "Iteration 39555: loss = 6.9860055e-11,1.5362027e-10\n",
      "Iteration 39560: loss = 6.9851284e-11,1.5362071e-10\n",
      "Iteration 39565: loss = 6.984024e-11,1.5362095e-10\n",
      "Iteration 39570: loss = 6.983199e-11,1.5362131e-10\n",
      "Iteration 39575: loss = 6.982289e-11,1.5362163e-10\n",
      "Iteration 39580: loss = 6.981409e-11,1.5362199e-10\n",
      "Iteration 39585: loss = 6.980503e-11,1.5362212e-10\n",
      "Iteration 39590: loss = 6.97945e-11,1.5362243e-10\n",
      "Iteration 39595: loss = 6.9782756e-11,1.5362418e-10\n",
      "Iteration 39600: loss = 6.976962e-11,1.5362656e-10\n",
      "Iteration 39605: loss = 6.975646e-11,1.5362908e-10\n",
      "Iteration 39610: loss = 6.974182e-11,1.5363122e-10\n",
      "Iteration 39615: loss = 6.9727946e-11,1.5363376e-10\n",
      "Iteration 39620: loss = 6.971648e-11,1.5363533e-10\n",
      "Iteration 39625: loss = 6.970554e-11,1.5363646e-10\n",
      "Iteration 39630: loss = 6.96948e-11,1.5363746e-10\n",
      "Iteration 39635: loss = 6.96827e-11,1.5363855e-10\n",
      "Iteration 39640: loss = 6.967276e-11,1.5363963e-10\n",
      "Iteration 39645: loss = 6.9662665e-11,1.5364067e-10\n",
      "Iteration 39650: loss = 6.9652624e-11,1.536417e-10\n",
      "Iteration 39655: loss = 6.9641376e-11,1.5364224e-10\n",
      "Iteration 39660: loss = 6.963164e-11,1.53643e-10\n",
      "Iteration 39665: loss = 6.96219e-11,1.5364365e-10\n",
      "Iteration 39670: loss = 6.9612864e-11,1.5364433e-10\n",
      "Iteration 39675: loss = 6.96035e-11,1.5364468e-10\n",
      "Iteration 39680: loss = 6.95927e-11,1.536451e-10\n",
      "Iteration 39685: loss = 6.958357e-11,1.5364549e-10\n",
      "Iteration 39690: loss = 6.9574756e-11,1.5364572e-10\n",
      "Iteration 39695: loss = 6.9565714e-11,1.536459e-10\n",
      "Iteration 39700: loss = 6.955768e-11,1.5364648e-10\n",
      "Iteration 39705: loss = 6.954724e-11,1.5364676e-10\n",
      "Iteration 39710: loss = 6.953857e-11,1.5364694e-10\n",
      "Iteration 39715: loss = 6.952964e-11,1.53647e-10\n",
      "Iteration 39720: loss = 6.952104e-11,1.5364751e-10\n",
      "Iteration 39725: loss = 6.9510835e-11,1.5364766e-10\n",
      "Iteration 39730: loss = 6.950268e-11,1.536476e-10\n",
      "Iteration 39735: loss = 6.949423e-11,1.5364773e-10\n",
      "Iteration 39740: loss = 6.9485556e-11,1.5364801e-10\n",
      "Iteration 39745: loss = 6.94769e-11,1.5364819e-10\n",
      "Iteration 39750: loss = 6.946694e-11,1.5364818e-10\n",
      "Iteration 39755: loss = 6.945824e-11,1.5364854e-10\n",
      "Iteration 39760: loss = 6.94504e-11,1.5364851e-10\n",
      "Iteration 39765: loss = 6.9442195e-11,1.5364864e-10\n",
      "Iteration 39770: loss = 6.9432494e-11,1.5364851e-10\n",
      "Iteration 39775: loss = 6.942454e-11,1.5364848e-10\n",
      "Iteration 39780: loss = 6.8481325e-11,1.5364926e-10\n",
      "Iteration 39785: loss = 6.8470014e-11,1.5365079e-10\n",
      "Iteration 39790: loss = 6.845757e-11,1.5365223e-10\n",
      "Iteration 39795: loss = 6.845153e-11,1.5365184e-10\n",
      "Iteration 39800: loss = 6.8443556e-11,1.5365155e-10\n",
      "Iteration 39805: loss = 6.843505e-11,1.5365155e-10\n",
      "Iteration 39810: loss = 6.842648e-11,1.5365148e-10\n",
      "Iteration 39815: loss = 6.8418625e-11,1.5365148e-10\n",
      "Iteration 39820: loss = 6.8410444e-11,1.5365106e-10\n",
      "Iteration 39825: loss = 6.8402374e-11,1.53651e-10\n",
      "Iteration 39830: loss = 6.83942e-11,1.5365081e-10\n",
      "Iteration 39835: loss = 6.838657e-11,1.5365027e-10\n",
      "Iteration 39840: loss = 6.8380974e-11,1.5364948e-10\n",
      "Iteration 39845: loss = 6.837419e-11,1.536488e-10\n",
      "Iteration 39850: loss = 6.83668e-11,1.5364816e-10\n",
      "Iteration 39855: loss = 6.835996e-11,1.5364728e-10\n",
      "Iteration 39860: loss = 6.8353094e-11,1.536464e-10\n",
      "Iteration 39865: loss = 6.834663e-11,1.5364526e-10\n",
      "Iteration 39870: loss = 6.834066e-11,1.536441e-10\n",
      "Iteration 39875: loss = 6.8328794e-11,1.5364648e-10\n",
      "Iteration 39880: loss = 6.831634e-11,1.5364852e-10\n",
      "Iteration 39885: loss = 6.830405e-11,1.5365054e-10\n",
      "Iteration 39890: loss = 6.829035e-11,1.5365245e-10\n",
      "Iteration 39895: loss = 6.828132e-11,1.5365315e-10\n",
      "Iteration 39900: loss = 6.8271694e-11,1.5365384e-10\n",
      "Iteration 39905: loss = 6.8262916e-11,1.5365434e-10\n",
      "Iteration 39910: loss = 6.825185e-11,1.5365482e-10\n",
      "Iteration 39915: loss = 6.8243265e-11,1.5365545e-10\n",
      "Iteration 39920: loss = 6.823371e-11,1.5365602e-10\n",
      "Iteration 39925: loss = 6.822498e-11,1.5365667e-10\n",
      "Iteration 39930: loss = 6.8215704e-11,1.5365718e-10\n",
      "Iteration 39935: loss = 6.820539e-11,1.5365778e-10\n",
      "Iteration 39940: loss = 6.819621e-11,1.5365786e-10\n",
      "Iteration 39945: loss = 6.818776e-11,1.5365817e-10\n",
      "Iteration 39950: loss = 6.817894e-11,1.5365885e-10\n",
      "Iteration 39955: loss = 6.8168964e-11,1.5365943e-10\n",
      "Iteration 39960: loss = 6.815987e-11,1.5365977e-10\n",
      "Iteration 39965: loss = 6.8151366e-11,1.5365997e-10\n",
      "Iteration 39970: loss = 6.721874e-11,1.5366046e-10\n",
      "Iteration 39975: loss = 6.7216115e-11,1.5365764e-10\n",
      "Iteration 39980: loss = 6.7212146e-11,1.5365534e-10\n",
      "Iteration 39985: loss = 6.7208294e-11,1.5365327e-10\n",
      "Iteration 39990: loss = 6.7204166e-11,1.5365106e-10\n",
      "Iteration 39995: loss = 6.720024e-11,1.5364901e-10\n",
      "Iteration 40000: loss = 6.719599e-11,1.5364693e-10\n",
      "Iteration 40005: loss = 6.719381e-11,1.5364486e-10\n",
      "Iteration 40010: loss = 6.718969e-11,1.5364254e-10\n",
      "Iteration 40015: loss = 6.7186166e-11,1.5364024e-10\n",
      "Iteration 40020: loss = 6.7182294e-11,1.5363799e-10\n",
      "Iteration 40025: loss = 6.7178714e-11,1.5363558e-10\n",
      "Iteration 40030: loss = 6.717478e-11,1.5363333e-10\n",
      "Iteration 40035: loss = 6.717149e-11,1.5363102e-10\n",
      "Iteration 40040: loss = 6.7167515e-11,1.5362862e-10\n",
      "Iteration 40045: loss = 6.716432e-11,1.5362638e-10\n",
      "Iteration 40050: loss = 6.716214e-11,1.5362395e-10\n",
      "Iteration 40055: loss = 6.715887e-11,1.5362155e-10\n",
      "Iteration 40060: loss = 6.715496e-11,1.5361916e-10\n",
      "Iteration 40065: loss = 6.715168e-11,1.5361691e-10\n",
      "Iteration 40070: loss = 6.7147864e-11,1.5361439e-10\n",
      "Iteration 40075: loss = 6.714471e-11,1.5361201e-10\n",
      "Iteration 40080: loss = 6.714076e-11,1.536096e-10\n",
      "Iteration 40085: loss = 6.713783e-11,1.5360718e-10\n",
      "Iteration 40090: loss = 6.713423e-11,1.5360489e-10\n",
      "Iteration 40095: loss = 6.7132994e-11,1.536022e-10\n",
      "Iteration 40100: loss = 6.712946e-11,1.5359969e-10\n",
      "Iteration 40105: loss = 6.712681e-11,1.5359715e-10\n",
      "Iteration 40110: loss = 6.712345e-11,1.5359432e-10\n",
      "Iteration 40115: loss = 6.71207e-11,1.5359172e-10\n",
      "Iteration 40120: loss = 6.7117534e-11,1.535892e-10\n",
      "Iteration 40125: loss = 6.711468e-11,1.5358655e-10\n",
      "Iteration 40130: loss = 6.7111365e-11,1.5358399e-10\n",
      "Iteration 40135: loss = 6.7107424e-11,1.5358192e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40140: loss = 6.710261e-11,1.5358084e-10\n",
      "Iteration 40145: loss = 6.709496e-11,1.5358086e-10\n",
      "Iteration 40150: loss = 6.708661e-11,1.5358069e-10\n",
      "Iteration 40155: loss = 6.7078974e-11,1.5358059e-10\n",
      "Iteration 40160: loss = 6.707084e-11,1.5358038e-10\n",
      "Iteration 40165: loss = 6.706323e-11,1.5358011e-10\n",
      "Iteration 40170: loss = 6.7055535e-11,1.535796e-10\n",
      "Iteration 40175: loss = 6.705093e-11,1.5357776e-10\n",
      "Iteration 40180: loss = 6.7046375e-11,1.5357593e-10\n",
      "Iteration 40185: loss = 6.704409e-11,1.5357399e-10\n",
      "Iteration 40190: loss = 6.7039756e-11,1.5357192e-10\n",
      "Iteration 40195: loss = 6.703595e-11,1.5356982e-10\n",
      "Iteration 40200: loss = 6.703161e-11,1.5356783e-10\n",
      "Iteration 40205: loss = 6.702779e-11,1.5356567e-10\n",
      "Iteration 40210: loss = 6.702335e-11,1.5356366e-10\n",
      "Iteration 40215: loss = 6.701961e-11,1.5356155e-10\n",
      "Iteration 40220: loss = 6.7015705e-11,1.5355936e-10\n",
      "Iteration 40225: loss = 6.701235e-11,1.5355686e-10\n",
      "Iteration 40230: loss = 6.7010154e-11,1.5355447e-10\n",
      "Iteration 40235: loss = 6.7007094e-11,1.5355209e-10\n",
      "Iteration 40240: loss = 6.7003576e-11,1.5354955e-10\n",
      "Iteration 40245: loss = 6.700052e-11,1.535471e-10\n",
      "Iteration 40250: loss = 6.699687e-11,1.5354468e-10\n",
      "Iteration 40255: loss = 6.69941e-11,1.5354243e-10\n",
      "Iteration 40260: loss = 6.699017e-11,1.5353978e-10\n",
      "Iteration 40265: loss = 6.698734e-11,1.5353738e-10\n",
      "Iteration 40270: loss = 6.6983884e-11,1.5353482e-10\n",
      "Iteration 40275: loss = 6.698236e-11,1.5353252e-10\n",
      "Iteration 40280: loss = 6.6978804e-11,1.5353002e-10\n",
      "Iteration 40285: loss = 6.697578e-11,1.5352755e-10\n",
      "Iteration 40290: loss = 6.697218e-11,1.5352498e-10\n",
      "Iteration 40295: loss = 6.696927e-11,1.5352253e-10\n",
      "Iteration 40300: loss = 6.696588e-11,1.5352e-10\n",
      "Iteration 40305: loss = 6.6962845e-11,1.5351778e-10\n",
      "Iteration 40310: loss = 6.695935e-11,1.5351526e-10\n",
      "Iteration 40315: loss = 6.695652e-11,1.535127e-10\n",
      "Iteration 40320: loss = 6.6953214e-11,1.5351012e-10\n",
      "Iteration 40325: loss = 6.695195e-11,1.535075e-10\n",
      "Iteration 40330: loss = 6.694844e-11,1.5350497e-10\n",
      "Iteration 40335: loss = 6.694562e-11,1.535024e-10\n",
      "Iteration 40340: loss = 6.6942174e-11,1.5349996e-10\n",
      "Iteration 40345: loss = 6.6939454e-11,1.5349731e-10\n",
      "Iteration 40350: loss = 6.6935756e-11,1.5349491e-10\n",
      "Iteration 40355: loss = 6.693316e-11,1.5349219e-10\n",
      "Iteration 40360: loss = 6.6928525e-11,1.534903e-10\n",
      "Iteration 40365: loss = 6.692418e-11,1.5348868e-10\n",
      "Iteration 40370: loss = 6.692245e-11,1.5348602e-10\n",
      "Iteration 40375: loss = 6.691274e-11,1.53487e-10\n",
      "Iteration 40380: loss = 6.689938e-11,1.5348944e-10\n",
      "Iteration 40385: loss = 6.688639e-11,1.5349191e-10\n",
      "Iteration 40390: loss = 6.6873077e-11,1.5349418e-10\n",
      "Iteration 40395: loss = 6.68603e-11,1.5349685e-10\n",
      "Iteration 40400: loss = 6.684717e-11,1.5349921e-10\n",
      "Iteration 40405: loss = 6.683449e-11,1.5350154e-10\n",
      "Iteration 40410: loss = 6.682182e-11,1.5350367e-10\n",
      "Iteration 40415: loss = 6.681135e-11,1.5350597e-10\n",
      "Iteration 40420: loss = 6.6798865e-11,1.5350796e-10\n",
      "Iteration 40425: loss = 6.678714e-11,1.5350991e-10\n",
      "Iteration 40430: loss = 6.6774926e-11,1.5351177e-10\n",
      "Iteration 40435: loss = 6.6763206e-11,1.5351366e-10\n",
      "Iteration 40440: loss = 6.675105e-11,1.5351553e-10\n",
      "Iteration 40445: loss = 6.6739676e-11,1.535176e-10\n",
      "Iteration 40450: loss = 6.67292e-11,1.5351842e-10\n",
      "Iteration 40455: loss = 6.6720136e-11,1.5351913e-10\n",
      "Iteration 40460: loss = 6.671204e-11,1.5351961e-10\n",
      "Iteration 40465: loss = 6.670113e-11,1.5352115e-10\n",
      "Iteration 40470: loss = 6.66896e-11,1.5352301e-10\n",
      "Iteration 40475: loss = 6.667881e-11,1.5352444e-10\n",
      "Iteration 40480: loss = 6.6667394e-11,1.53526e-10\n",
      "Iteration 40485: loss = 6.665665e-11,1.5352736e-10\n",
      "Iteration 40490: loss = 6.664605e-11,1.535285e-10\n",
      "Iteration 40495: loss = 6.6636016e-11,1.5352952e-10\n",
      "Iteration 40500: loss = 6.662557e-11,1.5353116e-10\n",
      "Iteration 40505: loss = 6.6618024e-11,1.535315e-10\n",
      "Iteration 40510: loss = 6.660722e-11,1.5353269e-10\n",
      "Iteration 40515: loss = 6.568267e-11,1.5353407e-10\n",
      "Iteration 40520: loss = 6.567036e-11,1.5353617e-10\n",
      "Iteration 40525: loss = 6.5657854e-11,1.5353885e-10\n",
      "Iteration 40530: loss = 6.5644046e-11,1.5354135e-10\n",
      "Iteration 40535: loss = 6.563106e-11,1.5354389e-10\n",
      "Iteration 40540: loss = 6.5619433e-11,1.5354595e-10\n",
      "Iteration 40545: loss = 6.560809e-11,1.5354779e-10\n",
      "Iteration 40550: loss = 6.559863e-11,1.5354931e-10\n",
      "Iteration 40555: loss = 6.558701e-11,1.535509e-10\n",
      "Iteration 40560: loss = 6.557631e-11,1.5355217e-10\n",
      "Iteration 40565: loss = 6.5564866e-11,1.5355414e-10\n",
      "Iteration 40570: loss = 6.555623e-11,1.5355481e-10\n",
      "Iteration 40575: loss = 6.5550704e-11,1.5355342e-10\n",
      "Iteration 40580: loss = 6.554668e-11,1.5355191e-10\n",
      "Iteration 40585: loss = 6.5540635e-11,1.5355066e-10\n",
      "Iteration 40590: loss = 6.553494e-11,1.5354916e-10\n",
      "Iteration 40595: loss = 6.552797e-11,1.5354926e-10\n",
      "Iteration 40600: loss = 6.551666e-11,1.5355116e-10\n",
      "Iteration 40605: loss = 6.5505455e-11,1.5355307e-10\n",
      "Iteration 40610: loss = 6.5495546e-11,1.5355464e-10\n",
      "Iteration 40615: loss = 6.5483806e-11,1.5355618e-10\n",
      "Iteration 40620: loss = 6.547307e-11,1.5355778e-10\n",
      "Iteration 40625: loss = 6.5462545e-11,1.5355936e-10\n",
      "Iteration 40630: loss = 6.545212e-11,1.5356051e-10\n",
      "Iteration 40635: loss = 6.544207e-11,1.5356179e-10\n",
      "Iteration 40640: loss = 6.5431695e-11,1.535628e-10\n",
      "Iteration 40645: loss = 6.45163e-11,1.5356433e-10\n",
      "Iteration 40650: loss = 6.451264e-11,1.5356284e-10\n",
      "Iteration 40655: loss = 6.4506026e-11,1.5356204e-10\n",
      "Iteration 40660: loss = 6.449897e-11,1.5356236e-10\n",
      "Iteration 40665: loss = 6.4490295e-11,1.5356252e-10\n",
      "Iteration 40670: loss = 6.448122e-11,1.5356277e-10\n",
      "Iteration 40675: loss = 6.447414e-11,1.5356277e-10\n",
      "Iteration 40680: loss = 6.4465856e-11,1.5356304e-10\n",
      "Iteration 40685: loss = 6.4459334e-11,1.5356277e-10\n",
      "Iteration 40690: loss = 6.44508e-11,1.5356273e-10\n",
      "Iteration 40695: loss = 6.444375e-11,1.5356307e-10\n",
      "Iteration 40700: loss = 6.4434534e-11,1.5356362e-10\n",
      "Iteration 40705: loss = 6.442736e-11,1.5356369e-10\n",
      "Iteration 40710: loss = 6.441866e-11,1.535639e-10\n",
      "Iteration 40715: loss = 6.441026e-11,1.5356416e-10\n",
      "Iteration 40720: loss = 6.440339e-11,1.5356405e-10\n",
      "Iteration 40725: loss = 6.43954e-11,1.5356416e-10\n",
      "Iteration 40730: loss = 6.438822e-11,1.535641e-10\n",
      "Iteration 40735: loss = 6.438048e-11,1.5356394e-10\n",
      "Iteration 40740: loss = 6.4374416e-11,1.5356372e-10\n",
      "Iteration 40745: loss = 6.4368094e-11,1.5356308e-10\n",
      "Iteration 40750: loss = 6.4364354e-11,1.5356127e-10\n",
      "Iteration 40755: loss = 6.4359296e-11,1.5355966e-10\n",
      "Iteration 40760: loss = 6.435453e-11,1.5355775e-10\n",
      "Iteration 40765: loss = 6.435127e-11,1.53556e-10\n",
      "Iteration 40770: loss = 6.4346674e-11,1.535541e-10\n",
      "Iteration 40775: loss = 6.434398e-11,1.5355245e-10\n",
      "Iteration 40780: loss = 6.433929e-11,1.535505e-10\n",
      "Iteration 40785: loss = 6.4336175e-11,1.5354855e-10\n",
      "Iteration 40790: loss = 6.43316e-11,1.5354677e-10\n",
      "Iteration 40795: loss = 6.432709e-11,1.5354477e-10\n",
      "Iteration 40800: loss = 6.4324095e-11,1.5354286e-10\n",
      "Iteration 40805: loss = 6.431998e-11,1.5354096e-10\n",
      "Iteration 40810: loss = 6.4317045e-11,1.535392e-10\n",
      "Iteration 40815: loss = 6.4312465e-11,1.535373e-10\n",
      "Iteration 40820: loss = 6.4309384e-11,1.5353531e-10\n",
      "Iteration 40825: loss = 6.430469e-11,1.5353312e-10\n",
      "Iteration 40830: loss = 6.4301654e-11,1.5353127e-10\n",
      "Iteration 40835: loss = 6.4297845e-11,1.535295e-10\n",
      "Iteration 40840: loss = 6.4293335e-11,1.5352745e-10\n",
      "Iteration 40845: loss = 6.429019e-11,1.5352547e-10\n",
      "Iteration 40850: loss = 6.428582e-11,1.5352361e-10\n",
      "Iteration 40855: loss = 6.428278e-11,1.5352151e-10\n",
      "Iteration 40860: loss = 6.427831e-11,1.5351963e-10\n",
      "Iteration 40865: loss = 6.427518e-11,1.5351792e-10\n",
      "Iteration 40870: loss = 6.4271234e-11,1.5351595e-10\n",
      "Iteration 40875: loss = 6.426673e-11,1.5351394e-10\n",
      "Iteration 40880: loss = 6.42637e-11,1.5351198e-10\n",
      "Iteration 40885: loss = 6.4259376e-11,1.5351004e-10\n",
      "Iteration 40890: loss = 6.4256246e-11,1.5350811e-10\n",
      "Iteration 40895: loss = 6.425115e-11,1.535065e-10\n",
      "Iteration 40900: loss = 6.424805e-11,1.5350489e-10\n",
      "Iteration 40905: loss = 6.424266e-11,1.5350325e-10\n",
      "Iteration 40910: loss = 6.4238705e-11,1.5350202e-10\n",
      "Iteration 40915: loss = 6.42335e-11,1.535004e-10\n",
      "Iteration 40920: loss = 6.422833e-11,1.5349877e-10\n",
      "Iteration 40925: loss = 6.4224695e-11,1.5349734e-10\n",
      "Iteration 40930: loss = 6.422e-11,1.5349572e-10\n",
      "Iteration 40935: loss = 6.421632e-11,1.5349419e-10\n",
      "Iteration 40940: loss = 6.421136e-11,1.5349247e-10\n",
      "Iteration 40945: loss = 6.4207806e-11,1.534908e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40950: loss = 6.420276e-11,1.5348914e-10\n",
      "Iteration 40955: loss = 6.4199396e-11,1.5348747e-10\n",
      "Iteration 40960: loss = 6.4194296e-11,1.5348596e-10\n",
      "Iteration 40965: loss = 6.419005e-11,1.5348413e-10\n",
      "Iteration 40970: loss = 6.4187065e-11,1.53482e-10\n",
      "Iteration 40975: loss = 6.4182736e-11,1.5348013e-10\n",
      "Iteration 40980: loss = 6.417975e-11,1.5347823e-10\n",
      "Iteration 40985: loss = 6.4175186e-11,1.5347638e-10\n",
      "Iteration 40990: loss = 6.417229e-11,1.5347448e-10\n",
      "Iteration 40995: loss = 6.416845e-11,1.5347232e-10\n",
      "Iteration 41000: loss = 6.4164396e-11,1.5347022e-10\n",
      "Iteration 41005: loss = 6.416143e-11,1.5346838e-10\n",
      "Iteration 41010: loss = 6.4157124e-11,1.5346636e-10\n",
      "Iteration 41015: loss = 6.415441e-11,1.5346435e-10\n",
      "Iteration 41020: loss = 6.415024e-11,1.534623e-10\n",
      "Iteration 41025: loss = 6.4148194e-11,1.5346019e-10\n",
      "Iteration 41030: loss = 6.4144016e-11,1.5345822e-10\n",
      "Iteration 41035: loss = 6.414144e-11,1.5345604e-10\n",
      "Iteration 41040: loss = 6.4137355e-11,1.5345392e-10\n",
      "Iteration 41045: loss = 6.4133414e-11,1.5345165e-10\n",
      "Iteration 41050: loss = 6.4130694e-11,1.5344966e-10\n",
      "Iteration 41055: loss = 6.412737e-11,1.5344734e-10\n",
      "Iteration 41060: loss = 6.41247e-11,1.5344526e-10\n",
      "Iteration 41065: loss = 6.4120584e-11,1.5344331e-10\n",
      "Iteration 41070: loss = 6.411801e-11,1.5344112e-10\n",
      "Iteration 41075: loss = 6.4114006e-11,1.5343901e-10\n",
      "Iteration 41080: loss = 6.4109974e-11,1.5343685e-10\n",
      "Iteration 41085: loss = 6.410739e-11,1.5343482e-10\n",
      "Iteration 41090: loss = 6.410374e-11,1.5343271e-10\n",
      "Iteration 41095: loss = 6.410116e-11,1.5343052e-10\n",
      "Iteration 41100: loss = 6.409722e-11,1.5342849e-10\n",
      "Iteration 41105: loss = 6.4094834e-11,1.5342626e-10\n",
      "Iteration 41110: loss = 6.409063e-11,1.5342419e-10\n",
      "Iteration 41115: loss = 6.408802e-11,1.5342212e-10\n",
      "Iteration 41120: loss = 6.4084536e-11,1.534199e-10\n",
      "Iteration 41125: loss = 6.408052e-11,1.5341763e-10\n",
      "Iteration 41130: loss = 6.4078076e-11,1.5341547e-10\n",
      "Iteration 41135: loss = 6.407414e-11,1.5341331e-10\n",
      "Iteration 41140: loss = 6.407182e-11,1.5341102e-10\n",
      "Iteration 41145: loss = 6.4067786e-11,1.5340892e-10\n",
      "Iteration 41150: loss = 6.406563e-11,1.5340683e-10\n",
      "Iteration 41155: loss = 6.406169e-11,1.5340458e-10\n",
      "Iteration 41160: loss = 6.405911e-11,1.5340253e-10\n",
      "Iteration 41165: loss = 6.4054796e-11,1.5340068e-10\n",
      "Iteration 41170: loss = 6.4050924e-11,1.5339865e-10\n",
      "Iteration 41175: loss = 6.404838e-11,1.533965e-10\n",
      "Iteration 41180: loss = 6.4044374e-11,1.5339435e-10\n",
      "Iteration 41185: loss = 6.404242e-11,1.5339202e-10\n",
      "Iteration 41190: loss = 6.4038295e-11,1.5339005e-10\n",
      "Iteration 41195: loss = 6.4035645e-11,1.5338776e-10\n",
      "Iteration 41200: loss = 6.4031926e-11,1.5338551e-10\n",
      "Iteration 41205: loss = 6.402837e-11,1.5338328e-10\n",
      "Iteration 41210: loss = 6.402617e-11,1.5338092e-10\n",
      "Iteration 41215: loss = 6.4022905e-11,1.5337895e-10\n",
      "Iteration 41220: loss = 6.402012e-11,1.5337676e-10\n",
      "Iteration 41225: loss = 6.401632e-11,1.5337455e-10\n",
      "Iteration 41230: loss = 6.4014245e-11,1.5337222e-10\n",
      "Iteration 41235: loss = 6.401045e-11,1.5336975e-10\n",
      "Iteration 41240: loss = 6.400799e-11,1.5336762e-10\n",
      "Iteration 41245: loss = 6.400466e-11,1.5336525e-10\n",
      "Iteration 41250: loss = 6.400121e-11,1.5336298e-10\n",
      "Iteration 41255: loss = 6.3998896e-11,1.5336081e-10\n",
      "Iteration 41260: loss = 6.3994976e-11,1.5335863e-10\n",
      "Iteration 41265: loss = 6.399271e-11,1.5335638e-10\n",
      "Iteration 41270: loss = 6.3988974e-11,1.5335412e-10\n",
      "Iteration 41275: loss = 6.39866e-11,1.5335189e-10\n",
      "Iteration 41280: loss = 6.398295e-11,1.5334974e-10\n",
      "Iteration 41285: loss = 6.398057e-11,1.5334758e-10\n",
      "Iteration 41290: loss = 6.3976616e-11,1.5334552e-10\n",
      "Iteration 41295: loss = 6.397246e-11,1.5334335e-10\n",
      "Iteration 41300: loss = 6.3970086e-11,1.533412e-10\n",
      "Iteration 41305: loss = 6.396612e-11,1.5333909e-10\n",
      "Iteration 41310: loss = 6.396423e-11,1.5333676e-10\n",
      "Iteration 41315: loss = 6.396026e-11,1.5333479e-10\n",
      "Iteration 41320: loss = 6.39575e-11,1.5333274e-10\n",
      "Iteration 41325: loss = 6.395321e-11,1.5333093e-10\n",
      "Iteration 41330: loss = 6.3948256e-11,1.5332915e-10\n",
      "Iteration 41335: loss = 6.394522e-11,1.5332721e-10\n",
      "Iteration 41340: loss = 6.39411e-11,1.533253e-10\n",
      "Iteration 41345: loss = 6.393793e-11,1.5332358e-10\n",
      "Iteration 41350: loss = 6.393355e-11,1.5332192e-10\n",
      "Iteration 41355: loss = 6.393055e-11,1.5332004e-10\n",
      "Iteration 41360: loss = 6.392596e-11,1.5331808e-10\n",
      "Iteration 41365: loss = 6.392306e-11,1.5331617e-10\n",
      "Iteration 41370: loss = 6.3919287e-11,1.5331414e-10\n",
      "Iteration 41375: loss = 6.3914825e-11,1.5331239e-10\n",
      "Iteration 41380: loss = 6.391187e-11,1.5331027e-10\n",
      "Iteration 41385: loss = 6.390757e-11,1.533084e-10\n",
      "Iteration 41390: loss = 6.390474e-11,1.5330648e-10\n",
      "Iteration 41395: loss = 6.3900496e-11,1.5330447e-10\n",
      "Iteration 41400: loss = 6.389769e-11,1.5330254e-10\n",
      "Iteration 41405: loss = 6.38938e-11,1.533005e-10\n",
      "Iteration 41410: loss = 6.388961e-11,1.5329862e-10\n",
      "Iteration 41415: loss = 6.388686e-11,1.5329664e-10\n",
      "Iteration 41420: loss = 6.388277e-11,1.5329454e-10\n",
      "Iteration 41425: loss = 6.388006e-11,1.5329261e-10\n",
      "Iteration 41430: loss = 6.3876036e-11,1.532905e-10\n",
      "Iteration 41435: loss = 6.387412e-11,1.5328844e-10\n",
      "Iteration 41440: loss = 6.386996e-11,1.5328634e-10\n",
      "Iteration 41445: loss = 6.386744e-11,1.5328429e-10\n",
      "Iteration 41450: loss = 6.386346e-11,1.5328215e-10\n",
      "Iteration 41455: loss = 6.3859466e-11,1.532802e-10\n",
      "Iteration 41460: loss = 6.385683e-11,1.5327814e-10\n",
      "Iteration 41465: loss = 6.385326e-11,1.5327589e-10\n",
      "Iteration 41470: loss = 6.385064e-11,1.5327373e-10\n",
      "Iteration 41475: loss = 6.384662e-11,1.5327173e-10\n",
      "Iteration 41480: loss = 6.384405e-11,1.5326959e-10\n",
      "Iteration 41485: loss = 6.294489e-11,1.532676e-10\n",
      "Iteration 41490: loss = 6.294155e-11,1.5326596e-10\n",
      "Iteration 41495: loss = 6.293092e-11,1.532672e-10\n",
      "Iteration 41500: loss = 6.2922036e-11,1.5326851e-10\n",
      "Iteration 41505: loss = 6.291508e-11,1.5326826e-10\n",
      "Iteration 41510: loss = 6.290831e-11,1.532682e-10\n",
      "Iteration 41515: loss = 6.290004e-11,1.5326818e-10\n",
      "Iteration 41520: loss = 6.289353e-11,1.5326801e-10\n",
      "Iteration 41525: loss = 6.288685e-11,1.532679e-10\n",
      "Iteration 41530: loss = 6.2880916e-11,1.5326772e-10\n",
      "Iteration 41535: loss = 6.287309e-11,1.5326751e-10\n",
      "Iteration 41540: loss = 6.286671e-11,1.5326726e-10\n",
      "Iteration 41545: loss = 6.286035e-11,1.5326677e-10\n",
      "Iteration 41550: loss = 6.28542e-11,1.5326654e-10\n",
      "Iteration 41555: loss = 6.284236e-11,1.5326887e-10\n",
      "Iteration 41560: loss = 6.282969e-11,1.5327142e-10\n",
      "Iteration 41565: loss = 6.281792e-11,1.532743e-10\n",
      "Iteration 41570: loss = 6.280495e-11,1.5327681e-10\n",
      "Iteration 41575: loss = 6.2793576e-11,1.5327957e-10\n",
      "Iteration 41580: loss = 6.278084e-11,1.53282e-10\n",
      "Iteration 41585: loss = 6.276979e-11,1.5328441e-10\n",
      "Iteration 41590: loss = 6.275817e-11,1.5328659e-10\n",
      "Iteration 41595: loss = 6.1861745e-11,1.532886e-10\n",
      "Iteration 41600: loss = 6.1852835e-11,1.5328971e-10\n",
      "Iteration 41605: loss = 6.184356e-11,1.5329069e-10\n",
      "Iteration 41610: loss = 6.183443e-11,1.5329217e-10\n",
      "Iteration 41615: loss = 6.1824795e-11,1.5329285e-10\n",
      "Iteration 41620: loss = 6.181811e-11,1.532931e-10\n",
      "Iteration 41625: loss = 6.181097e-11,1.5329316e-10\n",
      "Iteration 41630: loss = 6.180443e-11,1.5329303e-10\n",
      "Iteration 41635: loss = 6.179663e-11,1.5329284e-10\n",
      "Iteration 41640: loss = 6.178997e-11,1.5329268e-10\n",
      "Iteration 41645: loss = 6.17842e-11,1.5329234e-10\n",
      "Iteration 41650: loss = 6.177791e-11,1.5329212e-10\n",
      "Iteration 41655: loss = 6.177066e-11,1.5329182e-10\n",
      "Iteration 41660: loss = 6.176451e-11,1.5329148e-10\n",
      "Iteration 41665: loss = 6.175904e-11,1.5329085e-10\n",
      "Iteration 41670: loss = 6.175318e-11,1.5329035e-10\n",
      "Iteration 41675: loss = 6.174803e-11,1.5328949e-10\n",
      "Iteration 41680: loss = 6.174088e-11,1.5328902e-10\n",
      "Iteration 41685: loss = 6.173568e-11,1.5328837e-10\n",
      "Iteration 41690: loss = 6.173014e-11,1.5328765e-10\n",
      "Iteration 41695: loss = 6.1725035e-11,1.5328715e-10\n",
      "Iteration 41700: loss = 6.1718096e-11,1.532866e-10\n",
      "Iteration 41705: loss = 6.171306e-11,1.5328583e-10\n",
      "Iteration 41710: loss = 6.170761e-11,1.53285e-10\n",
      "Iteration 41715: loss = 6.170265e-11,1.5328422e-10\n",
      "Iteration 41720: loss = 6.169596e-11,1.5328333e-10\n",
      "Iteration 41725: loss = 6.169135e-11,1.5328241e-10\n",
      "Iteration 41730: loss = 6.168646e-11,1.5328139e-10\n",
      "Iteration 41735: loss = 6.168193e-11,1.5328029e-10\n",
      "Iteration 41740: loss = 6.16771e-11,1.5327915e-10\n",
      "Iteration 41745: loss = 6.16712e-11,1.5327814e-10\n",
      "Iteration 41750: loss = 6.1665506e-11,1.5327746e-10\n",
      "Iteration 41755: loss = 6.1660316e-11,1.5327681e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 41760: loss = 6.165459e-11,1.5327625e-10\n",
      "Iteration 41765: loss = 6.164783e-11,1.5327567e-10\n",
      "Iteration 41770: loss = 6.164185e-11,1.5327516e-10\n",
      "Iteration 41775: loss = 6.163633e-11,1.5327484e-10\n",
      "Iteration 41780: loss = 6.163015e-11,1.5327462e-10\n",
      "Iteration 41785: loss = 6.1622825e-11,1.5327423e-10\n",
      "Iteration 41790: loss = 6.161666e-11,1.5327369e-10\n",
      "Iteration 41795: loss = 6.161125e-11,1.5327328e-10\n",
      "Iteration 41800: loss = 6.160505e-11,1.532729e-10\n",
      "Iteration 41805: loss = 6.159834e-11,1.5327237e-10\n",
      "Iteration 41810: loss = 6.1592405e-11,1.532717e-10\n",
      "Iteration 41815: loss = 6.158708e-11,1.532714e-10\n",
      "Iteration 41820: loss = 6.1581185e-11,1.5327092e-10\n",
      "Iteration 41825: loss = 6.157592e-11,1.5327026e-10\n",
      "Iteration 41830: loss = 6.156855e-11,1.5326976e-10\n",
      "Iteration 41835: loss = 6.156336e-11,1.5326902e-10\n",
      "Iteration 41840: loss = 6.15577e-11,1.5326848e-10\n",
      "Iteration 41845: loss = 6.1552215e-11,1.532678e-10\n",
      "Iteration 41850: loss = 6.1545914e-11,1.5326715e-10\n",
      "Iteration 41855: loss = 6.154057e-11,1.5326629e-10\n",
      "Iteration 41860: loss = 6.1535714e-11,1.5326551e-10\n",
      "Iteration 41865: loss = 6.15304e-11,1.5326466e-10\n",
      "Iteration 41870: loss = 6.152437e-11,1.5326378e-10\n",
      "Iteration 41875: loss = 6.1518984e-11,1.5326304e-10\n",
      "Iteration 41880: loss = 6.151437e-11,1.5326213e-10\n",
      "Iteration 41885: loss = 6.150913e-11,1.5326124e-10\n",
      "Iteration 41890: loss = 6.150478e-11,1.5326028e-10\n",
      "Iteration 41895: loss = 6.1498036e-11,1.5325936e-10\n",
      "Iteration 41900: loss = 6.14937e-11,1.5325825e-10\n",
      "Iteration 41905: loss = 6.148879e-11,1.5325727e-10\n",
      "Iteration 41910: loss = 6.1484755e-11,1.5325603e-10\n",
      "Iteration 41915: loss = 6.1478635e-11,1.5325499e-10\n",
      "Iteration 41920: loss = 6.1474624e-11,1.5325369e-10\n",
      "Iteration 41925: loss = 6.147025e-11,1.5325252e-10\n",
      "Iteration 41930: loss = 6.1466096e-11,1.5325125e-10\n",
      "Iteration 41935: loss = 6.146011e-11,1.5325012e-10\n",
      "Iteration 41940: loss = 6.145612e-11,1.532487e-10\n",
      "Iteration 41945: loss = 6.145177e-11,1.5324768e-10\n",
      "Iteration 41950: loss = 6.1447875e-11,1.5324648e-10\n",
      "Iteration 41955: loss = 6.144322e-11,1.5324525e-10\n",
      "Iteration 41960: loss = 6.1437876e-11,1.5324403e-10\n",
      "Iteration 41965: loss = 6.143351e-11,1.5324264e-10\n",
      "Iteration 41970: loss = 6.142974e-11,1.5324121e-10\n",
      "Iteration 41975: loss = 6.142566e-11,1.5323964e-10\n",
      "Iteration 41980: loss = 6.14207e-11,1.5323835e-10\n",
      "Iteration 41985: loss = 6.141657e-11,1.5323688e-10\n",
      "Iteration 41990: loss = 6.1413034e-11,1.5323527e-10\n",
      "Iteration 41995: loss = 6.140894e-11,1.532338e-10\n",
      "Iteration 42000: loss = 6.1404035e-11,1.5323227e-10\n",
      "Iteration 42005: loss = 6.140021e-11,1.5323073e-10\n",
      "Iteration 42010: loss = 6.139667e-11,1.5322915e-10\n",
      "Iteration 42015: loss = 6.139272e-11,1.5322758e-10\n",
      "Iteration 42020: loss = 6.138928e-11,1.5322608e-10\n",
      "Iteration 42025: loss = 6.138406e-11,1.5322452e-10\n",
      "Iteration 42030: loss = 6.138062e-11,1.5322295e-10\n",
      "Iteration 42035: loss = 6.137673e-11,1.5322149e-10\n",
      "Iteration 42040: loss = 6.137333e-11,1.5321983e-10\n",
      "Iteration 42045: loss = 6.136807e-11,1.5321844e-10\n",
      "Iteration 42050: loss = 6.13641e-11,1.5321687e-10\n",
      "Iteration 42055: loss = 6.1360764e-11,1.5321527e-10\n",
      "Iteration 42060: loss = 6.135698e-11,1.532136e-10\n",
      "Iteration 42065: loss = 6.1352146e-11,1.5321192e-10\n",
      "Iteration 42070: loss = 6.1348364e-11,1.5321028e-10\n",
      "Iteration 42075: loss = 6.1345096e-11,1.5320874e-10\n",
      "Iteration 42080: loss = 6.1341064e-11,1.5320717e-10\n",
      "Iteration 42085: loss = 6.1337956e-11,1.5320555e-10\n",
      "Iteration 42090: loss = 6.133271e-11,1.5320409e-10\n",
      "Iteration 42095: loss = 6.132935e-11,1.5320248e-10\n",
      "Iteration 42100: loss = 6.132558e-11,1.5320083e-10\n",
      "Iteration 42105: loss = 6.1322364e-11,1.5319923e-10\n",
      "Iteration 42110: loss = 6.131697e-11,1.5319773e-10\n",
      "Iteration 42115: loss = 6.1313975e-11,1.5319615e-10\n",
      "Iteration 42120: loss = 6.13102e-11,1.531947e-10\n",
      "Iteration 42125: loss = 6.1307015e-11,1.5319288e-10\n",
      "Iteration 42130: loss = 6.130218e-11,1.531911e-10\n",
      "Iteration 42135: loss = 6.1299424e-11,1.5318924e-10\n",
      "Iteration 42140: loss = 6.129603e-11,1.5318737e-10\n",
      "Iteration 42145: loss = 6.1293214e-11,1.5318562e-10\n",
      "Iteration 42150: loss = 6.128965e-11,1.5318388e-10\n",
      "Iteration 42155: loss = 6.128507e-11,1.5318226e-10\n",
      "Iteration 42160: loss = 6.128174e-11,1.5318041e-10\n",
      "Iteration 42165: loss = 6.12788e-11,1.5317858e-10\n",
      "Iteration 42170: loss = 6.1275374e-11,1.5317686e-10\n",
      "Iteration 42175: loss = 6.1271134e-11,1.53175e-10\n",
      "Iteration 42180: loss = 6.126768e-11,1.5317335e-10\n",
      "Iteration 42185: loss = 6.126485e-11,1.5317152e-10\n",
      "Iteration 42190: loss = 6.126142e-11,1.5316966e-10\n",
      "Iteration 42195: loss = 6.1256944e-11,1.5316784e-10\n",
      "Iteration 42200: loss = 6.125355e-11,1.5316604e-10\n",
      "Iteration 42205: loss = 6.125073e-11,1.5316429e-10\n",
      "Iteration 42210: loss = 6.124737e-11,1.531626e-10\n",
      "Iteration 42215: loss = 6.1244405e-11,1.5316075e-10\n",
      "Iteration 42220: loss = 6.123969e-11,1.531589e-10\n",
      "Iteration 42225: loss = 6.1236766e-11,1.53157e-10\n",
      "Iteration 42230: loss = 6.1233324e-11,1.5315536e-10\n",
      "Iteration 42235: loss = 6.123035e-11,1.5315338e-10\n",
      "Iteration 42240: loss = 6.122525e-11,1.531518e-10\n",
      "Iteration 42245: loss = 6.1221805e-11,1.5315048e-10\n",
      "Iteration 42250: loss = 6.121766e-11,1.5314895e-10\n",
      "Iteration 42255: loss = 6.121408e-11,1.5314763e-10\n",
      "Iteration 42260: loss = 6.120849e-11,1.5314609e-10\n",
      "Iteration 42265: loss = 6.1204326e-11,1.5314486e-10\n",
      "Iteration 42270: loss = 6.120066e-11,1.5314335e-10\n",
      "Iteration 42275: loss = 6.1196416e-11,1.5314192e-10\n",
      "Iteration 42280: loss = 6.119288e-11,1.5314078e-10\n",
      "Iteration 42285: loss = 6.118748e-11,1.5313933e-10\n",
      "Iteration 42290: loss = 6.118375e-11,1.5313788e-10\n",
      "Iteration 42295: loss = 6.117983e-11,1.5313635e-10\n",
      "Iteration 42300: loss = 6.1176224e-11,1.5313478e-10\n",
      "Iteration 42305: loss = 6.117016e-11,1.5313387e-10\n",
      "Iteration 42310: loss = 6.1165704e-11,1.531329e-10\n",
      "Iteration 42315: loss = 6.116049e-11,1.5313195e-10\n",
      "Iteration 42320: loss = 6.1155934e-11,1.5313117e-10\n",
      "Iteration 42325: loss = 6.1149474e-11,1.5313037e-10\n",
      "Iteration 42330: loss = 6.114481e-11,1.5312948e-10\n",
      "Iteration 42335: loss = 6.1139926e-11,1.5312855e-10\n",
      "Iteration 42340: loss = 6.025918e-11,1.5312768e-10\n",
      "Iteration 42345: loss = 6.0252116e-11,1.5312707e-10\n",
      "Iteration 42350: loss = 6.0245815e-11,1.5312712e-10\n",
      "Iteration 42355: loss = 6.023674e-11,1.5312827e-10\n",
      "Iteration 42360: loss = 6.022789e-11,1.5313008e-10\n",
      "Iteration 42365: loss = 6.022028e-11,1.5313073e-10\n",
      "Iteration 42370: loss = 6.02122e-11,1.5313084e-10\n",
      "Iteration 42375: loss = 6.020524e-11,1.5313094e-10\n",
      "Iteration 42380: loss = 6.019878e-11,1.5313119e-10\n",
      "Iteration 42385: loss = 6.0192386e-11,1.5313131e-10\n",
      "Iteration 42390: loss = 6.018488e-11,1.5313113e-10\n",
      "Iteration 42395: loss = 6.0178744e-11,1.5313073e-10\n",
      "Iteration 42400: loss = 6.017289e-11,1.5313065e-10\n",
      "Iteration 42405: loss = 6.0166504e-11,1.5313062e-10\n",
      "Iteration 42410: loss = 6.0159655e-11,1.5313056e-10\n",
      "Iteration 42415: loss = 6.015433e-11,1.5312997e-10\n",
      "Iteration 42420: loss = 5.9281864e-11,1.5312937e-10\n",
      "Iteration 42425: loss = 5.9270235e-11,1.531322e-10\n",
      "Iteration 42430: loss = 5.9259465e-11,1.5313421e-10\n",
      "Iteration 42435: loss = 5.925196e-11,1.5313471e-10\n",
      "Iteration 42440: loss = 5.9244464e-11,1.5313532e-10\n",
      "Iteration 42445: loss = 5.923707e-11,1.5313563e-10\n",
      "Iteration 42450: loss = 5.923125e-11,1.5313614e-10\n",
      "Iteration 42455: loss = 5.922394e-11,1.5313657e-10\n",
      "Iteration 42460: loss = 5.921631e-11,1.5313703e-10\n",
      "Iteration 42465: loss = 5.920911e-11,1.5313723e-10\n",
      "Iteration 42470: loss = 5.9202136e-11,1.5313734e-10\n",
      "Iteration 42475: loss = 5.919517e-11,1.5313736e-10\n",
      "Iteration 42480: loss = 5.918538e-11,1.5313886e-10\n",
      "Iteration 42485: loss = 5.917265e-11,1.5314236e-10\n",
      "Iteration 42490: loss = 5.9160614e-11,1.531457e-10\n",
      "Iteration 42495: loss = 5.914826e-11,1.531491e-10\n",
      "Iteration 42500: loss = 5.913495e-11,1.5315234e-10\n",
      "Iteration 42505: loss = 5.9123116e-11,1.5315565e-10\n",
      "Iteration 42510: loss = 5.911166e-11,1.5315857e-10\n",
      "Iteration 42515: loss = 5.909966e-11,1.5316143e-10\n",
      "Iteration 42520: loss = 5.90883e-11,1.5316434e-10\n",
      "Iteration 42525: loss = 5.9075744e-11,1.5316712e-10\n",
      "Iteration 42530: loss = 5.906454e-11,1.5316974e-10\n",
      "Iteration 42535: loss = 5.9055684e-11,1.5317182e-10\n",
      "Iteration 42540: loss = 5.9048696e-11,1.5317173e-10\n",
      "Iteration 42545: loss = 5.904269e-11,1.5317163e-10\n",
      "Iteration 42550: loss = 5.9036255e-11,1.5317135e-10\n",
      "Iteration 42555: loss = 5.9030135e-11,1.5317117e-10\n",
      "Iteration 42560: loss = 5.9024e-11,1.5317098e-10\n",
      "Iteration 42565: loss = 5.901793e-11,1.5317082e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42570: loss = 5.90114e-11,1.5317068e-10\n",
      "Iteration 42575: loss = 5.900382e-11,1.5317178e-10\n",
      "Iteration 42580: loss = 5.8991194e-11,1.5317445e-10\n",
      "Iteration 42585: loss = 5.898012e-11,1.5317754e-10\n",
      "Iteration 42590: loss = 5.89691e-11,1.5318005e-10\n",
      "Iteration 42595: loss = 5.895779e-11,1.5318288e-10\n",
      "Iteration 42600: loss = 5.89456e-11,1.5318546e-10\n",
      "Iteration 42605: loss = 5.8934795e-11,1.531881e-10\n",
      "Iteration 42610: loss = 5.892418e-11,1.531906e-10\n",
      "Iteration 42615: loss = 5.8913256e-11,1.5319324e-10\n",
      "Iteration 42620: loss = 5.890774e-11,1.5319274e-10\n",
      "Iteration 42625: loss = 5.890144e-11,1.5319236e-10\n",
      "Iteration 42630: loss = 5.889559e-11,1.5319214e-10\n",
      "Iteration 42635: loss = 5.8889595e-11,1.5319179e-10\n",
      "Iteration 42640: loss = 5.888386e-11,1.5319154e-10\n",
      "Iteration 42645: loss = 5.887838e-11,1.5319128e-10\n",
      "Iteration 42650: loss = 5.887192e-11,1.5319089e-10\n",
      "Iteration 42655: loss = 5.886792e-11,1.5319018e-10\n",
      "Iteration 42660: loss = 5.8862366e-11,1.5318968e-10\n",
      "Iteration 42665: loss = 5.885661e-11,1.5318924e-10\n",
      "Iteration 42670: loss = 5.8851195e-11,1.5318857e-10\n",
      "Iteration 42675: loss = 5.884578e-11,1.5318818e-10\n",
      "Iteration 42680: loss = 5.883987e-11,1.5318764e-10\n",
      "Iteration 42685: loss = 5.8836075e-11,1.5318624e-10\n",
      "Iteration 42690: loss = 5.8832765e-11,1.5318442e-10\n",
      "Iteration 42695: loss = 5.882952e-11,1.5318266e-10\n",
      "Iteration 42700: loss = 5.882573e-11,1.5318086e-10\n",
      "Iteration 42705: loss = 5.8822024e-11,1.5318036e-10\n",
      "Iteration 42710: loss = 5.881783e-11,1.5317914e-10\n",
      "Iteration 42715: loss = 5.88148e-11,1.5317728e-10\n",
      "Iteration 42720: loss = 5.881017e-11,1.5317606e-10\n",
      "Iteration 42725: loss = 5.880638e-11,1.5317478e-10\n",
      "Iteration 42730: loss = 5.8801346e-11,1.5317375e-10\n",
      "Iteration 42735: loss = 5.879825e-11,1.5317213e-10\n",
      "Iteration 42740: loss = 5.879309e-11,1.5317111e-10\n",
      "Iteration 42745: loss = 5.879038e-11,1.5316937e-10\n",
      "Iteration 42750: loss = 5.878636e-11,1.5316778e-10\n",
      "Iteration 42755: loss = 5.878285e-11,1.5316703e-10\n",
      "Iteration 42760: loss = 5.877981e-11,1.5316495e-10\n",
      "Iteration 42765: loss = 5.877648e-11,1.531631e-10\n",
      "Iteration 42770: loss = 5.8771425e-11,1.5316196e-10\n",
      "Iteration 42775: loss = 5.876811e-11,1.5316047e-10\n",
      "Iteration 42780: loss = 5.8765e-11,1.5315851e-10\n",
      "Iteration 42785: loss = 5.876053e-11,1.5315739e-10\n",
      "Iteration 42790: loss = 5.875675e-11,1.5315611e-10\n",
      "Iteration 42795: loss = 5.875377e-11,1.53154e-10\n",
      "Iteration 42800: loss = 5.874948e-11,1.5315291e-10\n",
      "Iteration 42805: loss = 5.874518e-11,1.531515e-10\n",
      "Iteration 42810: loss = 5.874388e-11,1.5314938e-10\n",
      "Iteration 42815: loss = 5.8739485e-11,1.5314827e-10\n",
      "Iteration 42820: loss = 5.8734934e-11,1.5314724e-10\n",
      "Iteration 42825: loss = 5.87308e-11,1.5314609e-10\n",
      "Iteration 42830: loss = 5.8727544e-11,1.5314416e-10\n",
      "Iteration 42835: loss = 5.872364e-11,1.531428e-10\n",
      "Iteration 42840: loss = 5.872037e-11,1.5314082e-10\n",
      "Iteration 42845: loss = 5.8715845e-11,1.531398e-10\n",
      "Iteration 42850: loss = 5.871132e-11,1.531385e-10\n",
      "Iteration 42855: loss = 5.8708545e-11,1.5313632e-10\n",
      "Iteration 42860: loss = 5.8705735e-11,1.5313514e-10\n",
      "Iteration 42865: loss = 5.870164e-11,1.5313395e-10\n",
      "Iteration 42870: loss = 5.869897e-11,1.5313165e-10\n",
      "Iteration 42875: loss = 5.869481e-11,1.5313047e-10\n",
      "Iteration 42880: loss = 5.869113e-11,1.5312887e-10\n",
      "Iteration 42885: loss = 5.868749e-11,1.5312718e-10\n",
      "Iteration 42890: loss = 5.8683954e-11,1.5312564e-10\n",
      "Iteration 42895: loss = 5.868043e-11,1.5312399e-10\n",
      "Iteration 42900: loss = 5.8676355e-11,1.5312279e-10\n",
      "Iteration 42905: loss = 5.867312e-11,1.5312086e-10\n",
      "Iteration 42910: loss = 5.8670464e-11,1.5311928e-10\n",
      "Iteration 42915: loss = 5.8667404e-11,1.5311752e-10\n",
      "Iteration 42920: loss = 5.8663206e-11,1.531161e-10\n",
      "Iteration 42925: loss = 5.8658585e-11,1.5311515e-10\n",
      "Iteration 42930: loss = 5.865363e-11,1.5311456e-10\n",
      "Iteration 42935: loss = 5.864796e-11,1.5311402e-10\n",
      "Iteration 42940: loss = 5.8641994e-11,1.531136e-10\n",
      "Iteration 42945: loss = 5.863638e-11,1.53113e-10\n",
      "Iteration 42950: loss = 5.863073e-11,1.5311276e-10\n",
      "Iteration 42955: loss = 5.862493e-11,1.5311243e-10\n",
      "Iteration 42960: loss = 5.862068e-11,1.5311179e-10\n",
      "Iteration 42965: loss = 5.861729e-11,1.5311019e-10\n",
      "Iteration 42970: loss = 5.861371e-11,1.5310865e-10\n",
      "Iteration 42975: loss = 5.8610096e-11,1.531072e-10\n",
      "Iteration 42980: loss = 5.8606245e-11,1.5310549e-10\n",
      "Iteration 42985: loss = 5.8602685e-11,1.5310388e-10\n",
      "Iteration 42990: loss = 5.859904e-11,1.5310231e-10\n",
      "Iteration 42995: loss = 5.859538e-11,1.5310075e-10\n",
      "Iteration 43000: loss = 5.859226e-11,1.5309906e-10\n",
      "Iteration 43005: loss = 5.858872e-11,1.5309755e-10\n",
      "Iteration 43010: loss = 5.858642e-11,1.530962e-10\n",
      "Iteration 43015: loss = 5.858276e-11,1.5309426e-10\n",
      "Iteration 43020: loss = 5.857904e-11,1.5309284e-10\n",
      "Iteration 43025: loss = 5.8575526e-11,1.5309128e-10\n",
      "Iteration 43030: loss = 5.8572376e-11,1.530895e-10\n",
      "Iteration 43035: loss = 5.8568796e-11,1.5308804e-10\n",
      "Iteration 43040: loss = 5.8565375e-11,1.5308614e-10\n",
      "Iteration 43045: loss = 5.8562155e-11,1.5308439e-10\n",
      "Iteration 43050: loss = 5.8558734e-11,1.5308277e-10\n",
      "Iteration 43055: loss = 5.855545e-11,1.5308102e-10\n",
      "Iteration 43060: loss = 5.855217e-11,1.5307926e-10\n",
      "Iteration 43065: loss = 5.855061e-11,1.5307754e-10\n",
      "Iteration 43070: loss = 5.8547216e-11,1.5307605e-10\n",
      "Iteration 43075: loss = 5.854397e-11,1.5307408e-10\n",
      "Iteration 43080: loss = 5.854072e-11,1.5307228e-10\n",
      "Iteration 43085: loss = 5.853762e-11,1.530703e-10\n",
      "Iteration 43090: loss = 5.853437e-11,1.5306863e-10\n",
      "Iteration 43095: loss = 5.853116e-11,1.5306673e-10\n",
      "Iteration 43100: loss = 5.852847e-11,1.5306481e-10\n",
      "Iteration 43105: loss = 5.8525296e-11,1.5306284e-10\n",
      "Iteration 43110: loss = 5.852225e-11,1.5306097e-10\n",
      "Iteration 43115: loss = 5.852032e-11,1.5305904e-10\n",
      "Iteration 43120: loss = 5.8517226e-11,1.530572e-10\n",
      "Iteration 43125: loss = 5.85142e-11,1.530555e-10\n",
      "Iteration 43130: loss = 5.8510904e-11,1.530534e-10\n",
      "Iteration 43135: loss = 5.8508386e-11,1.5305163e-10\n",
      "Iteration 43140: loss = 5.8505374e-11,1.5304982e-10\n",
      "Iteration 43145: loss = 5.8500975e-11,1.5304835e-10\n",
      "Iteration 43150: loss = 5.849741e-11,1.5304687e-10\n",
      "Iteration 43155: loss = 5.849455e-11,1.5304503e-10\n",
      "Iteration 43160: loss = 5.849148e-11,1.5304305e-10\n",
      "Iteration 43165: loss = 5.763209e-11,1.5304134e-10\n",
      "Iteration 43170: loss = 5.7625366e-11,1.5304161e-10\n",
      "Iteration 43175: loss = 5.7617966e-11,1.5304244e-10\n",
      "Iteration 43180: loss = 5.7610916e-11,1.5304286e-10\n",
      "Iteration 43185: loss = 5.7603703e-11,1.5304324e-10\n",
      "Iteration 43190: loss = 5.6749026e-11,1.5304355e-10\n",
      "Iteration 43195: loss = 5.6744e-11,1.5304301e-10\n",
      "Iteration 43200: loss = 5.6741025e-11,1.5304075e-10\n",
      "Iteration 43205: loss = 5.673989e-11,1.5303864e-10\n",
      "Iteration 43210: loss = 5.6736626e-11,1.5303658e-10\n",
      "Iteration 43215: loss = 5.6734516e-11,1.530348e-10\n",
      "Iteration 43220: loss = 5.673115e-11,1.5303275e-10\n",
      "Iteration 43225: loss = 5.6727834e-11,1.5303085e-10\n",
      "Iteration 43230: loss = 5.6725857e-11,1.5302899e-10\n",
      "Iteration 43235: loss = 5.672254e-11,1.5302694e-10\n",
      "Iteration 43240: loss = 5.6721572e-11,1.530248e-10\n",
      "Iteration 43245: loss = 5.6718626e-11,1.5302251e-10\n",
      "Iteration 43250: loss = 5.671709e-11,1.5302043e-10\n",
      "Iteration 43255: loss = 5.671404e-11,1.530187e-10\n",
      "Iteration 43260: loss = 5.6710508e-11,1.5301647e-10\n",
      "Iteration 43265: loss = 5.6708457e-11,1.5301449e-10\n",
      "Iteration 43270: loss = 5.6705307e-11,1.5301263e-10\n",
      "Iteration 43275: loss = 5.670406e-11,1.5301048e-10\n",
      "Iteration 43280: loss = 5.6700755e-11,1.5300852e-10\n",
      "Iteration 43285: loss = 5.6699107e-11,1.5300644e-10\n",
      "Iteration 43290: loss = 5.6695995e-11,1.5300428e-10\n",
      "Iteration 43295: loss = 5.6692807e-11,1.5300222e-10\n",
      "Iteration 43300: loss = 5.6691346e-11,1.530001e-10\n",
      "Iteration 43305: loss = 5.6689046e-11,1.5299766e-10\n",
      "Iteration 43310: loss = 5.6689858e-11,1.5299405e-10\n",
      "Iteration 43315: loss = 5.6689264e-11,1.5299058e-10\n",
      "Iteration 43320: loss = 5.6690482e-11,1.5298698e-10\n",
      "Iteration 43325: loss = 5.6689663e-11,1.5298351e-10\n",
      "Iteration 43330: loss = 5.668943e-11,1.5298005e-10\n",
      "Iteration 43335: loss = 5.6690097e-11,1.5297641e-10\n",
      "Iteration 43340: loss = 5.6689976e-11,1.529728e-10\n",
      "Iteration 43345: loss = 5.6690846e-11,1.529693e-10\n",
      "Iteration 43350: loss = 5.6690725e-11,1.5296563e-10\n",
      "Iteration 43355: loss = 5.6691613e-11,1.5296198e-10\n",
      "Iteration 43360: loss = 5.669155e-11,1.529582e-10\n",
      "Iteration 43365: loss = 5.669092e-11,1.5295462e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43370: loss = 5.669233e-11,1.5295101e-10\n",
      "Iteration 43375: loss = 5.669179e-11,1.5294752e-10\n",
      "Iteration 43380: loss = 5.6689934e-11,1.529452e-10\n",
      "Iteration 43385: loss = 5.668475e-11,1.5294489e-10\n",
      "Iteration 43390: loss = 5.6679234e-11,1.529443e-10\n",
      "Iteration 43395: loss = 5.66741e-11,1.5294357e-10\n",
      "Iteration 43400: loss = 5.6669048e-11,1.5294305e-10\n",
      "Iteration 43405: loss = 5.666435e-11,1.5294223e-10\n",
      "Iteration 43410: loss = 5.6660523e-11,1.5294142e-10\n",
      "Iteration 43415: loss = 5.6655815e-11,1.5294067e-10\n",
      "Iteration 43420: loss = 5.665095e-11,1.5293974e-10\n",
      "Iteration 43425: loss = 5.6646705e-11,1.5293893e-10\n",
      "Iteration 43430: loss = 5.6641847e-11,1.5293783e-10\n",
      "Iteration 43435: loss = 5.6637462e-11,1.5293697e-10\n",
      "Iteration 43440: loss = 5.66326e-11,1.5293604e-10\n",
      "Iteration 43445: loss = 5.6628303e-11,1.5293523e-10\n",
      "Iteration 43450: loss = 5.6623563e-11,1.529343e-10\n",
      "Iteration 43455: loss = 5.6619275e-11,1.5293328e-10\n",
      "Iteration 43460: loss = 5.661432e-11,1.5293239e-10\n",
      "Iteration 43465: loss = 5.6611632e-11,1.5293135e-10\n",
      "Iteration 43470: loss = 5.660687e-11,1.5293052e-10\n",
      "Iteration 43475: loss = 5.660261e-11,1.529295e-10\n",
      "Iteration 43480: loss = 5.659775e-11,1.529286e-10\n",
      "Iteration 43485: loss = 5.65931e-11,1.5292757e-10\n",
      "Iteration 43490: loss = 5.6589112e-11,1.5292641e-10\n",
      "Iteration 43495: loss = 5.658442e-11,1.5292528e-10\n",
      "Iteration 43500: loss = 5.6580397e-11,1.5292445e-10\n",
      "Iteration 43505: loss = 5.6575866e-11,1.5292313e-10\n",
      "Iteration 43510: loss = 5.6571872e-11,1.5292195e-10\n",
      "Iteration 43515: loss = 5.656889e-11,1.5292081e-10\n",
      "Iteration 43520: loss = 5.656503e-11,1.5291954e-10\n",
      "Iteration 43525: loss = 5.6560822e-11,1.5291832e-10\n",
      "Iteration 43530: loss = 5.6556985e-11,1.5291703e-10\n",
      "Iteration 43535: loss = 5.655285e-11,1.5291587e-10\n",
      "Iteration 43540: loss = 5.6549033e-11,1.5291458e-10\n",
      "Iteration 43545: loss = 5.6544852e-11,1.5291335e-10\n",
      "Iteration 43550: loss = 5.6541234e-11,1.5291202e-10\n",
      "Iteration 43555: loss = 5.6538035e-11,1.5291021e-10\n",
      "Iteration 43560: loss = 5.6540366e-11,1.5290597e-10\n",
      "Iteration 43565: loss = 5.6539523e-11,1.5290266e-10\n",
      "Iteration 43570: loss = 5.6541823e-11,1.528985e-10\n",
      "Iteration 43575: loss = 5.6539707e-11,1.5289546e-10\n",
      "Iteration 43580: loss = 5.653647e-11,1.5289389e-10\n",
      "Iteration 43585: loss = 5.65388e-11,1.5288978e-10\n",
      "Iteration 43590: loss = 5.6538958e-11,1.5288568e-10\n",
      "Iteration 43595: loss = 5.6539926e-11,1.5288167e-10\n",
      "Iteration 43600: loss = 5.6536293e-11,1.5288061e-10\n",
      "Iteration 43605: loss = 5.6537025e-11,1.5287634e-10\n",
      "Iteration 43610: loss = 5.6538847e-11,1.5287227e-10\n",
      "Iteration 43615: loss = 5.6538316e-11,1.5286898e-10\n",
      "Iteration 43620: loss = 5.6533184e-11,1.5286866e-10\n",
      "Iteration 43625: loss = 5.652734e-11,1.5286836e-10\n",
      "Iteration 43630: loss = 5.6521104e-11,1.5286813e-10\n",
      "Iteration 43635: loss = 5.6516448e-11,1.528683e-10\n",
      "Iteration 43640: loss = 5.6510085e-11,1.5286805e-10\n",
      "Iteration 43645: loss = 5.650561e-11,1.5286783e-10\n",
      "Iteration 43650: loss = 5.6499205e-11,1.5286741e-10\n",
      "Iteration 43655: loss = 5.6494767e-11,1.5286725e-10\n",
      "Iteration 43660: loss = 5.6488397e-11,1.5286711e-10\n",
      "Iteration 43665: loss = 5.648285e-11,1.5286657e-10\n",
      "Iteration 43670: loss = 5.6477992e-11,1.5286625e-10\n",
      "Iteration 43675: loss = 5.6472504e-11,1.5286596e-10\n",
      "Iteration 43680: loss = 5.6467896e-11,1.5286558e-10\n",
      "Iteration 43685: loss = 5.646218e-11,1.5286493e-10\n",
      "Iteration 43690: loss = 5.6458182e-11,1.5286424e-10\n",
      "Iteration 43695: loss = 5.645247e-11,1.5286365e-10\n",
      "Iteration 43700: loss = 5.6447083e-11,1.5286317e-10\n",
      "Iteration 43705: loss = 5.644267e-11,1.528627e-10\n",
      "Iteration 43710: loss = 5.643755e-11,1.5286214e-10\n",
      "Iteration 43715: loss = 5.6433136e-11,1.5286164e-10\n",
      "Iteration 43720: loss = 5.6427862e-11,1.5286106e-10\n",
      "Iteration 43725: loss = 5.6423467e-11,1.5286054e-10\n",
      "Iteration 43730: loss = 5.6418283e-11,1.5285992e-10\n",
      "Iteration 43735: loss = 5.641265e-11,1.5285938e-10\n",
      "Iteration 43740: loss = 5.640896e-11,1.5285878e-10\n",
      "Iteration 43745: loss = 5.6403156e-11,1.5285812e-10\n",
      "Iteration 43750: loss = 5.6399593e-11,1.5285735e-10\n",
      "Iteration 43755: loss = 5.6394004e-11,1.5285662e-10\n",
      "Iteration 43760: loss = 5.639e-11,1.5285603e-10\n",
      "Iteration 43765: loss = 5.6382985e-11,1.5285605e-10\n",
      "Iteration 43770: loss = 5.6372827e-11,1.5285859e-10\n",
      "Iteration 43775: loss = 5.6362123e-11,1.5286132e-10\n",
      "Iteration 43780: loss = 5.6352093e-11,1.5286361e-10\n",
      "Iteration 43785: loss = 5.6341654e-11,1.5286616e-10\n",
      "Iteration 43790: loss = 5.6332716e-11,1.5286837e-10\n",
      "Iteration 43795: loss = 5.632325e-11,1.528707e-10\n",
      "Iteration 43800: loss = 5.631332e-11,1.5287258e-10\n",
      "Iteration 43805: loss = 5.6303906e-11,1.5287502e-10\n",
      "Iteration 43810: loss = 5.6294167e-11,1.5287682e-10\n",
      "Iteration 43815: loss = 5.628494e-11,1.528792e-10\n",
      "Iteration 43820: loss = 5.6275335e-11,1.5288104e-10\n",
      "Iteration 43825: loss = 5.626624e-11,1.5288282e-10\n",
      "Iteration 43830: loss = 5.625672e-11,1.5288486e-10\n",
      "Iteration 43835: loss = 5.6247566e-11,1.5288668e-10\n",
      "Iteration 43840: loss = 5.6239274e-11,1.528887e-10\n",
      "Iteration 43845: loss = 5.6230267e-11,1.5289034e-10\n",
      "Iteration 43850: loss = 5.6220712e-11,1.528927e-10\n",
      "Iteration 43855: loss = 5.6211674e-11,1.5289443e-10\n",
      "Iteration 43860: loss = 5.6205787e-11,1.5289382e-10\n",
      "Iteration 43865: loss = 5.620032e-11,1.5289417e-10\n",
      "Iteration 43870: loss = 5.619343e-11,1.5289445e-10\n",
      "Iteration 43875: loss = 5.618841e-11,1.5289447e-10\n",
      "Iteration 43880: loss = 5.6182014e-11,1.5289442e-10\n",
      "Iteration 43885: loss = 5.6178392e-11,1.5289339e-10\n",
      "Iteration 43890: loss = 5.6173916e-11,1.5289235e-10\n",
      "Iteration 43895: loss = 5.616917e-11,1.5289123e-10\n",
      "Iteration 43900: loss = 5.616613e-11,1.5289026e-10\n",
      "Iteration 43905: loss = 5.6161353e-11,1.5288905e-10\n",
      "Iteration 43910: loss = 5.615795e-11,1.5288809e-10\n",
      "Iteration 43915: loss = 5.6153592e-11,1.5288705e-10\n",
      "Iteration 43920: loss = 5.6150192e-11,1.5288598e-10\n",
      "Iteration 43925: loss = 5.6145862e-11,1.528851e-10\n",
      "Iteration 43930: loss = 5.6141005e-11,1.5288396e-10\n",
      "Iteration 43935: loss = 5.6138136e-11,1.5288285e-10\n",
      "Iteration 43940: loss = 5.613342e-11,1.5288185e-10\n",
      "Iteration 43945: loss = 5.6130312e-11,1.5288067e-10\n",
      "Iteration 43950: loss = 5.612567e-11,1.5287963e-10\n",
      "Iteration 43955: loss = 5.6121108e-11,1.5287951e-10\n",
      "Iteration 43960: loss = 5.611156e-11,1.528811e-10\n",
      "Iteration 43965: loss = 5.610337e-11,1.528821e-10\n",
      "Iteration 43970: loss = 5.6097616e-11,1.528828e-10\n",
      "Iteration 43975: loss = 5.6089845e-11,1.5288357e-10\n",
      "Iteration 43980: loss = 5.6083766e-11,1.5288425e-10\n",
      "Iteration 43985: loss = 5.607653e-11,1.5288484e-10\n",
      "Iteration 43990: loss = 5.60698e-11,1.5288538e-10\n",
      "Iteration 43995: loss = 5.6063834e-11,1.528855e-10\n",
      "Iteration 44000: loss = 5.6057336e-11,1.5288584e-10\n",
      "Iteration 44005: loss = 5.6051757e-11,1.5288623e-10\n",
      "Iteration 44010: loss = 5.604506e-11,1.5288669e-10\n",
      "Iteration 44015: loss = 5.6039326e-11,1.5288695e-10\n",
      "Iteration 44020: loss = 5.603269e-11,1.5288733e-10\n",
      "Iteration 44025: loss = 5.6025806e-11,1.5288758e-10\n",
      "Iteration 44030: loss = 5.6020483e-11,1.5288773e-10\n",
      "Iteration 44035: loss = 5.6013694e-11,1.5288786e-10\n",
      "Iteration 44040: loss = 5.6008764e-11,1.5288808e-10\n",
      "Iteration 44045: loss = 5.6002036e-11,1.5288804e-10\n",
      "Iteration 44050: loss = 5.5997346e-11,1.528881e-10\n",
      "Iteration 44055: loss = 5.5990736e-11,1.5288835e-10\n",
      "Iteration 44060: loss = 5.59847e-11,1.5288848e-10\n",
      "Iteration 44065: loss = 5.597943e-11,1.5288854e-10\n",
      "Iteration 44070: loss = 5.597299e-11,1.5288848e-10\n",
      "Iteration 44075: loss = 5.5968268e-11,1.5288863e-10\n",
      "Iteration 44080: loss = 5.596175e-11,1.5288854e-10\n",
      "Iteration 44085: loss = 5.5956902e-11,1.5288873e-10\n",
      "Iteration 44090: loss = 5.595059e-11,1.528887e-10\n",
      "Iteration 44095: loss = 5.5944503e-11,1.5288877e-10\n",
      "Iteration 44100: loss = 5.5101174e-11,1.5288878e-10\n",
      "Iteration 44105: loss = 5.5095e-11,1.5288894e-10\n",
      "Iteration 44110: loss = 5.5089155e-11,1.528893e-10\n",
      "Iteration 44115: loss = 5.5081894e-11,1.5289e-10\n",
      "Iteration 44120: loss = 5.5075538e-11,1.5289119e-10\n",
      "Iteration 44125: loss = 5.5067766e-11,1.5289195e-10\n",
      "Iteration 44130: loss = 5.423065e-11,1.5289263e-10\n",
      "Iteration 44135: loss = 5.4224125e-11,1.5289411e-10\n",
      "Iteration 44140: loss = 5.4215604e-11,1.5289513e-10\n",
      "Iteration 44145: loss = 5.4208735e-11,1.5289667e-10\n",
      "Iteration 44150: loss = 5.420005e-11,1.5289815e-10\n",
      "Iteration 44155: loss = 5.4192928e-11,1.5289946e-10\n",
      "Iteration 44160: loss = 5.4184407e-11,1.5290072e-10\n",
      "Iteration 44165: loss = 5.4176538e-11,1.52902e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44170: loss = 5.4169284e-11,1.5290325e-10\n",
      "Iteration 44175: loss = 5.4161023e-11,1.5290436e-10\n",
      "Iteration 44180: loss = 5.4155968e-11,1.5290444e-10\n",
      "Iteration 44185: loss = 5.415014e-11,1.5290422e-10\n",
      "Iteration 44190: loss = 5.414578e-11,1.5290388e-10\n",
      "Iteration 44195: loss = 5.41401e-11,1.5290362e-10\n",
      "Iteration 44200: loss = 5.413463e-11,1.5290322e-10\n",
      "Iteration 44205: loss = 5.413031e-11,1.5290282e-10\n",
      "Iteration 44210: loss = 5.412536e-11,1.5290236e-10\n",
      "Iteration 44215: loss = 5.412147e-11,1.5290189e-10\n",
      "Iteration 44220: loss = 5.4116444e-11,1.5290147e-10\n",
      "Iteration 44225: loss = 5.4112704e-11,1.5290089e-10\n",
      "Iteration 44230: loss = 5.4107222e-11,1.5290025e-10\n",
      "Iteration 44235: loss = 5.410213e-11,1.5289942e-10\n",
      "Iteration 44240: loss = 5.4098576e-11,1.5289889e-10\n",
      "Iteration 44245: loss = 5.40937e-11,1.5289803e-10\n",
      "Iteration 44250: loss = 5.4090236e-11,1.5289722e-10\n",
      "Iteration 44255: loss = 5.4084928e-11,1.5289649e-10\n",
      "Iteration 44260: loss = 5.4081458e-11,1.5289564e-10\n",
      "Iteration 44265: loss = 5.4076826e-11,1.5289507e-10\n",
      "Iteration 44270: loss = 5.4072125e-11,1.5289427e-10\n",
      "Iteration 44275: loss = 5.406882e-11,1.528933e-10\n",
      "Iteration 44280: loss = 5.40641e-11,1.5289248e-10\n",
      "Iteration 44285: loss = 5.4060364e-11,1.528917e-10\n",
      "Iteration 44290: loss = 5.4055687e-11,1.5289088e-10\n",
      "Iteration 44295: loss = 5.4052398e-11,1.5289021e-10\n",
      "Iteration 44300: loss = 5.4047874e-11,1.5288928e-10\n",
      "Iteration 44305: loss = 5.404283e-11,1.5288865e-10\n",
      "Iteration 44310: loss = 5.403836e-11,1.5288819e-10\n",
      "Iteration 44315: loss = 5.4034027e-11,1.5288737e-10\n",
      "Iteration 44320: loss = 5.4032594e-11,1.52885e-10\n",
      "Iteration 44325: loss = 5.4031085e-11,1.5288307e-10\n",
      "Iteration 44330: loss = 5.4028542e-11,1.528804e-10\n",
      "Iteration 44335: loss = 5.4028768e-11,1.5287727e-10\n",
      "Iteration 44340: loss = 5.402873e-11,1.5287412e-10\n",
      "Iteration 44345: loss = 5.4028854e-11,1.5287105e-10\n",
      "Iteration 44350: loss = 5.4027366e-11,1.5286795e-10\n",
      "Iteration 44355: loss = 5.402748e-11,1.5286479e-10\n",
      "Iteration 44360: loss = 5.4027657e-11,1.5286168e-10\n",
      "Iteration 44365: loss = 5.4027772e-11,1.5285868e-10\n",
      "Iteration 44370: loss = 5.4026547e-11,1.528555e-10\n",
      "Iteration 44375: loss = 5.402626e-11,1.5285269e-10\n",
      "Iteration 44380: loss = 5.4026436e-11,1.528491e-10\n",
      "Iteration 44385: loss = 5.4026703e-11,1.5284607e-10\n",
      "Iteration 44390: loss = 5.402566e-11,1.5284285e-10\n",
      "Iteration 44395: loss = 5.4025756e-11,1.5283959e-10\n",
      "Iteration 44400: loss = 5.4025742e-11,1.5283654e-10\n",
      "Iteration 44405: loss = 5.40261e-11,1.5283325e-10\n",
      "Iteration 44410: loss = 5.4025246e-11,1.5283003e-10\n",
      "Iteration 44415: loss = 5.4025576e-11,1.5282692e-10\n",
      "Iteration 44420: loss = 5.4026037e-11,1.5282348e-10\n",
      "Iteration 44425: loss = 5.40195e-11,1.5282359e-10\n",
      "Iteration 44430: loss = 5.401506e-11,1.5282343e-10\n",
      "Iteration 44435: loss = 5.4009436e-11,1.5282314e-10\n",
      "Iteration 44440: loss = 5.400487e-11,1.5282305e-10\n",
      "Iteration 44445: loss = 5.3999197e-11,1.5282287e-10\n",
      "Iteration 44450: loss = 5.3994812e-11,1.5282274e-10\n",
      "Iteration 44455: loss = 5.398872e-11,1.5282248e-10\n",
      "Iteration 44460: loss = 5.3982846e-11,1.5282231e-10\n",
      "Iteration 44465: loss = 5.39786e-11,1.5282207e-10\n",
      "Iteration 44470: loss = 5.3973197e-11,1.5282184e-10\n",
      "Iteration 44475: loss = 5.396912e-11,1.5282133e-10\n",
      "Iteration 44480: loss = 5.3963393e-11,1.5282112e-10\n",
      "Iteration 44485: loss = 5.3959216e-11,1.5282084e-10\n",
      "Iteration 44490: loss = 5.3953855e-11,1.5282048e-10\n",
      "Iteration 44495: loss = 5.394861e-11,1.5282003e-10\n",
      "Iteration 44500: loss = 5.3944616e-11,1.5281988e-10\n",
      "Iteration 44505: loss = 5.3938992e-11,1.528193e-10\n",
      "Iteration 44510: loss = 5.3935134e-11,1.5281872e-10\n",
      "Iteration 44515: loss = 5.3930072e-11,1.5281815e-10\n",
      "Iteration 44520: loss = 5.392646e-11,1.5281741e-10\n",
      "Iteration 44525: loss = 5.3921798e-11,1.5281677e-10\n",
      "Iteration 44530: loss = 5.3916524e-11,1.5281611e-10\n",
      "Iteration 44535: loss = 5.3913023e-11,1.5281543e-10\n",
      "Iteration 44540: loss = 5.390843e-11,1.5281484e-10\n",
      "Iteration 44545: loss = 5.390489e-11,1.5281407e-10\n",
      "Iteration 44550: loss = 5.390025e-11,1.5281343e-10\n",
      "Iteration 44555: loss = 5.3896474e-11,1.5281276e-10\n",
      "Iteration 44560: loss = 5.3891825e-11,1.5281205e-10\n",
      "Iteration 44565: loss = 5.3886992e-11,1.5281133e-10\n",
      "Iteration 44570: loss = 5.388356e-11,1.5281065e-10\n",
      "Iteration 44575: loss = 5.3878974e-11,1.528099e-10\n",
      "Iteration 44580: loss = 5.3875373e-11,1.5280915e-10\n",
      "Iteration 44585: loss = 5.3870755e-11,1.5280825e-10\n",
      "Iteration 44590: loss = 5.3867227e-11,1.528075e-10\n",
      "Iteration 44595: loss = 5.386282e-11,1.5280664e-10\n",
      "Iteration 44600: loss = 5.385818e-11,1.5280593e-10\n",
      "Iteration 44605: loss = 5.38542e-11,1.5280496e-10\n",
      "Iteration 44610: loss = 5.384954e-11,1.5280413e-10\n",
      "Iteration 44615: loss = 5.3846257e-11,1.5280331e-10\n",
      "Iteration 44620: loss = 5.3841664e-11,1.5280258e-10\n",
      "Iteration 44625: loss = 5.383831e-11,1.5280169e-10\n",
      "Iteration 44630: loss = 5.383328e-11,1.5280097e-10\n",
      "Iteration 44635: loss = 5.3828653e-11,1.5280008e-10\n",
      "Iteration 44640: loss = 5.3825323e-11,1.5279959e-10\n",
      "Iteration 44645: loss = 5.3820778e-11,1.527988e-10\n",
      "Iteration 44650: loss = 5.3817575e-11,1.5279784e-10\n",
      "Iteration 44655: loss = 5.3813093e-11,1.5279696e-10\n",
      "Iteration 44660: loss = 5.3808152e-11,1.5279614e-10\n",
      "Iteration 44665: loss = 5.3804877e-11,1.5279525e-10\n",
      "Iteration 44670: loss = 5.380059e-11,1.5279432e-10\n",
      "Iteration 44675: loss = 5.3797244e-11,1.5279349e-10\n",
      "Iteration 44680: loss = 5.3792634e-11,1.5279253e-10\n",
      "Iteration 44685: loss = 5.3789067e-11,1.5279196e-10\n",
      "Iteration 44690: loss = 5.3784536e-11,1.5279086e-10\n",
      "Iteration 44695: loss = 5.3780275e-11,1.5279011e-10\n",
      "Iteration 44700: loss = 5.377719e-11,1.5278906e-10\n",
      "Iteration 44705: loss = 5.3773114e-11,1.5278792e-10\n",
      "Iteration 44710: loss = 5.3769808e-11,1.5278688e-10\n",
      "Iteration 44715: loss = 5.376594e-11,1.527858e-10\n",
      "Iteration 44720: loss = 5.3762928e-11,1.5278455e-10\n",
      "Iteration 44725: loss = 5.3758973e-11,1.5278344e-10\n",
      "Iteration 44730: loss = 5.375494e-11,1.5278237e-10\n",
      "Iteration 44735: loss = 5.375181e-11,1.5278126e-10\n",
      "Iteration 44740: loss = 5.3747656e-11,1.5278001e-10\n",
      "Iteration 44745: loss = 5.374513e-11,1.5277898e-10\n",
      "Iteration 44750: loss = 5.3741112e-11,1.5277782e-10\n",
      "Iteration 44755: loss = 5.3738427e-11,1.5277675e-10\n",
      "Iteration 44760: loss = 5.373382e-11,1.5277556e-10\n",
      "Iteration 44765: loss = 5.3729892e-11,1.5277443e-10\n",
      "Iteration 44770: loss = 5.3727134e-11,1.5277339e-10\n",
      "Iteration 44775: loss = 5.3723196e-11,1.5277224e-10\n",
      "Iteration 44780: loss = 5.37204e-11,1.5277112e-10\n",
      "Iteration 44785: loss = 5.371596e-11,1.5277e-10\n",
      "Iteration 44790: loss = 5.3713384e-11,1.5276888e-10\n",
      "Iteration 44795: loss = 5.3709173e-11,1.5276783e-10\n",
      "Iteration 44800: loss = 5.3705224e-11,1.5276658e-10\n",
      "Iteration 44805: loss = 5.3702515e-11,1.5276548e-10\n",
      "Iteration 44810: loss = 5.369817e-11,1.5276432e-10\n",
      "Iteration 44815: loss = 5.369544e-11,1.5276322e-10\n",
      "Iteration 44820: loss = 5.3691423e-11,1.5276225e-10\n",
      "Iteration 44825: loss = 5.3688724e-11,1.52761e-10\n",
      "Iteration 44830: loss = 5.368474e-11,1.5275985e-10\n",
      "Iteration 44835: loss = 5.36806e-11,1.5275854e-10\n",
      "Iteration 44840: loss = 5.3678048e-11,1.5275718e-10\n",
      "Iteration 44845: loss = 5.367444e-11,1.5275592e-10\n",
      "Iteration 44850: loss = 5.3671862e-11,1.5275463e-10\n",
      "Iteration 44855: loss = 5.3668264e-11,1.5275334e-10\n",
      "Iteration 44860: loss = 5.3665145e-11,1.5275221e-10\n",
      "Iteration 44865: loss = 5.3658956e-11,1.5275259e-10\n",
      "Iteration 44870: loss = 5.3652888e-11,1.5275264e-10\n",
      "Iteration 44875: loss = 5.36499e-11,1.5275162e-10\n",
      "Iteration 44880: loss = 5.3645755e-11,1.5275049e-10\n",
      "Iteration 44885: loss = 5.364144e-11,1.527503e-10\n",
      "Iteration 44890: loss = 5.363572e-11,1.5275001e-10\n",
      "Iteration 44895: loss = 5.3632283e-11,1.5274923e-10\n",
      "Iteration 44900: loss = 5.362627e-11,1.5274958e-10\n",
      "Iteration 44905: loss = 5.3619765e-11,1.5274994e-10\n",
      "Iteration 44910: loss = 5.3614512e-11,1.527504e-10\n",
      "Iteration 44915: loss = 5.3613707e-11,1.5274751e-10\n",
      "Iteration 44920: loss = 5.3612226e-11,1.5274466e-10\n",
      "Iteration 44925: loss = 5.3608302e-11,1.527444e-10\n",
      "Iteration 44930: loss = 5.36019e-11,1.5274468e-10\n",
      "Iteration 44935: loss = 5.359702e-11,1.5274497e-10\n",
      "Iteration 44940: loss = 5.3590268e-11,1.5274539e-10\n",
      "Iteration 44945: loss = 5.358412e-11,1.5274548e-10\n",
      "Iteration 44950: loss = 5.3578916e-11,1.5274583e-10\n",
      "Iteration 44955: loss = 5.3572632e-11,1.5274622e-10\n",
      "Iteration 44960: loss = 5.3567827e-11,1.5274645e-10\n",
      "Iteration 44965: loss = 5.3561308e-11,1.527465e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44970: loss = 5.3555202e-11,1.5274661e-10\n",
      "Iteration 44975: loss = 5.355048e-11,1.5274684e-10\n",
      "Iteration 44980: loss = 5.354446e-11,1.527468e-10\n",
      "Iteration 44985: loss = 5.3539884e-11,1.527468e-10\n",
      "Iteration 44990: loss = 5.353368e-11,1.5274686e-10\n",
      "Iteration 44995: loss = 5.352878e-11,1.5274716e-10\n",
      "Iteration 45000: loss = 5.3525028e-11,1.527459e-10\n",
      "Iteration 45005: loss = 5.3525035e-11,1.5274279e-10\n",
      "Iteration 45010: loss = 5.3523713e-11,1.5274e-10\n",
      "Iteration 45015: loss = 5.3523307e-11,1.5273699e-10\n",
      "Iteration 45020: loss = 5.3523266e-11,1.5273409e-10\n",
      "Iteration 45025: loss = 5.3523103e-11,1.5273122e-10\n",
      "Iteration 45030: loss = 5.3522065e-11,1.5272801e-10\n",
      "Iteration 45035: loss = 5.352195e-11,1.527252e-10\n",
      "Iteration 45040: loss = 5.3521843e-11,1.5272222e-10\n",
      "Iteration 45045: loss = 5.3520216e-11,1.5271992e-10\n",
      "Iteration 45050: loss = 5.351254e-11,1.5272088e-10\n",
      "Iteration 45055: loss = 5.350126e-11,1.5272508e-10\n",
      "Iteration 45060: loss = 5.3488745e-11,1.5272913e-10\n",
      "Iteration 45065: loss = 5.265796e-11,1.5273345e-10\n",
      "Iteration 45070: loss = 5.2649742e-11,1.5273491e-10\n",
      "Iteration 45075: loss = 5.2642214e-11,1.5273653e-10\n",
      "Iteration 45080: loss = 5.263472e-11,1.5273793e-10\n",
      "Iteration 45085: loss = 5.2627014e-11,1.5274004e-10\n",
      "Iteration 45090: loss = 5.261815e-11,1.5274171e-10\n",
      "Iteration 45095: loss = 5.2610142e-11,1.5274343e-10\n",
      "Iteration 45100: loss = 5.2602398e-11,1.5274493e-10\n",
      "Iteration 45105: loss = 5.2595168e-11,1.5274633e-10\n",
      "Iteration 45110: loss = 5.258673e-11,1.5274801e-10\n",
      "Iteration 45115: loss = 5.2579496e-11,1.5274942e-10\n",
      "Iteration 45120: loss = 5.257354e-11,1.5274995e-10\n",
      "Iteration 45125: loss = 5.2567856e-11,1.5275049e-10\n",
      "Iteration 45130: loss = 5.2561216e-11,1.5275094e-10\n",
      "Iteration 45135: loss = 5.25557e-11,1.527515e-10\n",
      "Iteration 45140: loss = 5.255035e-11,1.527517e-10\n",
      "Iteration 45145: loss = 5.1735394e-11,1.5275213e-10\n",
      "Iteration 45150: loss = 5.1728594e-11,1.5275244e-10\n",
      "Iteration 45155: loss = 5.17225e-11,1.52753e-10\n",
      "Iteration 45160: loss = 5.171658e-11,1.5275342e-10\n",
      "Iteration 45165: loss = 5.171048e-11,1.5275424e-10\n",
      "Iteration 45170: loss = 5.1704432e-11,1.5275509e-10\n",
      "Iteration 45175: loss = 5.1697188e-11,1.5275556e-10\n",
      "Iteration 45180: loss = 5.1691217e-11,1.5275615e-10\n",
      "Iteration 45185: loss = 5.1685753e-11,1.5275685e-10\n",
      "Iteration 45190: loss = 5.16798e-11,1.5275785e-10\n",
      "Iteration 45195: loss = 5.167292e-11,1.5275828e-10\n",
      "Iteration 45200: loss = 5.1667115e-11,1.5275893e-10\n",
      "Iteration 45205: loss = 5.1661408e-11,1.5275922e-10\n",
      "Iteration 45210: loss = 5.1655607e-11,1.5275986e-10\n",
      "Iteration 45215: loss = 5.1648855e-11,1.5276039e-10\n",
      "Iteration 45220: loss = 5.1643495e-11,1.5276114e-10\n",
      "Iteration 45225: loss = 5.163806e-11,1.5276154e-10\n",
      "Iteration 45230: loss = 5.16326e-11,1.5276175e-10\n",
      "Iteration 45235: loss = 5.1625915e-11,1.5276218e-10\n",
      "Iteration 45240: loss = 5.1620444e-11,1.5276247e-10\n",
      "Iteration 45245: loss = 5.16152e-11,1.5276284e-10\n",
      "Iteration 45250: loss = 5.160976e-11,1.5276305e-10\n",
      "Iteration 45255: loss = 5.1603433e-11,1.5276325e-10\n",
      "Iteration 45260: loss = 5.1598864e-11,1.5276354e-10\n",
      "Iteration 45265: loss = 5.159395e-11,1.5276323e-10\n",
      "Iteration 45270: loss = 5.1588747e-11,1.5276354e-10\n",
      "Iteration 45275: loss = 5.1581947e-11,1.5276398e-10\n",
      "Iteration 45280: loss = 5.157645e-11,1.5276455e-10\n",
      "Iteration 45285: loss = 5.157091e-11,1.5276487e-10\n",
      "Iteration 45290: loss = 5.1565335e-11,1.5276518e-10\n",
      "Iteration 45295: loss = 5.1558854e-11,1.5276588e-10\n",
      "Iteration 45300: loss = 5.1553217e-11,1.5276654e-10\n",
      "Iteration 45305: loss = 5.1547613e-11,1.5276685e-10\n",
      "Iteration 45310: loss = 5.1542163e-11,1.5276741e-10\n",
      "Iteration 45315: loss = 5.153539e-11,1.5276769e-10\n",
      "Iteration 45320: loss = 5.1529864e-11,1.5276817e-10\n",
      "Iteration 45325: loss = 5.1524604e-11,1.5276835e-10\n",
      "Iteration 45330: loss = 5.151939e-11,1.5276845e-10\n",
      "Iteration 45335: loss = 5.151346e-11,1.5276892e-10\n",
      "Iteration 45340: loss = 5.1508298e-11,1.527691e-10\n",
      "Iteration 45345: loss = 5.1503274e-11,1.527693e-10\n",
      "Iteration 45350: loss = 5.1498358e-11,1.5276931e-10\n",
      "Iteration 45355: loss = 5.149216e-11,1.527694e-10\n",
      "Iteration 45360: loss = 5.1487047e-11,1.5276949e-10\n",
      "Iteration 45365: loss = 5.14821e-11,1.5276963e-10\n",
      "Iteration 45370: loss = 5.1477825e-11,1.5276967e-10\n",
      "Iteration 45375: loss = 5.147294e-11,1.5276969e-10\n",
      "Iteration 45380: loss = 5.1466942e-11,1.5276956e-10\n",
      "Iteration 45385: loss = 5.1462046e-11,1.5276948e-10\n",
      "Iteration 45390: loss = 5.145766e-11,1.5276946e-10\n",
      "Iteration 45395: loss = 5.1452998e-11,1.5276938e-10\n",
      "Iteration 45400: loss = 5.1447277e-11,1.527692e-10\n",
      "Iteration 45405: loss = 5.1443374e-11,1.5276908e-10\n",
      "Iteration 45410: loss = 5.1438864e-11,1.5276895e-10\n",
      "Iteration 45415: loss = 5.143437e-11,1.5276849e-10\n",
      "Iteration 45420: loss = 5.1428917e-11,1.5276812e-10\n",
      "Iteration 45425: loss = 5.1424753e-11,1.5276765e-10\n",
      "Iteration 45430: loss = 5.1420614e-11,1.5276731e-10\n",
      "Iteration 45435: loss = 5.1416652e-11,1.5276691e-10\n",
      "Iteration 45440: loss = 5.141117e-11,1.5276641e-10\n",
      "Iteration 45445: loss = 5.1407587e-11,1.5276602e-10\n",
      "Iteration 45450: loss = 5.1403593e-11,1.5276552e-10\n",
      "Iteration 45455: loss = 5.1399274e-11,1.5276515e-10\n",
      "Iteration 45460: loss = 5.139406e-11,1.5276475e-10\n",
      "Iteration 45465: loss = 5.1388168e-11,1.5276552e-10\n",
      "Iteration 45470: loss = 5.1381177e-11,1.5276708e-10\n",
      "Iteration 45475: loss = 5.137411e-11,1.527683e-10\n",
      "Iteration 45480: loss = 5.136629e-11,1.5276974e-10\n",
      "Iteration 45485: loss = 5.1359098e-11,1.527713e-10\n",
      "Iteration 45490: loss = 5.1352134e-11,1.5277249e-10\n",
      "Iteration 45495: loss = 5.1344946e-11,1.5277396e-10\n",
      "Iteration 45500: loss = 5.1337077e-11,1.5277545e-10\n",
      "Iteration 45505: loss = 5.133068e-11,1.527768e-10\n",
      "Iteration 45510: loss = 5.1324223e-11,1.527779e-10\n",
      "Iteration 45515: loss = 5.1317766e-11,1.5277855e-10\n",
      "Iteration 45520: loss = 5.1310595e-11,1.5277962e-10\n",
      "Iteration 45525: loss = 5.1304506e-11,1.527806e-10\n",
      "Iteration 45530: loss = 5.1298174e-11,1.527817e-10\n",
      "Iteration 45535: loss = 5.129197e-11,1.527826e-10\n",
      "Iteration 45540: loss = 5.1284615e-11,1.5278337e-10\n",
      "Iteration 45545: loss = 5.12784e-11,1.5278422e-10\n",
      "Iteration 45550: loss = 5.127226e-11,1.5278505e-10\n",
      "Iteration 45555: loss = 5.126687e-11,1.5278569e-10\n",
      "Iteration 45560: loss = 5.126104e-11,1.5278658e-10\n",
      "Iteration 45565: loss = 5.1253956e-11,1.5278737e-10\n",
      "Iteration 45570: loss = 5.1248072e-11,1.5278799e-10\n",
      "Iteration 45575: loss = 5.1242215e-11,1.5278889e-10\n",
      "Iteration 45580: loss = 5.123624e-11,1.5278974e-10\n",
      "Iteration 45585: loss = 5.1229253e-11,1.5279032e-10\n",
      "Iteration 45590: loss = 5.1223813e-11,1.5279097e-10\n",
      "Iteration 45595: loss = 5.1218436e-11,1.5279157e-10\n",
      "Iteration 45600: loss = 5.1212867e-11,1.5279202e-10\n",
      "Iteration 45605: loss = 5.1206164e-11,1.5279247e-10\n",
      "Iteration 45610: loss = 5.12006e-11,1.5279307e-10\n",
      "Iteration 45615: loss = 5.119505e-11,1.5279336e-10\n",
      "Iteration 45620: loss = 5.1189667e-11,1.5279393e-10\n",
      "Iteration 45625: loss = 5.1183013e-11,1.5279464e-10\n",
      "Iteration 45630: loss = 5.1177954e-11,1.5279517e-10\n",
      "Iteration 45635: loss = 5.1172153e-11,1.5279572e-10\n",
      "Iteration 45640: loss = 5.1166373e-11,1.5279651e-10\n",
      "Iteration 45645: loss = 5.1159382e-11,1.5279722e-10\n",
      "Iteration 45650: loss = 5.1153404e-11,1.5279791e-10\n",
      "Iteration 45655: loss = 5.1147666e-11,1.5279883e-10\n",
      "Iteration 45660: loss = 5.1141685e-11,1.527992e-10\n",
      "Iteration 45665: loss = 5.1134815e-11,1.5280001e-10\n",
      "Iteration 45670: loss = 5.1129333e-11,1.5280079e-10\n",
      "Iteration 45675: loss = 5.1123505e-11,1.5280165e-10\n",
      "Iteration 45680: loss = 5.1117808e-11,1.5280223e-10\n",
      "Iteration 45685: loss = 5.111261e-11,1.5280184e-10\n",
      "Iteration 45690: loss = 5.1108673e-11,1.5280127e-10\n",
      "Iteration 45695: loss = 5.1104797e-11,1.5280079e-10\n",
      "Iteration 45700: loss = 5.1101304e-11,1.528001e-10\n",
      "Iteration 45705: loss = 5.109616e-11,1.527996e-10\n",
      "Iteration 45710: loss = 5.109274e-11,1.527992e-10\n",
      "Iteration 45715: loss = 5.108844e-11,1.5279904e-10\n",
      "Iteration 45720: loss = 5.108411e-11,1.5279855e-10\n",
      "Iteration 45725: loss = 5.0277133e-11,1.5279882e-10\n",
      "Iteration 45730: loss = 5.0271922e-11,1.5279912e-10\n",
      "Iteration 45735: loss = 5.0269844e-11,1.5279732e-10\n",
      "Iteration 45740: loss = 5.0267745e-11,1.5279562e-10\n",
      "Iteration 45745: loss = 5.0266947e-11,1.5279311e-10\n",
      "Iteration 45750: loss = 5.0266e-11,1.5279045e-10\n",
      "Iteration 45755: loss = 5.026657e-11,1.5278788e-10\n",
      "Iteration 45760: loss = 5.0265656e-11,1.5278509e-10\n",
      "Iteration 45765: loss = 5.0265212e-11,1.5278243e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 45770: loss = 5.0264206e-11,1.5277979e-10\n",
      "Iteration 45775: loss = 5.0263845e-11,1.5277712e-10\n",
      "Iteration 45780: loss = 5.0262978e-11,1.5277438e-10\n",
      "Iteration 45785: loss = 5.026223e-11,1.5277166e-10\n",
      "Iteration 45790: loss = 5.026145e-11,1.5276914e-10\n",
      "Iteration 45795: loss = 5.0260896e-11,1.5276658e-10\n",
      "Iteration 45800: loss = 5.0260185e-11,1.5276362e-10\n",
      "Iteration 45805: loss = 5.0259536e-11,1.52761e-10\n",
      "Iteration 45810: loss = 5.025862e-11,1.5275814e-10\n",
      "Iteration 45815: loss = 5.0259425e-11,1.5275552e-10\n",
      "Iteration 45820: loss = 5.0258454e-11,1.5275274e-10\n",
      "Iteration 45825: loss = 5.0257642e-11,1.5274997e-10\n",
      "Iteration 45830: loss = 5.025717e-11,1.5274722e-10\n",
      "Iteration 45835: loss = 5.0256504e-11,1.5274448e-10\n",
      "Iteration 45840: loss = 5.025609e-11,1.5274178e-10\n",
      "Iteration 45845: loss = 5.025526e-11,1.5273882e-10\n",
      "Iteration 45850: loss = 5.025519e-11,1.527361e-10\n",
      "Iteration 45855: loss = 5.025459e-11,1.52733e-10\n",
      "Iteration 45860: loss = 5.0254707e-11,1.5273004e-10\n",
      "Iteration 45865: loss = 5.0254304e-11,1.5272718e-10\n",
      "Iteration 45870: loss = 5.025416e-11,1.527241e-10\n",
      "Iteration 45875: loss = 5.025483e-11,1.5272124e-10\n",
      "Iteration 45880: loss = 5.02549e-11,1.5271807e-10\n",
      "Iteration 45885: loss = 5.0254256e-11,1.5271517e-10\n",
      "Iteration 45890: loss = 5.0253992e-11,1.5271218e-10\n",
      "Iteration 45895: loss = 5.0253898e-11,1.527091e-10\n",
      "Iteration 45900: loss = 5.0253458e-11,1.5270604e-10\n",
      "Iteration 45905: loss = 5.0253437e-11,1.5270311e-10\n",
      "Iteration 45910: loss = 5.0252944e-11,1.5270013e-10\n",
      "Iteration 45915: loss = 5.025291e-11,1.5269694e-10\n",
      "Iteration 45920: loss = 5.0252514e-11,1.5269386e-10\n",
      "Iteration 45925: loss = 5.025241e-11,1.5269096e-10\n",
      "Iteration 45930: loss = 5.0253243e-11,1.5268804e-10\n",
      "Iteration 45935: loss = 5.0253104e-11,1.52685e-10\n",
      "Iteration 45940: loss = 5.025273e-11,1.5268184e-10\n",
      "Iteration 45945: loss = 5.0252635e-11,1.5267895e-10\n",
      "Iteration 45950: loss = 5.025216e-11,1.5267604e-10\n",
      "Iteration 45955: loss = 5.025172e-11,1.526731e-10\n",
      "Iteration 45960: loss = 5.0251706e-11,1.5267021e-10\n",
      "Iteration 45965: loss = 5.0251345e-11,1.5266717e-10\n",
      "Iteration 45970: loss = 5.0251577e-11,1.526639e-10\n",
      "Iteration 45975: loss = 5.02513e-11,1.526605e-10\n",
      "Iteration 45980: loss = 5.025143e-11,1.5265753e-10\n",
      "Iteration 45985: loss = 5.025122e-11,1.5265453e-10\n",
      "Iteration 45990: loss = 5.0252608e-11,1.5265153e-10\n",
      "Iteration 45995: loss = 5.0252257e-11,1.526485e-10\n",
      "Iteration 46000: loss = 5.025243e-11,1.5264524e-10\n",
      "Iteration 46005: loss = 5.0252084e-11,1.5264226e-10\n",
      "Iteration 46010: loss = 5.0252427e-11,1.5263915e-10\n",
      "Iteration 46015: loss = 5.0251955e-11,1.5263608e-10\n",
      "Iteration 46020: loss = 5.0252247e-11,1.5263311e-10\n",
      "Iteration 46025: loss = 5.025184e-11,1.5262985e-10\n",
      "Iteration 46030: loss = 5.025153e-11,1.5262669e-10\n",
      "Iteration 46035: loss = 5.0251716e-11,1.5262377e-10\n",
      "Iteration 46040: loss = 5.025146e-11,1.5262064e-10\n",
      "Iteration 46045: loss = 5.0251487e-11,1.5261727e-10\n",
      "Iteration 46050: loss = 5.0252486e-11,1.5261434e-10\n",
      "Iteration 46055: loss = 5.025272e-11,1.5261134e-10\n",
      "Iteration 46060: loss = 5.0252358e-11,1.5260815e-10\n",
      "Iteration 46065: loss = 5.0252563e-11,1.5260514e-10\n",
      "Iteration 46070: loss = 5.0252313e-11,1.5260215e-10\n",
      "Iteration 46075: loss = 5.025232e-11,1.5259896e-10\n",
      "Iteration 46080: loss = 5.0252007e-11,1.52596e-10\n",
      "Iteration 46085: loss = 5.0252e-11,1.5259272e-10\n",
      "Iteration 46090: loss = 5.02518e-11,1.5258958e-10\n",
      "Iteration 46095: loss = 5.0251556e-11,1.5258672e-10\n",
      "Iteration 46100: loss = 5.025178e-11,1.5258346e-10\n",
      "Iteration 46105: loss = 5.0251466e-11,1.5258028e-10\n",
      "Iteration 46110: loss = 5.0252708e-11,1.5257727e-10\n",
      "Iteration 46115: loss = 5.0252413e-11,1.5257429e-10\n",
      "Iteration 46120: loss = 5.0252417e-11,1.5257115e-10\n",
      "Iteration 46125: loss = 5.0252153e-11,1.5256807e-10\n",
      "Iteration 46130: loss = 5.025211e-11,1.5256489e-10\n",
      "Iteration 46135: loss = 5.0252056e-11,1.5256191e-10\n",
      "Iteration 46140: loss = 5.0252105e-11,1.5255879e-10\n",
      "Iteration 46145: loss = 5.0251674e-11,1.5255588e-10\n",
      "Iteration 46150: loss = 5.0251792e-11,1.5255286e-10\n",
      "Iteration 46155: loss = 5.02514e-11,1.5254974e-10\n",
      "Iteration 46160: loss = 5.0251105e-11,1.5254653e-10\n",
      "Iteration 46165: loss = 5.0252583e-11,1.5254374e-10\n",
      "Iteration 46170: loss = 5.0252014e-11,1.5254034e-10\n",
      "Iteration 46175: loss = 5.0252153e-11,1.5253726e-10\n",
      "Iteration 46180: loss = 5.0251966e-11,1.525342e-10\n",
      "Iteration 46185: loss = 5.025212e-11,1.5253106e-10\n",
      "Iteration 46190: loss = 5.0251678e-11,1.5252806e-10\n",
      "Iteration 46195: loss = 5.0251903e-11,1.5252509e-10\n",
      "Iteration 46200: loss = 5.0251494e-11,1.5252202e-10\n",
      "Iteration 46205: loss = 5.0251556e-11,1.5251904e-10\n",
      "Iteration 46210: loss = 5.025116e-11,1.5251614e-10\n",
      "Iteration 46215: loss = 5.0251373e-11,1.5251293e-10\n",
      "Iteration 46220: loss = 5.025084e-11,1.5251e-10\n",
      "Iteration 46225: loss = 5.0252132e-11,1.5250685e-10\n",
      "Iteration 46230: loss = 5.0251803e-11,1.5250384e-10\n",
      "Iteration 46235: loss = 5.025147e-11,1.5250108e-10\n",
      "Iteration 46240: loss = 5.0251556e-11,1.524976e-10\n",
      "Iteration 46245: loss = 5.0251386e-11,1.5249448e-10\n",
      "Iteration 46250: loss = 5.025129e-11,1.5249166e-10\n",
      "Iteration 46255: loss = 5.0250956e-11,1.5248866e-10\n",
      "Iteration 46260: loss = 5.025111e-11,1.5248547e-10\n",
      "Iteration 46265: loss = 5.0250554e-11,1.5248247e-10\n",
      "Iteration 46270: loss = 5.0250703e-11,1.5247949e-10\n",
      "Iteration 46275: loss = 5.0250554e-11,1.5247643e-10\n",
      "Iteration 46280: loss = 5.0250613e-11,1.524734e-10\n",
      "Iteration 46285: loss = 5.0251327e-11,1.5247031e-10\n",
      "Iteration 46290: loss = 5.0251362e-11,1.5246733e-10\n",
      "Iteration 46295: loss = 5.0251248e-11,1.5246399e-10\n",
      "Iteration 46300: loss = 5.0250762e-11,1.5246107e-10\n",
      "Iteration 46305: loss = 5.0250745e-11,1.5245799e-10\n",
      "Iteration 46310: loss = 5.0250557e-11,1.524551e-10\n",
      "Iteration 46315: loss = 5.025055e-11,1.5245184e-10\n",
      "Iteration 46320: loss = 5.0250207e-11,1.5244876e-10\n",
      "Iteration 46325: loss = 5.025032e-11,1.5244583e-10\n",
      "Iteration 46330: loss = 5.0249915e-11,1.5244284e-10\n",
      "Iteration 46335: loss = 5.025011e-11,1.5243985e-10\n",
      "Iteration 46340: loss = 5.0250904e-11,1.5243665e-10\n",
      "Iteration 46345: loss = 5.0250932e-11,1.5243348e-10\n",
      "Iteration 46350: loss = 5.0250665e-11,1.5243053e-10\n",
      "Iteration 46355: loss = 5.025085e-11,1.5242758e-10\n",
      "Iteration 46360: loss = 5.0250425e-11,1.5242466e-10\n",
      "Iteration 46365: loss = 5.025042e-11,1.5242159e-10\n",
      "Iteration 46370: loss = 5.025028e-11,1.5241844e-10\n",
      "Iteration 46375: loss = 5.0249985e-11,1.5241527e-10\n",
      "Iteration 46380: loss = 5.0250092e-11,1.5241204e-10\n",
      "Iteration 46385: loss = 5.0249627e-11,1.524091e-10\n",
      "Iteration 46390: loss = 5.024968e-11,1.5240606e-10\n",
      "Iteration 46395: loss = 5.024914e-11,1.5240323e-10\n",
      "Iteration 46400: loss = 5.025047e-11,1.5240002e-10\n",
      "Iteration 46405: loss = 5.025017e-11,1.5239705e-10\n",
      "Iteration 46410: loss = 5.0250137e-11,1.5239396e-10\n",
      "Iteration 46415: loss = 5.025001e-11,1.5239093e-10\n",
      "Iteration 46420: loss = 5.0250044e-11,1.5238776e-10\n",
      "Iteration 46425: loss = 5.02498e-11,1.5238481e-10\n",
      "Iteration 46430: loss = 5.024979e-11,1.5238164e-10\n",
      "Iteration 46435: loss = 5.0249527e-11,1.523788e-10\n",
      "Iteration 46440: loss = 5.024901e-11,1.5237563e-10\n",
      "Iteration 46445: loss = 5.0249253e-11,1.523727e-10\n",
      "Iteration 46450: loss = 5.024912e-11,1.5236956e-10\n",
      "Iteration 46455: loss = 5.024885e-11,1.5236656e-10\n",
      "Iteration 46460: loss = 5.0249693e-11,1.5236346e-10\n",
      "Iteration 46465: loss = 5.024983e-11,1.5236024e-10\n",
      "Iteration 46470: loss = 5.0249516e-11,1.5235724e-10\n",
      "Iteration 46475: loss = 5.0249582e-11,1.5235432e-10\n",
      "Iteration 46480: loss = 5.0249385e-11,1.5235102e-10\n",
      "Iteration 46485: loss = 5.0249277e-11,1.5234805e-10\n",
      "Iteration 46490: loss = 5.0248836e-11,1.5234519e-10\n",
      "Iteration 46495: loss = 5.0249083e-11,1.5234206e-10\n",
      "Iteration 46500: loss = 5.0248857e-11,1.5233902e-10\n",
      "Iteration 46505: loss = 5.0248514e-11,1.52336e-10\n",
      "Iteration 46510: loss = 5.0248472e-11,1.5233273e-10\n",
      "Iteration 46515: loss = 5.024942e-11,1.5232977e-10\n",
      "Iteration 46520: loss = 5.024937e-11,1.523269e-10\n",
      "Iteration 46525: loss = 5.0249027e-11,1.5232379e-10\n",
      "Iteration 46530: loss = 5.0249305e-11,1.5232078e-10\n",
      "Iteration 46535: loss = 5.0248763e-11,1.523178e-10\n",
      "Iteration 46540: loss = 5.0248788e-11,1.523148e-10\n",
      "Iteration 46545: loss = 5.0248538e-11,1.5231169e-10\n",
      "Iteration 46550: loss = 5.0248677e-11,1.5230864e-10\n",
      "Iteration 46555: loss = 5.0248156e-11,1.5230564e-10\n",
      "Iteration 46560: loss = 5.0248306e-11,1.5230259e-10\n",
      "Iteration 46565: loss = 5.0247934e-11,1.5229945e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46570: loss = 5.0247796e-11,1.5229654e-10\n",
      "Iteration 46575: loss = 5.0248788e-11,1.5229357e-10\n",
      "Iteration 46580: loss = 5.0248472e-11,1.5229057e-10\n",
      "Iteration 46585: loss = 5.0248593e-11,1.522873e-10\n",
      "Iteration 46590: loss = 5.0248205e-11,1.5228417e-10\n",
      "Iteration 46595: loss = 5.0248288e-11,1.5228131e-10\n",
      "Iteration 46600: loss = 5.02477e-11,1.522782e-10\n",
      "Iteration 46605: loss = 5.0247934e-11,1.5227518e-10\n",
      "Iteration 46610: loss = 5.024743e-11,1.5227228e-10\n",
      "Iteration 46615: loss = 5.02476e-11,1.5226924e-10\n",
      "Iteration 46620: loss = 5.0247445e-11,1.5226598e-10\n",
      "Iteration 46625: loss = 5.0247553e-11,1.5226297e-10\n",
      "Iteration 46630: loss = 5.024721e-11,1.5225993e-10\n",
      "Iteration 46635: loss = 5.02485e-11,1.5225689e-10\n",
      "Iteration 46640: loss = 5.0248042e-11,1.5225421e-10\n",
      "Iteration 46645: loss = 5.0247657e-11,1.5225088e-10\n",
      "Iteration 46650: loss = 5.0247865e-11,1.5224795e-10\n",
      "Iteration 46655: loss = 5.0247553e-11,1.5224483e-10\n",
      "Iteration 46660: loss = 5.0247622e-11,1.5224183e-10\n",
      "Iteration 46665: loss = 5.024726e-11,1.522388e-10\n",
      "Iteration 46670: loss = 5.024721e-11,1.5223586e-10\n",
      "Iteration 46675: loss = 5.0246796e-11,1.5223281e-10\n",
      "Iteration 46680: loss = 5.0246984e-11,1.5222967e-10\n",
      "Iteration 46685: loss = 5.0246696e-11,1.5222668e-10\n",
      "Iteration 46690: loss = 5.0246623e-11,1.5222375e-10\n",
      "Iteration 46695: loss = 5.024749e-11,1.5222083e-10\n",
      "Iteration 46700: loss = 5.024753e-11,1.5221772e-10\n",
      "Iteration 46705: loss = 5.024733e-11,1.5221466e-10\n",
      "Iteration 46710: loss = 5.0246852e-11,1.5221173e-10\n",
      "Iteration 46715: loss = 5.0246737e-11,1.522087e-10\n",
      "Iteration 46720: loss = 5.0246567e-11,1.5220573e-10\n",
      "Iteration 46725: loss = 5.0246668e-11,1.5220261e-10\n",
      "Iteration 46730: loss = 5.0246196e-11,1.5219946e-10\n",
      "Iteration 46735: loss = 5.0246207e-11,1.5219652e-10\n",
      "Iteration 46740: loss = 5.0245783e-11,1.521936e-10\n",
      "Iteration 46745: loss = 5.024592e-11,1.5219054e-10\n",
      "Iteration 46750: loss = 5.0246824e-11,1.5218746e-10\n",
      "Iteration 46755: loss = 5.0246845e-11,1.5218449e-10\n",
      "Iteration 46760: loss = 5.024643e-11,1.5218145e-10\n",
      "Iteration 46765: loss = 5.024643e-11,1.5217848e-10\n",
      "Iteration 46770: loss = 5.0245905e-11,1.5217544e-10\n",
      "Iteration 46775: loss = 5.0245915e-11,1.521724e-10\n",
      "Iteration 46780: loss = 5.0245808e-11,1.5216949e-10\n",
      "Iteration 46785: loss = 5.0245547e-11,1.5216622e-10\n",
      "Iteration 46790: loss = 5.0245506e-11,1.5216318e-10\n",
      "Iteration 46795: loss = 5.024516e-11,1.5216023e-10\n",
      "Iteration 46800: loss = 5.0245235e-11,1.5215715e-10\n",
      "Iteration 46805: loss = 5.02451e-11,1.5215423e-10\n",
      "Iteration 46810: loss = 5.0246234e-11,1.5215135e-10\n",
      "Iteration 46815: loss = 5.0245804e-11,1.5214827e-10\n",
      "Iteration 46820: loss = 5.0245877e-11,1.5214519e-10\n",
      "Iteration 46825: loss = 5.02455e-11,1.5214233e-10\n",
      "Iteration 46830: loss = 5.024543e-11,1.5213916e-10\n",
      "Iteration 46835: loss = 5.024503e-11,1.5213622e-10\n",
      "Iteration 46840: loss = 5.024528e-11,1.5213329e-10\n",
      "Iteration 46845: loss = 5.024492e-11,1.5213013e-10\n",
      "Iteration 46850: loss = 5.0244624e-11,1.5212699e-10\n",
      "Iteration 46855: loss = 5.024473e-11,1.521241e-10\n",
      "Iteration 46860: loss = 5.0244277e-11,1.5212093e-10\n",
      "Iteration 46865: loss = 5.0244378e-11,1.5211794e-10\n",
      "Iteration 46870: loss = 5.024517e-11,1.521149e-10\n",
      "Iteration 46875: loss = 5.0245183e-11,1.5211203e-10\n",
      "Iteration 46880: loss = 5.024498e-11,1.5210884e-10\n",
      "Iteration 46885: loss = 5.0245006e-11,1.5210586e-10\n",
      "Iteration 46890: loss = 5.024467e-11,1.521028e-10\n",
      "Iteration 46895: loss = 5.0244794e-11,1.5209983e-10\n",
      "Iteration 46900: loss = 5.024434e-11,1.5209661e-10\n",
      "Iteration 46905: loss = 5.024441e-11,1.520937e-10\n",
      "Iteration 46910: loss = 5.024431e-11,1.520906e-10\n",
      "Iteration 46915: loss = 5.0244395e-11,1.5208763e-10\n",
      "Iteration 46920: loss = 5.024416e-11,1.5208443e-10\n",
      "Iteration 46925: loss = 5.0245016e-11,1.5208135e-10\n",
      "Iteration 46930: loss = 5.0245103e-11,1.5207831e-10\n",
      "Iteration 46935: loss = 5.024475e-11,1.5207541e-10\n",
      "Iteration 46940: loss = 5.0244964e-11,1.5207231e-10\n",
      "Iteration 46945: loss = 5.0244527e-11,1.5206922e-10\n",
      "Iteration 46950: loss = 5.0244562e-11,1.5206625e-10\n",
      "Iteration 46955: loss = 5.0244222e-11,1.5206313e-10\n",
      "Iteration 46960: loss = 5.024445e-11,1.5206024e-10\n",
      "Iteration 46965: loss = 5.0244208e-11,1.5205717e-10\n",
      "Iteration 46970: loss = 5.024425e-11,1.5205415e-10\n",
      "Iteration 46975: loss = 5.024398e-11,1.5205098e-10\n",
      "Iteration 46980: loss = 5.0244166e-11,1.52048e-10\n",
      "Iteration 46985: loss = 5.024504e-11,1.5204504e-10\n",
      "Iteration 46990: loss = 5.024481e-11,1.52042e-10\n",
      "Iteration 46995: loss = 5.0244846e-11,1.5203862e-10\n",
      "Iteration 47000: loss = 5.024473e-11,1.5203579e-10\n",
      "Iteration 47005: loss = 5.0244902e-11,1.5203266e-10\n",
      "Iteration 47010: loss = 5.024452e-11,1.5202944e-10\n",
      "Iteration 47015: loss = 5.0244656e-11,1.5202618e-10\n",
      "Iteration 47020: loss = 5.0244406e-11,1.5202328e-10\n",
      "Iteration 47025: loss = 5.0244708e-11,1.5201998e-10\n",
      "Iteration 47030: loss = 5.024436e-11,1.520169e-10\n",
      "Iteration 47035: loss = 5.0244465e-11,1.5201387e-10\n",
      "Iteration 47040: loss = 5.0244166e-11,1.5201086e-10\n",
      "Iteration 47045: loss = 5.0245638e-11,1.5200753e-10\n",
      "Iteration 47050: loss = 5.0245263e-11,1.5200445e-10\n",
      "Iteration 47055: loss = 5.024511e-11,1.520013e-10\n",
      "Iteration 47060: loss = 5.0245117e-11,1.5199858e-10\n",
      "Iteration 47065: loss = 5.024492e-11,1.5199535e-10\n",
      "Iteration 47070: loss = 5.0245082e-11,1.5199238e-10\n",
      "Iteration 47075: loss = 5.0244708e-11,1.519893e-10\n",
      "Iteration 47080: loss = 5.0245072e-11,1.519862e-10\n",
      "Iteration 47085: loss = 5.0244805e-11,1.5198304e-10\n",
      "Iteration 47090: loss = 5.0244846e-11,1.5198007e-10\n",
      "Iteration 47095: loss = 5.0244548e-11,1.5197703e-10\n",
      "Iteration 47100: loss = 5.0245873e-11,1.5197396e-10\n",
      "Iteration 47105: loss = 5.0245672e-11,1.5197095e-10\n",
      "Iteration 47110: loss = 5.0245735e-11,1.519678e-10\n",
      "Iteration 47115: loss = 5.0245433e-11,1.5196455e-10\n",
      "Iteration 47120: loss = 5.0245696e-11,1.5196153e-10\n",
      "Iteration 47125: loss = 5.02455e-11,1.5195847e-10\n",
      "Iteration 47130: loss = 5.0245103e-11,1.5195542e-10\n",
      "Iteration 47135: loss = 5.0245367e-11,1.5195246e-10\n",
      "Iteration 47140: loss = 5.0245124e-11,1.5194936e-10\n",
      "Iteration 47145: loss = 5.024516e-11,1.5194634e-10\n",
      "Iteration 47150: loss = 5.0244916e-11,1.5194342e-10\n",
      "Iteration 47155: loss = 5.0245016e-11,1.5194018e-10\n",
      "Iteration 47160: loss = 5.0245846e-11,1.5193724e-10\n",
      "Iteration 47165: loss = 5.0246095e-11,1.5193403e-10\n",
      "Iteration 47170: loss = 5.024575e-11,1.5193083e-10\n",
      "Iteration 47175: loss = 5.02459e-11,1.5192797e-10\n",
      "Iteration 47180: loss = 5.0245655e-11,1.5192486e-10\n",
      "Iteration 47185: loss = 5.0245804e-11,1.5192178e-10\n",
      "Iteration 47190: loss = 5.024557e-11,1.519187e-10\n",
      "Iteration 47195: loss = 5.0245214e-11,1.5191573e-10\n",
      "Iteration 47200: loss = 5.024539e-11,1.5191275e-10\n",
      "Iteration 47205: loss = 5.0245034e-11,1.5190967e-10\n",
      "Iteration 47210: loss = 5.024514e-11,1.5190653e-10\n",
      "Iteration 47215: loss = 5.024486e-11,1.5190346e-10\n",
      "Iteration 47220: loss = 5.0246248e-11,1.5190038e-10\n",
      "Iteration 47225: loss = 5.0245873e-11,1.5189697e-10\n",
      "Iteration 47230: loss = 5.0246137e-11,1.518941e-10\n",
      "Iteration 47235: loss = 5.0245922e-11,1.5189111e-10\n",
      "Iteration 47240: loss = 5.024599e-11,1.5188824e-10\n",
      "Iteration 47245: loss = 5.0245308e-11,1.5188534e-10\n",
      "Iteration 47250: loss = 5.0245207e-11,1.518825e-10\n",
      "Iteration 47255: loss = 5.024486e-11,1.5187943e-10\n",
      "Iteration 47260: loss = 5.024442e-11,1.5187633e-10\n",
      "Iteration 47265: loss = 5.0244423e-11,1.518734e-10\n",
      "Iteration 47270: loss = 5.0243965e-11,1.5187042e-10\n",
      "Iteration 47275: loss = 5.0245294e-11,1.5186744e-10\n",
      "Iteration 47280: loss = 5.024511e-11,1.518645e-10\n",
      "Iteration 47285: loss = 5.0245252e-11,1.5186126e-10\n",
      "Iteration 47290: loss = 5.0244933e-11,1.5185837e-10\n",
      "Iteration 47295: loss = 5.0245027e-11,1.5185528e-10\n",
      "Iteration 47300: loss = 5.0244725e-11,1.5185234e-10\n",
      "Iteration 47305: loss = 5.0244836e-11,1.5184898e-10\n",
      "Iteration 47310: loss = 5.0244628e-11,1.518462e-10\n",
      "Iteration 47315: loss = 5.0244725e-11,1.5184312e-10\n",
      "Iteration 47320: loss = 5.024454e-11,1.5184037e-10\n",
      "Iteration 47325: loss = 5.0244642e-11,1.5183699e-10\n",
      "Iteration 47330: loss = 5.0244267e-11,1.5183375e-10\n",
      "Iteration 47335: loss = 5.024535e-11,1.518309e-10\n",
      "Iteration 47340: loss = 5.0245582e-11,1.5182772e-10\n",
      "Iteration 47345: loss = 5.0245252e-11,1.5182472e-10\n",
      "Iteration 47350: loss = 5.0245294e-11,1.5182165e-10\n",
      "Iteration 47355: loss = 5.0244964e-11,1.5181872e-10\n",
      "Iteration 47360: loss = 5.0245166e-11,1.5181569e-10\n",
      "Iteration 47365: loss = 5.0244853e-11,1.5181266e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 47370: loss = 5.0245072e-11,1.5180945e-10\n",
      "Iteration 47375: loss = 5.0244756e-11,1.518064e-10\n",
      "Iteration 47380: loss = 5.0244767e-11,1.5180353e-10\n",
      "Iteration 47385: loss = 5.0244437e-11,1.5180052e-10\n",
      "Iteration 47390: loss = 5.0244572e-11,1.517977e-10\n",
      "Iteration 47395: loss = 5.0245516e-11,1.517945e-10\n",
      "Iteration 47400: loss = 5.0245294e-11,1.5179125e-10\n",
      "Iteration 47405: loss = 5.0245464e-11,1.5178828e-10\n",
      "Iteration 47410: loss = 5.0245152e-11,1.5178521e-10\n",
      "Iteration 47415: loss = 5.024521e-11,1.517822e-10\n",
      "Iteration 47420: loss = 5.0245117e-11,1.517792e-10\n",
      "Iteration 47425: loss = 5.024527e-11,1.5177609e-10\n",
      "Iteration 47430: loss = 5.0244947e-11,1.5177276e-10\n",
      "Iteration 47435: loss = 5.0245183e-11,1.5176985e-10\n",
      "Iteration 47440: loss = 5.024477e-11,1.517668e-10\n",
      "Iteration 47445: loss = 5.0245003e-11,1.5176346e-10\n",
      "Iteration 47450: loss = 5.0244735e-11,1.5176056e-10\n",
      "Iteration 47455: loss = 5.024618e-11,1.5175758e-10\n",
      "Iteration 47460: loss = 5.0245818e-11,1.5175466e-10\n",
      "Iteration 47465: loss = 5.0246068e-11,1.5175128e-10\n",
      "Iteration 47470: loss = 5.0245835e-11,1.5174834e-10\n",
      "Iteration 47475: loss = 5.024543e-11,1.5174502e-10\n",
      "Iteration 47480: loss = 5.0245513e-11,1.517421e-10\n",
      "Iteration 47485: loss = 5.024521e-11,1.5173927e-10\n",
      "Iteration 47490: loss = 5.024532e-11,1.5173608e-10\n",
      "Iteration 47495: loss = 5.0245228e-11,1.5173317e-10\n",
      "Iteration 47500: loss = 5.0245478e-11,1.5173018e-10\n",
      "Iteration 47505: loss = 5.024488e-11,1.5172709e-10\n",
      "Iteration 47510: loss = 5.0246224e-11,1.5172416e-10\n",
      "Iteration 47515: loss = 5.0246005e-11,1.51721e-10\n",
      "Iteration 47520: loss = 5.0246043e-11,1.5171814e-10\n",
      "Iteration 47525: loss = 5.024589e-11,1.517148e-10\n",
      "Iteration 47530: loss = 5.0246026e-11,1.51712e-10\n",
      "Iteration 47535: loss = 5.0245735e-11,1.5170888e-10\n",
      "Iteration 47540: loss = 5.0245433e-11,1.5170581e-10\n",
      "Iteration 47545: loss = 5.024552e-11,1.5170253e-10\n",
      "Iteration 47550: loss = 5.0245256e-11,1.5169954e-10\n",
      "Iteration 47555: loss = 5.0245395e-11,1.5169666e-10\n",
      "Iteration 47560: loss = 5.0245114e-11,1.5169363e-10\n",
      "Iteration 47565: loss = 5.0245298e-11,1.5169083e-10\n",
      "Iteration 47570: loss = 5.024608e-11,1.5168762e-10\n",
      "Iteration 47575: loss = 5.0246227e-11,1.5168468e-10\n",
      "Iteration 47580: loss = 5.0245887e-11,1.5168174e-10\n",
      "Iteration 47585: loss = 5.024599e-11,1.5167848e-10\n",
      "Iteration 47590: loss = 5.0245776e-11,1.5167534e-10\n",
      "Iteration 47595: loss = 5.0245915e-11,1.5167226e-10\n",
      "Iteration 47600: loss = 5.0245554e-11,1.5166912e-10\n",
      "Iteration 47605: loss = 5.0245252e-11,1.5166593e-10\n",
      "Iteration 47610: loss = 5.024528e-11,1.5166303e-10\n",
      "Iteration 47615: loss = 5.0244753e-11,1.516602e-10\n",
      "Iteration 47620: loss = 5.0244742e-11,1.516572e-10\n",
      "Iteration 47625: loss = 5.0244642e-11,1.5165441e-10\n",
      "Iteration 47630: loss = 5.0245835e-11,1.5165133e-10\n",
      "Iteration 47635: loss = 5.0245596e-11,1.5164832e-10\n",
      "Iteration 47640: loss = 5.0245714e-11,1.5164521e-10\n",
      "Iteration 47645: loss = 5.0245388e-11,1.5164235e-10\n",
      "Iteration 47650: loss = 5.0245558e-11,1.5163916e-10\n",
      "Iteration 47655: loss = 5.0245284e-11,1.5163616e-10\n",
      "Iteration 47660: loss = 5.024531e-11,1.5163293e-10\n",
      "Iteration 47665: loss = 5.024506e-11,1.5163004e-10\n",
      "Iteration 47670: loss = 5.0245138e-11,1.5162704e-10\n",
      "Iteration 47675: loss = 5.0244923e-11,1.5162403e-10\n",
      "Iteration 47680: loss = 5.024464e-11,1.5162095e-10\n",
      "Iteration 47685: loss = 5.0245932e-11,1.5161783e-10\n",
      "Iteration 47690: loss = 5.024578e-11,1.5161478e-10\n",
      "Iteration 47695: loss = 5.024595e-11,1.5161186e-10\n",
      "Iteration 47700: loss = 5.0245585e-11,1.5160885e-10\n",
      "Iteration 47705: loss = 5.0245707e-11,1.516057e-10\n",
      "Iteration 47710: loss = 5.0245346e-11,1.5160281e-10\n",
      "Iteration 47715: loss = 5.0245568e-11,1.5159997e-10\n",
      "Iteration 47720: loss = 5.0245252e-11,1.515968e-10\n",
      "Iteration 47725: loss = 5.0245346e-11,1.5159365e-10\n",
      "Iteration 47730: loss = 5.02452e-11,1.5159073e-10\n",
      "Iteration 47735: loss = 5.024513e-11,1.5158746e-10\n",
      "Iteration 47740: loss = 5.024485e-11,1.5158448e-10\n",
      "Iteration 47745: loss = 5.024582e-11,1.5158129e-10\n",
      "Iteration 47750: loss = 5.0245974e-11,1.5157851e-10\n",
      "Iteration 47755: loss = 5.024553e-11,1.5157545e-10\n",
      "Iteration 47760: loss = 5.02457e-11,1.5157223e-10\n",
      "Iteration 47765: loss = 5.0245388e-11,1.5156923e-10\n",
      "Iteration 47770: loss = 5.0245492e-11,1.5156623e-10\n",
      "Iteration 47775: loss = 5.0245277e-11,1.5156316e-10\n",
      "Iteration 47780: loss = 5.0245447e-11,1.5156013e-10\n",
      "Iteration 47785: loss = 5.024513e-11,1.5155714e-10\n",
      "Iteration 47790: loss = 5.024516e-11,1.5155406e-10\n",
      "Iteration 47795: loss = 5.0244958e-11,1.5155109e-10\n",
      "Iteration 47800: loss = 5.0245048e-11,1.51548e-10\n",
      "Iteration 47805: loss = 5.0245915e-11,1.5154503e-10\n",
      "Iteration 47810: loss = 5.0245513e-11,1.5154195e-10\n",
      "Iteration 47815: loss = 5.0245766e-11,1.515389e-10\n",
      "Iteration 47820: loss = 5.024538e-11,1.5153591e-10\n",
      "Iteration 47825: loss = 5.02456e-11,1.51533e-10\n",
      "Iteration 47830: loss = 5.024532e-11,1.5152993e-10\n",
      "Iteration 47835: loss = 5.024547e-11,1.5152687e-10\n",
      "Iteration 47840: loss = 5.0245114e-11,1.5152392e-10\n",
      "Iteration 47845: loss = 5.024528e-11,1.5152085e-10\n",
      "Iteration 47850: loss = 5.0244902e-11,1.5151802e-10\n",
      "Iteration 47855: loss = 5.024146e-11,1.5151731e-10\n",
      "Iteration 47860: loss = 5.0236464e-11,1.5151795e-10\n",
      "Iteration 47865: loss = 5.0230545e-11,1.5151844e-10\n",
      "Iteration 47870: loss = 5.022427e-11,1.5151935e-10\n",
      "Iteration 47875: loss = 5.0218375e-11,1.5152005e-10\n",
      "Iteration 47880: loss = 5.02121e-11,1.515209e-10\n",
      "Iteration 47885: loss = 5.0205843e-11,1.5152182e-10\n",
      "Iteration 47890: loss = 5.0200136e-11,1.5152252e-10\n",
      "Iteration 47895: loss = 5.0194047e-11,1.5152327e-10\n",
      "Iteration 47900: loss = 5.0188433e-11,1.5152377e-10\n",
      "Iteration 47905: loss = 5.0182383e-11,1.5152446e-10\n",
      "Iteration 47910: loss = 5.0176727e-11,1.5152532e-10\n",
      "Iteration 47915: loss = 5.017081e-11,1.5152614e-10\n",
      "Iteration 47920: loss = 5.016629e-11,1.5152665e-10\n",
      "Iteration 47925: loss = 5.0160275e-11,1.515273e-10\n",
      "Iteration 47930: loss = 5.0154682e-11,1.5152803e-10\n",
      "Iteration 47935: loss = 5.0148732e-11,1.5152832e-10\n",
      "Iteration 47940: loss = 5.0143393e-11,1.5152896e-10\n",
      "Iteration 47945: loss = 5.0137398e-11,1.5152957e-10\n",
      "Iteration 47950: loss = 5.0131527e-11,1.5152998e-10\n",
      "Iteration 47955: loss = 5.0126275e-11,1.5153048e-10\n",
      "Iteration 47960: loss = 5.0120418e-11,1.5153112e-10\n",
      "Iteration 47965: loss = 4.9324045e-11,1.5153176e-10\n",
      "Iteration 47970: loss = 4.9317512e-11,1.5153255e-10\n",
      "Iteration 47975: loss = 4.931087e-11,1.5153384e-10\n",
      "Iteration 47980: loss = 4.930441e-11,1.5153587e-10\n",
      "Iteration 47985: loss = 4.929566e-11,1.515385e-10\n",
      "Iteration 47990: loss = 4.928617e-11,1.5154111e-10\n",
      "Iteration 47995: loss = 4.927721e-11,1.5154433e-10\n",
      "Iteration 48000: loss = 4.9267906e-11,1.5154711e-10\n",
      "Iteration 48005: loss = 4.9258774e-11,1.5154972e-10\n",
      "Iteration 48010: loss = 4.924997e-11,1.515524e-10\n",
      "Iteration 48015: loss = 4.924135e-11,1.5155471e-10\n",
      "Iteration 48020: loss = 4.923265e-11,1.5155754e-10\n",
      "Iteration 48025: loss = 4.9223927e-11,1.5156018e-10\n",
      "Iteration 48030: loss = 4.921552e-11,1.5156239e-10\n",
      "Iteration 48035: loss = 4.920803e-11,1.5156507e-10\n",
      "Iteration 48040: loss = 4.9199856e-11,1.5156723e-10\n",
      "Iteration 48045: loss = 4.91938e-11,1.515683e-10\n",
      "Iteration 48050: loss = 4.9187477e-11,1.5156904e-10\n",
      "Iteration 48055: loss = 4.9181437e-11,1.5156992e-10\n",
      "Iteration 48060: loss = 4.917533e-11,1.5157103e-10\n",
      "Iteration 48065: loss = 4.916939e-11,1.5157202e-10\n",
      "Iteration 48070: loss = 4.9163635e-11,1.515726e-10\n",
      "Iteration 48075: loss = 4.9157456e-11,1.5157318e-10\n",
      "Iteration 48080: loss = 4.9151443e-11,1.5157445e-10\n",
      "Iteration 48085: loss = 4.9143963e-11,1.5157596e-10\n",
      "Iteration 48090: loss = 4.9136032e-11,1.515783e-10\n",
      "Iteration 48095: loss = 4.9128302e-11,1.515808e-10\n",
      "Iteration 48100: loss = 4.9120544e-11,1.5158291e-10\n",
      "Iteration 48105: loss = 4.911402e-11,1.5158381e-10\n",
      "Iteration 48110: loss = 4.9107874e-11,1.5158488e-10\n",
      "Iteration 48115: loss = 4.9101844e-11,1.5158574e-10\n",
      "Iteration 48120: loss = 4.9095686e-11,1.5158655e-10\n",
      "Iteration 48125: loss = 4.9089316e-11,1.5158769e-10\n",
      "Iteration 48130: loss = 4.9083196e-11,1.515886e-10\n",
      "Iteration 48135: loss = 4.907708e-11,1.5158973e-10\n",
      "Iteration 48140: loss = 4.9071292e-11,1.5159057e-10\n",
      "Iteration 48145: loss = 4.9065373e-11,1.5159138e-10\n",
      "Iteration 48150: loss = 4.9058993e-11,1.515922e-10\n",
      "Iteration 48155: loss = 4.905446e-11,1.5159313e-10\n",
      "Iteration 48160: loss = 4.9048567e-11,1.5159396e-10\n",
      "Iteration 48165: loss = 4.9042766e-11,1.5159467e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48170: loss = 4.9037024e-11,1.5159549e-10\n",
      "Iteration 48175: loss = 4.903096e-11,1.515964e-10\n",
      "Iteration 48180: loss = 4.9025183e-11,1.5159712e-10\n",
      "Iteration 48185: loss = 4.901954e-11,1.5159798e-10\n",
      "Iteration 48190: loss = 4.9013748e-11,1.5159883e-10\n",
      "Iteration 48195: loss = 4.9008474e-11,1.5159954e-10\n",
      "Iteration 48200: loss = 4.9002663e-11,1.5159994e-10\n",
      "Iteration 48205: loss = 4.899718e-11,1.5160043e-10\n",
      "Iteration 48210: loss = 4.8991974e-11,1.5160091e-10\n",
      "Iteration 48215: loss = 4.8987873e-11,1.5160133e-10\n",
      "Iteration 48220: loss = 4.8982457e-11,1.5160191e-10\n",
      "Iteration 48225: loss = 4.8976892e-11,1.5160231e-10\n",
      "Iteration 48230: loss = 4.8971483e-11,1.5160281e-10\n",
      "Iteration 48235: loss = 4.8966237e-11,1.5160356e-10\n",
      "Iteration 48240: loss = 4.8960825e-11,1.516039e-10\n",
      "Iteration 48245: loss = 4.8955336e-11,1.5160445e-10\n",
      "Iteration 48250: loss = 4.895005e-11,1.516049e-10\n",
      "Iteration 48255: loss = 4.8944956e-11,1.5160545e-10\n",
      "Iteration 48260: loss = 4.8939897e-11,1.5160571e-10\n",
      "Iteration 48265: loss = 4.893468e-11,1.5160594e-10\n",
      "Iteration 48270: loss = 4.8930304e-11,1.5160637e-10\n",
      "Iteration 48275: loss = 4.8925242e-11,1.5160678e-10\n",
      "Iteration 48280: loss = 4.892013e-11,1.5160717e-10\n",
      "Iteration 48285: loss = 4.8915056e-11,1.516075e-10\n",
      "Iteration 48290: loss = 4.8910057e-11,1.516079e-10\n",
      "Iteration 48295: loss = 4.8904686e-11,1.5160852e-10\n",
      "Iteration 48300: loss = 4.8899537e-11,1.5160878e-10\n",
      "Iteration 48305: loss = 4.8894517e-11,1.5160911e-10\n",
      "Iteration 48310: loss = 4.8889625e-11,1.5160914e-10\n",
      "Iteration 48315: loss = 4.8884886e-11,1.5160932e-10\n",
      "Iteration 48320: loss = 4.8879668e-11,1.5160956e-10\n",
      "Iteration 48325: loss = 4.887486e-11,1.5160974e-10\n",
      "Iteration 48330: loss = 4.887131e-11,1.5160985e-10\n",
      "Iteration 48335: loss = 4.8866536e-11,1.5160995e-10\n",
      "Iteration 48340: loss = 4.8862043e-11,1.5161017e-10\n",
      "Iteration 48345: loss = 4.8857054e-11,1.5161022e-10\n",
      "Iteration 48350: loss = 4.8852544e-11,1.5161021e-10\n",
      "Iteration 48355: loss = 4.884797e-11,1.516104e-10\n",
      "Iteration 48360: loss = 4.88434e-11,1.5161042e-10\n",
      "Iteration 48365: loss = 4.8838832e-11,1.5161045e-10\n",
      "Iteration 48370: loss = 4.8833774e-11,1.5161043e-10\n",
      "Iteration 48375: loss = 4.8829232e-11,1.516104e-10\n",
      "Iteration 48380: loss = 4.8824778e-11,1.516104e-10\n",
      "Iteration 48385: loss = 4.8820326e-11,1.5161053e-10\n",
      "Iteration 48390: loss = 4.8816534e-11,1.5161063e-10\n",
      "Iteration 48395: loss = 4.8811847e-11,1.5161086e-10\n",
      "Iteration 48400: loss = 4.8806983e-11,1.516111e-10\n",
      "Iteration 48405: loss = 4.880196e-11,1.516113e-10\n",
      "Iteration 48410: loss = 4.879688e-11,1.5161172e-10\n",
      "Iteration 48415: loss = 4.8791304e-11,1.51612e-10\n",
      "Iteration 48420: loss = 4.8786284e-11,1.5161268e-10\n",
      "Iteration 48425: loss = 4.878108e-11,1.5161306e-10\n",
      "Iteration 48430: loss = 4.877587e-11,1.5161339e-10\n",
      "Iteration 48435: loss = 4.8770838e-11,1.5161371e-10\n",
      "Iteration 48440: loss = 4.8765308e-11,1.516144e-10\n",
      "Iteration 48445: loss = 4.8761366e-11,1.5161473e-10\n",
      "Iteration 48450: loss = 4.875644e-11,1.5161512e-10\n",
      "Iteration 48455: loss = 4.8751374e-11,1.5161533e-10\n",
      "Iteration 48460: loss = 4.8746448e-11,1.5161568e-10\n",
      "Iteration 48465: loss = 4.874089e-11,1.5161625e-10\n",
      "Iteration 48470: loss = 4.8735457e-11,1.5161689e-10\n",
      "Iteration 48475: loss = 4.8729604e-11,1.5161788e-10\n",
      "Iteration 48480: loss = 4.872372e-11,1.5161865e-10\n",
      "Iteration 48485: loss = 4.8717988e-11,1.5161954e-10\n",
      "Iteration 48490: loss = 4.79302e-11,1.5162042e-10\n",
      "Iteration 48495: loss = 4.792944e-11,1.5161854e-10\n",
      "Iteration 48500: loss = 4.792677e-11,1.5161718e-10\n",
      "Iteration 48505: loss = 4.7925008e-11,1.516158e-10\n",
      "Iteration 48510: loss = 4.7922208e-11,1.5161451e-10\n",
      "Iteration 48515: loss = 4.791882e-11,1.5161336e-10\n",
      "Iteration 48520: loss = 4.7917087e-11,1.5161213e-10\n",
      "Iteration 48525: loss = 4.7914187e-11,1.5161095e-10\n",
      "Iteration 48530: loss = 4.79123e-11,1.5160961e-10\n",
      "Iteration 48535: loss = 4.7909048e-11,1.5160838e-10\n",
      "Iteration 48540: loss = 4.7905912e-11,1.5160719e-10\n",
      "Iteration 48545: loss = 4.790436e-11,1.5160581e-10\n",
      "Iteration 48550: loss = 4.7901766e-11,1.5160471e-10\n",
      "Iteration 48555: loss = 4.790025e-11,1.5160313e-10\n",
      "Iteration 48560: loss = 4.789731e-11,1.5160162e-10\n",
      "Iteration 48565: loss = 4.7895844e-11,1.516002e-10\n",
      "Iteration 48570: loss = 4.789343e-11,1.5159873e-10\n",
      "Iteration 48575: loss = 4.789084e-11,1.5159722e-10\n",
      "Iteration 48580: loss = 4.788973e-11,1.5159567e-10\n",
      "Iteration 48585: loss = 4.7886962e-11,1.5159404e-10\n",
      "Iteration 48590: loss = 4.7885928e-11,1.5159231e-10\n",
      "Iteration 48595: loss = 4.7883975e-11,1.5159053e-10\n",
      "Iteration 48600: loss = 4.7881962e-11,1.5158866e-10\n",
      "Iteration 48605: loss = 4.7881154e-11,1.5158678e-10\n",
      "Iteration 48610: loss = 4.7878864e-11,1.5158486e-10\n",
      "Iteration 48615: loss = 4.7878177e-11,1.5158302e-10\n",
      "Iteration 48620: loss = 4.787643e-11,1.5158103e-10\n",
      "Iteration 48625: loss = 4.7874538e-11,1.5157918e-10\n",
      "Iteration 48630: loss = 4.78739e-11,1.5157717e-10\n",
      "Iteration 48635: loss = 4.7871766e-11,1.5157517e-10\n",
      "Iteration 48640: loss = 4.787125e-11,1.5157313e-10\n",
      "Iteration 48645: loss = 4.786946e-11,1.5157117e-10\n",
      "Iteration 48650: loss = 4.7868792e-11,1.5156924e-10\n",
      "Iteration 48655: loss = 4.7867005e-11,1.5156716e-10\n",
      "Iteration 48660: loss = 4.7864875e-11,1.5156537e-10\n",
      "Iteration 48665: loss = 4.7864313e-11,1.5156333e-10\n",
      "Iteration 48670: loss = 4.7862794e-11,1.515612e-10\n",
      "Iteration 48675: loss = 4.786251e-11,1.5155896e-10\n",
      "Iteration 48680: loss = 4.7860684e-11,1.5155684e-10\n",
      "Iteration 48685: loss = 4.7859206e-11,1.5155452e-10\n",
      "Iteration 48690: loss = 4.785886e-11,1.5155216e-10\n",
      "Iteration 48695: loss = 4.7857388e-11,1.5155018e-10\n",
      "Iteration 48700: loss = 4.785732e-11,1.5154804e-10\n",
      "Iteration 48705: loss = 4.7855737e-11,1.5154566e-10\n",
      "Iteration 48710: loss = 4.7855577e-11,1.5154339e-10\n",
      "Iteration 48715: loss = 4.7854373e-11,1.5154111e-10\n",
      "Iteration 48720: loss = 4.7853086e-11,1.5153868e-10\n",
      "Iteration 48725: loss = 4.785297e-11,1.5153635e-10\n",
      "Iteration 48730: loss = 4.785122e-11,1.5153404e-10\n",
      "Iteration 48735: loss = 4.7851303e-11,1.5153165e-10\n",
      "Iteration 48740: loss = 4.784997e-11,1.5152925e-10\n",
      "Iteration 48745: loss = 4.784872e-11,1.5152705e-10\n",
      "Iteration 48750: loss = 4.784864e-11,1.515247e-10\n",
      "Iteration 48755: loss = 4.784695e-11,1.515226e-10\n",
      "Iteration 48760: loss = 4.7846976e-11,1.5152016e-10\n",
      "Iteration 48765: loss = 4.784566e-11,1.5151763e-10\n",
      "Iteration 48770: loss = 4.7845616e-11,1.5151527e-10\n",
      "Iteration 48775: loss = 4.7844492e-11,1.5151297e-10\n",
      "Iteration 48780: loss = 4.7842817e-11,1.5151068e-10\n",
      "Iteration 48785: loss = 4.784266e-11,1.515083e-10\n",
      "Iteration 48790: loss = 4.784152e-11,1.5150602e-10\n",
      "Iteration 48795: loss = 4.784136e-11,1.5150362e-10\n",
      "Iteration 48800: loss = 4.7839645e-11,1.5150128e-10\n",
      "Iteration 48805: loss = 4.783852e-11,1.5149912e-10\n",
      "Iteration 48810: loss = 4.783843e-11,1.5149673e-10\n",
      "Iteration 48815: loss = 4.7837272e-11,1.5149444e-10\n",
      "Iteration 48820: loss = 4.7837234e-11,1.514921e-10\n",
      "Iteration 48825: loss = 4.7835614e-11,1.5148967e-10\n",
      "Iteration 48830: loss = 4.7834462e-11,1.5148717e-10\n",
      "Iteration 48835: loss = 4.783443e-11,1.5148485e-10\n",
      "Iteration 48840: loss = 4.783332e-11,1.5148258e-10\n",
      "Iteration 48845: loss = 4.7833265e-11,1.5148009e-10\n",
      "Iteration 48850: loss = 4.7831683e-11,1.5147784e-10\n",
      "Iteration 48855: loss = 4.783163e-11,1.5147539e-10\n",
      "Iteration 48860: loss = 4.7830486e-11,1.5147296e-10\n",
      "Iteration 48865: loss = 4.7829334e-11,1.5147053e-10\n",
      "Iteration 48870: loss = 4.782932e-11,1.5146828e-10\n",
      "Iteration 48875: loss = 4.7827728e-11,1.5146594e-10\n",
      "Iteration 48880: loss = 4.7827815e-11,1.5146367e-10\n",
      "Iteration 48885: loss = 4.782662e-11,1.5146118e-10\n",
      "Iteration 48890: loss = 4.7825372e-11,1.514589e-10\n",
      "Iteration 48895: loss = 4.782534e-11,1.514565e-10\n",
      "Iteration 48900: loss = 4.78238e-11,1.5145414e-10\n",
      "Iteration 48905: loss = 4.782381e-11,1.5145174e-10\n",
      "Iteration 48910: loss = 4.782257e-11,1.514494e-10\n",
      "Iteration 48915: loss = 4.7822607e-11,1.5144684e-10\n",
      "Iteration 48920: loss = 4.7821472e-11,1.5144462e-10\n",
      "Iteration 48925: loss = 4.781764e-11,1.514443e-10\n",
      "Iteration 48930: loss = 4.7812018e-11,1.5144505e-10\n",
      "Iteration 48935: loss = 4.7806526e-11,1.5144579e-10\n",
      "Iteration 48940: loss = 4.780092e-11,1.5144658e-10\n",
      "Iteration 48945: loss = 4.7795268e-11,1.5144737e-10\n",
      "Iteration 48950: loss = 4.7790022e-11,1.5144781e-10\n",
      "Iteration 48955: loss = 4.778489e-11,1.514484e-10\n",
      "Iteration 48960: loss = 4.7779808e-11,1.5144892e-10\n",
      "Iteration 48965: loss = 4.7774635e-11,1.5144944e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48970: loss = 4.7770357e-11,1.5144999e-10\n",
      "Iteration 48975: loss = 4.7765136e-11,1.5145077e-10\n",
      "Iteration 48980: loss = 4.7760147e-11,1.5145103e-10\n",
      "Iteration 48985: loss = 4.7754998e-11,1.5145162e-10\n",
      "Iteration 48990: loss = 4.7749877e-11,1.5145205e-10\n",
      "Iteration 48995: loss = 4.7744392e-11,1.5145267e-10\n",
      "Iteration 49000: loss = 4.773949e-11,1.5145307e-10\n",
      "Iteration 49005: loss = 4.7734632e-11,1.5145356e-10\n",
      "Iteration 49010: loss = 4.7729487e-11,1.5145402e-10\n",
      "Iteration 49015: loss = 4.772456e-11,1.5145443e-10\n",
      "Iteration 49020: loss = 4.6947165e-11,1.5145482e-10\n",
      "Iteration 49025: loss = 4.771981e-11,1.5145195e-10\n",
      "Iteration 49030: loss = 4.6946936e-11,1.514495e-10\n",
      "Iteration 49035: loss = 4.694688e-11,1.5144704e-10\n",
      "Iteration 49040: loss = 4.6945437e-11,1.5144493e-10\n",
      "Iteration 49045: loss = 4.694159e-11,1.5144411e-10\n",
      "Iteration 49050: loss = 4.6940212e-11,1.5144272e-10\n",
      "Iteration 49055: loss = 4.6938235e-11,1.5144098e-10\n",
      "Iteration 49060: loss = 4.6937138e-11,1.5143913e-10\n",
      "Iteration 49065: loss = 4.6934973e-11,1.5143736e-10\n",
      "Iteration 49070: loss = 4.6934127e-11,1.514358e-10\n",
      "Iteration 49075: loss = 4.6932166e-11,1.5143362e-10\n",
      "Iteration 49080: loss = 4.6930463e-11,1.514317e-10\n",
      "Iteration 49085: loss = 4.693015e-11,1.5142965e-10\n",
      "Iteration 49090: loss = 4.6928395e-11,1.5142754e-10\n",
      "Iteration 49095: loss = 4.692774e-11,1.5142543e-10\n",
      "Iteration 49100: loss = 4.6926078e-11,1.5142357e-10\n",
      "Iteration 49105: loss = 4.6924378e-11,1.5142147e-10\n",
      "Iteration 49110: loss = 4.6923836e-11,1.5141935e-10\n",
      "Iteration 49115: loss = 4.6922258e-11,1.5141716e-10\n",
      "Iteration 49120: loss = 4.692184e-11,1.5141495e-10\n",
      "Iteration 49125: loss = 4.6920693e-11,1.5141283e-10\n",
      "Iteration 49130: loss = 4.6920235e-11,1.514107e-10\n",
      "Iteration 49135: loss = 4.6918854e-11,1.514085e-10\n",
      "Iteration 49140: loss = 4.6917317e-11,1.5140642e-10\n",
      "Iteration 49145: loss = 4.691705e-11,1.5140389e-10\n",
      "Iteration 49150: loss = 4.691562e-11,1.5140192e-10\n",
      "Iteration 49155: loss = 4.6915267e-11,1.5139966e-10\n",
      "Iteration 49160: loss = 4.6913848e-11,1.513975e-10\n",
      "Iteration 49165: loss = 4.691292e-11,1.5139512e-10\n",
      "Iteration 49170: loss = 4.6912505e-11,1.5139319e-10\n",
      "Iteration 49175: loss = 4.6910975e-11,1.5139079e-10\n",
      "Iteration 49180: loss = 4.6910763e-11,1.5138864e-10\n",
      "Iteration 49185: loss = 4.690927e-11,1.5138638e-10\n",
      "Iteration 49190: loss = 4.690777e-11,1.513843e-10\n",
      "Iteration 49195: loss = 4.6907575e-11,1.5138192e-10\n",
      "Iteration 49200: loss = 4.690615e-11,1.5137966e-10\n",
      "Iteration 49205: loss = 4.690621e-11,1.5137756e-10\n",
      "Iteration 49210: loss = 4.6904817e-11,1.5137516e-10\n",
      "Iteration 49215: loss = 4.6904463e-11,1.5137293e-10\n",
      "Iteration 49220: loss = 4.6902992e-11,1.5137082e-10\n",
      "Iteration 49225: loss = 4.6901708e-11,1.513685e-10\n",
      "Iteration 49230: loss = 4.6901507e-11,1.5136617e-10\n",
      "Iteration 49235: loss = 4.6900154e-11,1.5136375e-10\n",
      "Iteration 49240: loss = 4.6899786e-11,1.5136138e-10\n",
      "Iteration 49245: loss = 4.6898933e-11,1.5135923e-10\n",
      "Iteration 49250: loss = 4.6897278e-11,1.5135705e-10\n",
      "Iteration 49255: loss = 4.689733e-11,1.5135462e-10\n",
      "Iteration 49260: loss = 4.689613e-11,1.5135201e-10\n",
      "Iteration 49265: loss = 4.689606e-11,1.5134993e-10\n",
      "Iteration 49270: loss = 4.6894877e-11,1.5134746e-10\n",
      "Iteration 49275: loss = 4.689484e-11,1.5134509e-10\n",
      "Iteration 49280: loss = 4.6893628e-11,1.5134283e-10\n",
      "Iteration 49285: loss = 4.6892514e-11,1.5134041e-10\n",
      "Iteration 49290: loss = 4.689281e-11,1.5133794e-10\n",
      "Iteration 49295: loss = 4.689156e-11,1.5133543e-10\n",
      "Iteration 49300: loss = 4.6891543e-11,1.513332e-10\n",
      "Iteration 49305: loss = 4.689042e-11,1.5133089e-10\n",
      "Iteration 49310: loss = 4.6889093e-11,1.513284e-10\n",
      "Iteration 49315: loss = 4.6889184e-11,1.5132598e-10\n",
      "Iteration 49320: loss = 4.688804e-11,1.5132366e-10\n",
      "Iteration 49325: loss = 4.688797e-11,1.5132118e-10\n",
      "Iteration 49330: loss = 4.6887178e-11,1.5131855e-10\n",
      "Iteration 49335: loss = 4.688716e-11,1.5131639e-10\n",
      "Iteration 49340: loss = 4.6885985e-11,1.5131399e-10\n",
      "Iteration 49345: loss = 4.688482e-11,1.5131144e-10\n",
      "Iteration 49350: loss = 4.6884777e-11,1.5130913e-10\n",
      "Iteration 49355: loss = 4.6883688e-11,1.5130666e-10\n",
      "Iteration 49360: loss = 4.6883514e-11,1.5130444e-10\n",
      "Iteration 49365: loss = 4.688239e-11,1.5130197e-10\n",
      "Iteration 49370: loss = 4.688157e-11,1.5129967e-10\n",
      "Iteration 49375: loss = 4.6881537e-11,1.5129725e-10\n",
      "Iteration 49380: loss = 4.6880392e-11,1.5129473e-10\n",
      "Iteration 49385: loss = 4.6880402e-11,1.5129241e-10\n",
      "Iteration 49390: loss = 4.6879233e-11,1.5129005e-10\n",
      "Iteration 49395: loss = 4.687913e-11,1.5128757e-10\n",
      "Iteration 49400: loss = 4.6877908e-11,1.5128529e-10\n",
      "Iteration 49405: loss = 4.6876673e-11,1.512826e-10\n",
      "Iteration 49410: loss = 4.6877165e-11,1.5128032e-10\n",
      "Iteration 49415: loss = 4.6875906e-11,1.5127785e-10\n",
      "Iteration 49420: loss = 4.6875854e-11,1.5127571e-10\n",
      "Iteration 49425: loss = 4.6874702e-11,1.5127315e-10\n",
      "Iteration 49430: loss = 4.687337e-11,1.5127083e-10\n",
      "Iteration 49435: loss = 4.6873255e-11,1.5126851e-10\n",
      "Iteration 49440: loss = 4.6872325e-11,1.5126617e-10\n",
      "Iteration 49445: loss = 4.6872246e-11,1.5126347e-10\n",
      "Iteration 49450: loss = 4.687141e-11,1.5126106e-10\n",
      "Iteration 49455: loss = 4.687033e-11,1.512586e-10\n",
      "Iteration 49460: loss = 4.6870216e-11,1.5125631e-10\n",
      "Iteration 49465: loss = 4.6869e-11,1.5125395e-10\n",
      "Iteration 49470: loss = 4.686902e-11,1.5125157e-10\n",
      "Iteration 49475: loss = 4.6867857e-11,1.512491e-10\n",
      "Iteration 49480: loss = 4.686776e-11,1.5124671e-10\n",
      "Iteration 49485: loss = 4.6866667e-11,1.5124431e-10\n",
      "Iteration 49490: loss = 4.6865428e-11,1.5124199e-10\n",
      "Iteration 49495: loss = 4.6865758e-11,1.5123962e-10\n",
      "Iteration 49500: loss = 4.6864526e-11,1.5123734e-10\n",
      "Iteration 49505: loss = 4.686447e-11,1.5123489e-10\n",
      "Iteration 49510: loss = 4.6863333e-11,1.512325e-10\n",
      "Iteration 49515: loss = 4.6862108e-11,1.5123015e-10\n",
      "Iteration 49520: loss = 4.6862177e-11,1.5122766e-10\n",
      "Iteration 49525: loss = 4.6860977e-11,1.512252e-10\n",
      "Iteration 49530: loss = 4.6861005e-11,1.5122273e-10\n",
      "Iteration 49535: loss = 4.6860352e-11,1.512204e-10\n",
      "Iteration 49540: loss = 4.6860325e-11,1.5121798e-10\n",
      "Iteration 49545: loss = 4.6859173e-11,1.5121546e-10\n",
      "Iteration 49550: loss = 4.6858194e-11,1.5121321e-10\n",
      "Iteration 49555: loss = 4.6858125e-11,1.5121085e-10\n",
      "Iteration 49560: loss = 4.6856945e-11,1.512084e-10\n",
      "Iteration 49565: loss = 4.6856935e-11,1.5120565e-10\n",
      "Iteration 49570: loss = 4.6855856e-11,1.5120327e-10\n",
      "Iteration 49575: loss = 4.68518e-11,1.5120336e-10\n",
      "Iteration 49580: loss = 4.684603e-11,1.512046e-10\n",
      "Iteration 49585: loss = 4.683949e-11,1.5120602e-10\n",
      "Iteration 49590: loss = 4.6833714e-11,1.5120737e-10\n",
      "Iteration 49595: loss = 4.6827143e-11,1.5120878e-10\n",
      "Iteration 49600: loss = 4.6821533e-11,1.5121004e-10\n",
      "Iteration 49605: loss = 4.6815038e-11,1.5121117e-10\n",
      "Iteration 49610: loss = 4.680879e-11,1.5121211e-10\n",
      "Iteration 49615: loss = 4.6803898e-11,1.5121324e-10\n",
      "Iteration 49620: loss = 4.679754e-11,1.5121457e-10\n",
      "Iteration 49625: loss = 4.6792303e-11,1.512157e-10\n",
      "Iteration 49630: loss = 4.6786103e-11,1.512167e-10\n",
      "Iteration 49635: loss = 4.677982e-11,1.5121804e-10\n",
      "Iteration 49640: loss = 4.677468e-11,1.5121888e-10\n",
      "Iteration 49645: loss = 4.676826e-11,1.5122e-10\n",
      "Iteration 49650: loss = 4.6762705e-11,1.5122123e-10\n",
      "Iteration 49655: loss = 4.67562e-11,1.512227e-10\n",
      "Iteration 49660: loss = 4.674904e-11,1.5122462e-10\n",
      "Iteration 49665: loss = 4.6746884e-11,1.5122364e-10\n",
      "Iteration 49670: loss = 4.6741142e-11,1.5122437e-10\n",
      "Iteration 49675: loss = 4.6734117e-11,1.5122678e-10\n",
      "Iteration 49680: loss = 4.672635e-11,1.5122904e-10\n",
      "Iteration 49685: loss = 4.6719458e-11,1.5123096e-10\n",
      "Iteration 49690: loss = 4.6711756e-11,1.5123312e-10\n",
      "Iteration 49695: loss = 4.6703922e-11,1.5123541e-10\n",
      "Iteration 49700: loss = 4.66976e-11,1.512374e-10\n",
      "Iteration 49705: loss = 4.6689957e-11,1.5123952e-10\n",
      "Iteration 49710: loss = 4.668359e-11,1.5124148e-10\n",
      "Iteration 49715: loss = 4.6676326e-11,1.5124327e-10\n",
      "Iteration 49720: loss = 4.6669127e-11,1.5124481e-10\n",
      "Iteration 49725: loss = 4.6663316e-11,1.5124661e-10\n",
      "Iteration 49730: loss = 4.6656373e-11,1.5124829e-10\n",
      "Iteration 49735: loss = 4.6650586e-11,1.512496e-10\n",
      "Iteration 49740: loss = 4.664417e-11,1.512511e-10\n",
      "Iteration 49745: loss = 4.663843e-11,1.5125284e-10\n",
      "Iteration 49750: loss = 4.6631587e-11,1.5125395e-10\n",
      "Iteration 49755: loss = 4.662472e-11,1.5125565e-10\n",
      "Iteration 49760: loss = 4.6619108e-11,1.5125715e-10\n",
      "Iteration 49765: loss = 4.6612426e-11,1.5125848e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49770: loss = 4.6606958e-11,1.5125981e-10\n",
      "Iteration 49775: loss = 4.6600505e-11,1.5126114e-10\n",
      "Iteration 49780: loss = 4.659449e-11,1.5126252e-10\n",
      "Iteration 49785: loss = 4.6589264e-11,1.512637e-10\n",
      "Iteration 49790: loss = 4.6582894e-11,1.5126479e-10\n",
      "Iteration 49795: loss = 4.657784e-11,1.5126611e-10\n",
      "Iteration 49800: loss = 4.657143e-11,1.5126714e-10\n",
      "Iteration 49805: loss = 4.6566334e-11,1.5126847e-10\n",
      "Iteration 49810: loss = 4.6560224e-11,1.512693e-10\n",
      "Iteration 49815: loss = 4.6554097e-11,1.5127041e-10\n",
      "Iteration 49820: loss = 4.6549573e-11,1.5127133e-10\n",
      "Iteration 49825: loss = 4.6543578e-11,1.5127233e-10\n",
      "Iteration 49830: loss = 4.6538672e-11,1.5127335e-10\n",
      "Iteration 49835: loss = 4.6532642e-11,1.5127433e-10\n",
      "Iteration 49840: loss = 4.6526585e-11,1.5127537e-10\n",
      "Iteration 49845: loss = 4.652176e-11,1.5127635e-10\n",
      "Iteration 49850: loss = 4.6515666e-11,1.5127721e-10\n",
      "Iteration 49855: loss = 4.6510826e-11,1.5127812e-10\n",
      "Iteration 49860: loss = 4.6505258e-11,1.5127902e-10\n",
      "Iteration 49865: loss = 4.6499266e-11,1.5127993e-10\n",
      "Iteration 49870: loss = 4.6494582e-11,1.512811e-10\n",
      "Iteration 49875: loss = 4.6488497e-11,1.5128192e-10\n",
      "Iteration 49880: loss = 4.6483678e-11,1.5128289e-10\n",
      "Iteration 49885: loss = 4.6477783e-11,1.5128382e-10\n",
      "Iteration 49890: loss = 4.6473086e-11,1.512845e-10\n",
      "Iteration 49895: loss = 4.6467292e-11,1.5128536e-10\n",
      "Iteration 49900: loss = 4.646171e-11,1.512861e-10\n",
      "Iteration 49905: loss = 4.6457102e-11,1.51287e-10\n",
      "Iteration 49910: loss = 4.645143e-11,1.5128766e-10\n",
      "Iteration 49915: loss = 4.6446846e-11,1.5128886e-10\n",
      "Iteration 49920: loss = 4.644113e-11,1.5128955e-10\n",
      "Iteration 49925: loss = 4.6435484e-11,1.5129023e-10\n",
      "Iteration 49930: loss = 4.6430897e-11,1.5129116e-10\n",
      "Iteration 49935: loss = 4.6425336e-11,1.5129176e-10\n",
      "Iteration 49940: loss = 4.6420794e-11,1.5129253e-10\n",
      "Iteration 49945: loss = 4.5652707e-11,1.512933e-10\n",
      "Iteration 49950: loss = 4.5647253e-11,1.5129459e-10\n",
      "Iteration 49955: loss = 4.5640408e-11,1.5129623e-10\n",
      "Iteration 49960: loss = 4.5633164e-11,1.5129802e-10\n",
      "Iteration 49965: loss = 4.5627113e-11,1.5129983e-10\n",
      "Iteration 49970: loss = 4.5619897e-11,1.5130189e-10\n",
      "Iteration 49975: loss = 4.56138e-11,1.5130364e-10\n",
      "Iteration 49980: loss = 4.5606546e-11,1.5130552e-10\n",
      "Iteration 49985: loss = 4.5601866e-11,1.5130615e-10\n",
      "Iteration 49990: loss = 4.5599774e-11,1.5130494e-10\n",
      "Iteration 49995: loss = 4.55974e-11,1.5130401e-10\n",
      "Iteration 50000: loss = 4.5594004e-11,1.5130297e-10\n",
      "Iteration 50005: loss = 4.55916e-11,1.5130197e-10\n",
      "Iteration 50010: loss = 4.5589255e-11,1.5130101e-10\n",
      "Iteration 50015: loss = 4.5587114e-11,1.513e-10\n",
      "Iteration 50020: loss = 4.5584e-11,1.512989e-10\n",
      "Iteration 50025: loss = 4.5582278e-11,1.5129778e-10\n",
      "Iteration 50030: loss = 4.5580217e-11,1.5129657e-10\n",
      "Iteration 50035: loss = 4.5578152e-11,1.5129556e-10\n",
      "Iteration 50040: loss = 4.5575033e-11,1.5129442e-10\n",
      "Iteration 50045: loss = 4.557282e-11,1.5129327e-10\n",
      "Iteration 50050: loss = 4.557089e-11,1.5129195e-10\n",
      "Iteration 50055: loss = 4.5568754e-11,1.5129077e-10\n",
      "Iteration 50060: loss = 4.5565645e-11,1.5128948e-10\n",
      "Iteration 50065: loss = 4.556061e-11,1.5129092e-10\n",
      "Iteration 50070: loss = 4.5553426e-11,1.5129302e-10\n",
      "Iteration 50075: loss = 4.5546147e-11,1.5129487e-10\n",
      "Iteration 50080: loss = 4.5540047e-11,1.5129667e-10\n",
      "Iteration 50085: loss = 4.553308e-11,1.5129857e-10\n",
      "Iteration 50090: loss = 4.5527204e-11,1.5130025e-10\n",
      "Iteration 50095: loss = 4.5520487e-11,1.5130186e-10\n",
      "Iteration 50100: loss = 4.5514866e-11,1.5130328e-10\n",
      "Iteration 50105: loss = 4.5508652e-11,1.5130455e-10\n",
      "Iteration 50110: loss = 4.5502237e-11,1.5130601e-10\n",
      "Iteration 50115: loss = 4.5496718e-11,1.513075e-10\n",
      "Iteration 50120: loss = 4.549035e-11,1.5130894e-10\n",
      "Iteration 50125: loss = 4.548502e-11,1.5131016e-10\n",
      "Iteration 50130: loss = 4.547844e-11,1.5131175e-10\n",
      "Iteration 50135: loss = 4.547354e-11,1.5131206e-10\n",
      "Iteration 50140: loss = 4.5471748e-11,1.5131063e-10\n",
      "Iteration 50145: loss = 4.546993e-11,1.513092e-10\n",
      "Iteration 50150: loss = 4.546746e-11,1.5130808e-10\n",
      "Iteration 50155: loss = 4.5465628e-11,1.5130677e-10\n",
      "Iteration 50160: loss = 4.5463747e-11,1.513055e-10\n",
      "Iteration 50165: loss = 4.546187e-11,1.5130415e-10\n",
      "Iteration 50170: loss = 4.5460493e-11,1.5130192e-10\n",
      "Iteration 50175: loss = 4.5460257e-11,1.5129943e-10\n",
      "Iteration 50180: loss = 4.545994e-11,1.5129699e-10\n",
      "Iteration 50185: loss = 4.5459924e-11,1.5129449e-10\n",
      "Iteration 50190: loss = 4.5458675e-11,1.512919e-10\n",
      "Iteration 50195: loss = 4.5458665e-11,1.5128954e-10\n",
      "Iteration 50200: loss = 4.545828e-11,1.5128693e-10\n",
      "Iteration 50205: loss = 4.5458432e-11,1.5128467e-10\n",
      "Iteration 50210: loss = 4.5457124e-11,1.5128213e-10\n",
      "Iteration 50215: loss = 4.5457093e-11,1.5127968e-10\n",
      "Iteration 50220: loss = 4.5456795e-11,1.5127725e-10\n",
      "Iteration 50225: loss = 4.5457003e-11,1.5127474e-10\n",
      "Iteration 50230: loss = 4.5455594e-11,1.5127208e-10\n",
      "Iteration 50235: loss = 4.5455282e-11,1.5126962e-10\n",
      "Iteration 50240: loss = 4.5455407e-11,1.5126722e-10\n",
      "Iteration 50245: loss = 4.545508e-11,1.5126475e-10\n",
      "Iteration 50250: loss = 4.545414e-11,1.5126234e-10\n",
      "Iteration 50255: loss = 4.5453905e-11,1.5125982e-10\n",
      "Iteration 50260: loss = 4.545399e-11,1.5125724e-10\n",
      "Iteration 50265: loss = 4.545371e-11,1.512546e-10\n",
      "Iteration 50270: loss = 4.5452822e-11,1.5125226e-10\n",
      "Iteration 50275: loss = 4.5451403e-11,1.5125025e-10\n",
      "Iteration 50280: loss = 4.5446976e-11,1.5125098e-10\n",
      "Iteration 50285: loss = 4.54419e-11,1.5125183e-10\n",
      "Iteration 50290: loss = 4.5436484e-11,1.5125243e-10\n",
      "Iteration 50295: loss = 4.5432578e-11,1.5125298e-10\n",
      "Iteration 50300: loss = 4.5427075e-11,1.5125347e-10\n",
      "Iteration 50305: loss = 4.5423405e-11,1.5125405e-10\n",
      "Iteration 50310: loss = 4.5417933e-11,1.5125443e-10\n",
      "Iteration 50315: loss = 4.5413045e-11,1.5125487e-10\n",
      "Iteration 50320: loss = 4.5409138e-11,1.5125533e-10\n",
      "Iteration 50325: loss = 4.5404333e-11,1.5125559e-10\n",
      "Iteration 50330: loss = 4.5400253e-11,1.5125609e-10\n",
      "Iteration 50335: loss = 4.539519e-11,1.5125633e-10\n",
      "Iteration 50340: loss = 4.5391607e-11,1.5125665e-10\n",
      "Iteration 50345: loss = 4.5386517e-11,1.512569e-10\n",
      "Iteration 50350: loss = 4.538187e-11,1.5125728e-10\n",
      "Iteration 50355: loss = 4.4626327e-11,1.512562e-10\n",
      "Iteration 50360: loss = 4.4624724e-11,1.51255e-10\n",
      "Iteration 50365: loss = 4.4622524e-11,1.5125404e-10\n",
      "Iteration 50370: loss = 4.462042e-11,1.5125294e-10\n",
      "Iteration 50375: loss = 4.461688e-11,1.5125212e-10\n",
      "Iteration 50380: loss = 4.4614885e-11,1.5125107e-10\n",
      "Iteration 50385: loss = 4.4612345e-11,1.512502e-10\n",
      "Iteration 50390: loss = 4.4610034e-11,1.5124922e-10\n",
      "Iteration 50395: loss = 4.4606933e-11,1.5124818e-10\n",
      "Iteration 50400: loss = 4.4604504e-11,1.512472e-10\n",
      "Iteration 50405: loss = 4.4602516e-11,1.5124631e-10\n",
      "Iteration 50410: loss = 4.4600188e-11,1.5124534e-10\n",
      "Iteration 50415: loss = 4.459731e-11,1.5124431e-10\n",
      "Iteration 50420: loss = 4.459515e-11,1.512431e-10\n",
      "Iteration 50425: loss = 4.4593412e-11,1.5124207e-10\n",
      "Iteration 50430: loss = 4.4591386e-11,1.5124085e-10\n",
      "Iteration 50435: loss = 4.4588416e-11,1.512394e-10\n",
      "Iteration 50440: loss = 4.458697e-11,1.5123801e-10\n",
      "Iteration 50445: loss = 4.4585283e-11,1.5123687e-10\n",
      "Iteration 50450: loss = 4.4582657e-11,1.5123547e-10\n",
      "Iteration 50455: loss = 4.458098e-11,1.512341e-10\n",
      "Iteration 50460: loss = 4.457967e-11,1.5123248e-10\n",
      "Iteration 50465: loss = 4.4578095e-11,1.5123108e-10\n",
      "Iteration 50470: loss = 4.4575968e-11,1.5122942e-10\n",
      "Iteration 50475: loss = 4.457454e-11,1.5122774e-10\n",
      "Iteration 50480: loss = 4.4573373e-11,1.512263e-10\n",
      "Iteration 50485: loss = 4.4571843e-11,1.5122448e-10\n",
      "Iteration 50490: loss = 4.4569317e-11,1.5122306e-10\n",
      "Iteration 50495: loss = 4.4568186e-11,1.5122142e-10\n",
      "Iteration 50500: loss = 4.4566722e-11,1.5121983e-10\n",
      "Iteration 50505: loss = 4.456559e-11,1.5121826e-10\n",
      "Iteration 50510: loss = 4.4563048e-11,1.5121651e-10\n",
      "Iteration 50515: loss = 4.456217e-11,1.5121485e-10\n",
      "Iteration 50520: loss = 4.4560886e-11,1.5121301e-10\n",
      "Iteration 50525: loss = 4.4560005e-11,1.5121138e-10\n",
      "Iteration 50530: loss = 4.4557736e-11,1.5120952e-10\n",
      "Iteration 50535: loss = 4.4556928e-11,1.5120766e-10\n",
      "Iteration 50540: loss = 4.4555717e-11,1.5120592e-10\n",
      "Iteration 50545: loss = 4.4554433e-11,1.5120424e-10\n",
      "Iteration 50550: loss = 4.4552473e-11,1.5120234e-10\n",
      "Iteration 50555: loss = 4.455131e-11,1.5120052e-10\n",
      "Iteration 50560: loss = 4.4550346e-11,1.5119873e-10\n",
      "Iteration 50565: loss = 4.4549118e-11,1.5119687e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50570: loss = 4.4547355e-11,1.5119513e-10\n",
      "Iteration 50575: loss = 4.4546016e-11,1.5119339e-10\n",
      "Iteration 50580: loss = 4.454517e-11,1.5119156e-10\n",
      "Iteration 50585: loss = 4.454391e-11,1.511897e-10\n",
      "Iteration 50590: loss = 4.4541645e-11,1.5118823e-10\n",
      "Iteration 50595: loss = 4.454084e-11,1.511864e-10\n",
      "Iteration 50600: loss = 4.453954e-11,1.5118445e-10\n",
      "Iteration 50605: loss = 4.453876e-11,1.5118265e-10\n",
      "Iteration 50610: loss = 4.4536482e-11,1.5118093e-10\n",
      "Iteration 50615: loss = 4.4535615e-11,1.5117922e-10\n",
      "Iteration 50620: loss = 4.4534463e-11,1.5117732e-10\n",
      "Iteration 50625: loss = 4.45336e-11,1.5117559e-10\n",
      "Iteration 50630: loss = 4.4531285e-11,1.5117382e-10\n",
      "Iteration 50635: loss = 4.453037e-11,1.5117205e-10\n",
      "Iteration 50640: loss = 4.4529238e-11,1.5117023e-10\n",
      "Iteration 50645: loss = 4.4527923e-11,1.5116831e-10\n",
      "Iteration 50650: loss = 4.452601e-11,1.5116663e-10\n",
      "Iteration 50655: loss = 4.452481e-11,1.5116484e-10\n",
      "Iteration 50660: loss = 4.4523902e-11,1.5116315e-10\n",
      "Iteration 50665: loss = 4.4521678e-11,1.5116132e-10\n",
      "Iteration 50670: loss = 4.4520786e-11,1.511596e-10\n",
      "Iteration 50675: loss = 4.4519537e-11,1.5115786e-10\n",
      "Iteration 50680: loss = 4.451869e-11,1.5115598e-10\n",
      "Iteration 50685: loss = 4.451649e-11,1.5115421e-10\n",
      "Iteration 50690: loss = 4.4515846e-11,1.5115242e-10\n",
      "Iteration 50695: loss = 4.4514708e-11,1.5115037e-10\n",
      "Iteration 50700: loss = 4.4513518e-11,1.5114851e-10\n",
      "Iteration 50705: loss = 4.45117e-11,1.5114651e-10\n",
      "Iteration 50710: loss = 4.4510576e-11,1.5114483e-10\n",
      "Iteration 50715: loss = 4.4509823e-11,1.5114306e-10\n",
      "Iteration 50720: loss = 4.4508706e-11,1.5114127e-10\n",
      "Iteration 50725: loss = 4.4507023e-11,1.5113946e-10\n",
      "Iteration 50730: loss = 4.450587e-11,1.5113745e-10\n",
      "Iteration 50735: loss = 4.450511e-11,1.5113565e-10\n",
      "Iteration 50740: loss = 4.4504022e-11,1.511339e-10\n",
      "Iteration 50745: loss = 4.4501805e-11,1.5113184e-10\n",
      "Iteration 50750: loss = 4.4501125e-11,1.5113004e-10\n",
      "Iteration 50755: loss = 4.449993e-11,1.5112808e-10\n",
      "Iteration 50760: loss = 4.4499283e-11,1.5112632e-10\n",
      "Iteration 50765: loss = 4.4497097e-11,1.511245e-10\n",
      "Iteration 50770: loss = 4.449638e-11,1.5112266e-10\n",
      "Iteration 50775: loss = 4.4495314e-11,1.5112073e-10\n",
      "Iteration 50780: loss = 4.4494533e-11,1.5111892e-10\n",
      "Iteration 50785: loss = 4.4492413e-11,1.5111701e-10\n",
      "Iteration 50790: loss = 4.4491726e-11,1.5111525e-10\n",
      "Iteration 50795: loss = 4.4490595e-11,1.5111326e-10\n",
      "Iteration 50800: loss = 4.448952e-11,1.5111154e-10\n",
      "Iteration 50805: loss = 4.44877e-11,1.5110965e-10\n",
      "Iteration 50810: loss = 4.448665e-11,1.5110772e-10\n",
      "Iteration 50815: loss = 4.4486043e-11,1.5110578e-10\n",
      "Iteration 50820: loss = 4.4484944e-11,1.5110388e-10\n",
      "Iteration 50825: loss = 4.448318e-11,1.5110198e-10\n",
      "Iteration 50830: loss = 4.448228e-11,1.511e-10\n",
      "Iteration 50835: loss = 4.4481762e-11,1.51098e-10\n",
      "Iteration 50840: loss = 4.4480936e-11,1.5109604e-10\n",
      "Iteration 50845: loss = 4.447934e-11,1.5109394e-10\n",
      "Iteration 50850: loss = 4.4478365e-11,1.5109203e-10\n",
      "Iteration 50855: loss = 4.447737e-11,1.5109025e-10\n",
      "Iteration 50860: loss = 4.4476853e-11,1.5108814e-10\n",
      "Iteration 50865: loss = 4.447495e-11,1.5108623e-10\n",
      "Iteration 50870: loss = 4.447444e-11,1.5108426e-10\n",
      "Iteration 50875: loss = 4.4473442e-11,1.510822e-10\n",
      "Iteration 50880: loss = 4.4471843e-11,1.5108004e-10\n",
      "Iteration 50885: loss = 4.4470965e-11,1.5107815e-10\n",
      "Iteration 50890: loss = 4.4470493e-11,1.5107626e-10\n",
      "Iteration 50895: loss = 4.4469702e-11,1.5107414e-10\n",
      "Iteration 50900: loss = 4.446767e-11,1.5107222e-10\n",
      "Iteration 50905: loss = 4.4467225e-11,1.5107e-10\n",
      "Iteration 50910: loss = 4.4466448e-11,1.5106802e-10\n",
      "Iteration 50915: loss = 4.44659e-11,1.5106605e-10\n",
      "Iteration 50920: loss = 4.446409e-11,1.5106383e-10\n",
      "Iteration 50925: loss = 4.44635e-11,1.5106205e-10\n",
      "Iteration 50930: loss = 4.4462666e-11,1.5105994e-10\n",
      "Iteration 50935: loss = 4.4462222e-11,1.5105778e-10\n",
      "Iteration 50940: loss = 4.4460435e-11,1.5105567e-10\n",
      "Iteration 50945: loss = 4.4459884e-11,1.5105378e-10\n",
      "Iteration 50950: loss = 4.445905e-11,1.510517e-10\n",
      "Iteration 50955: loss = 4.445831e-11,1.5104959e-10\n",
      "Iteration 50960: loss = 4.445672e-11,1.5104754e-10\n",
      "Iteration 50965: loss = 4.4455894e-11,1.510454e-10\n",
      "Iteration 50970: loss = 4.445548e-11,1.5104348e-10\n",
      "Iteration 50975: loss = 4.4454638e-11,1.5104151e-10\n",
      "Iteration 50980: loss = 4.4453097e-11,1.5103932e-10\n",
      "Iteration 50985: loss = 4.4452265e-11,1.5103734e-10\n",
      "Iteration 50990: loss = 4.445183e-11,1.5103528e-10\n",
      "Iteration 50995: loss = 4.4450985e-11,1.5103327e-10\n",
      "Iteration 51000: loss = 4.4449385e-11,1.5103131e-10\n",
      "Iteration 51005: loss = 4.4448754e-11,1.5102908e-10\n",
      "Iteration 51010: loss = 4.444782e-11,1.51027e-10\n",
      "Iteration 51015: loss = 4.4447418e-11,1.5102498e-10\n",
      "Iteration 51020: loss = 4.4445576e-11,1.5102282e-10\n",
      "Iteration 51025: loss = 4.4445076e-11,1.5102086e-10\n",
      "Iteration 51030: loss = 4.4444327e-11,1.5101889e-10\n",
      "Iteration 51035: loss = 4.4443893e-11,1.5101687e-10\n",
      "Iteration 51040: loss = 4.4442047e-11,1.5101499e-10\n",
      "Iteration 51045: loss = 4.444159e-11,1.5101273e-10\n",
      "Iteration 51050: loss = 4.4440684e-11,1.5101081e-10\n",
      "Iteration 51055: loss = 4.4439882e-11,1.5100855e-10\n",
      "Iteration 51060: loss = 4.4438498e-11,1.5100654e-10\n",
      "Iteration 51065: loss = 4.443751e-11,1.5100468e-10\n",
      "Iteration 51070: loss = 4.443719e-11,1.510026e-10\n",
      "Iteration 51075: loss = 4.443626e-11,1.5100038e-10\n",
      "Iteration 51080: loss = 4.4434473e-11,1.5099852e-10\n",
      "Iteration 51085: loss = 4.4433363e-11,1.5099673e-10\n",
      "Iteration 51090: loss = 4.4432166e-11,1.509951e-10\n",
      "Iteration 51095: loss = 4.4429415e-11,1.5099377e-10\n",
      "Iteration 51100: loss = 4.442817e-11,1.5099245e-10\n",
      "Iteration 51105: loss = 4.4426338e-11,1.5099101e-10\n",
      "Iteration 51110: loss = 4.442444e-11,1.5098964e-10\n",
      "Iteration 51115: loss = 4.4421907e-11,1.5098835e-10\n",
      "Iteration 51120: loss = 4.4420134e-11,1.5098717e-10\n",
      "Iteration 51125: loss = 4.4418833e-11,1.5098572e-10\n",
      "Iteration 51130: loss = 4.4417286e-11,1.5098409e-10\n",
      "Iteration 51135: loss = 4.4415128e-11,1.5098257e-10\n",
      "Iteration 51140: loss = 4.4413778e-11,1.5098092e-10\n",
      "Iteration 51145: loss = 4.4412616e-11,1.5097938e-10\n",
      "Iteration 51150: loss = 4.441125e-11,1.5097781e-10\n",
      "Iteration 51155: loss = 4.4408754e-11,1.5097608e-10\n",
      "Iteration 51160: loss = 4.4407842e-11,1.509743e-10\n",
      "Iteration 51165: loss = 4.4406496e-11,1.5097282e-10\n",
      "Iteration 51170: loss = 4.4405483e-11,1.5097121e-10\n",
      "Iteration 51175: loss = 4.4402936e-11,1.5096954e-10\n",
      "Iteration 51180: loss = 4.4401937e-11,1.5096785e-10\n",
      "Iteration 51185: loss = 4.4400497e-11,1.5096609e-10\n",
      "Iteration 51190: loss = 4.4399623e-11,1.5096459e-10\n",
      "Iteration 51195: loss = 4.439719e-11,1.5096283e-10\n",
      "Iteration 51200: loss = 4.4396205e-11,1.509611e-10\n",
      "Iteration 51205: loss = 4.439488e-11,1.5095922e-10\n",
      "Iteration 51210: loss = 4.439361e-11,1.5095757e-10\n",
      "Iteration 51215: loss = 4.4391605e-11,1.5095589e-10\n",
      "Iteration 51220: loss = 4.4390353e-11,1.5095422e-10\n",
      "Iteration 51225: loss = 4.4389367e-11,1.509525e-10\n",
      "Iteration 51230: loss = 4.438808e-11,1.5095089e-10\n",
      "Iteration 51235: loss = 4.4386137e-11,1.5094907e-10\n",
      "Iteration 51240: loss = 4.4385107e-11,1.5094732e-10\n",
      "Iteration 51245: loss = 4.4384108e-11,1.5094555e-10\n",
      "Iteration 51250: loss = 4.43829e-11,1.5094365e-10\n",
      "Iteration 51255: loss = 4.438105e-11,1.5094179e-10\n",
      "Iteration 51260: loss = 4.4379774e-11,1.509401e-10\n",
      "Iteration 51265: loss = 4.437867e-11,1.5093835e-10\n",
      "Iteration 51270: loss = 4.4378112e-11,1.5093654e-10\n",
      "Iteration 51275: loss = 4.4376013e-11,1.5093447e-10\n",
      "Iteration 51280: loss = 4.4375698e-11,1.5093239e-10\n",
      "Iteration 51285: loss = 4.4374657e-11,1.509303e-10\n",
      "Iteration 51290: loss = 4.4373148e-11,1.5092842e-10\n",
      "Iteration 51295: loss = 4.4372353e-11,1.5092626e-10\n",
      "Iteration 51300: loss = 4.4371753e-11,1.5092445e-10\n",
      "Iteration 51305: loss = 4.4371007e-11,1.5092241e-10\n",
      "Iteration 51310: loss = 4.4369022e-11,1.5092039e-10\n",
      "Iteration 51315: loss = 4.4368564e-11,1.5091842e-10\n",
      "Iteration 51320: loss = 4.436769e-11,1.5091631e-10\n",
      "Iteration 51325: loss = 4.436719e-11,1.5091448e-10\n",
      "Iteration 51330: loss = 4.436516e-11,1.5091244e-10\n",
      "Iteration 51335: loss = 4.4364633e-11,1.5091042e-10\n",
      "Iteration 51340: loss = 4.4363846e-11,1.509085e-10\n",
      "Iteration 51345: loss = 4.4363374e-11,1.509063e-10\n",
      "Iteration 51350: loss = 4.4361428e-11,1.509043e-10\n",
      "Iteration 51355: loss = 4.4360893e-11,1.5090248e-10\n",
      "Iteration 51360: loss = 4.4360057e-11,1.5090038e-10\n",
      "Iteration 51365: loss = 4.435922e-11,1.5089828e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51370: loss = 4.43577e-11,1.508964e-10\n",
      "Iteration 51375: loss = 4.435677e-11,1.5089446e-10\n",
      "Iteration 51380: loss = 4.4356116e-11,1.5089255e-10\n",
      "Iteration 51385: loss = 4.4355356e-11,1.5089036e-10\n",
      "Iteration 51390: loss = 4.4353812e-11,1.5088841e-10\n",
      "Iteration 51395: loss = 4.4352962e-11,1.5088628e-10\n",
      "Iteration 51400: loss = 4.4352452e-11,1.5088437e-10\n",
      "Iteration 51405: loss = 4.4351703e-11,1.5088232e-10\n",
      "Iteration 51410: loss = 4.4350103e-11,1.5088021e-10\n",
      "Iteration 51415: loss = 4.43492e-11,1.5087828e-10\n",
      "Iteration 51420: loss = 4.4348247e-11,1.5087631e-10\n",
      "Iteration 51425: loss = 4.434785e-11,1.5087429e-10\n",
      "Iteration 51430: loss = 4.4346016e-11,1.5087231e-10\n",
      "Iteration 51435: loss = 4.4345475e-11,1.5087022e-10\n",
      "Iteration 51440: loss = 4.434469e-11,1.5086815e-10\n",
      "Iteration 51445: loss = 4.4344136e-11,1.5086607e-10\n",
      "Iteration 51450: loss = 4.434218e-11,1.5086404e-10\n",
      "Iteration 51455: loss = 4.4341805e-11,1.5086205e-10\n",
      "Iteration 51460: loss = 4.4340937e-11,1.5086007e-10\n",
      "Iteration 51465: loss = 4.43401e-11,1.5085824e-10\n",
      "Iteration 51470: loss = 4.4338578e-11,1.5085608e-10\n",
      "Iteration 51475: loss = 4.433771e-11,1.5085408e-10\n",
      "Iteration 51480: loss = 4.4337266e-11,1.5085205e-10\n",
      "Iteration 51485: loss = 4.4336385e-11,1.5085008e-10\n",
      "Iteration 51490: loss = 4.4334914e-11,1.5084817e-10\n",
      "Iteration 51495: loss = 4.4334075e-11,1.5084581e-10\n",
      "Iteration 51500: loss = 4.433365e-11,1.5084398e-10\n",
      "Iteration 51505: loss = 4.4331743e-11,1.5084185e-10\n",
      "Iteration 51510: loss = 4.4331202e-11,1.5083969e-10\n",
      "Iteration 51515: loss = 4.4330404e-11,1.5083781e-10\n",
      "Iteration 51520: loss = 4.4329537e-11,1.5083591e-10\n",
      "Iteration 51525: loss = 4.4328177e-11,1.5083372e-10\n",
      "Iteration 51530: loss = 4.4327375e-11,1.5083167e-10\n",
      "Iteration 51535: loss = 4.4326803e-11,1.5082977e-10\n",
      "Iteration 51540: loss = 4.4326095e-11,1.5082774e-10\n",
      "Iteration 51545: loss = 4.432445e-11,1.5082564e-10\n",
      "Iteration 51550: loss = 4.432378e-11,1.5082381e-10\n",
      "Iteration 51555: loss = 4.4323295e-11,1.5082154e-10\n",
      "Iteration 51560: loss = 4.432241e-11,1.508196e-10\n",
      "Iteration 51565: loss = 4.4320884e-11,1.5081744e-10\n",
      "Iteration 51570: loss = 4.4320186e-11,1.5081536e-10\n",
      "Iteration 51575: loss = 4.43194e-11,1.5081326e-10\n",
      "Iteration 51580: loss = 4.4318948e-11,1.5081132e-10\n",
      "Iteration 51585: loss = 4.431694e-11,1.5080925e-10\n",
      "Iteration 51590: loss = 4.4316523e-11,1.5080734e-10\n",
      "Iteration 51595: loss = 4.431577e-11,1.5080531e-10\n",
      "Iteration 51600: loss = 4.431533e-11,1.5080318e-10\n",
      "Iteration 51605: loss = 4.431347e-11,1.5080129e-10\n",
      "Iteration 51610: loss = 4.4312887e-11,1.5079889e-10\n",
      "Iteration 51615: loss = 4.431226e-11,1.5079675e-10\n",
      "Iteration 51620: loss = 4.4311377e-11,1.507949e-10\n",
      "Iteration 51625: loss = 4.4309997e-11,1.5079288e-10\n",
      "Iteration 51630: loss = 4.4309185e-11,1.5079073e-10\n",
      "Iteration 51635: loss = 4.430875e-11,1.5078892e-10\n",
      "Iteration 51640: loss = 4.4308685e-11,1.5078627e-10\n",
      "Iteration 51645: loss = 4.4306895e-11,1.50784e-10\n",
      "Iteration 51650: loss = 4.4306437e-11,1.5078215e-10\n",
      "Iteration 51655: loss = 4.4306402e-11,1.5077967e-10\n",
      "Iteration 51660: loss = 4.4306014e-11,1.5077738e-10\n",
      "Iteration 51665: loss = 4.4304505e-11,1.5077523e-10\n",
      "Iteration 51670: loss = 4.4304452e-11,1.507728e-10\n",
      "Iteration 51675: loss = 4.4304078e-11,1.5077066e-10\n",
      "Iteration 51680: loss = 4.4303613e-11,1.5076855e-10\n",
      "Iteration 51685: loss = 4.4301698e-11,1.5076636e-10\n",
      "Iteration 51690: loss = 4.4301965e-11,1.5076346e-10\n",
      "Iteration 51695: loss = 4.4301392e-11,1.5076163e-10\n",
      "Iteration 51700: loss = 4.4300563e-11,1.5075974e-10\n",
      "Iteration 51705: loss = 4.4298915e-11,1.5075771e-10\n",
      "Iteration 51710: loss = 4.429952e-11,1.50755e-10\n",
      "Iteration 51715: loss = 4.4298634e-11,1.5075288e-10\n",
      "Iteration 51720: loss = 4.4296743e-11,1.5075097e-10\n",
      "Iteration 51725: loss = 4.4297423e-11,1.5074854e-10\n",
      "Iteration 51730: loss = 4.4296757e-11,1.5074628e-10\n",
      "Iteration 51735: loss = 4.4296216e-11,1.5074406e-10\n",
      "Iteration 51740: loss = 4.4294984e-11,1.5074153e-10\n",
      "Iteration 51745: loss = 4.4294388e-11,1.5073956e-10\n",
      "Iteration 51750: loss = 4.429458e-11,1.5073684e-10\n",
      "Iteration 51755: loss = 4.4294065e-11,1.5073459e-10\n",
      "Iteration 51760: loss = 4.429258e-11,1.5073262e-10\n",
      "Iteration 51765: loss = 4.429267e-11,1.5072982e-10\n",
      "Iteration 51770: loss = 4.4291806e-11,1.5072792e-10\n",
      "Iteration 51775: loss = 4.4291265e-11,1.5072585e-10\n",
      "Iteration 51780: loss = 4.4290415e-11,1.5072331e-10\n",
      "Iteration 51785: loss = 4.428956e-11,1.5072134e-10\n",
      "Iteration 51790: loss = 4.428936e-11,1.5071892e-10\n",
      "Iteration 51795: loss = 4.4288833e-11,1.5071691e-10\n",
      "Iteration 51800: loss = 4.4287924e-11,1.5071457e-10\n",
      "Iteration 51805: loss = 4.428744e-11,1.5071214e-10\n",
      "Iteration 51810: loss = 4.4286883e-11,1.5071015e-10\n",
      "Iteration 51815: loss = 4.428624e-11,1.5070799e-10\n",
      "Iteration 51820: loss = 4.428538e-11,1.5070545e-10\n",
      "Iteration 51825: loss = 4.428449e-11,1.5070352e-10\n",
      "Iteration 51830: loss = 4.4284593e-11,1.5070113e-10\n",
      "Iteration 51835: loss = 4.4284337e-11,1.5069868e-10\n",
      "Iteration 51840: loss = 4.42832e-11,1.5069622e-10\n",
      "Iteration 51845: loss = 4.42829e-11,1.5069389e-10\n",
      "Iteration 51850: loss = 4.4282373e-11,1.5069172e-10\n",
      "Iteration 51855: loss = 4.4282477e-11,1.5068922e-10\n",
      "Iteration 51860: loss = 4.428052e-11,1.5068717e-10\n",
      "Iteration 51865: loss = 4.4280746e-11,1.5068449e-10\n",
      "Iteration 51870: loss = 4.4280236e-11,1.5068266e-10\n",
      "Iteration 51875: loss = 4.42803e-11,1.5068022e-10\n",
      "Iteration 51880: loss = 4.4278452e-11,1.506781e-10\n",
      "Iteration 51885: loss = 4.4278525e-11,1.5067558e-10\n",
      "Iteration 51890: loss = 4.427803e-11,1.5067345e-10\n",
      "Iteration 51895: loss = 4.4277488e-11,1.5067143e-10\n",
      "Iteration 51900: loss = 4.4276555e-11,1.5066867e-10\n",
      "Iteration 51905: loss = 4.4276027e-11,1.5066659e-10\n",
      "Iteration 51910: loss = 4.4275403e-11,1.5066454e-10\n",
      "Iteration 51915: loss = 4.4275337e-11,1.506622e-10\n",
      "Iteration 51920: loss = 4.4274095e-11,1.5065973e-10\n",
      "Iteration 51925: loss = 4.42734e-11,1.5065746e-10\n",
      "Iteration 51930: loss = 4.4273044e-11,1.506554e-10\n",
      "Iteration 51935: loss = 4.4272124e-11,1.506528e-10\n",
      "Iteration 51940: loss = 4.4271333e-11,1.5065062e-10\n",
      "Iteration 51945: loss = 4.4270674e-11,1.5064887e-10\n",
      "Iteration 51950: loss = 4.4271083e-11,1.5064636e-10\n",
      "Iteration 51955: loss = 4.4269612e-11,1.506438e-10\n",
      "Iteration 51960: loss = 4.426892e-11,1.5064186e-10\n",
      "Iteration 51965: loss = 4.426919e-11,1.5063911e-10\n",
      "Iteration 51970: loss = 4.4268766e-11,1.5063684e-10\n",
      "Iteration 51975: loss = 4.4266982e-11,1.506351e-10\n",
      "Iteration 51980: loss = 4.4266965e-11,1.5063274e-10\n",
      "Iteration 51985: loss = 4.4267322e-11,1.506301e-10\n",
      "Iteration 51990: loss = 4.4266053e-11,1.5062829e-10\n",
      "Iteration 51995: loss = 4.4263926e-11,1.5062661e-10\n",
      "Iteration 52000: loss = 4.4264564e-11,1.5062367e-10\n",
      "Iteration 52005: loss = 4.4264606e-11,1.5062121e-10\n",
      "Iteration 52010: loss = 4.426375e-11,1.5061931e-10\n",
      "Iteration 52015: loss = 4.4262632e-11,1.5061685e-10\n",
      "Iteration 52020: loss = 4.4262673e-11,1.5061426e-10\n",
      "Iteration 52025: loss = 4.4261882e-11,1.5061229e-10\n",
      "Iteration 52030: loss = 4.4262562e-11,1.5060944e-10\n",
      "Iteration 52035: loss = 4.4260262e-11,1.5060754e-10\n",
      "Iteration 52040: loss = 4.4260415e-11,1.5060536e-10\n",
      "Iteration 52045: loss = 4.426025e-11,1.506028e-10\n",
      "Iteration 52050: loss = 4.426023e-11,1.5060037e-10\n",
      "Iteration 52055: loss = 4.4258472e-11,1.5059824e-10\n",
      "Iteration 52060: loss = 4.425835e-11,1.5059597e-10\n",
      "Iteration 52065: loss = 4.425756e-11,1.5059393e-10\n",
      "Iteration 52070: loss = 4.4257903e-11,1.5059116e-10\n",
      "Iteration 52075: loss = 4.425608e-11,1.5058935e-10\n",
      "Iteration 52080: loss = 4.4256047e-11,1.5058696e-10\n",
      "Iteration 52085: loss = 4.425603e-11,1.5058449e-10\n",
      "Iteration 52090: loss = 4.425522e-11,1.5058263e-10\n",
      "Iteration 52095: loss = 4.4254097e-11,1.5057984e-10\n",
      "Iteration 52100: loss = 4.4254073e-11,1.5057768e-10\n",
      "Iteration 52105: loss = 4.4253726e-11,1.5057532e-10\n",
      "Iteration 52110: loss = 4.425403e-11,1.5057258e-10\n",
      "Iteration 52115: loss = 4.4252213e-11,1.5057092e-10\n",
      "Iteration 52120: loss = 4.4252168e-11,1.5056843e-10\n",
      "Iteration 52125: loss = 4.425206e-11,1.5056584e-10\n",
      "Iteration 52130: loss = 4.425026e-11,1.5056385e-10\n",
      "Iteration 52135: loss = 4.425101e-11,1.5056095e-10\n",
      "Iteration 52140: loss = 4.4249878e-11,1.5055915e-10\n",
      "Iteration 52145: loss = 4.4249843e-11,1.5055697e-10\n",
      "Iteration 52150: loss = 4.4249052e-11,1.5055411e-10\n",
      "Iteration 52155: loss = 4.4248678e-11,1.5055213e-10\n",
      "Iteration 52160: loss = 4.4247942e-11,1.5055021e-10\n",
      "Iteration 52165: loss = 4.424847e-11,1.5054706e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52170: loss = 4.4246763e-11,1.5054495e-10\n",
      "Iteration 52175: loss = 4.4246235e-11,1.5054262e-10\n",
      "Iteration 52180: loss = 4.4245926e-11,1.5054046e-10\n",
      "Iteration 52185: loss = 4.424627e-11,1.5053794e-10\n",
      "Iteration 52190: loss = 4.424504e-11,1.5053564e-10\n",
      "Iteration 52195: loss = 4.4244344e-11,1.5053354e-10\n",
      "Iteration 52200: loss = 4.424465e-11,1.505309e-10\n",
      "Iteration 52205: loss = 4.424341e-11,1.5052913e-10\n",
      "Iteration 52210: loss = 4.4242856e-11,1.5052642e-10\n",
      "Iteration 52215: loss = 4.4242318e-11,1.5052432e-10\n",
      "Iteration 52220: loss = 4.424228e-11,1.5052207e-10\n",
      "Iteration 52225: loss = 4.4242197e-11,1.505194e-10\n",
      "Iteration 52230: loss = 4.424104e-11,1.5051715e-10\n",
      "Iteration 52235: loss = 4.4240316e-11,1.5051528e-10\n",
      "Iteration 52240: loss = 4.4240552e-11,1.505125e-10\n",
      "Iteration 52245: loss = 4.423961e-11,1.5051052e-10\n",
      "Iteration 52250: loss = 4.4239223e-11,1.505077e-10\n",
      "Iteration 52255: loss = 4.423835e-11,1.5050561e-10\n",
      "Iteration 52260: loss = 4.4238408e-11,1.505032e-10\n",
      "Iteration 52265: loss = 4.4237617e-11,1.5050149e-10\n",
      "Iteration 52270: loss = 4.423482e-11,1.5050011e-10\n",
      "Iteration 52275: loss = 4.423356e-11,1.5049877e-10\n",
      "Iteration 52280: loss = 4.4231705e-11,1.5049749e-10\n",
      "Iteration 52285: loss = 4.4229804e-11,1.5049623e-10\n",
      "Iteration 52290: loss = 4.42273e-11,1.5049499e-10\n",
      "Iteration 52295: loss = 4.4225342e-11,1.5049367e-10\n",
      "Iteration 52300: loss = 4.4223892e-11,1.5049247e-10\n",
      "Iteration 52305: loss = 4.4221998e-11,1.5049124e-10\n",
      "Iteration 52310: loss = 4.4219312e-11,1.5048975e-10\n",
      "Iteration 52315: loss = 4.421757e-11,1.5048854e-10\n",
      "Iteration 52320: loss = 4.4217227e-11,1.5048633e-10\n",
      "Iteration 52325: loss = 4.4217203e-11,1.5048407e-10\n",
      "Iteration 52330: loss = 4.4216162e-11,1.5048168e-10\n",
      "Iteration 52335: loss = 4.421576e-11,1.5047924e-10\n",
      "Iteration 52340: loss = 4.4215923e-11,1.504767e-10\n",
      "Iteration 52345: loss = 4.421483e-11,1.5047423e-10\n",
      "Iteration 52350: loss = 4.421498e-11,1.5047177e-10\n",
      "Iteration 52355: loss = 4.421508e-11,1.504693e-10\n",
      "Iteration 52360: loss = 4.4214795e-11,1.5046689e-10\n",
      "Iteration 52365: loss = 4.421384e-11,1.5046445e-10\n",
      "Iteration 52370: loss = 4.421384e-11,1.5046192e-10\n",
      "Iteration 52375: loss = 4.4213206e-11,1.504599e-10\n",
      "Iteration 52380: loss = 4.42133e-11,1.5045731e-10\n",
      "Iteration 52385: loss = 4.4212273e-11,1.5045482e-10\n",
      "Iteration 52390: loss = 4.421162e-11,1.504525e-10\n",
      "Iteration 52395: loss = 4.4210968e-11,1.5045055e-10\n",
      "Iteration 52400: loss = 4.4211117e-11,1.5044815e-10\n",
      "Iteration 52405: loss = 4.421016e-11,1.504456e-10\n",
      "Iteration 52410: loss = 4.4209757e-11,1.5044332e-10\n",
      "Iteration 52415: loss = 4.420958e-11,1.5044127e-10\n",
      "Iteration 52420: loss = 4.420923e-11,1.5043891e-10\n",
      "Iteration 52425: loss = 4.4207932e-11,1.5043675e-10\n",
      "Iteration 52430: loss = 4.420776e-11,1.5043412e-10\n",
      "Iteration 52435: loss = 4.42078e-11,1.5043179e-10\n",
      "Iteration 52440: loss = 4.420758e-11,1.5042963e-10\n",
      "Iteration 52445: loss = 4.420626e-11,1.5042725e-10\n",
      "Iteration 52450: loss = 4.4206028e-11,1.5042485e-10\n",
      "Iteration 52455: loss = 4.4205722e-11,1.504227e-10\n",
      "Iteration 52460: loss = 4.420593e-11,1.5042018e-10\n",
      "Iteration 52465: loss = 4.4204678e-11,1.5041768e-10\n",
      "Iteration 52470: loss = 4.420436e-11,1.5041564e-10\n",
      "Iteration 52475: loss = 4.4204043e-11,1.5041314e-10\n",
      "Iteration 52480: loss = 4.4203873e-11,1.5041074e-10\n",
      "Iteration 52485: loss = 4.420295e-11,1.5040835e-10\n",
      "Iteration 52490: loss = 4.4202666e-11,1.5040608e-10\n",
      "Iteration 52495: loss = 4.4202253e-11,1.5040397e-10\n",
      "Iteration 52500: loss = 4.4202503e-11,1.504013e-10\n",
      "Iteration 52505: loss = 4.420178e-11,1.5039842e-10\n",
      "Iteration 52510: loss = 4.4201174e-11,1.5039606e-10\n",
      "Iteration 52515: loss = 4.4201125e-11,1.503937e-10\n",
      "Iteration 52520: loss = 4.4201337e-11,1.503912e-10\n",
      "Iteration 52525: loss = 4.419905e-11,1.5038956e-10\n",
      "Iteration 52530: loss = 4.4199328e-11,1.5038697e-10\n",
      "Iteration 52535: loss = 4.419953e-11,1.5038437e-10\n",
      "Iteration 52540: loss = 4.41986e-11,1.5038236e-10\n",
      "Iteration 52545: loss = 4.4197906e-11,1.503799e-10\n",
      "Iteration 52550: loss = 4.4198162e-11,1.5037736e-10\n",
      "Iteration 52555: loss = 4.4197205e-11,1.5037543e-10\n",
      "Iteration 52560: loss = 4.4195647e-11,1.503735e-10\n",
      "Iteration 52565: loss = 4.4195952e-11,1.5037098e-10\n",
      "Iteration 52570: loss = 4.4196157e-11,1.5036844e-10\n",
      "Iteration 52575: loss = 4.4195245e-11,1.5036651e-10\n",
      "Iteration 52580: loss = 4.419416e-11,1.5036399e-10\n",
      "Iteration 52585: loss = 4.419394e-11,1.5036146e-10\n",
      "Iteration 52590: loss = 4.4193524e-11,1.5035961e-10\n",
      "Iteration 52595: loss = 4.419338e-11,1.50357e-10\n",
      "Iteration 52600: loss = 4.4191827e-11,1.5035498e-10\n",
      "Iteration 52605: loss = 4.419165e-11,1.5035248e-10\n",
      "Iteration 52610: loss = 4.419194e-11,1.5034987e-10\n",
      "Iteration 52615: loss = 4.4191567e-11,1.5034765e-10\n",
      "Iteration 52620: loss = 4.419005e-11,1.5034551e-10\n",
      "Iteration 52625: loss = 4.4190273e-11,1.5034281e-10\n",
      "Iteration 52630: loss = 4.4190134e-11,1.5034027e-10\n",
      "Iteration 52635: loss = 4.4190065e-11,1.50338e-10\n",
      "Iteration 52640: loss = 4.4189315e-11,1.5033547e-10\n",
      "Iteration 52645: loss = 4.4188094e-11,1.5033381e-10\n",
      "Iteration 52650: loss = 4.4188375e-11,1.5033128e-10\n",
      "Iteration 52655: loss = 4.4188455e-11,1.5032878e-10\n",
      "Iteration 52660: loss = 4.4186797e-11,1.5032672e-10\n",
      "Iteration 52665: loss = 4.4186765e-11,1.5032421e-10\n",
      "Iteration 52670: loss = 4.4186595e-11,1.5032209e-10\n",
      "Iteration 52675: loss = 4.418642e-11,1.5031933e-10\n",
      "Iteration 52680: loss = 4.41856e-11,1.5031704e-10\n",
      "Iteration 52685: loss = 4.418522e-11,1.5031477e-10\n",
      "Iteration 52690: loss = 4.418522e-11,1.503122e-10\n",
      "Iteration 52695: loss = 4.4185513e-11,1.5030947e-10\n",
      "Iteration 52700: loss = 4.418387e-11,1.5030742e-10\n",
      "Iteration 52705: loss = 4.418385e-11,1.5030516e-10\n",
      "Iteration 52710: loss = 4.4183355e-11,1.5030306e-10\n",
      "Iteration 52715: loss = 4.41836e-11,1.503004e-10\n",
      "Iteration 52720: loss = 4.4182477e-11,1.502978e-10\n",
      "Iteration 52725: loss = 4.4182064e-11,1.5029582e-10\n",
      "Iteration 52730: loss = 4.4181988e-11,1.5029335e-10\n",
      "Iteration 52735: loss = 4.4181547e-11,1.5029128e-10\n",
      "Iteration 52740: loss = 4.4180885e-11,1.5028864e-10\n",
      "Iteration 52745: loss = 4.4180756e-11,1.5028603e-10\n",
      "Iteration 52750: loss = 4.4180316e-11,1.5028391e-10\n",
      "Iteration 52755: loss = 4.4179216e-11,1.5028138e-10\n",
      "Iteration 52760: loss = 4.4179115e-11,1.5027904e-10\n",
      "Iteration 52765: loss = 4.4178262e-11,1.5027721e-10\n",
      "Iteration 52770: loss = 4.417847e-11,1.5027458e-10\n",
      "Iteration 52775: loss = 4.4177804e-11,1.5027189e-10\n",
      "Iteration 52780: loss = 4.4177648e-11,1.5026949e-10\n",
      "Iteration 52785: loss = 4.4176874e-11,1.5026758e-10\n",
      "Iteration 52790: loss = 4.417723e-11,1.5026498e-10\n",
      "Iteration 52795: loss = 4.417601e-11,1.502625e-10\n",
      "Iteration 52800: loss = 4.4175635e-11,1.502605e-10\n",
      "Iteration 52805: loss = 4.4175174e-11,1.5025804e-10\n",
      "Iteration 52810: loss = 4.417546e-11,1.502554e-10\n",
      "Iteration 52815: loss = 4.4174633e-11,1.5025274e-10\n",
      "Iteration 52820: loss = 4.4174258e-11,1.5025065e-10\n",
      "Iteration 52825: loss = 4.4174137e-11,1.5024831e-10\n",
      "Iteration 52830: loss = 4.4174386e-11,1.5024579e-10\n",
      "Iteration 52835: loss = 4.41734e-11,1.5024323e-10\n",
      "Iteration 52840: loss = 4.417286e-11,1.5024115e-10\n",
      "Iteration 52845: loss = 4.417316e-11,1.5023857e-10\n",
      "Iteration 52850: loss = 4.417268e-11,1.5023638e-10\n",
      "Iteration 52855: loss = 4.417156e-11,1.5023371e-10\n",
      "Iteration 52860: loss = 4.4171455e-11,1.5023133e-10\n",
      "Iteration 52865: loss = 4.4171684e-11,1.5022877e-10\n",
      "Iteration 52870: loss = 4.4171278e-11,1.5022668e-10\n",
      "Iteration 52875: loss = 4.4170514e-11,1.5022397e-10\n",
      "Iteration 52880: loss = 4.4170754e-11,1.5022139e-10\n",
      "Iteration 52885: loss = 4.417026e-11,1.5021931e-10\n",
      "Iteration 52890: loss = 4.4170608e-11,1.5021662e-10\n",
      "Iteration 52895: loss = 4.416904e-11,1.5021434e-10\n",
      "Iteration 52900: loss = 4.416888e-11,1.502119e-10\n",
      "Iteration 52905: loss = 4.416915e-11,1.5020934e-10\n",
      "Iteration 52910: loss = 4.4168266e-11,1.5020744e-10\n",
      "Iteration 52915: loss = 4.4167545e-11,1.5020483e-10\n",
      "Iteration 52920: loss = 4.4167805e-11,1.502023e-10\n",
      "Iteration 52925: loss = 4.4167336e-11,1.5020012e-10\n",
      "Iteration 52930: loss = 4.416709e-11,1.501979e-10\n",
      "Iteration 52935: loss = 4.3422155e-11,1.5019555e-10\n",
      "Iteration 52940: loss = 4.3421197e-11,1.5019354e-10\n",
      "Iteration 52945: loss = 4.342059e-11,1.5019178e-10\n",
      "Iteration 52950: loss = 4.3420014e-11,1.501897e-10\n",
      "Iteration 52955: loss = 4.341837e-11,1.5018796e-10\n",
      "Iteration 52960: loss = 4.3417676e-11,1.5018603e-10\n",
      "Iteration 52965: loss = 4.341351e-11,1.5018625e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52970: loss = 4.3406556e-11,1.5018786e-10\n",
      "Iteration 52975: loss = 4.3400644e-11,1.5018994e-10\n",
      "Iteration 52980: loss = 4.3394715e-11,1.5019191e-10\n",
      "Iteration 52985: loss = 4.3388407e-11,1.5019357e-10\n",
      "Iteration 52990: loss = 4.3381614e-11,1.5019576e-10\n",
      "Iteration 52995: loss = 4.3375557e-11,1.5019752e-10\n",
      "Iteration 53000: loss = 4.3369686e-11,1.5019946e-10\n",
      "Iteration 53005: loss = 4.336383e-11,1.5020137e-10\n",
      "Iteration 53010: loss = 4.3356662e-11,1.5020322e-10\n",
      "Iteration 53015: loss = 4.3350823e-11,1.5020488e-10\n",
      "Iteration 53020: loss = 4.3345254e-11,1.5020654e-10\n",
      "Iteration 53025: loss = 4.333985e-11,1.5020823e-10\n",
      "Iteration 53030: loss = 4.333287e-11,1.5020972e-10\n",
      "Iteration 53035: loss = 4.3327526e-11,1.502112e-10\n",
      "Iteration 53040: loss = 4.332218e-11,1.5021256e-10\n",
      "Iteration 53045: loss = 4.331695e-11,1.5021401e-10\n",
      "Iteration 53050: loss = 4.3310577e-11,1.5021559e-10\n",
      "Iteration 53055: loss = 4.33049e-11,1.5021692e-10\n",
      "Iteration 53060: loss = 4.3299836e-11,1.5021848e-10\n",
      "Iteration 53065: loss = 4.3294517e-11,1.5021986e-10\n",
      "Iteration 53070: loss = 4.3288238e-11,1.5022128e-10\n",
      "Iteration 53075: loss = 4.3282846e-11,1.5022236e-10\n",
      "Iteration 53080: loss = 4.3277756e-11,1.5022388e-10\n",
      "Iteration 53085: loss = 4.327273e-11,1.5022512e-10\n",
      "Iteration 53090: loss = 4.326646e-11,1.5022633e-10\n",
      "Iteration 53095: loss = 4.3261245e-11,1.5022798e-10\n",
      "Iteration 53100: loss = 4.3255535e-11,1.5022959e-10\n",
      "Iteration 53105: loss = 4.325018e-11,1.5023094e-10\n",
      "Iteration 53110: loss = 4.324349e-11,1.5023263e-10\n",
      "Iteration 53115: loss = 4.323805e-11,1.5023416e-10\n",
      "Iteration 53120: loss = 4.323226e-11,1.5023566e-10\n",
      "Iteration 53125: loss = 4.3226884e-11,1.5023727e-10\n",
      "Iteration 53130: loss = 4.32204e-11,1.502389e-10\n",
      "Iteration 53135: loss = 4.321507e-11,1.5024038e-10\n",
      "Iteration 53140: loss = 4.3209814e-11,1.5024171e-10\n",
      "Iteration 53145: loss = 4.320415e-11,1.5024332e-10\n",
      "Iteration 53150: loss = 4.3197987e-11,1.5024515e-10\n",
      "Iteration 53155: loss = 4.319275e-11,1.502464e-10\n",
      "Iteration 53160: loss = 4.3187672e-11,1.5024795e-10\n",
      "Iteration 53165: loss = 4.318251e-11,1.5024923e-10\n",
      "Iteration 53170: loss = 4.3175973e-11,1.5025052e-10\n",
      "Iteration 53175: loss = 4.3170925e-11,1.502518e-10\n",
      "Iteration 53180: loss = 4.3164364e-11,1.5025411e-10\n",
      "Iteration 53185: loss = 4.3157894e-11,1.5025586e-10\n",
      "Iteration 53190: loss = 4.3152495e-11,1.5025721e-10\n",
      "Iteration 53195: loss = 4.3146892e-11,1.502589e-10\n",
      "Iteration 53200: loss = 4.3140158e-11,1.5026147e-10\n",
      "Iteration 53205: loss = 4.3133646e-11,1.5026315e-10\n",
      "Iteration 53210: loss = 4.312888e-11,1.5026429e-10\n",
      "Iteration 53215: loss = 4.3123383e-11,1.502655e-10\n",
      "Iteration 53220: loss = 4.238257e-11,1.5026823e-10\n",
      "Iteration 53225: loss = 4.2376994e-11,1.5026952e-10\n",
      "Iteration 53230: loss = 4.2374063e-11,1.5026898e-10\n",
      "Iteration 53235: loss = 4.237083e-11,1.5026853e-10\n",
      "Iteration 53240: loss = 4.2367557e-11,1.5026838e-10\n",
      "Iteration 53245: loss = 4.2364317e-11,1.5026802e-10\n",
      "Iteration 53250: loss = 4.236211e-11,1.5026783e-10\n",
      "Iteration 53255: loss = 4.235907e-11,1.5026771e-10\n",
      "Iteration 53260: loss = 4.235563e-11,1.502673e-10\n",
      "Iteration 53265: loss = 4.2352715e-11,1.5026683e-10\n",
      "Iteration 53270: loss = 4.2350096e-11,1.502663e-10\n",
      "Iteration 53275: loss = 4.2347337e-11,1.5026574e-10\n",
      "Iteration 53280: loss = 4.234425e-11,1.502652e-10\n",
      "Iteration 53285: loss = 4.234153e-11,1.5026452e-10\n",
      "Iteration 53290: loss = 4.23388e-11,1.5026388e-10\n",
      "Iteration 53295: loss = 4.2336124e-11,1.5026323e-10\n",
      "Iteration 53300: loss = 4.233346e-11,1.5026255e-10\n",
      "Iteration 53305: loss = 4.2330472e-11,1.5026187e-10\n",
      "Iteration 53310: loss = 4.2327836e-11,1.5026126e-10\n",
      "Iteration 53315: loss = 4.2325393e-11,1.5026039e-10\n",
      "Iteration 53320: loss = 4.2324033e-11,1.5025965e-10\n",
      "Iteration 53325: loss = 4.2321174e-11,1.5025871e-10\n",
      "Iteration 53330: loss = 4.231665e-11,1.5025933e-10\n",
      "Iteration 53335: loss = 4.231035e-11,1.5026182e-10\n",
      "Iteration 53340: loss = 4.2302894e-11,1.5026393e-10\n",
      "Iteration 53345: loss = 4.2296732e-11,1.5026626e-10\n",
      "Iteration 53350: loss = 4.2290043e-11,1.5026864e-10\n",
      "Iteration 53355: loss = 4.228377e-11,1.502707e-10\n",
      "Iteration 53360: loss = 4.227658e-11,1.5027335e-10\n",
      "Iteration 53365: loss = 4.227041e-11,1.5027546e-10\n",
      "Iteration 53370: loss = 4.2264133e-11,1.5027778e-10\n",
      "Iteration 53375: loss = 4.2257708e-11,1.5027994e-10\n",
      "Iteration 53380: loss = 4.22509e-11,1.5028175e-10\n",
      "Iteration 53385: loss = 4.224479e-11,1.50284e-10\n",
      "Iteration 53390: loss = 4.2242376e-11,1.502832e-10\n",
      "Iteration 53395: loss = 4.2239618e-11,1.5028237e-10\n",
      "Iteration 53400: loss = 4.223703e-11,1.5028152e-10\n",
      "Iteration 53405: loss = 4.223448e-11,1.502808e-10\n",
      "Iteration 53410: loss = 4.2233248e-11,1.502801e-10\n",
      "Iteration 53415: loss = 4.2230597e-11,1.502793e-10\n",
      "Iteration 53420: loss = 4.2227777e-11,1.5027862e-10\n",
      "Iteration 53425: loss = 4.222524e-11,1.502779e-10\n",
      "Iteration 53430: loss = 4.2222868e-11,1.5027712e-10\n",
      "Iteration 53435: loss = 4.222044e-11,1.5027626e-10\n",
      "Iteration 53440: loss = 4.2217774e-11,1.5027563e-10\n",
      "Iteration 53445: loss = 4.221526e-11,1.5027493e-10\n",
      "Iteration 53450: loss = 4.2212844e-11,1.5027402e-10\n",
      "Iteration 53455: loss = 4.221053e-11,1.5027313e-10\n",
      "Iteration 53460: loss = 4.220843e-11,1.5027214e-10\n",
      "Iteration 53465: loss = 4.220392e-11,1.5027293e-10\n",
      "Iteration 53470: loss = 4.21967e-11,1.5027521e-10\n",
      "Iteration 53475: loss = 4.2190657e-11,1.5027737e-10\n",
      "Iteration 53480: loss = 4.2184742e-11,1.5027908e-10\n",
      "Iteration 53485: loss = 4.217839e-11,1.5028134e-10\n",
      "Iteration 53490: loss = 4.2171475e-11,1.5028348e-10\n",
      "Iteration 53495: loss = 4.2165688e-11,1.5028559e-10\n",
      "Iteration 53500: loss = 4.2159953e-11,1.5028757e-10\n",
      "Iteration 53505: loss = 4.2153992e-11,1.5028959e-10\n",
      "Iteration 53510: loss = 4.214679e-11,1.5029158e-10\n",
      "Iteration 53515: loss = 4.214105e-11,1.5029347e-10\n",
      "Iteration 53520: loss = 4.2135694e-11,1.5029525e-10\n",
      "Iteration 53525: loss = 4.2131465e-11,1.502961e-10\n",
      "Iteration 53530: loss = 4.212497e-11,1.5029762e-10\n",
      "Iteration 53535: loss = 4.2122e-11,1.5029777e-10\n",
      "Iteration 53540: loss = 4.212e-11,1.5029672e-10\n",
      "Iteration 53545: loss = 4.2117587e-11,1.5029594e-10\n",
      "Iteration 53550: loss = 4.2114923e-11,1.5029522e-10\n",
      "Iteration 53555: loss = 4.2112164e-11,1.5029454e-10\n",
      "Iteration 53560: loss = 4.2109715e-11,1.5029376e-10\n",
      "Iteration 53565: loss = 4.2107328e-11,1.50293e-10\n",
      "Iteration 53570: loss = 4.2105028e-11,1.5029217e-10\n",
      "Iteration 53575: loss = 4.2102405e-11,1.5029131e-10\n",
      "Iteration 53580: loss = 4.210018e-11,1.5029042e-10\n",
      "Iteration 53585: loss = 4.209891e-11,1.5028859e-10\n",
      "Iteration 53590: loss = 4.2099473e-11,1.5028664e-10\n",
      "Iteration 53595: loss = 4.209878e-11,1.5028455e-10\n",
      "Iteration 53600: loss = 4.2097797e-11,1.5028265e-10\n",
      "Iteration 53605: loss = 4.2097017e-11,1.5028068e-10\n",
      "Iteration 53610: loss = 4.209627e-11,1.5027857e-10\n",
      "Iteration 53615: loss = 4.2095438e-11,1.5027654e-10\n",
      "Iteration 53620: loss = 4.209467e-11,1.5027447e-10\n",
      "Iteration 53625: loss = 4.2093763e-11,1.502724e-10\n",
      "Iteration 53630: loss = 4.2092954e-11,1.502705e-10\n",
      "Iteration 53635: loss = 4.209251e-11,1.5026819e-10\n",
      "Iteration 53640: loss = 4.209173e-11,1.5026627e-10\n",
      "Iteration 53645: loss = 4.209089e-11,1.5026419e-10\n",
      "Iteration 53650: loss = 4.209011e-11,1.5026219e-10\n",
      "Iteration 53655: loss = 4.2090317e-11,1.5025993e-10\n",
      "Iteration 53660: loss = 4.2089443e-11,1.5025797e-10\n",
      "Iteration 53665: loss = 4.2088662e-11,1.5025575e-10\n",
      "Iteration 53670: loss = 4.2087875e-11,1.502538e-10\n",
      "Iteration 53675: loss = 4.2087264e-11,1.5025181e-10\n",
      "Iteration 53680: loss = 4.20867e-11,1.502498e-10\n",
      "Iteration 53685: loss = 4.2085863e-11,1.5024781e-10\n",
      "Iteration 53690: loss = 4.208518e-11,1.5024572e-10\n",
      "Iteration 53695: loss = 4.2083285e-11,1.5024428e-10\n",
      "Iteration 53700: loss = 4.2082435e-11,1.5024235e-10\n",
      "Iteration 53705: loss = 4.208173e-11,1.502404e-10\n",
      "Iteration 53710: loss = 4.2080756e-11,1.5023847e-10\n",
      "Iteration 53715: loss = 4.2079954e-11,1.502367e-10\n",
      "Iteration 53720: loss = 4.2079045e-11,1.5023469e-10\n",
      "Iteration 53725: loss = 4.207921e-11,1.502327e-10\n",
      "Iteration 53730: loss = 4.20783e-11,1.5023083e-10\n",
      "Iteration 53735: loss = 4.207741e-11,1.5022894e-10\n",
      "Iteration 53740: loss = 4.2076388e-11,1.5022686e-10\n",
      "Iteration 53745: loss = 4.207547e-11,1.5022504e-10\n",
      "Iteration 53750: loss = 4.2075e-11,1.5022308e-10\n",
      "Iteration 53755: loss = 4.2074007e-11,1.5022097e-10\n",
      "Iteration 53760: loss = 4.2073175e-11,1.5021893e-10\n",
      "Iteration 53765: loss = 4.207226e-11,1.5021703e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53770: loss = 4.207143e-11,1.5021501e-10\n",
      "Iteration 53775: loss = 4.207043e-11,1.5021326e-10\n",
      "Iteration 53780: loss = 4.2069518e-11,1.5021127e-10\n",
      "Iteration 53785: loss = 4.2068567e-11,1.5020939e-10\n",
      "Iteration 53790: loss = 4.206772e-11,1.5020721e-10\n",
      "Iteration 53795: loss = 4.206818e-11,1.502052e-10\n",
      "Iteration 53800: loss = 4.2067308e-11,1.5020324e-10\n",
      "Iteration 53805: loss = 4.206645e-11,1.5020143e-10\n",
      "Iteration 53810: loss = 4.2065587e-11,1.5019941e-10\n",
      "Iteration 53815: loss = 4.2064695e-11,1.501974e-10\n",
      "Iteration 53820: loss = 4.20637e-11,1.5019547e-10\n",
      "Iteration 53825: loss = 4.20628e-11,1.501936e-10\n",
      "Iteration 53830: loss = 4.2061955e-11,1.5019172e-10\n",
      "Iteration 53835: loss = 4.2061032e-11,1.501898e-10\n",
      "Iteration 53840: loss = 4.2060067e-11,1.5018768e-10\n",
      "Iteration 53845: loss = 4.2059564e-11,1.5018577e-10\n",
      "Iteration 53850: loss = 4.2058666e-11,1.5018396e-10\n",
      "Iteration 53855: loss = 4.2057798e-11,1.50182e-10\n",
      "Iteration 53860: loss = 4.205787e-11,1.5017994e-10\n",
      "Iteration 53865: loss = 4.2056875e-11,1.501779e-10\n",
      "Iteration 53870: loss = 4.2056063e-11,1.5017587e-10\n",
      "Iteration 53875: loss = 4.2055193e-11,1.501739e-10\n",
      "Iteration 53880: loss = 4.205432e-11,1.5017199e-10\n",
      "Iteration 53885: loss = 4.2053392e-11,1.5017007e-10\n",
      "Iteration 53890: loss = 4.2052892e-11,1.5016816e-10\n",
      "Iteration 53895: loss = 4.2052032e-11,1.5016621e-10\n",
      "Iteration 53900: loss = 4.2051113e-11,1.5016421e-10\n",
      "Iteration 53905: loss = 4.2050183e-11,1.5016255e-10\n",
      "Iteration 53910: loss = 4.2049277e-11,1.5016066e-10\n",
      "Iteration 53915: loss = 4.2048365e-11,1.5015858e-10\n",
      "Iteration 53920: loss = 4.204753e-11,1.5015644e-10\n",
      "Iteration 53925: loss = 4.204655e-11,1.5015436e-10\n",
      "Iteration 53930: loss = 4.2046772e-11,1.5015252e-10\n",
      "Iteration 53935: loss = 4.2046144e-11,1.501507e-10\n",
      "Iteration 53940: loss = 4.2045353e-11,1.5014863e-10\n",
      "Iteration 53945: loss = 4.2044323e-11,1.5014684e-10\n",
      "Iteration 53950: loss = 4.2043494e-11,1.5014481e-10\n",
      "Iteration 53955: loss = 4.2042588e-11,1.501431e-10\n",
      "Iteration 53960: loss = 4.204166e-11,1.5014096e-10\n",
      "Iteration 53965: loss = 4.204083e-11,1.5013876e-10\n",
      "Iteration 53970: loss = 4.2039917e-11,1.5013672e-10\n",
      "Iteration 53975: loss = 4.2039008e-11,1.5013507e-10\n",
      "Iteration 53980: loss = 4.2038473e-11,1.5013307e-10\n",
      "Iteration 53985: loss = 4.2037443e-11,1.5013117e-10\n",
      "Iteration 53990: loss = 4.2036447e-11,1.5012913e-10\n",
      "Iteration 53995: loss = 4.203565e-11,1.5012726e-10\n",
      "Iteration 54000: loss = 4.2035736e-11,1.5012522e-10\n",
      "Iteration 54005: loss = 4.2034837e-11,1.5012336e-10\n",
      "Iteration 54010: loss = 4.20339e-11,1.5012147e-10\n",
      "Iteration 54015: loss = 4.203299e-11,1.5011956e-10\n",
      "Iteration 54020: loss = 4.203217e-11,1.5011756e-10\n",
      "Iteration 54025: loss = 4.203157e-11,1.5011563e-10\n",
      "Iteration 54030: loss = 4.2030737e-11,1.5011327e-10\n",
      "Iteration 54035: loss = 4.2029987e-11,1.5011124e-10\n",
      "Iteration 54040: loss = 4.202926e-11,1.5010933e-10\n",
      "Iteration 54045: loss = 4.2028343e-11,1.5010725e-10\n",
      "Iteration 54050: loss = 4.20273e-11,1.5010551e-10\n",
      "Iteration 54055: loss = 4.2026518e-11,1.5010365e-10\n",
      "Iteration 54060: loss = 4.20257e-11,1.5010156e-10\n",
      "Iteration 54065: loss = 4.202599e-11,1.5009927e-10\n",
      "Iteration 54070: loss = 4.2025522e-11,1.500973e-10\n",
      "Iteration 54075: loss = 4.2024707e-11,1.5009538e-10\n",
      "Iteration 54080: loss = 4.2023728e-11,1.5009344e-10\n",
      "Iteration 54085: loss = 4.2022896e-11,1.5009147e-10\n",
      "Iteration 54090: loss = 4.202216e-11,1.5008954e-10\n",
      "Iteration 54095: loss = 4.2021345e-11,1.5008754e-10\n",
      "Iteration 54100: loss = 4.2020484e-11,1.5008558e-10\n",
      "Iteration 54105: loss = 4.201972e-11,1.500834e-10\n",
      "Iteration 54110: loss = 4.2018892e-11,1.5008138e-10\n",
      "Iteration 54115: loss = 4.201833e-11,1.5007948e-10\n",
      "Iteration 54120: loss = 4.2017542e-11,1.5007767e-10\n",
      "Iteration 54125: loss = 4.2016616e-11,1.5007573e-10\n",
      "Iteration 54130: loss = 4.201581e-11,1.5007345e-10\n",
      "Iteration 54135: loss = 4.201599e-11,1.5007126e-10\n",
      "Iteration 54140: loss = 4.2015284e-11,1.5006937e-10\n",
      "Iteration 54145: loss = 4.2014493e-11,1.5006728e-10\n",
      "Iteration 54150: loss = 4.2013695e-11,1.5006524e-10\n",
      "Iteration 54155: loss = 4.2012963e-11,1.500633e-10\n",
      "Iteration 54160: loss = 4.201243e-11,1.5006128e-10\n",
      "Iteration 54165: loss = 4.201173e-11,1.5005919e-10\n",
      "Iteration 54170: loss = 4.2010825e-11,1.5005742e-10\n",
      "Iteration 54175: loss = 4.201005e-11,1.5005547e-10\n",
      "Iteration 54180: loss = 4.200919e-11,1.500533e-10\n",
      "Iteration 54185: loss = 4.2008577e-11,1.5005114e-10\n",
      "Iteration 54190: loss = 4.2007706e-11,1.5004897e-10\n",
      "Iteration 54195: loss = 4.2006815e-11,1.5004713e-10\n",
      "Iteration 54200: loss = 4.200701e-11,1.5004496e-10\n",
      "Iteration 54205: loss = 4.200655e-11,1.5004314e-10\n",
      "Iteration 54210: loss = 4.2005857e-11,1.5004109e-10\n",
      "Iteration 54215: loss = 4.2004945e-11,1.5003902e-10\n",
      "Iteration 54220: loss = 4.2004306e-11,1.5003687e-10\n",
      "Iteration 54225: loss = 4.2003605e-11,1.5003485e-10\n",
      "Iteration 54230: loss = 4.2002925e-11,1.500328e-10\n",
      "Iteration 54235: loss = 4.200219e-11,1.5003074e-10\n",
      "Iteration 54240: loss = 4.2001236e-11,1.5002871e-10\n",
      "Iteration 54245: loss = 4.2000486e-11,1.5002669e-10\n",
      "Iteration 54250: loss = 4.199982e-11,1.500247e-10\n",
      "Iteration 54255: loss = 4.1999324e-11,1.5002255e-10\n",
      "Iteration 54260: loss = 4.199857e-11,1.5002029e-10\n",
      "Iteration 54265: loss = 4.1997798e-11,1.500184e-10\n",
      "Iteration 54270: loss = 4.1998006e-11,1.5001637e-10\n",
      "Iteration 54275: loss = 4.199735e-11,1.5001421e-10\n",
      "Iteration 54280: loss = 4.1996878e-11,1.5001177e-10\n",
      "Iteration 54285: loss = 4.19961e-11,1.5001017e-10\n",
      "Iteration 54290: loss = 4.199534e-11,1.500081e-10\n",
      "Iteration 54295: loss = 4.199494e-11,1.5000592e-10\n",
      "Iteration 54300: loss = 4.199412e-11,1.5000398e-10\n",
      "Iteration 54305: loss = 4.1993357e-11,1.5000198e-10\n",
      "Iteration 54310: loss = 4.1992503e-11,1.4999982e-10\n",
      "Iteration 54315: loss = 4.1991743e-11,1.4999793e-10\n",
      "Iteration 54320: loss = 4.1990966e-11,1.4999597e-10\n",
      "Iteration 54325: loss = 4.199016e-11,1.4999377e-10\n",
      "Iteration 54330: loss = 4.1989374e-11,1.4999176e-10\n",
      "Iteration 54335: loss = 4.1988638e-11,1.4998947e-10\n",
      "Iteration 54340: loss = 4.1988874e-11,1.499875e-10\n",
      "Iteration 54345: loss = 4.1988326e-11,1.4998564e-10\n",
      "Iteration 54350: loss = 4.1987497e-11,1.4998355e-10\n",
      "Iteration 54355: loss = 4.1986813e-11,1.4998142e-10\n",
      "Iteration 54360: loss = 4.1986175e-11,1.499794e-10\n",
      "Iteration 54365: loss = 4.1985544e-11,1.4997717e-10\n",
      "Iteration 54370: loss = 4.125987e-11,1.4997525e-10\n",
      "Iteration 54375: loss = 4.1257622e-11,1.4997453e-10\n",
      "Iteration 54380: loss = 4.1254316e-11,1.499745e-10\n",
      "Iteration 54385: loss = 4.1250583e-11,1.4997453e-10\n",
      "Iteration 54390: loss = 4.1246895e-11,1.4997478e-10\n",
      "Iteration 54395: loss = 4.124419e-11,1.4997417e-10\n",
      "Iteration 54400: loss = 4.124162e-11,1.4997341e-10\n",
      "Iteration 54405: loss = 4.1240705e-11,1.4997252e-10\n",
      "Iteration 54410: loss = 4.1238304e-11,1.499717e-10\n",
      "Iteration 54415: loss = 4.1236053e-11,1.4997088e-10\n",
      "Iteration 54420: loss = 4.123388e-11,1.4996987e-10\n",
      "Iteration 54425: loss = 4.123187e-11,1.4996884e-10\n",
      "Iteration 54430: loss = 4.1229836e-11,1.4996773e-10\n",
      "Iteration 54435: loss = 4.1228063e-11,1.499666e-10\n",
      "Iteration 54440: loss = 4.1225995e-11,1.4996532e-10\n",
      "Iteration 54445: loss = 4.122411e-11,1.499643e-10\n",
      "Iteration 54450: loss = 4.1222612e-11,1.4996328e-10\n",
      "Iteration 54455: loss = 4.1220697e-11,1.4996206e-10\n",
      "Iteration 54460: loss = 4.1218733e-11,1.4996093e-10\n",
      "Iteration 54465: loss = 4.121688e-11,1.4995977e-10\n",
      "Iteration 54470: loss = 4.1214768e-11,1.4995863e-10\n",
      "Iteration 54475: loss = 4.121395e-11,1.4995744e-10\n",
      "Iteration 54480: loss = 4.1212225e-11,1.4995619e-10\n",
      "Iteration 54485: loss = 4.1210427e-11,1.4995488e-10\n",
      "Iteration 54490: loss = 4.120872e-11,1.4995372e-10\n",
      "Iteration 54495: loss = 4.120722e-11,1.4995204e-10\n",
      "Iteration 54500: loss = 4.1205563e-11,1.4995094e-10\n",
      "Iteration 54505: loss = 4.120403e-11,1.4994965e-10\n",
      "Iteration 54510: loss = 4.120249e-11,1.4994816e-10\n",
      "Iteration 54515: loss = 4.1200817e-11,1.4994672e-10\n",
      "Iteration 54520: loss = 4.1199395e-11,1.499452e-10\n",
      "Iteration 54525: loss = 4.1197813e-11,1.4994384e-10\n",
      "Iteration 54530: loss = 4.1196366e-11,1.4994227e-10\n",
      "Iteration 54535: loss = 4.1194836e-11,1.4994084e-10\n",
      "Iteration 54540: loss = 4.119359e-11,1.4993948e-10\n",
      "Iteration 54545: loss = 4.1193157e-11,1.4993803e-10\n",
      "Iteration 54550: loss = 4.1191706e-11,1.4993651e-10\n",
      "Iteration 54555: loss = 4.119023e-11,1.4993501e-10\n",
      "Iteration 54560: loss = 4.1188785e-11,1.4993347e-10\n",
      "Iteration 54565: loss = 4.1187387e-11,1.49932e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54570: loss = 4.1185833e-11,1.499306e-10\n",
      "Iteration 54575: loss = 4.1184566e-11,1.499292e-10\n",
      "Iteration 54580: loss = 4.1183206e-11,1.499276e-10\n",
      "Iteration 54585: loss = 4.1182127e-11,1.49926e-10\n",
      "Iteration 54590: loss = 4.1180764e-11,1.4992436e-10\n",
      "Iteration 54595: loss = 4.117942e-11,1.4992287e-10\n",
      "Iteration 54600: loss = 4.1178245e-11,1.4992152e-10\n",
      "Iteration 54605: loss = 4.1176753e-11,1.4991991e-10\n",
      "Iteration 54610: loss = 4.1176104e-11,1.4991831e-10\n",
      "Iteration 54615: loss = 4.117477e-11,1.4991691e-10\n",
      "Iteration 54620: loss = 4.1173426e-11,1.4991552e-10\n",
      "Iteration 54625: loss = 4.1171982e-11,1.4991391e-10\n",
      "Iteration 54630: loss = 4.117091e-11,1.499124e-10\n",
      "Iteration 54635: loss = 4.1169502e-11,1.499109e-10\n",
      "Iteration 54640: loss = 4.1168124e-11,1.4990939e-10\n",
      "Iteration 54645: loss = 4.1166733e-11,1.4990788e-10\n",
      "Iteration 54650: loss = 4.1165446e-11,1.499063e-10\n",
      "Iteration 54655: loss = 4.1164096e-11,1.4990462e-10\n",
      "Iteration 54660: loss = 4.1162625e-11,1.4990317e-10\n",
      "Iteration 54665: loss = 4.116126e-11,1.4990165e-10\n",
      "Iteration 54670: loss = 4.1160044e-11,1.4989988e-10\n",
      "Iteration 54675: loss = 4.1158913e-11,1.4989848e-10\n",
      "Iteration 54680: loss = 4.1158663e-11,1.498969e-10\n",
      "Iteration 54685: loss = 4.115759e-11,1.498952e-10\n",
      "Iteration 54690: loss = 4.1156422e-11,1.498935e-10\n",
      "Iteration 54695: loss = 4.1155274e-11,1.4989185e-10\n",
      "Iteration 54700: loss = 4.1154056e-11,1.4989016e-10\n",
      "Iteration 54705: loss = 4.115284e-11,1.498886e-10\n",
      "Iteration 54710: loss = 4.1151766e-11,1.498867e-10\n",
      "Iteration 54715: loss = 4.115064e-11,1.4988502e-10\n",
      "Iteration 54720: loss = 4.1149823e-11,1.4988329e-10\n",
      "Iteration 54725: loss = 4.114858e-11,1.4988158e-10\n",
      "Iteration 54730: loss = 4.1147464e-11,1.4987991e-10\n",
      "Iteration 54735: loss = 4.1146406e-11,1.4987826e-10\n",
      "Iteration 54740: loss = 4.114516e-11,1.498766e-10\n",
      "Iteration 54745: loss = 4.1143994e-11,1.4987456e-10\n",
      "Iteration 54750: loss = 4.114405e-11,1.49873e-10\n",
      "Iteration 54755: loss = 4.114283e-11,1.4987141e-10\n",
      "Iteration 54760: loss = 4.114172e-11,1.4986962e-10\n",
      "Iteration 54765: loss = 4.1140924e-11,1.4986798e-10\n",
      "Iteration 54770: loss = 4.1139824e-11,1.498662e-10\n",
      "Iteration 54775: loss = 4.1138638e-11,1.4986444e-10\n",
      "Iteration 54780: loss = 4.1137524e-11,1.4986275e-10\n",
      "Iteration 54785: loss = 4.1136334e-11,1.4986101e-10\n",
      "Iteration 54790: loss = 4.113522e-11,1.498592e-10\n",
      "Iteration 54795: loss = 4.113404e-11,1.4985754e-10\n",
      "Iteration 54800: loss = 4.113289e-11,1.4985585e-10\n",
      "Iteration 54805: loss = 4.113186e-11,1.4985423e-10\n",
      "Iteration 54810: loss = 4.113113e-11,1.4985249e-10\n",
      "Iteration 54815: loss = 4.1130935e-11,1.4985083e-10\n",
      "Iteration 54820: loss = 4.1130013e-11,1.4984915e-10\n",
      "Iteration 54825: loss = 4.112879e-11,1.498472e-10\n",
      "Iteration 54830: loss = 4.1127615e-11,1.4984557e-10\n",
      "Iteration 54835: loss = 4.1126543e-11,1.498437e-10\n",
      "Iteration 54840: loss = 4.112546e-11,1.4984206e-10\n",
      "Iteration 54845: loss = 4.112433e-11,1.4984033e-10\n",
      "Iteration 54850: loss = 4.1123275e-11,1.4983868e-10\n",
      "Iteration 54855: loss = 4.1122137e-11,1.4983687e-10\n",
      "Iteration 54860: loss = 4.1121315e-11,1.4983517e-10\n",
      "Iteration 54865: loss = 4.1120284e-11,1.4983328e-10\n",
      "Iteration 54870: loss = 4.111919e-11,1.498315e-10\n",
      "Iteration 54875: loss = 4.111803e-11,1.4982973e-10\n",
      "Iteration 54880: loss = 4.1116957e-11,1.4982804e-10\n",
      "Iteration 54885: loss = 4.1116933e-11,1.4982621e-10\n",
      "Iteration 54890: loss = 4.1115774e-11,1.4982442e-10\n",
      "Iteration 54895: loss = 4.111469e-11,1.4982277e-10\n",
      "Iteration 54900: loss = 4.111363e-11,1.498211e-10\n",
      "Iteration 54905: loss = 4.111297e-11,1.4981927e-10\n",
      "Iteration 54910: loss = 4.111173e-11,1.4981755e-10\n",
      "Iteration 54915: loss = 4.1110698e-11,1.4981576e-10\n",
      "Iteration 54920: loss = 4.1109626e-11,1.4981393e-10\n",
      "Iteration 54925: loss = 4.110853e-11,1.498121e-10\n",
      "Iteration 54930: loss = 4.1107447e-11,1.498105e-10\n",
      "Iteration 54935: loss = 4.1106393e-11,1.4980861e-10\n",
      "Iteration 54940: loss = 4.1105282e-11,1.4980692e-10\n",
      "Iteration 54945: loss = 4.1104276e-11,1.4980506e-10\n",
      "Iteration 54950: loss = 4.110341e-11,1.4980339e-10\n",
      "Iteration 54955: loss = 4.11034e-11,1.4980184e-10\n",
      "Iteration 54960: loss = 4.110233e-11,1.4979995e-10\n",
      "Iteration 54965: loss = 4.110115e-11,1.4979819e-10\n",
      "Iteration 54970: loss = 4.11001e-11,1.4979651e-10\n",
      "Iteration 54975: loss = 4.109907e-11,1.4979486e-10\n",
      "Iteration 54980: loss = 4.1098014e-11,1.4979308e-10\n",
      "Iteration 54985: loss = 4.109693e-11,1.4979129e-10\n",
      "Iteration 54990: loss = 4.1095915e-11,1.4978954e-10\n",
      "Iteration 54995: loss = 4.1095214e-11,1.4978768e-10\n",
      "Iteration 55000: loss = 4.1094225e-11,1.4978592e-10\n",
      "Iteration 55005: loss = 4.109334e-11,1.4978394e-10\n",
      "Iteration 55010: loss = 4.1092445e-11,1.4978224e-10\n",
      "Iteration 55015: loss = 4.109152e-11,1.4978033e-10\n",
      "Iteration 55020: loss = 4.1091696e-11,1.4977827e-10\n",
      "Iteration 55025: loss = 4.109084e-11,1.4977634e-10\n",
      "Iteration 55030: loss = 4.1089937e-11,1.4977465e-10\n",
      "Iteration 55035: loss = 4.108903e-11,1.4977278e-10\n",
      "Iteration 55040: loss = 4.1088594e-11,1.4977086e-10\n",
      "Iteration 55045: loss = 4.1087658e-11,1.4976888e-10\n",
      "Iteration 55050: loss = 4.1086783e-11,1.4976687e-10\n",
      "Iteration 55055: loss = 4.108597e-11,1.4976495e-10\n",
      "Iteration 55060: loss = 4.1085125e-11,1.4976305e-10\n",
      "Iteration 55065: loss = 4.1084258e-11,1.4976115e-10\n",
      "Iteration 55070: loss = 4.108347e-11,1.497593e-10\n",
      "Iteration 55075: loss = 4.108257e-11,1.4975736e-10\n",
      "Iteration 55080: loss = 4.1081832e-11,1.4975553e-10\n",
      "Iteration 55085: loss = 4.1081305e-11,1.4975346e-10\n",
      "Iteration 55090: loss = 4.1081517e-11,1.4975152e-10\n",
      "Iteration 55095: loss = 4.1080653e-11,1.4974964e-10\n",
      "Iteration 55100: loss = 4.1079848e-11,1.497477e-10\n",
      "Iteration 55105: loss = 4.1079074e-11,1.4974587e-10\n",
      "Iteration 55110: loss = 4.1078252e-11,1.4974383e-10\n",
      "Iteration 55115: loss = 4.1077423e-11,1.4974191e-10\n",
      "Iteration 55120: loss = 4.1076618e-11,1.4973987e-10\n",
      "Iteration 55125: loss = 4.1075827e-11,1.4973792e-10\n",
      "Iteration 55130: loss = 4.10753e-11,1.4973606e-10\n",
      "Iteration 55135: loss = 4.1074564e-11,1.4973423e-10\n",
      "Iteration 55140: loss = 4.107373e-11,1.4973214e-10\n",
      "Iteration 55145: loss = 4.1072975e-11,1.4973027e-10\n",
      "Iteration 55150: loss = 4.10721e-11,1.4972823e-10\n",
      "Iteration 55155: loss = 4.1072253e-11,1.4972634e-10\n",
      "Iteration 55160: loss = 4.1071476e-11,1.4972432e-10\n",
      "Iteration 55165: loss = 4.1070675e-11,1.4972258e-10\n",
      "Iteration 55170: loss = 4.1069894e-11,1.4972054e-10\n",
      "Iteration 55175: loss = 4.1069374e-11,1.4971847e-10\n",
      "Iteration 55180: loss = 4.106844e-11,1.4971671e-10\n",
      "Iteration 55185: loss = 4.1067694e-11,1.4971463e-10\n",
      "Iteration 55190: loss = 4.1066876e-11,1.4971283e-10\n",
      "Iteration 55195: loss = 4.1066022e-11,1.4971085e-10\n",
      "Iteration 55200: loss = 4.1065203e-11,1.4970894e-10\n",
      "Iteration 55205: loss = 4.1064402e-11,1.4970702e-10\n",
      "Iteration 55210: loss = 4.1063677e-11,1.4970508e-10\n",
      "Iteration 55215: loss = 4.1062792e-11,1.49703e-10\n",
      "Iteration 55220: loss = 4.1062344e-11,1.4970124e-10\n",
      "Iteration 55225: loss = 4.1062525e-11,1.4969939e-10\n",
      "Iteration 55230: loss = 4.1061727e-11,1.4969748e-10\n",
      "Iteration 55235: loss = 4.1060915e-11,1.4969528e-10\n",
      "Iteration 55240: loss = 4.1060214e-11,1.4969355e-10\n",
      "Iteration 55245: loss = 4.105945e-11,1.4969154e-10\n",
      "Iteration 55250: loss = 4.1058695e-11,1.4968943e-10\n",
      "Iteration 55255: loss = 4.1057966e-11,1.4968739e-10\n",
      "Iteration 55260: loss = 4.105724e-11,1.4968536e-10\n",
      "Iteration 55265: loss = 4.105687e-11,1.496834e-10\n",
      "Iteration 55270: loss = 4.1056058e-11,1.4968132e-10\n",
      "Iteration 55275: loss = 4.1055517e-11,1.4967926e-10\n",
      "Iteration 55280: loss = 4.1054646e-11,1.4967746e-10\n",
      "Iteration 55285: loss = 4.1053962e-11,1.4967541e-10\n",
      "Iteration 55290: loss = 4.1053157e-11,1.4967345e-10\n",
      "Iteration 55295: loss = 4.105348e-11,1.4967128e-10\n",
      "Iteration 55300: loss = 4.1052797e-11,1.4966944e-10\n",
      "Iteration 55305: loss = 4.105209e-11,1.4966733e-10\n",
      "Iteration 55310: loss = 4.1051676e-11,1.4966542e-10\n",
      "Iteration 55315: loss = 4.105091e-11,1.4966339e-10\n",
      "Iteration 55320: loss = 4.105007e-11,1.4966142e-10\n",
      "Iteration 55325: loss = 4.1049383e-11,1.4965937e-10\n",
      "Iteration 55330: loss = 4.1048834e-11,1.496574e-10\n",
      "Iteration 55335: loss = 4.104794e-11,1.4965545e-10\n",
      "Iteration 55340: loss = 4.104719e-11,1.4965355e-10\n",
      "Iteration 55345: loss = 4.1046683e-11,1.4965144e-10\n",
      "Iteration 55350: loss = 4.104583e-11,1.496494e-10\n",
      "Iteration 55355: loss = 4.1045143e-11,1.4964724e-10\n",
      "Iteration 55360: loss = 4.10456e-11,1.496454e-10\n",
      "Iteration 55365: loss = 4.1044928e-11,1.496434e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55370: loss = 4.104432e-11,1.4964122e-10\n",
      "Iteration 55375: loss = 4.104357e-11,1.4963937e-10\n",
      "Iteration 55380: loss = 4.104286e-11,1.4963736e-10\n",
      "Iteration 55385: loss = 4.1042034e-11,1.4963543e-10\n",
      "Iteration 55390: loss = 4.1041323e-11,1.4963328e-10\n",
      "Iteration 55395: loss = 4.104064e-11,1.4963139e-10\n",
      "Iteration 55400: loss = 4.103999e-11,1.4962942e-10\n",
      "Iteration 55405: loss = 4.1039477e-11,1.4962748e-10\n",
      "Iteration 55410: loss = 4.1038766e-11,1.4962548e-10\n",
      "Iteration 55415: loss = 4.1038176e-11,1.4962317e-10\n",
      "Iteration 55420: loss = 4.1037493e-11,1.496211e-10\n",
      "Iteration 55425: loss = 4.1036896e-11,1.4961907e-10\n",
      "Iteration 55430: loss = 4.103717e-11,1.4961712e-10\n",
      "Iteration 55435: loss = 4.103663e-11,1.4961507e-10\n",
      "Iteration 55440: loss = 4.103584e-11,1.4961309e-10\n",
      "Iteration 55445: loss = 4.103523e-11,1.4961099e-10\n",
      "Iteration 55450: loss = 4.103481e-11,1.4960885e-10\n",
      "Iteration 55455: loss = 4.1034082e-11,1.4960698e-10\n",
      "Iteration 55460: loss = 4.1033153e-11,1.4960497e-10\n",
      "Iteration 55465: loss = 4.103196e-11,1.4960343e-10\n",
      "Iteration 55470: loss = 4.102875e-11,1.4960325e-10\n",
      "Iteration 55475: loss = 4.1025683e-11,1.4960314e-10\n",
      "Iteration 55480: loss = 4.102237e-11,1.4960326e-10\n",
      "Iteration 55485: loss = 4.101906e-11,1.4960322e-10\n",
      "Iteration 55490: loss = 4.101584e-11,1.4960307e-10\n",
      "Iteration 55495: loss = 4.1012815e-11,1.4960298e-10\n",
      "Iteration 55500: loss = 4.1010747e-11,1.4960296e-10\n",
      "Iteration 55505: loss = 4.100763e-11,1.4960261e-10\n",
      "Iteration 55510: loss = 4.1004724e-11,1.4960233e-10\n",
      "Iteration 55515: loss = 4.1001758e-11,1.4960211e-10\n",
      "Iteration 55520: loss = 4.0998687e-11,1.4960179e-10\n",
      "Iteration 55525: loss = 4.099678e-11,1.4960078e-10\n",
      "Iteration 55530: loss = 4.0995738e-11,1.4959907e-10\n",
      "Iteration 55535: loss = 4.0994454e-11,1.4959745e-10\n",
      "Iteration 55540: loss = 4.0993663e-11,1.4959568e-10\n",
      "Iteration 55545: loss = 4.0992407e-11,1.4959414e-10\n",
      "Iteration 55550: loss = 4.099135e-11,1.495924e-10\n",
      "Iteration 55555: loss = 4.0990183e-11,1.4959073e-10\n",
      "Iteration 55560: loss = 4.098812e-11,1.4958965e-10\n",
      "Iteration 55565: loss = 4.0987658e-11,1.4958826e-10\n",
      "Iteration 55570: loss = 4.0986506e-11,1.4958676e-10\n",
      "Iteration 55575: loss = 4.0985354e-11,1.4958498e-10\n",
      "Iteration 55580: loss = 4.098265e-11,1.4958462e-10\n",
      "Iteration 55585: loss = 4.0981538e-11,1.4958311e-10\n",
      "Iteration 55590: loss = 4.0980327e-11,1.4958139e-10\n",
      "Iteration 55595: loss = 4.0979237e-11,1.4957986e-10\n",
      "Iteration 55600: loss = 4.0976732e-11,1.4957915e-10\n",
      "Iteration 55605: loss = 4.0973572e-11,1.4957899e-10\n",
      "Iteration 55610: loss = 4.0970376e-11,1.4957888e-10\n",
      "Iteration 55615: loss = 4.096706e-11,1.4957886e-10\n",
      "Iteration 55620: loss = 4.0963854e-11,1.4957874e-10\n",
      "Iteration 55625: loss = 4.096053e-11,1.4957863e-10\n",
      "Iteration 55630: loss = 4.0957654e-11,1.4957875e-10\n",
      "Iteration 55635: loss = 4.0955583e-11,1.4957849e-10\n",
      "Iteration 55640: loss = 4.095242e-11,1.495782e-10\n",
      "Iteration 55645: loss = 4.0949345e-11,1.4957807e-10\n",
      "Iteration 55650: loss = 4.0946233e-11,1.4957777e-10\n",
      "Iteration 55655: loss = 4.0943183e-11,1.4957777e-10\n",
      "Iteration 55660: loss = 4.094021e-11,1.4957752e-10\n",
      "Iteration 55665: loss = 4.093724e-11,1.4957734e-10\n",
      "Iteration 55670: loss = 4.0934187e-11,1.4957716e-10\n",
      "Iteration 55675: loss = 4.093158e-11,1.4957696e-10\n",
      "Iteration 55680: loss = 4.0928566e-11,1.4957666e-10\n",
      "Iteration 55685: loss = 4.0925586e-11,1.4957632e-10\n",
      "Iteration 55690: loss = 4.0922793e-11,1.4957588e-10\n",
      "Iteration 55695: loss = 4.0920073e-11,1.4957549e-10\n",
      "Iteration 55700: loss = 4.0917204e-11,1.4957505e-10\n",
      "Iteration 55705: loss = 4.0915552e-11,1.4957444e-10\n",
      "Iteration 55710: loss = 4.0912926e-11,1.4957396e-10\n",
      "Iteration 55715: loss = 4.091033e-11,1.4957337e-10\n",
      "Iteration 55720: loss = 4.0907888e-11,1.4957306e-10\n",
      "Iteration 55725: loss = 4.0189976e-11,1.4957269e-10\n",
      "Iteration 55730: loss = 4.0185928e-11,1.4957321e-10\n",
      "Iteration 55735: loss = 4.0177583e-11,1.4957698e-10\n",
      "Iteration 55740: loss = 4.0169288e-11,1.4958093e-10\n",
      "Iteration 55745: loss = 4.0163355e-11,1.495831e-10\n",
      "Iteration 55750: loss = 4.0160236e-11,1.4958262e-10\n",
      "Iteration 55755: loss = 4.0158505e-11,1.4958228e-10\n",
      "Iteration 55760: loss = 4.015521e-11,1.4958187e-10\n",
      "Iteration 55765: loss = 4.0152468e-11,1.4958146e-10\n",
      "Iteration 55770: loss = 4.0150192e-11,1.4958129e-10\n",
      "Iteration 55775: loss = 4.014411e-11,1.4958336e-10\n",
      "Iteration 55780: loss = 4.0137685e-11,1.4958626e-10\n",
      "Iteration 55785: loss = 4.0130847e-11,1.4958894e-10\n",
      "Iteration 55790: loss = 4.0124352e-11,1.495917e-10\n",
      "Iteration 55795: loss = 4.0117576e-11,1.4959424e-10\n",
      "Iteration 55800: loss = 4.0111244e-11,1.49597e-10\n",
      "Iteration 55805: loss = 4.0104458e-11,1.4959944e-10\n",
      "Iteration 55810: loss = 4.009809e-11,1.4960203e-10\n",
      "Iteration 55815: loss = 4.0091364e-11,1.4960458e-10\n",
      "Iteration 55820: loss = 4.0084772e-11,1.4960709e-10\n",
      "Iteration 55825: loss = 4.00795e-11,1.4960949e-10\n",
      "Iteration 55830: loss = 4.007283e-11,1.4961193e-10\n",
      "Iteration 55835: loss = 4.006645e-11,1.4961471e-10\n",
      "Iteration 55840: loss = 4.0060077e-11,1.4961701e-10\n",
      "Iteration 55845: loss = 4.0053984e-11,1.4961939e-10\n",
      "Iteration 55850: loss = 4.0047975e-11,1.4962197e-10\n",
      "Iteration 55855: loss = 4.004185e-11,1.4962401e-10\n",
      "Iteration 55860: loss = 4.0035985e-11,1.4962628e-10\n",
      "Iteration 55865: loss = 4.003012e-11,1.4962839e-10\n",
      "Iteration 55870: loss = 4.0024522e-11,1.4963039e-10\n",
      "Iteration 55875: loss = 4.0018752e-11,1.4963208e-10\n",
      "Iteration 55880: loss = 4.0013257e-11,1.4963393e-10\n",
      "Iteration 55885: loss = 4.000746e-11,1.4963611e-10\n",
      "Iteration 55890: loss = 4.000301e-11,1.4963769e-10\n",
      "Iteration 55895: loss = 3.999732e-11,1.496398e-10\n",
      "Iteration 55900: loss = 3.999168e-11,1.496416e-10\n",
      "Iteration 55905: loss = 3.9986302e-11,1.4964324e-10\n",
      "Iteration 55910: loss = 3.998063e-11,1.4964507e-10\n",
      "Iteration 55915: loss = 3.9975356e-11,1.4964661e-10\n",
      "Iteration 55920: loss = 3.9969725e-11,1.4964852e-10\n",
      "Iteration 55925: loss = 3.996452e-11,1.4965053e-10\n",
      "Iteration 55930: loss = 3.9958935e-11,1.4965221e-10\n",
      "Iteration 55935: loss = 3.995343e-11,1.4965408e-10\n",
      "Iteration 55940: loss = 3.99482e-11,1.4965572e-10\n",
      "Iteration 55945: loss = 3.9942747e-11,1.4965758e-10\n",
      "Iteration 55950: loss = 3.993754e-11,1.4965923e-10\n",
      "Iteration 55955: loss = 3.993211e-11,1.496607e-10\n",
      "Iteration 55960: loss = 3.992802e-11,1.4966249e-10\n",
      "Iteration 55965: loss = 3.9922666e-11,1.4966411e-10\n",
      "Iteration 55970: loss = 3.991757e-11,1.4966572e-10\n",
      "Iteration 55975: loss = 3.9912355e-11,1.4966718e-10\n",
      "Iteration 55980: loss = 3.9907123e-11,1.4966847e-10\n",
      "Iteration 55985: loss = 3.9902182e-11,1.4967022e-10\n",
      "Iteration 55990: loss = 3.989695e-11,1.4967172e-10\n",
      "Iteration 55995: loss = 3.9892145e-11,1.4967322e-10\n",
      "Iteration 56000: loss = 3.98869e-11,1.4967486e-10\n",
      "Iteration 56005: loss = 3.988196e-11,1.496764e-10\n",
      "Iteration 56010: loss = 3.9876647e-11,1.4967802e-10\n",
      "Iteration 56015: loss = 3.987177e-11,1.4967952e-10\n",
      "Iteration 56020: loss = 3.98665e-11,1.4968114e-10\n",
      "Iteration 56025: loss = 3.9861347e-11,1.4968266e-10\n",
      "Iteration 56030: loss = 3.9151574e-11,1.4968415e-10\n",
      "Iteration 56035: loss = 3.91476e-11,1.4968436e-10\n",
      "Iteration 56040: loss = 3.9144826e-11,1.4968475e-10\n",
      "Iteration 56045: loss = 3.9140653e-11,1.4968501e-10\n",
      "Iteration 56050: loss = 3.9136278e-11,1.4968587e-10\n",
      "Iteration 56055: loss = 3.9132836e-11,1.4968685e-10\n",
      "Iteration 56060: loss = 3.9130078e-11,1.4968662e-10\n",
      "Iteration 56065: loss = 3.9127857e-11,1.4968628e-10\n",
      "Iteration 56070: loss = 3.9125193e-11,1.4968582e-10\n",
      "Iteration 56075: loss = 3.9122056e-11,1.4968549e-10\n",
      "Iteration 56080: loss = 3.9119905e-11,1.4968507e-10\n",
      "Iteration 56085: loss = 3.911714e-11,1.4968468e-10\n",
      "Iteration 56090: loss = 3.911499e-11,1.4968415e-10\n",
      "Iteration 56095: loss = 3.911238e-11,1.4968386e-10\n",
      "Iteration 56100: loss = 3.9109636e-11,1.4968321e-10\n",
      "Iteration 56105: loss = 3.91082e-11,1.4968254e-10\n",
      "Iteration 56110: loss = 3.9105445e-11,1.496818e-10\n",
      "Iteration 56115: loss = 3.910423e-11,1.4968105e-10\n",
      "Iteration 56120: loss = 3.9101628e-11,1.4968027e-10\n",
      "Iteration 56125: loss = 3.9098988e-11,1.4967955e-10\n",
      "Iteration 56130: loss = 3.9097624e-11,1.4967876e-10\n",
      "Iteration 56135: loss = 3.9095168e-11,1.4967795e-10\n",
      "Iteration 56140: loss = 3.9093558e-11,1.4967745e-10\n",
      "Iteration 56145: loss = 3.9090425e-11,1.4967713e-10\n",
      "Iteration 56150: loss = 3.908761e-11,1.4967692e-10\n",
      "Iteration 56155: loss = 3.9085245e-11,1.4967651e-10\n",
      "Iteration 56160: loss = 3.9082043e-11,1.4967642e-10\n",
      "Iteration 56165: loss = 3.9079965e-11,1.4967613e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56170: loss = 3.9076766e-11,1.4967576e-10\n",
      "Iteration 56175: loss = 3.9073758e-11,1.4967565e-10\n",
      "Iteration 56180: loss = 3.9071493e-11,1.4967533e-10\n",
      "Iteration 56185: loss = 3.906885e-11,1.4967518e-10\n",
      "Iteration 56190: loss = 3.9066795e-11,1.4967488e-10\n",
      "Iteration 56195: loss = 3.9064158e-11,1.4967447e-10\n",
      "Iteration 56200: loss = 3.9061237e-11,1.4967393e-10\n",
      "Iteration 56205: loss = 3.9059252e-11,1.4967336e-10\n",
      "Iteration 56210: loss = 3.90568e-11,1.4967286e-10\n",
      "Iteration 56215: loss = 3.905484e-11,1.496726e-10\n",
      "Iteration 56220: loss = 3.9052327e-11,1.496718e-10\n",
      "Iteration 56225: loss = 3.904939e-11,1.4967141e-10\n",
      "Iteration 56230: loss = 3.9048112e-11,1.4967069e-10\n",
      "Iteration 56235: loss = 3.9045437e-11,1.4966986e-10\n",
      "Iteration 56240: loss = 3.9044278e-11,1.4966918e-10\n",
      "Iteration 56245: loss = 3.904167e-11,1.4966842e-10\n",
      "Iteration 56250: loss = 3.903912e-11,1.496676e-10\n",
      "Iteration 56255: loss = 3.9037853e-11,1.4966672e-10\n",
      "Iteration 56260: loss = 3.903533e-11,1.4966602e-10\n",
      "Iteration 56265: loss = 3.9034023e-11,1.4966525e-10\n",
      "Iteration 56270: loss = 3.9031486e-11,1.4966442e-10\n",
      "Iteration 56275: loss = 3.9029575e-11,1.4966349e-10\n",
      "Iteration 56280: loss = 3.9028267e-11,1.4966253e-10\n",
      "Iteration 56285: loss = 3.9025977e-11,1.4966166e-10\n",
      "Iteration 56290: loss = 3.9024933e-11,1.4966053e-10\n",
      "Iteration 56295: loss = 3.902266e-11,1.496595e-10\n",
      "Iteration 56300: loss = 3.9020776e-11,1.4965854e-10\n",
      "Iteration 56305: loss = 3.9019437e-11,1.4965752e-10\n",
      "Iteration 56310: loss = 3.9017515e-11,1.496566e-10\n",
      "Iteration 56315: loss = 3.9016124e-11,1.4965554e-10\n",
      "Iteration 56320: loss = 3.9014268e-11,1.4965455e-10\n",
      "Iteration 56325: loss = 3.9012012e-11,1.4965354e-10\n",
      "Iteration 56330: loss = 3.9010743e-11,1.4965246e-10\n",
      "Iteration 56335: loss = 3.900905e-11,1.496514e-10\n",
      "Iteration 56340: loss = 3.9007783e-11,1.4965026e-10\n",
      "Iteration 56345: loss = 3.9006024e-11,1.4964927e-10\n",
      "Iteration 56350: loss = 3.900379e-11,1.4964835e-10\n",
      "Iteration 56355: loss = 3.9002936e-11,1.4964704e-10\n",
      "Iteration 56360: loss = 3.9000705e-11,1.4964603e-10\n",
      "Iteration 56365: loss = 3.899955e-11,1.4964494e-10\n",
      "Iteration 56370: loss = 3.8997795e-11,1.4964395e-10\n",
      "Iteration 56375: loss = 3.899561e-11,1.4964269e-10\n",
      "Iteration 56380: loss = 3.899478e-11,1.4964155e-10\n",
      "Iteration 56385: loss = 3.8992625e-11,1.4964055e-10\n",
      "Iteration 56390: loss = 3.8991754e-11,1.4963941e-10\n",
      "Iteration 56395: loss = 3.8989645e-11,1.4963839e-10\n",
      "Iteration 56400: loss = 3.8988007e-11,1.496372e-10\n",
      "Iteration 56405: loss = 3.898699e-11,1.4963603e-10\n",
      "Iteration 56410: loss = 3.898495e-11,1.496348e-10\n",
      "Iteration 56415: loss = 3.8984458e-11,1.4963351e-10\n",
      "Iteration 56420: loss = 3.8982595e-11,1.4963211e-10\n",
      "Iteration 56425: loss = 3.8981905e-11,1.496309e-10\n",
      "Iteration 56430: loss = 3.897994e-11,1.4962973e-10\n",
      "Iteration 56435: loss = 3.8978442e-11,1.4962832e-10\n",
      "Iteration 56440: loss = 3.8977502e-11,1.4962703e-10\n",
      "Iteration 56445: loss = 3.897591e-11,1.4962584e-10\n",
      "Iteration 56450: loss = 3.897503e-11,1.4962459e-10\n",
      "Iteration 56455: loss = 3.8973196e-11,1.4962323e-10\n",
      "Iteration 56460: loss = 3.8971684e-11,1.4962195e-10\n",
      "Iteration 56465: loss = 3.8970705e-11,1.4962057e-10\n",
      "Iteration 56470: loss = 3.8969255e-11,1.4961936e-10\n",
      "Iteration 56475: loss = 3.8968245e-11,1.4961798e-10\n",
      "Iteration 56480: loss = 3.8966694e-11,1.4961668e-10\n",
      "Iteration 56485: loss = 3.896472e-11,1.4961545e-10\n",
      "Iteration 56490: loss = 3.8963846e-11,1.4961421e-10\n",
      "Iteration 56495: loss = 3.89624e-11,1.4961299e-10\n",
      "Iteration 56500: loss = 3.896144e-11,1.4961155e-10\n",
      "Iteration 56505: loss = 3.8959853e-11,1.4961032e-10\n",
      "Iteration 56510: loss = 3.895806e-11,1.4960899e-10\n",
      "Iteration 56515: loss = 3.895743e-11,1.496076e-10\n",
      "Iteration 56520: loss = 3.8955523e-11,1.496063e-10\n",
      "Iteration 56525: loss = 3.8955006e-11,1.496051e-10\n",
      "Iteration 56530: loss = 3.8953063e-11,1.4960384e-10\n",
      "Iteration 56535: loss = 3.8951314e-11,1.4960251e-10\n",
      "Iteration 56540: loss = 3.8950763e-11,1.496012e-10\n",
      "Iteration 56545: loss = 3.8949104e-11,1.4959983e-10\n",
      "Iteration 56550: loss = 3.894853e-11,1.4959836e-10\n",
      "Iteration 56555: loss = 3.8946752e-11,1.4959699e-10\n",
      "Iteration 56560: loss = 3.8945427e-11,1.4959582e-10\n",
      "Iteration 56565: loss = 3.8944504e-11,1.4959446e-10\n",
      "Iteration 56570: loss = 3.8942728e-11,1.4959298e-10\n",
      "Iteration 56575: loss = 3.894221e-11,1.4959162e-10\n",
      "Iteration 56580: loss = 3.894046e-11,1.4959028e-10\n",
      "Iteration 56585: loss = 3.8939088e-11,1.4958884e-10\n",
      "Iteration 56590: loss = 3.893816e-11,1.4958743e-10\n",
      "Iteration 56595: loss = 3.893683e-11,1.495861e-10\n",
      "Iteration 56600: loss = 3.893594e-11,1.4958468e-10\n",
      "Iteration 56605: loss = 3.8934463e-11,1.4958332e-10\n",
      "Iteration 56610: loss = 3.8932732e-11,1.4958203e-10\n",
      "Iteration 56615: loss = 3.8931955e-11,1.4958068e-10\n",
      "Iteration 56620: loss = 3.8930473e-11,1.4957921e-10\n",
      "Iteration 56625: loss = 3.8929766e-11,1.4957774e-10\n",
      "Iteration 56630: loss = 3.892839e-11,1.4957646e-10\n",
      "Iteration 56635: loss = 3.8926657e-11,1.49575e-10\n",
      "Iteration 56640: loss = 3.8926154e-11,1.4957363e-10\n",
      "Iteration 56645: loss = 3.892454e-11,1.4957223e-10\n",
      "Iteration 56650: loss = 3.8924027e-11,1.4957077e-10\n",
      "Iteration 56655: loss = 3.892232e-11,1.495695e-10\n",
      "Iteration 56660: loss = 3.8920516e-11,1.4956812e-10\n",
      "Iteration 56665: loss = 3.8920322e-11,1.4956644e-10\n",
      "Iteration 56670: loss = 3.891883e-11,1.4956483e-10\n",
      "Iteration 56675: loss = 3.8918566e-11,1.4956336e-10\n",
      "Iteration 56680: loss = 3.891684e-11,1.4956189e-10\n",
      "Iteration 56685: loss = 3.8915534e-11,1.4956038e-10\n",
      "Iteration 56690: loss = 3.891497e-11,1.4955874e-10\n",
      "Iteration 56695: loss = 3.8913425e-11,1.4955727e-10\n",
      "Iteration 56700: loss = 3.8913168e-11,1.4955563e-10\n",
      "Iteration 56705: loss = 3.8911496e-11,1.4955408e-10\n",
      "Iteration 56710: loss = 3.8910052e-11,1.4955279e-10\n",
      "Iteration 56715: loss = 3.890964e-11,1.4955126e-10\n",
      "Iteration 56720: loss = 3.890847e-11,1.4954972e-10\n",
      "Iteration 56725: loss = 3.890782e-11,1.4954812e-10\n",
      "Iteration 56730: loss = 3.8906527e-11,1.495465e-10\n",
      "Iteration 56735: loss = 3.8904862e-11,1.4954514e-10\n",
      "Iteration 56740: loss = 3.890419e-11,1.4954359e-10\n",
      "Iteration 56745: loss = 3.890291e-11,1.495421e-10\n",
      "Iteration 56750: loss = 3.8902104e-11,1.4954071e-10\n",
      "Iteration 56755: loss = 3.8900758e-11,1.4953934e-10\n",
      "Iteration 56760: loss = 3.8899203e-11,1.4953783e-10\n",
      "Iteration 56765: loss = 3.8898818e-11,1.4953629e-10\n",
      "Iteration 56770: loss = 3.8896993e-11,1.4953488e-10\n",
      "Iteration 56775: loss = 3.8896355e-11,1.4953352e-10\n",
      "Iteration 56780: loss = 3.8895175e-11,1.4953211e-10\n",
      "Iteration 56785: loss = 3.889362e-11,1.4953058e-10\n",
      "Iteration 56790: loss = 3.889317e-11,1.4952914e-10\n",
      "Iteration 56795: loss = 3.8891307e-11,1.4952778e-10\n",
      "Iteration 56800: loss = 3.8890918e-11,1.4952645e-10\n",
      "Iteration 56805: loss = 3.888911e-11,1.4952503e-10\n",
      "Iteration 56810: loss = 3.8888916e-11,1.4952369e-10\n",
      "Iteration 56815: loss = 3.888723e-11,1.4952208e-10\n",
      "Iteration 56820: loss = 3.8885544e-11,1.4952065e-10\n",
      "Iteration 56825: loss = 3.888534e-11,1.4951901e-10\n",
      "Iteration 56830: loss = 3.8883726e-11,1.4951745e-10\n",
      "Iteration 56835: loss = 3.8883213e-11,1.4951612e-10\n",
      "Iteration 56840: loss = 3.8881478e-11,1.495148e-10\n",
      "Iteration 56845: loss = 3.8880222e-11,1.4951326e-10\n",
      "Iteration 56850: loss = 3.8879663e-11,1.4951171e-10\n",
      "Iteration 56855: loss = 3.8878237e-11,1.4951017e-10\n",
      "Iteration 56860: loss = 3.8877675e-11,1.4950871e-10\n",
      "Iteration 56865: loss = 3.887605e-11,1.4950724e-10\n",
      "Iteration 56870: loss = 3.8874792e-11,1.4950555e-10\n",
      "Iteration 56875: loss = 3.8874074e-11,1.4950417e-10\n",
      "Iteration 56880: loss = 3.8872586e-11,1.4950267e-10\n",
      "Iteration 56885: loss = 3.8872e-11,1.495013e-10\n",
      "Iteration 56890: loss = 3.887085e-11,1.494998e-10\n",
      "Iteration 56895: loss = 3.8869043e-11,1.4949857e-10\n",
      "Iteration 56900: loss = 3.8868252e-11,1.4949682e-10\n",
      "Iteration 56905: loss = 3.8866965e-11,1.494956e-10\n",
      "Iteration 56910: loss = 3.88663e-11,1.49494e-10\n",
      "Iteration 56915: loss = 3.8865005e-11,1.4949258e-10\n",
      "Iteration 56920: loss = 3.8863305e-11,1.4949111e-10\n",
      "Iteration 56925: loss = 3.8862843e-11,1.494896e-10\n",
      "Iteration 56930: loss = 3.8861196e-11,1.4948813e-10\n",
      "Iteration 56935: loss = 3.8861022e-11,1.4948644e-10\n",
      "Iteration 56940: loss = 3.8859273e-11,1.4948517e-10\n",
      "Iteration 56945: loss = 3.8857705e-11,1.494839e-10\n",
      "Iteration 56950: loss = 3.8857265e-11,1.4948245e-10\n",
      "Iteration 56955: loss = 3.88555e-11,1.4948097e-10\n",
      "Iteration 56960: loss = 3.8855006e-11,1.4947946e-10\n",
      "Iteration 56965: loss = 3.885335e-11,1.4947814e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56970: loss = 3.8852112e-11,1.4947679e-10\n",
      "Iteration 56975: loss = 3.8851256e-11,1.494753e-10\n",
      "Iteration 56980: loss = 3.8849892e-11,1.4947388e-10\n",
      "Iteration 56985: loss = 3.884917e-11,1.4947252e-10\n",
      "Iteration 56990: loss = 3.8847533e-11,1.4947112e-10\n",
      "Iteration 56995: loss = 3.884619e-11,1.4946966e-10\n",
      "Iteration 57000: loss = 3.8845455e-11,1.4946824e-10\n",
      "Iteration 57005: loss = 3.8844133e-11,1.4946683e-10\n",
      "Iteration 57010: loss = 3.8843304e-11,1.4946525e-10\n",
      "Iteration 57015: loss = 3.8842e-11,1.4946386e-10\n",
      "Iteration 57020: loss = 3.8840254e-11,1.4946258e-10\n",
      "Iteration 57025: loss = 3.8839553e-11,1.4946118e-10\n",
      "Iteration 57030: loss = 3.883815e-11,1.4945968e-10\n",
      "Iteration 57035: loss = 3.8837322e-11,1.4945818e-10\n",
      "Iteration 57040: loss = 3.8836084e-11,1.4945685e-10\n",
      "Iteration 57045: loss = 3.8834228e-11,1.4945548e-10\n",
      "Iteration 57050: loss = 3.8833815e-11,1.4945402e-10\n",
      "Iteration 57055: loss = 3.8832174e-11,1.494526e-10\n",
      "Iteration 57060: loss = 3.8831664e-11,1.4945112e-10\n",
      "Iteration 57065: loss = 3.8830023e-11,1.494498e-10\n",
      "Iteration 57070: loss = 3.882833e-11,1.4944854e-10\n",
      "Iteration 57075: loss = 3.8827844e-11,1.4944705e-10\n",
      "Iteration 57080: loss = 3.8826276e-11,1.4944566e-10\n",
      "Iteration 57085: loss = 3.8825807e-11,1.4944429e-10\n",
      "Iteration 57090: loss = 3.8824232e-11,1.4944279e-10\n",
      "Iteration 57095: loss = 3.8823042e-11,1.4944126e-10\n",
      "Iteration 57100: loss = 3.8822442e-11,1.4943978e-10\n",
      "Iteration 57105: loss = 3.8820915e-11,1.4943828e-10\n",
      "Iteration 57110: loss = 3.8820367e-11,1.4943696e-10\n",
      "Iteration 57115: loss = 3.8818726e-11,1.4943552e-10\n",
      "Iteration 57120: loss = 3.881739e-11,1.4943408e-10\n",
      "Iteration 57125: loss = 3.8816558e-11,1.4943259e-10\n",
      "Iteration 57130: loss = 3.881524e-11,1.494311e-10\n",
      "Iteration 57135: loss = 3.8814552e-11,1.494298e-10\n",
      "Iteration 57140: loss = 3.8813185e-11,1.4942844e-10\n",
      "Iteration 57145: loss = 3.8811617e-11,1.4942679e-10\n",
      "Iteration 57150: loss = 3.8810764e-11,1.4942557e-10\n",
      "Iteration 57155: loss = 3.8809473e-11,1.4942407e-10\n",
      "Iteration 57160: loss = 3.8808772e-11,1.4942263e-10\n",
      "Iteration 57165: loss = 3.8807402e-11,1.4942111e-10\n",
      "Iteration 57170: loss = 3.880579e-11,1.4941968e-10\n",
      "Iteration 57175: loss = 3.8805317e-11,1.494183e-10\n",
      "Iteration 57180: loss = 3.8803637e-11,1.4941692e-10\n",
      "Iteration 57185: loss = 3.8803186e-11,1.4941559e-10\n",
      "Iteration 57190: loss = 3.8801688e-11,1.4941387e-10\n",
      "Iteration 57195: loss = 3.8801087e-11,1.4941251e-10\n",
      "Iteration 57200: loss = 3.8799818e-11,1.4941096e-10\n",
      "Iteration 57205: loss = 3.8798218e-11,1.494095e-10\n",
      "Iteration 57210: loss = 3.8797795e-11,1.4940801e-10\n",
      "Iteration 57215: loss = 3.879613e-11,1.4940663e-10\n",
      "Iteration 57220: loss = 3.8795644e-11,1.4940524e-10\n",
      "Iteration 57225: loss = 3.8794027e-11,1.4940377e-10\n",
      "Iteration 57230: loss = 3.8792424e-11,1.4940227e-10\n",
      "Iteration 57235: loss = 3.879231e-11,1.4940046e-10\n",
      "Iteration 57240: loss = 3.8790565e-11,1.4939935e-10\n",
      "Iteration 57245: loss = 3.8790225e-11,1.4939797e-10\n",
      "Iteration 57250: loss = 3.8788563e-11,1.4939647e-10\n",
      "Iteration 57255: loss = 3.8787317e-11,1.4939483e-10\n",
      "Iteration 57260: loss = 3.8786783e-11,1.4939339e-10\n",
      "Iteration 57265: loss = 3.8785686e-11,1.493918e-10\n",
      "Iteration 57270: loss = 3.8784906e-11,1.4939035e-10\n",
      "Iteration 57275: loss = 3.8783293e-11,1.4938888e-10\n",
      "Iteration 57280: loss = 3.878193e-11,1.493874e-10\n",
      "Iteration 57285: loss = 3.8781298e-11,1.4938593e-10\n",
      "Iteration 57290: loss = 3.8780094e-11,1.4938434e-10\n",
      "Iteration 57295: loss = 3.877942e-11,1.4938276e-10\n",
      "Iteration 57300: loss = 3.8778154e-11,1.4938137e-10\n",
      "Iteration 57305: loss = 3.8776485e-11,1.4937995e-10\n",
      "Iteration 57310: loss = 3.8775736e-11,1.493786e-10\n",
      "Iteration 57315: loss = 3.8773377e-11,1.4937789e-10\n",
      "Iteration 57320: loss = 3.877262e-11,1.4937665e-10\n",
      "Iteration 57325: loss = 3.8771358e-11,1.493751e-10\n",
      "Iteration 57330: loss = 3.8769606e-11,1.4937371e-10\n",
      "Iteration 57335: loss = 3.8769318e-11,1.4937222e-10\n",
      "Iteration 57340: loss = 3.876754e-11,1.4937088e-10\n",
      "Iteration 57345: loss = 3.876627e-11,1.4937021e-10\n",
      "Iteration 57350: loss = 3.87646e-11,1.4936877e-10\n",
      "Iteration 57355: loss = 3.8763e-11,1.4936719e-10\n",
      "Iteration 57360: loss = 3.8762712e-11,1.4936556e-10\n",
      "Iteration 57365: loss = 3.8761213e-11,1.493642e-10\n",
      "Iteration 57370: loss = 3.876088e-11,1.493626e-10\n",
      "Iteration 57375: loss = 3.8758375e-11,1.4936177e-10\n",
      "Iteration 57380: loss = 3.875632e-11,1.4936101e-10\n",
      "Iteration 57385: loss = 3.8755926e-11,1.4935941e-10\n",
      "Iteration 57390: loss = 3.8754295e-11,1.493583e-10\n",
      "Iteration 57395: loss = 3.875397e-11,1.4935655e-10\n",
      "Iteration 57400: loss = 3.8751002e-11,1.4935614e-10\n",
      "Iteration 57405: loss = 3.8750114e-11,1.4935458e-10\n",
      "Iteration 57410: loss = 3.8749007e-11,1.4935321e-10\n",
      "Iteration 57415: loss = 3.8746728e-11,1.493528e-10\n",
      "Iteration 57420: loss = 3.8746378e-11,1.4935089e-10\n",
      "Iteration 57425: loss = 3.8744133e-11,1.4935046e-10\n",
      "Iteration 57430: loss = 3.8044755e-11,1.4935016e-10\n",
      "Iteration 57435: loss = 3.8042458e-11,1.4935e-10\n",
      "Iteration 57440: loss = 3.803911e-11,1.4935013e-10\n",
      "Iteration 57445: loss = 3.803585e-11,1.4935068e-10\n",
      "Iteration 57450: loss = 3.8031793e-11,1.4935157e-10\n",
      "Iteration 57455: loss = 3.8027196e-11,1.4935234e-10\n",
      "Iteration 57460: loss = 3.8023827e-11,1.4935336e-10\n",
      "Iteration 57465: loss = 3.8019185e-11,1.4935438e-10\n",
      "Iteration 57470: loss = 3.8015865e-11,1.4935536e-10\n",
      "Iteration 57475: loss = 3.8011483e-11,1.4935615e-10\n",
      "Iteration 57480: loss = 3.8007156e-11,1.4935704e-10\n",
      "Iteration 57485: loss = 3.8003937e-11,1.4935803e-10\n",
      "Iteration 57490: loss = 3.799955e-11,1.4935875e-10\n",
      "Iteration 57495: loss = 3.7996866e-11,1.493592e-10\n",
      "Iteration 57500: loss = 3.7993903e-11,1.4935883e-10\n",
      "Iteration 57505: loss = 3.7991384e-11,1.4935829e-10\n",
      "Iteration 57510: loss = 3.7989892e-11,1.493577e-10\n",
      "Iteration 57515: loss = 3.7987627e-11,1.4935703e-10\n",
      "Iteration 57520: loss = 3.798588e-11,1.4935625e-10\n",
      "Iteration 57525: loss = 3.7983477e-11,1.4935561e-10\n",
      "Iteration 57530: loss = 3.798119e-11,1.4935483e-10\n",
      "Iteration 57535: loss = 3.797981e-11,1.4935417e-10\n",
      "Iteration 57540: loss = 3.7977423e-11,1.4935359e-10\n",
      "Iteration 57545: loss = 3.7975817e-11,1.4935278e-10\n",
      "Iteration 57550: loss = 3.7973524e-11,1.4935209e-10\n",
      "Iteration 57555: loss = 3.797133e-11,1.4935128e-10\n",
      "Iteration 57560: loss = 3.7969957e-11,1.4935045e-10\n",
      "Iteration 57565: loss = 3.796747e-11,1.4934967e-10\n",
      "Iteration 57570: loss = 3.7966175e-11,1.4934884e-10\n",
      "Iteration 57575: loss = 3.7963934e-11,1.4934826e-10\n",
      "Iteration 57580: loss = 3.796174e-11,1.4934744e-10\n",
      "Iteration 57585: loss = 3.7960152e-11,1.493467e-10\n",
      "Iteration 57590: loss = 3.795808e-11,1.4934576e-10\n",
      "Iteration 57595: loss = 3.7956895e-11,1.493448e-10\n",
      "Iteration 57600: loss = 3.7954962e-11,1.4934386e-10\n",
      "Iteration 57605: loss = 3.79539e-11,1.4934308e-10\n",
      "Iteration 57610: loss = 3.7951642e-11,1.4934216e-10\n",
      "Iteration 57615: loss = 3.7949723e-11,1.4934129e-10\n",
      "Iteration 57620: loss = 3.7948592e-11,1.493401e-10\n",
      "Iteration 57625: loss = 3.794665e-11,1.4933914e-10\n",
      "Iteration 57630: loss = 3.79453e-11,1.4933804e-10\n",
      "Iteration 57635: loss = 3.7943614e-11,1.493371e-10\n",
      "Iteration 57640: loss = 3.794175e-11,1.4933595e-10\n",
      "Iteration 57645: loss = 3.7941116e-11,1.4933484e-10\n",
      "Iteration 57650: loss = 3.793885e-11,1.4933381e-10\n",
      "Iteration 57655: loss = 3.7938076e-11,1.4933259e-10\n",
      "Iteration 57660: loss = 3.7936255e-11,1.4933142e-10\n",
      "Iteration 57665: loss = 3.7934537e-11,1.4933045e-10\n",
      "Iteration 57670: loss = 3.7933653e-11,1.4932927e-10\n",
      "Iteration 57675: loss = 3.7931606e-11,1.493282e-10\n",
      "Iteration 57680: loss = 3.7930787e-11,1.4932702e-10\n",
      "Iteration 57685: loss = 3.7929142e-11,1.4932586e-10\n",
      "Iteration 57690: loss = 3.7927318e-11,1.4932473e-10\n",
      "Iteration 57695: loss = 3.7926294e-11,1.4932362e-10\n",
      "Iteration 57700: loss = 3.792441e-11,1.493225e-10\n",
      "Iteration 57705: loss = 3.7923702e-11,1.4932128e-10\n",
      "Iteration 57710: loss = 3.7922034e-11,1.4932017e-10\n",
      "Iteration 57715: loss = 3.791997e-11,1.4931895e-10\n",
      "Iteration 57720: loss = 3.7919105e-11,1.4931784e-10\n",
      "Iteration 57725: loss = 3.7917513e-11,1.493165e-10\n",
      "Iteration 57730: loss = 3.7916694e-11,1.4931538e-10\n",
      "Iteration 57735: loss = 3.7914654e-11,1.4931413e-10\n",
      "Iteration 57740: loss = 3.7913013e-11,1.493129e-10\n",
      "Iteration 57745: loss = 3.7912284e-11,1.4931165e-10\n",
      "Iteration 57750: loss = 3.7910595e-11,1.4931069e-10\n",
      "Iteration 57755: loss = 3.7909925e-11,1.4930945e-10\n",
      "Iteration 57760: loss = 3.7907982e-11,1.4930833e-10\n",
      "Iteration 57765: loss = 3.7906237e-11,1.4930697e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57770: loss = 3.7905463e-11,1.4930593e-10\n",
      "Iteration 57775: loss = 3.7903767e-11,1.4930455e-10\n",
      "Iteration 57780: loss = 3.7902716e-11,1.4930357e-10\n",
      "Iteration 57785: loss = 3.790113e-11,1.4930235e-10\n",
      "Iteration 57790: loss = 3.7899493e-11,1.493012e-10\n",
      "Iteration 57795: loss = 3.7898712e-11,1.4929982e-10\n",
      "Iteration 57800: loss = 3.7896745e-11,1.4929877e-10\n",
      "Iteration 57805: loss = 3.7896016e-11,1.492975e-10\n",
      "Iteration 57810: loss = 3.789438e-11,1.4929624e-10\n",
      "Iteration 57815: loss = 3.789238e-11,1.4929544e-10\n",
      "Iteration 57820: loss = 3.7891013e-11,1.4929467e-10\n",
      "Iteration 57825: loss = 3.7888512e-11,1.4929405e-10\n",
      "Iteration 57830: loss = 3.788708e-11,1.4929336e-10\n",
      "Iteration 57835: loss = 3.788486e-11,1.4929269e-10\n",
      "Iteration 57840: loss = 3.788257e-11,1.4929202e-10\n",
      "Iteration 57845: loss = 3.7880706e-11,1.4929144e-10\n",
      "Iteration 57850: loss = 3.787844e-11,1.4929069e-10\n",
      "Iteration 57855: loss = 3.7877174e-11,1.4929001e-10\n",
      "Iteration 57860: loss = 3.787497e-11,1.492893e-10\n",
      "Iteration 57865: loss = 3.787237e-11,1.4928851e-10\n",
      "Iteration 57870: loss = 3.787094e-11,1.4928785e-10\n",
      "Iteration 57875: loss = 3.786888e-11,1.4928697e-10\n",
      "Iteration 57880: loss = 3.7867463e-11,1.492862e-10\n",
      "Iteration 57885: loss = 3.7865225e-11,1.4928549e-10\n",
      "Iteration 57890: loss = 3.7862796e-11,1.4928463e-10\n",
      "Iteration 57895: loss = 3.7861343e-11,1.4928393e-10\n",
      "Iteration 57900: loss = 3.7859233e-11,1.4928318e-10\n",
      "Iteration 57905: loss = 3.7857942e-11,1.4928249e-10\n",
      "Iteration 57910: loss = 3.785544e-11,1.4928186e-10\n",
      "Iteration 57915: loss = 3.7853446e-11,1.4928073e-10\n",
      "Iteration 57920: loss = 3.785239e-11,1.4928005e-10\n",
      "Iteration 57925: loss = 3.7850317e-11,1.4927903e-10\n",
      "Iteration 57930: loss = 3.784879e-11,1.4927837e-10\n",
      "Iteration 57935: loss = 3.7846847e-11,1.4927747e-10\n",
      "Iteration 57940: loss = 3.7844842e-11,1.4927658e-10\n",
      "Iteration 57945: loss = 3.784376e-11,1.4927554e-10\n",
      "Iteration 57950: loss = 3.784183e-11,1.4927462e-10\n",
      "Iteration 57955: loss = 3.784039e-11,1.4927383e-10\n",
      "Iteration 57960: loss = 3.783834e-11,1.4927294e-10\n",
      "Iteration 57965: loss = 3.7836484e-11,1.4927204e-10\n",
      "Iteration 57970: loss = 3.7835454e-11,1.492709e-10\n",
      "Iteration 57975: loss = 3.783327e-11,1.4926985e-10\n",
      "Iteration 57980: loss = 3.7832137e-11,1.4926894e-10\n",
      "Iteration 57985: loss = 3.7830204e-11,1.492679e-10\n",
      "Iteration 57990: loss = 3.7829236e-11,1.4926697e-10\n",
      "Iteration 57995: loss = 3.7827092e-11,1.4926597e-10\n",
      "Iteration 58000: loss = 3.7825087e-11,1.4926507e-10\n",
      "Iteration 58005: loss = 3.782413e-11,1.4926405e-10\n",
      "Iteration 58010: loss = 3.7822263e-11,1.4926292e-10\n",
      "Iteration 58015: loss = 3.782129e-11,1.4926202e-10\n",
      "Iteration 58020: loss = 3.781916e-11,1.49261e-10\n",
      "Iteration 58025: loss = 3.781726e-11,1.492598e-10\n",
      "Iteration 58030: loss = 3.7816292e-11,1.4925883e-10\n",
      "Iteration 58035: loss = 3.7814498e-11,1.4925809e-10\n",
      "Iteration 58040: loss = 3.781329e-11,1.4925691e-10\n",
      "Iteration 58045: loss = 3.7811337e-11,1.4925589e-10\n",
      "Iteration 58050: loss = 3.780957e-11,1.4925483e-10\n",
      "Iteration 58055: loss = 3.780858e-11,1.4925397e-10\n",
      "Iteration 58060: loss = 3.780645e-11,1.492528e-10\n",
      "Iteration 58065: loss = 3.7805484e-11,1.4925187e-10\n",
      "Iteration 58070: loss = 3.780368e-11,1.4925085e-10\n",
      "Iteration 58075: loss = 3.7801824e-11,1.4924983e-10\n",
      "Iteration 58080: loss = 3.7800967e-11,1.4924878e-10\n",
      "Iteration 58085: loss = 3.779878e-11,1.49248e-10\n",
      "Iteration 58090: loss = 3.7797824e-11,1.492467e-10\n",
      "Iteration 58095: loss = 3.779597e-11,1.4924573e-10\n",
      "Iteration 58100: loss = 3.779428e-11,1.4924467e-10\n",
      "Iteration 58105: loss = 3.77931e-11,1.4924342e-10\n",
      "Iteration 58110: loss = 3.7791343e-11,1.4924237e-10\n",
      "Iteration 58115: loss = 3.779056e-11,1.4924116e-10\n",
      "Iteration 58120: loss = 3.7788838e-11,1.4924008e-10\n",
      "Iteration 58125: loss = 3.7099934e-11,1.4923908e-10\n",
      "Iteration 58130: loss = 3.7098956e-11,1.4923796e-10\n",
      "Iteration 58135: loss = 3.7097016e-11,1.4923715e-10\n",
      "Iteration 58140: loss = 3.709742e-11,1.4923439e-10\n",
      "Iteration 58145: loss = 3.7099677e-11,1.4923054e-10\n",
      "Iteration 58150: loss = 3.7788186e-11,1.4922684e-10\n",
      "Iteration 58155: loss = 3.709949e-11,1.4922608e-10\n",
      "Iteration 58160: loss = 3.70973e-11,1.4922517e-10\n",
      "Iteration 58165: loss = 3.7099757e-11,1.4922125e-10\n",
      "Iteration 58170: loss = 3.7099143e-11,1.4921933e-10\n",
      "Iteration 58175: loss = 3.7098834e-11,1.492173e-10\n",
      "Iteration 58180: loss = 3.709959e-11,1.4921439e-10\n",
      "Iteration 58185: loss = 3.7099185e-11,1.492129e-10\n",
      "Iteration 58190: loss = 3.7099462e-11,1.492103e-10\n",
      "Iteration 58195: loss = 3.7099185e-11,1.4920826e-10\n",
      "Iteration 58200: loss = 3.709887e-11,1.4920624e-10\n",
      "Iteration 58205: loss = 3.778726e-11,1.4920284e-10\n",
      "Iteration 58210: loss = 3.709792e-11,1.4920216e-10\n",
      "Iteration 58215: loss = 3.7098515e-11,1.491993e-10\n",
      "Iteration 58220: loss = 3.7099265e-11,1.4919652e-10\n",
      "Iteration 58225: loss = 3.709887e-11,1.4919446e-10\n",
      "Iteration 58230: loss = 3.709764e-11,1.4919374e-10\n",
      "Iteration 58235: loss = 3.7786628e-11,1.4918962e-10\n",
      "Iteration 58240: loss = 3.7098602e-11,1.491881e-10\n",
      "Iteration 58245: loss = 3.7787006e-11,1.4918478e-10\n",
      "Iteration 58250: loss = 3.7098057e-11,1.4918403e-10\n",
      "Iteration 58255: loss = 3.7095566e-11,1.4918332e-10\n",
      "Iteration 58260: loss = 3.7097884e-11,1.4917906e-10\n",
      "Iteration 58265: loss = 3.7098578e-11,1.4917656e-10\n",
      "Iteration 58270: loss = 3.7097405e-11,1.491758e-10\n",
      "Iteration 58275: loss = 3.7098564e-11,1.491717e-10\n",
      "Iteration 58280: loss = 3.7098397e-11,1.4916948e-10\n",
      "Iteration 58285: loss = 3.709717e-11,1.4916872e-10\n",
      "Iteration 58290: loss = 3.7095084e-11,1.4916804e-10\n",
      "Iteration 58295: loss = 3.7093915e-11,1.4916723e-10\n",
      "Iteration 58300: loss = 3.7093325e-11,1.4916518e-10\n",
      "Iteration 58305: loss = 3.7095767e-11,1.4916099e-10\n",
      "Iteration 58310: loss = 3.709737e-11,1.4915712e-10\n",
      "Iteration 58315: loss = 3.7098862e-11,1.491537e-10\n",
      "Iteration 58320: loss = 3.709733e-11,1.4915287e-10\n",
      "Iteration 58325: loss = 3.7095202e-11,1.4915211e-10\n",
      "Iteration 58330: loss = 3.709308e-11,1.491513e-10\n",
      "Iteration 58335: loss = 3.708736e-11,1.4915422e-10\n",
      "Iteration 58340: loss = 3.7080356e-11,1.4915712e-10\n",
      "Iteration 58345: loss = 3.7074625e-11,1.4915999e-10\n",
      "Iteration 58350: loss = 3.706795e-11,1.4916303e-10\n",
      "Iteration 58355: loss = 3.7061403e-11,1.4916576e-10\n",
      "Iteration 58360: loss = 3.7055806e-11,1.4916879e-10\n",
      "Iteration 58365: loss = 3.7049055e-11,1.4917138e-10\n",
      "Iteration 58370: loss = 3.7043518e-11,1.4917409e-10\n",
      "Iteration 58375: loss = 3.703714e-11,1.4917699e-10\n",
      "Iteration 58380: loss = 3.7030667e-11,1.4917992e-10\n",
      "Iteration 58385: loss = 3.70249e-11,1.4918264e-10\n",
      "Iteration 58390: loss = 3.7018433e-11,1.4918528e-10\n",
      "Iteration 58395: loss = 3.7012948e-11,1.4918816e-10\n",
      "Iteration 58400: loss = 3.7006665e-11,1.4919087e-10\n",
      "Iteration 58405: loss = 3.6999955e-11,1.491935e-10\n",
      "Iteration 58410: loss = 3.6994522e-11,1.4919642e-10\n",
      "Iteration 58415: loss = 3.6988284e-11,1.4919903e-10\n",
      "Iteration 58420: loss = 3.6982976e-11,1.4920146e-10\n",
      "Iteration 58425: loss = 3.6976835e-11,1.4920443e-10\n",
      "Iteration 58430: loss = 3.6970278e-11,1.4920667e-10\n",
      "Iteration 58435: loss = 3.6965177e-11,1.4920945e-10\n",
      "Iteration 58440: loss = 3.6959075e-11,1.4921175e-10\n",
      "Iteration 58445: loss = 3.695415e-11,1.4921422e-10\n",
      "Iteration 58450: loss = 3.6947844e-11,1.492166e-10\n",
      "Iteration 58455: loss = 3.6941984e-11,1.4921894e-10\n",
      "Iteration 58460: loss = 3.6938463e-11,1.492198e-10\n",
      "Iteration 58465: loss = 3.693658e-11,1.492192e-10\n",
      "Iteration 58470: loss = 3.693425e-11,1.4921908e-10\n",
      "Iteration 58475: loss = 3.6931996e-11,1.4921894e-10\n",
      "Iteration 58480: loss = 3.6929036e-11,1.4921873e-10\n",
      "Iteration 58485: loss = 3.6926687e-11,1.4921847e-10\n",
      "Iteration 58490: loss = 3.692466e-11,1.4921829e-10\n",
      "Iteration 58495: loss = 3.6922267e-11,1.4921783e-10\n",
      "Iteration 58500: loss = 3.691942e-11,1.4921775e-10\n",
      "Iteration 58505: loss = 3.691732e-11,1.4921733e-10\n",
      "Iteration 58510: loss = 3.6915273e-11,1.4921724e-10\n",
      "Iteration 58515: loss = 3.691211e-11,1.4921682e-10\n",
      "Iteration 58520: loss = 3.6910135e-11,1.4921653e-10\n",
      "Iteration 58525: loss = 3.690817e-11,1.4921608e-10\n",
      "Iteration 58530: loss = 3.6906162e-11,1.4921583e-10\n",
      "Iteration 58535: loss = 3.690305e-11,1.4921553e-10\n",
      "Iteration 58540: loss = 3.6901183e-11,1.4921486e-10\n",
      "Iteration 58545: loss = 3.689952e-11,1.4921443e-10\n",
      "Iteration 58550: loss = 3.689769e-11,1.4921386e-10\n",
      "Iteration 58555: loss = 3.6895067e-11,1.4921336e-10\n",
      "Iteration 58560: loss = 3.6893117e-11,1.4921282e-10\n",
      "Iteration 58565: loss = 3.689145e-11,1.4921224e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58570: loss = 3.6889842e-11,1.4921164e-10\n",
      "Iteration 58575: loss = 3.688745e-11,1.4921106e-10\n",
      "Iteration 58580: loss = 3.688557e-11,1.4921042e-10\n",
      "Iteration 58585: loss = 3.6883916e-11,1.492099e-10\n",
      "Iteration 58590: loss = 3.688146e-11,1.492092e-10\n",
      "Iteration 58595: loss = 3.6879937e-11,1.4920863e-10\n",
      "Iteration 58600: loss = 3.6878032e-11,1.4920773e-10\n",
      "Iteration 58605: loss = 3.6876502e-11,1.4920726e-10\n",
      "Iteration 58610: loss = 3.687407e-11,1.4920631e-10\n",
      "Iteration 58615: loss = 3.6872477e-11,1.4920576e-10\n",
      "Iteration 58620: loss = 3.6870958e-11,1.4920506e-10\n",
      "Iteration 58625: loss = 3.686914e-11,1.4920425e-10\n",
      "Iteration 58630: loss = 3.6867e-11,1.4920354e-10\n",
      "Iteration 58635: loss = 3.6865542e-11,1.492027e-10\n",
      "Iteration 58640: loss = 3.6864116e-11,1.4920187e-10\n",
      "Iteration 58645: loss = 3.6861535e-11,1.4920093e-10\n",
      "Iteration 58650: loss = 3.6860206e-11,1.4920011e-10\n",
      "Iteration 58655: loss = 3.6858672e-11,1.4919935e-10\n",
      "Iteration 58660: loss = 3.685731e-11,1.491985e-10\n",
      "Iteration 58665: loss = 3.6854773e-11,1.4919778e-10\n",
      "Iteration 58670: loss = 3.685337e-11,1.4919688e-10\n",
      "Iteration 58675: loss = 3.6852073e-11,1.491961e-10\n",
      "Iteration 58680: loss = 3.6850696e-11,1.491953e-10\n",
      "Iteration 58685: loss = 3.6848382e-11,1.4919442e-10\n",
      "Iteration 58690: loss = 3.6846592e-11,1.4919366e-10\n",
      "Iteration 58695: loss = 3.68453e-11,1.4919277e-10\n",
      "Iteration 58700: loss = 3.6844115e-11,1.491918e-10\n",
      "Iteration 58705: loss = 3.684189e-11,1.4919085e-10\n",
      "Iteration 58710: loss = 3.684048e-11,1.4918994e-10\n",
      "Iteration 58715: loss = 3.6839205e-11,1.4918894e-10\n",
      "Iteration 58720: loss = 3.6837148e-11,1.491879e-10\n",
      "Iteration 58725: loss = 3.6835982e-11,1.49187e-10\n",
      "Iteration 58730: loss = 3.683449e-11,1.4918594e-10\n",
      "Iteration 58735: loss = 3.683324e-11,1.4918504e-10\n",
      "Iteration 58740: loss = 3.683114e-11,1.4918405e-10\n",
      "Iteration 58745: loss = 3.6829966e-11,1.4918311e-10\n",
      "Iteration 58750: loss = 3.6828506e-11,1.4918197e-10\n",
      "Iteration 58755: loss = 3.682731e-11,1.4918104e-10\n",
      "Iteration 58760: loss = 3.6825192e-11,1.4918e-10\n",
      "Iteration 58765: loss = 3.6823922e-11,1.4917911e-10\n",
      "Iteration 58770: loss = 3.6822833e-11,1.491781e-10\n",
      "Iteration 58775: loss = 3.6821407e-11,1.4917716e-10\n",
      "Iteration 58780: loss = 3.6819214e-11,1.4917624e-10\n",
      "Iteration 58785: loss = 3.6818062e-11,1.4917527e-10\n",
      "Iteration 58790: loss = 3.6816973e-11,1.4917412e-10\n",
      "Iteration 58795: loss = 3.6814506e-11,1.4917315e-10\n",
      "Iteration 58800: loss = 3.6813372e-11,1.4917198e-10\n",
      "Iteration 58805: loss = 3.681225e-11,1.4917113e-10\n",
      "Iteration 58810: loss = 3.6810978e-11,1.4917015e-10\n",
      "Iteration 58815: loss = 3.6808556e-11,1.4916926e-10\n",
      "Iteration 58820: loss = 3.6807408e-11,1.4916836e-10\n",
      "Iteration 58825: loss = 3.6806218e-11,1.4916737e-10\n",
      "Iteration 58830: loss = 3.6805087e-11,1.491663e-10\n",
      "Iteration 58835: loss = 3.680301e-11,1.4916533e-10\n",
      "Iteration 58840: loss = 3.6801538e-11,1.4916433e-10\n",
      "Iteration 58845: loss = 3.6800448e-11,1.491632e-10\n",
      "Iteration 58850: loss = 3.6798294e-11,1.4916232e-10\n",
      "Iteration 58855: loss = 3.679724e-11,1.4916128e-10\n",
      "Iteration 58860: loss = 3.679575e-11,1.4916017e-10\n",
      "Iteration 58865: loss = 3.67946e-11,1.4915916e-10\n",
      "Iteration 58870: loss = 3.6792496e-11,1.4915806e-10\n",
      "Iteration 58875: loss = 3.679133e-11,1.4915703e-10\n",
      "Iteration 58880: loss = 3.6789977e-11,1.49156e-10\n",
      "Iteration 58885: loss = 3.6788822e-11,1.4915508e-10\n",
      "Iteration 58890: loss = 3.6786758e-11,1.4915406e-10\n",
      "Iteration 58895: loss = 3.678556e-11,1.4915298e-10\n",
      "Iteration 58900: loss = 3.67844e-11,1.4915198e-10\n",
      "Iteration 58905: loss = 3.678304e-11,1.4915096e-10\n",
      "Iteration 58910: loss = 3.678098e-11,1.4915005e-10\n",
      "Iteration 58915: loss = 3.6779912e-11,1.4914896e-10\n",
      "Iteration 58920: loss = 3.677872e-11,1.4914817e-10\n",
      "Iteration 58925: loss = 3.6776384e-11,1.4914678e-10\n",
      "Iteration 58930: loss = 3.677529e-11,1.4914586e-10\n",
      "Iteration 58935: loss = 3.6774177e-11,1.491449e-10\n",
      "Iteration 58940: loss = 3.677303e-11,1.4914389e-10\n",
      "Iteration 58945: loss = 3.677074e-11,1.4914299e-10\n",
      "Iteration 58950: loss = 3.676963e-11,1.4914192e-10\n",
      "Iteration 58955: loss = 3.6768536e-11,1.4914085e-10\n",
      "Iteration 58960: loss = 3.676733e-11,1.4913978e-10\n",
      "Iteration 58965: loss = 3.6765334e-11,1.4913855e-10\n",
      "Iteration 58970: loss = 3.6763915e-11,1.4913792e-10\n",
      "Iteration 58975: loss = 3.6762853e-11,1.4913672e-10\n",
      "Iteration 58980: loss = 3.6761674e-11,1.4913557e-10\n",
      "Iteration 58985: loss = 3.6759696e-11,1.491347e-10\n",
      "Iteration 58990: loss = 3.6758183e-11,1.4913373e-10\n",
      "Iteration 58995: loss = 3.6757066e-11,1.4913269e-10\n",
      "Iteration 59000: loss = 3.67552e-11,1.4913155e-10\n",
      "Iteration 59005: loss = 3.6754013e-11,1.4913046e-10\n",
      "Iteration 59010: loss = 3.6752656e-11,1.491295e-10\n",
      "Iteration 59015: loss = 3.67515e-11,1.4912842e-10\n",
      "Iteration 59020: loss = 3.6749485e-11,1.4912735e-10\n",
      "Iteration 59025: loss = 3.6748427e-11,1.4912628e-10\n",
      "Iteration 59030: loss = 3.674719e-11,1.491253e-10\n",
      "Iteration 59035: loss = 3.6745877e-11,1.4912435e-10\n",
      "Iteration 59040: loss = 3.6743875e-11,1.4912335e-10\n",
      "Iteration 59045: loss = 3.6742706e-11,1.4912223e-10\n",
      "Iteration 59050: loss = 3.6741655e-11,1.4912119e-10\n",
      "Iteration 59055: loss = 3.6739334e-11,1.491201e-10\n",
      "Iteration 59060: loss = 3.673828e-11,1.4911913e-10\n",
      "Iteration 59065: loss = 3.6737207e-11,1.4911798e-10\n",
      "Iteration 59070: loss = 3.6736097e-11,1.49117e-10\n",
      "Iteration 59075: loss = 3.6733883e-11,1.4911603e-10\n",
      "Iteration 59080: loss = 3.67327e-11,1.4911497e-10\n",
      "Iteration 59085: loss = 3.6731684e-11,1.4911385e-10\n",
      "Iteration 59090: loss = 3.6730577e-11,1.4911275e-10\n",
      "Iteration 59095: loss = 3.6728596e-11,1.4911176e-10\n",
      "Iteration 59100: loss = 3.6727253e-11,1.4911068e-10\n",
      "Iteration 59105: loss = 3.6726247e-11,1.4910956e-10\n",
      "Iteration 59110: loss = 3.672523e-11,1.4910866e-10\n",
      "Iteration 59115: loss = 3.672318e-11,1.4910752e-10\n",
      "Iteration 59120: loss = 3.6721896e-11,1.4910644e-10\n",
      "Iteration 59125: loss = 3.672087e-11,1.4910537e-10\n",
      "Iteration 59130: loss = 3.6718975e-11,1.491042e-10\n",
      "Iteration 59135: loss = 3.6717913e-11,1.4910317e-10\n",
      "Iteration 59140: loss = 3.6716602e-11,1.4910201e-10\n",
      "Iteration 59145: loss = 3.6715773e-11,1.4910087e-10\n",
      "Iteration 59150: loss = 3.671378e-11,1.490996e-10\n",
      "Iteration 59155: loss = 3.6712907e-11,1.4909851e-10\n",
      "Iteration 59160: loss = 3.671192e-11,1.4909739e-10\n",
      "Iteration 59165: loss = 3.671065e-11,1.4909633e-10\n",
      "Iteration 59170: loss = 3.6708844e-11,1.4909518e-10\n",
      "Iteration 59175: loss = 3.670782e-11,1.4909397e-10\n",
      "Iteration 59180: loss = 3.6706888e-11,1.4909277e-10\n",
      "Iteration 59185: loss = 3.6705517e-11,1.490916e-10\n",
      "Iteration 59190: loss = 3.670369e-11,1.4909049e-10\n",
      "Iteration 59195: loss = 3.670259e-11,1.4908955e-10\n",
      "Iteration 59200: loss = 3.670158e-11,1.4908838e-10\n",
      "Iteration 59205: loss = 3.669935e-11,1.490873e-10\n",
      "Iteration 59210: loss = 3.6696608e-11,1.4908787e-10\n",
      "Iteration 59215: loss = 3.6693423e-11,1.4908873e-10\n",
      "Iteration 59220: loss = 3.6690116e-11,1.4908952e-10\n",
      "Iteration 59225: loss = 3.6685554e-11,1.4909034e-10\n",
      "Iteration 59230: loss = 3.6682126e-11,1.4909111e-10\n",
      "Iteration 59235: loss = 3.6678858e-11,1.4909202e-10\n",
      "Iteration 59240: loss = 3.667548e-11,1.4909285e-10\n",
      "Iteration 59245: loss = 3.6671256e-11,1.4909377e-10\n",
      "Iteration 59250: loss = 3.6667735e-11,1.4909446e-10\n",
      "Iteration 59255: loss = 3.6664765e-11,1.4909496e-10\n",
      "Iteration 59260: loss = 3.6661726e-11,1.490955e-10\n",
      "Iteration 59265: loss = 3.6657784e-11,1.4909624e-10\n",
      "Iteration 59270: loss = 3.665453e-11,1.4909686e-10\n",
      "Iteration 59275: loss = 3.6651612e-11,1.490974e-10\n",
      "Iteration 59280: loss = 3.6647727e-11,1.4909773e-10\n",
      "Iteration 59285: loss = 3.6644795e-11,1.4909834e-10\n",
      "Iteration 59290: loss = 3.664152e-11,1.4909897e-10\n",
      "Iteration 59295: loss = 3.663871e-11,1.4909933e-10\n",
      "Iteration 59300: loss = 3.5957542e-11,1.4909995e-10\n",
      "Iteration 59305: loss = 3.595408e-11,1.4910083e-10\n",
      "Iteration 59310: loss = 3.5950486e-11,1.4910202e-10\n",
      "Iteration 59315: loss = 3.594718e-11,1.4910267e-10\n",
      "Iteration 59320: loss = 3.5943425e-11,1.4910295e-10\n",
      "Iteration 59325: loss = 3.594057e-11,1.4910359e-10\n",
      "Iteration 59330: loss = 3.5937774e-11,1.4910378e-10\n",
      "Iteration 59335: loss = 3.593415e-11,1.4910434e-10\n",
      "Iteration 59340: loss = 3.5931494e-11,1.4910471e-10\n",
      "Iteration 59345: loss = 3.592923e-11,1.491049e-10\n",
      "Iteration 59350: loss = 3.59267e-11,1.4910521e-10\n",
      "Iteration 59355: loss = 3.5923438e-11,1.4910512e-10\n",
      "Iteration 59360: loss = 3.5921068e-11,1.4910513e-10\n",
      "Iteration 59365: loss = 3.5918758e-11,1.4910513e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59370: loss = 3.591637e-11,1.491049e-10\n",
      "Iteration 59375: loss = 3.5913158e-11,1.4910485e-10\n",
      "Iteration 59380: loss = 3.5910882e-11,1.4910476e-10\n",
      "Iteration 59385: loss = 3.590851e-11,1.4910478e-10\n",
      "Iteration 59390: loss = 3.5906274e-11,1.491045e-10\n",
      "Iteration 59395: loss = 3.5903395e-11,1.4910442e-10\n",
      "Iteration 59400: loss = 3.5901403e-11,1.4910437e-10\n",
      "Iteration 59405: loss = 3.5899176e-11,1.4910423e-10\n",
      "Iteration 59410: loss = 3.589624e-11,1.4910392e-10\n",
      "Iteration 59415: loss = 3.5894236e-11,1.4910352e-10\n",
      "Iteration 59420: loss = 3.5892154e-11,1.4910319e-10\n",
      "Iteration 59425: loss = 3.5890475e-11,1.4910279e-10\n",
      "Iteration 59430: loss = 3.588767e-11,1.4910222e-10\n",
      "Iteration 59435: loss = 3.5885853e-11,1.4910176e-10\n",
      "Iteration 59440: loss = 3.5884008e-11,1.4910137e-10\n",
      "Iteration 59445: loss = 3.588208e-11,1.4910101e-10\n",
      "Iteration 59450: loss = 3.5879678e-11,1.4910054e-10\n",
      "Iteration 59455: loss = 3.5877742e-11,1.4910012e-10\n",
      "Iteration 59460: loss = 3.5876063e-11,1.4909965e-10\n",
      "Iteration 59465: loss = 3.5874307e-11,1.4909927e-10\n",
      "Iteration 59470: loss = 3.5871625e-11,1.4909858e-10\n",
      "Iteration 59475: loss = 3.5869904e-11,1.4909825e-10\n",
      "Iteration 59480: loss = 3.5868145e-11,1.4909773e-10\n",
      "Iteration 59485: loss = 3.58655e-11,1.4909711e-10\n",
      "Iteration 59490: loss = 3.586362e-11,1.4909665e-10\n",
      "Iteration 59495: loss = 3.5861814e-11,1.4909612e-10\n",
      "Iteration 59500: loss = 3.5860356e-11,1.490956e-10\n",
      "Iteration 59505: loss = 3.5857765e-11,1.4909507e-10\n",
      "Iteration 59510: loss = 3.5855964e-11,1.4909471e-10\n",
      "Iteration 59515: loss = 3.5853886e-11,1.4909425e-10\n",
      "Iteration 59520: loss = 3.585143e-11,1.4909454e-10\n",
      "Iteration 59525: loss = 3.5848217e-11,1.4909449e-10\n",
      "Iteration 59530: loss = 3.5845656e-11,1.4909447e-10\n",
      "Iteration 59535: loss = 3.5843033e-11,1.4909474e-10\n",
      "Iteration 59540: loss = 3.583965e-11,1.490949e-10\n",
      "Iteration 59545: loss = 3.583696e-11,1.4909532e-10\n",
      "Iteration 59550: loss = 3.5834884e-11,1.4909515e-10\n",
      "Iteration 59555: loss = 3.583244e-11,1.4909521e-10\n",
      "Iteration 59560: loss = 3.5829065e-11,1.4909524e-10\n",
      "Iteration 59565: loss = 3.5826682e-11,1.4909524e-10\n",
      "Iteration 59570: loss = 3.5824208e-11,1.4909533e-10\n",
      "Iteration 59575: loss = 3.582184e-11,1.4909537e-10\n",
      "Iteration 59580: loss = 3.581866e-11,1.4909537e-10\n",
      "Iteration 59585: loss = 3.5816492e-11,1.4909515e-10\n",
      "Iteration 59590: loss = 3.581437e-11,1.4909499e-10\n",
      "Iteration 59595: loss = 3.58123e-11,1.4909471e-10\n",
      "Iteration 59600: loss = 3.5809317e-11,1.4909457e-10\n",
      "Iteration 59605: loss = 3.5807603e-11,1.4909426e-10\n",
      "Iteration 59610: loss = 3.580576e-11,1.4909396e-10\n",
      "Iteration 59615: loss = 3.580301e-11,1.490937e-10\n",
      "Iteration 59620: loss = 3.5801202e-11,1.4909318e-10\n",
      "Iteration 59625: loss = 3.579925e-11,1.490926e-10\n",
      "Iteration 59630: loss = 3.579738e-11,1.4909228e-10\n",
      "Iteration 59635: loss = 3.5794704e-11,1.4909185e-10\n",
      "Iteration 59640: loss = 3.5792935e-11,1.4909136e-10\n",
      "Iteration 59645: loss = 3.579116e-11,1.4909088e-10\n",
      "Iteration 59650: loss = 3.578934e-11,1.4909061e-10\n",
      "Iteration 59655: loss = 3.5786873e-11,1.4909014e-10\n",
      "Iteration 59660: loss = 3.5785073e-11,1.4908969e-10\n",
      "Iteration 59665: loss = 3.578328e-11,1.4908924e-10\n",
      "Iteration 59670: loss = 3.578152e-11,1.4908871e-10\n",
      "Iteration 59675: loss = 3.577885e-11,1.4908816e-10\n",
      "Iteration 59680: loss = 3.5777076e-11,1.4908773e-10\n",
      "Iteration 59685: loss = 3.577538e-11,1.4908712e-10\n",
      "Iteration 59690: loss = 3.51047e-11,1.4908699e-10\n",
      "Iteration 59695: loss = 3.5102452e-11,1.4908666e-10\n",
      "Iteration 59700: loss = 3.5099753e-11,1.4908684e-10\n",
      "Iteration 59705: loss = 3.5097262e-11,1.4908708e-10\n",
      "Iteration 59710: loss = 3.5093494e-11,1.4908766e-10\n",
      "Iteration 59715: loss = 3.5090666e-11,1.4908806e-10\n",
      "Iteration 59720: loss = 3.5087804e-11,1.4908864e-10\n",
      "Iteration 59725: loss = 3.508497e-11,1.4908907e-10\n",
      "Iteration 59730: loss = 3.5081316e-11,1.4908967e-10\n",
      "Iteration 59735: loss = 3.507847e-11,1.4909002e-10\n",
      "Iteration 59740: loss = 3.5075744e-11,1.4909039e-10\n",
      "Iteration 59745: loss = 3.5072435e-11,1.4909068e-10\n",
      "Iteration 59750: loss = 3.506985e-11,1.490909e-10\n",
      "Iteration 59755: loss = 3.5067546e-11,1.490911e-10\n",
      "Iteration 59760: loss = 3.5065076e-11,1.4909166e-10\n",
      "Iteration 59765: loss = 3.5061513e-11,1.4909163e-10\n",
      "Iteration 59770: loss = 3.5059105e-11,1.4909175e-10\n",
      "Iteration 59775: loss = 3.505657e-11,1.490918e-10\n",
      "Iteration 59780: loss = 3.5053977e-11,1.4909192e-10\n",
      "Iteration 59785: loss = 3.5050823e-11,1.4909204e-10\n",
      "Iteration 59790: loss = 3.504839e-11,1.4909224e-10\n",
      "Iteration 59795: loss = 3.5046136e-11,1.490922e-10\n",
      "Iteration 59800: loss = 3.5043843e-11,1.4909225e-10\n",
      "Iteration 59805: loss = 3.504126e-11,1.4909204e-10\n",
      "Iteration 59810: loss = 3.50394e-11,1.4909171e-10\n",
      "Iteration 59815: loss = 3.5037428e-11,1.490913e-10\n",
      "Iteration 59820: loss = 3.5034586e-11,1.4909096e-10\n",
      "Iteration 59825: loss = 3.5032772e-11,1.4909046e-10\n",
      "Iteration 59830: loss = 3.5030843e-11,1.4909027e-10\n",
      "Iteration 59835: loss = 3.5028893e-11,1.4908971e-10\n",
      "Iteration 59840: loss = 3.5026215e-11,1.4908944e-10\n",
      "Iteration 59845: loss = 3.50243e-11,1.490891e-10\n",
      "Iteration 59850: loss = 3.5022468e-11,1.490888e-10\n",
      "Iteration 59855: loss = 3.5020594e-11,1.4908844e-10\n",
      "Iteration 59860: loss = 3.5018186e-11,1.4908813e-10\n",
      "Iteration 59865: loss = 3.501636e-11,1.4908774e-10\n",
      "Iteration 59870: loss = 3.501454e-11,1.4908694e-10\n",
      "Iteration 59875: loss = 3.5012798e-11,1.490868e-10\n",
      "Iteration 59880: loss = 3.5010096e-11,1.4908623e-10\n",
      "Iteration 59885: loss = 3.5008434e-11,1.4908566e-10\n",
      "Iteration 59890: loss = 3.5006605e-11,1.490851e-10\n",
      "Iteration 59895: loss = 3.500405e-11,1.4908462e-10\n",
      "Iteration 59900: loss = 3.5002338e-11,1.4908422e-10\n",
      "Iteration 59905: loss = 3.5000624e-11,1.4908369e-10\n",
      "Iteration 59910: loss = 3.4999188e-11,1.4908325e-10\n",
      "Iteration 59915: loss = 3.4996516e-11,1.4908277e-10\n",
      "Iteration 59920: loss = 3.4994795e-11,1.4908219e-10\n",
      "Iteration 59925: loss = 3.4993012e-11,1.4908169e-10\n",
      "Iteration 59930: loss = 3.4991388e-11,1.4908119e-10\n",
      "Iteration 59935: loss = 3.4988724e-11,1.4908061e-10\n",
      "Iteration 59940: loss = 3.4987017e-11,1.4908008e-10\n",
      "Iteration 59945: loss = 3.498527e-11,1.4907969e-10\n",
      "Iteration 59950: loss = 3.4982715e-11,1.4907904e-10\n",
      "Iteration 59955: loss = 3.4980973e-11,1.4907872e-10\n",
      "Iteration 59960: loss = 3.497955e-11,1.4907828e-10\n",
      "Iteration 59965: loss = 3.4977892e-11,1.4907775e-10\n",
      "Iteration 59970: loss = 3.4975235e-11,1.4907722e-10\n",
      "Iteration 59975: loss = 3.497359e-11,1.490767e-10\n",
      "Iteration 59980: loss = 3.4971952e-11,1.4907603e-10\n",
      "Iteration 59985: loss = 3.4970287e-11,1.4907564e-10\n",
      "Iteration 59990: loss = 3.496781e-11,1.4907503e-10\n",
      "Iteration 59995: loss = 3.4966068e-11,1.490746e-10\n",
      "Iteration 60000: loss = 3.496443e-11,1.4907402e-10\n",
      "Iteration 60005: loss = 3.496271e-11,1.4907338e-10\n",
      "Iteration 60010: loss = 3.4960503e-11,1.4907275e-10\n",
      "Iteration 60015: loss = 3.495908e-11,1.4907213e-10\n",
      "Iteration 60020: loss = 3.4957523e-11,1.4907142e-10\n",
      "Iteration 60025: loss = 3.495535e-11,1.4907055e-10\n",
      "Iteration 60030: loss = 3.4953908e-11,1.4906988e-10\n",
      "Iteration 60035: loss = 3.4952468e-11,1.4906909e-10\n",
      "Iteration 60040: loss = 3.4950893e-11,1.490684e-10\n",
      "Iteration 60045: loss = 3.494872e-11,1.4906765e-10\n",
      "Iteration 60050: loss = 3.4947278e-11,1.4906684e-10\n",
      "Iteration 60055: loss = 3.494586e-11,1.4906605e-10\n",
      "Iteration 60060: loss = 3.494437e-11,1.490654e-10\n",
      "Iteration 60065: loss = 3.4942417e-11,1.4906461e-10\n",
      "Iteration 60070: loss = 3.4941102e-11,1.4906396e-10\n",
      "Iteration 60075: loss = 3.4939673e-11,1.4906316e-10\n",
      "Iteration 60080: loss = 3.4938264e-11,1.4906243e-10\n",
      "Iteration 60085: loss = 3.4935933e-11,1.4906171e-10\n",
      "Iteration 60090: loss = 3.493454e-11,1.4906096e-10\n",
      "Iteration 60095: loss = 3.49331e-11,1.490602e-10\n",
      "Iteration 60100: loss = 3.4930815e-11,1.4905942e-10\n",
      "Iteration 60105: loss = 3.4929486e-11,1.4905885e-10\n",
      "Iteration 60110: loss = 3.4927915e-11,1.4905811e-10\n",
      "Iteration 60115: loss = 3.492689e-11,1.4905728e-10\n",
      "Iteration 60120: loss = 3.4924685e-11,1.4905657e-10\n",
      "Iteration 60125: loss = 3.492318e-11,1.490558e-10\n",
      "Iteration 60130: loss = 3.4921795e-11,1.4905505e-10\n",
      "Iteration 60135: loss = 3.4920424e-11,1.4905416e-10\n",
      "Iteration 60140: loss = 3.4918072e-11,1.4905338e-10\n",
      "Iteration 60145: loss = 3.491672e-11,1.4905271e-10\n",
      "Iteration 60150: loss = 3.49153e-11,1.4905187e-10\n",
      "Iteration 60155: loss = 3.491306e-11,1.4905113e-10\n",
      "Iteration 60160: loss = 3.4911633e-11,1.4905036e-10\n",
      "Iteration 60165: loss = 3.49106e-11,1.4904955e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60170: loss = 3.4909322e-11,1.4904888e-10\n",
      "Iteration 60175: loss = 3.490697e-11,1.4904802e-10\n",
      "Iteration 60180: loss = 3.490558e-11,1.4904708e-10\n",
      "Iteration 60185: loss = 3.4904166e-11,1.4904622e-10\n",
      "Iteration 60190: loss = 3.4902945e-11,1.4904543e-10\n",
      "Iteration 60195: loss = 3.4900787e-11,1.4904461e-10\n",
      "Iteration 60200: loss = 3.4899427e-11,1.4904386e-10\n",
      "Iteration 60205: loss = 3.4898015e-11,1.4904303e-10\n",
      "Iteration 60210: loss = 3.489666e-11,1.4904225e-10\n",
      "Iteration 60215: loss = 3.4894903e-11,1.490414e-10\n",
      "Iteration 60220: loss = 3.4893515e-11,1.4904057e-10\n",
      "Iteration 60225: loss = 3.4892204e-11,1.4903974e-10\n",
      "Iteration 60230: loss = 3.48901e-11,1.49039e-10\n",
      "Iteration 60235: loss = 3.4888804e-11,1.4903792e-10\n",
      "Iteration 60240: loss = 3.4887402e-11,1.4903721e-10\n",
      "Iteration 60245: loss = 3.4886257e-11,1.4903632e-10\n",
      "Iteration 60250: loss = 3.488416e-11,1.4903524e-10\n",
      "Iteration 60255: loss = 3.488304e-11,1.4903448e-10\n",
      "Iteration 60260: loss = 3.4881698e-11,1.490335e-10\n",
      "Iteration 60265: loss = 3.488057e-11,1.4903262e-10\n",
      "Iteration 60270: loss = 3.4878694e-11,1.4903181e-10\n",
      "Iteration 60275: loss = 3.4877577e-11,1.490309e-10\n",
      "Iteration 60280: loss = 3.4876255e-11,1.4903008e-10\n",
      "Iteration 60285: loss = 3.4875016e-11,1.4902912e-10\n",
      "Iteration 60290: loss = 3.4872973e-11,1.4902815e-10\n",
      "Iteration 60295: loss = 3.487172e-11,1.4902708e-10\n",
      "Iteration 60300: loss = 3.4870464e-11,1.4902625e-10\n",
      "Iteration 60305: loss = 3.486839e-11,1.490254e-10\n",
      "Iteration 60310: loss = 3.4867262e-11,1.4902449e-10\n",
      "Iteration 60315: loss = 3.4865954e-11,1.4902368e-10\n",
      "Iteration 60320: loss = 3.4865024e-11,1.4902267e-10\n",
      "Iteration 60325: loss = 3.4863126e-11,1.4902156e-10\n",
      "Iteration 60330: loss = 3.4861874e-11,1.4902062e-10\n",
      "Iteration 60335: loss = 3.4860847e-11,1.4901969e-10\n",
      "Iteration 60340: loss = 3.4859674e-11,1.4901866e-10\n",
      "Iteration 60345: loss = 3.485773e-11,1.4901752e-10\n",
      "Iteration 60350: loss = 3.485653e-11,1.4901662e-10\n",
      "Iteration 60355: loss = 3.485533e-11,1.4901569e-10\n",
      "Iteration 60360: loss = 3.4854168e-11,1.4901487e-10\n",
      "Iteration 60365: loss = 3.4852208e-11,1.4901372e-10\n",
      "Iteration 60370: loss = 3.4851403e-11,1.4901277e-10\n",
      "Iteration 60375: loss = 3.4850182e-11,1.4901191e-10\n",
      "Iteration 60380: loss = 3.4848145e-11,1.4901087e-10\n",
      "Iteration 60385: loss = 3.4847073e-11,1.4900994e-10\n",
      "Iteration 60390: loss = 3.4845824e-11,1.4900886e-10\n",
      "Iteration 60395: loss = 3.4844745e-11,1.4900785e-10\n",
      "Iteration 60400: loss = 3.4842747e-11,1.4900688e-10\n",
      "Iteration 60405: loss = 3.4841543e-11,1.4900595e-10\n",
      "Iteration 60410: loss = 3.4840495e-11,1.4900498e-10\n",
      "Iteration 60415: loss = 3.4839034e-11,1.490042e-10\n",
      "Iteration 60420: loss = 3.483683e-11,1.4900359e-10\n",
      "Iteration 60425: loss = 3.4835135e-11,1.4900312e-10\n",
      "Iteration 60430: loss = 3.4833466e-11,1.490026e-10\n",
      "Iteration 60435: loss = 3.483103e-11,1.4900198e-10\n",
      "Iteration 60440: loss = 3.482929e-11,1.4900155e-10\n",
      "Iteration 60445: loss = 3.4827544e-11,1.490009e-10\n",
      "Iteration 60450: loss = 3.482597e-11,1.4900053e-10\n",
      "Iteration 60455: loss = 3.482338e-11,1.4900005e-10\n",
      "Iteration 60460: loss = 3.4821777e-11,1.4899969e-10\n",
      "Iteration 60465: loss = 3.482002e-11,1.4899917e-10\n",
      "Iteration 60470: loss = 3.4818627e-11,1.4899887e-10\n",
      "Iteration 60475: loss = 3.481614e-11,1.489982e-10\n",
      "Iteration 60480: loss = 3.4814433e-11,1.4899774e-10\n",
      "Iteration 60485: loss = 3.4812833e-11,1.4899708e-10\n",
      "Iteration 60490: loss = 3.481113e-11,1.4899648e-10\n",
      "Iteration 60495: loss = 3.480863e-11,1.4899595e-10\n",
      "Iteration 60500: loss = 3.4806952e-11,1.489953e-10\n",
      "Iteration 60505: loss = 3.48052e-11,1.4899482e-10\n",
      "Iteration 60510: loss = 3.4802803e-11,1.4899426e-10\n",
      "Iteration 60515: loss = 3.480109e-11,1.4899369e-10\n",
      "Iteration 60520: loss = 3.479961e-11,1.4899307e-10\n",
      "Iteration 60525: loss = 3.4798248e-11,1.4899235e-10\n",
      "Iteration 60530: loss = 3.4795982e-11,1.489916e-10\n",
      "Iteration 60535: loss = 3.4794383e-11,1.4899096e-10\n",
      "Iteration 60540: loss = 3.4792887e-11,1.4899035e-10\n",
      "Iteration 60545: loss = 3.4791482e-11,1.4898964e-10\n",
      "Iteration 60550: loss = 3.478914e-11,1.4898896e-10\n",
      "Iteration 60555: loss = 3.4787534e-11,1.4898843e-10\n",
      "Iteration 60560: loss = 3.4786025e-11,1.4898774e-10\n",
      "Iteration 60565: loss = 3.4784623e-11,1.4898721e-10\n",
      "Iteration 60570: loss = 3.4782354e-11,1.4898652e-10\n",
      "Iteration 60575: loss = 3.4781036e-11,1.4898574e-10\n",
      "Iteration 60580: loss = 3.477948e-11,1.489851e-10\n",
      "Iteration 60585: loss = 3.477686e-11,1.4898476e-10\n",
      "Iteration 60590: loss = 3.4774707e-11,1.489843e-10\n",
      "Iteration 60595: loss = 3.4772598e-11,1.4898462e-10\n",
      "Iteration 60600: loss = 3.4770315e-11,1.4898452e-10\n",
      "Iteration 60605: loss = 3.4767283e-11,1.489846e-10\n",
      "Iteration 60610: loss = 3.4765014e-11,1.4898456e-10\n",
      "Iteration 60615: loss = 3.4762773e-11,1.4898462e-10\n",
      "Iteration 60620: loss = 3.4760604e-11,1.4898449e-10\n",
      "Iteration 60625: loss = 3.4757846e-11,1.4898435e-10\n",
      "Iteration 60630: loss = 3.4755743e-11,1.4898442e-10\n",
      "Iteration 60635: loss = 3.4753585e-11,1.4898435e-10\n",
      "Iteration 60640: loss = 3.4750623e-11,1.4898438e-10\n",
      "Iteration 60645: loss = 3.4748496e-11,1.4898413e-10\n",
      "Iteration 60650: loss = 3.4746542e-11,1.4898408e-10\n",
      "Iteration 60655: loss = 3.4744485e-11,1.4898383e-10\n",
      "Iteration 60660: loss = 3.4741703e-11,1.4898353e-10\n",
      "Iteration 60665: loss = 3.4739645e-11,1.4898342e-10\n",
      "Iteration 60670: loss = 3.4737813e-11,1.4898294e-10\n",
      "Iteration 60675: loss = 3.4736405e-11,1.4898269e-10\n",
      "Iteration 60680: loss = 3.47339e-11,1.489821e-10\n",
      "Iteration 60685: loss = 3.4732214e-11,1.4898154e-10\n",
      "Iteration 60690: loss = 3.473061e-11,1.489809e-10\n",
      "Iteration 60695: loss = 3.472909e-11,1.4898033e-10\n",
      "Iteration 60700: loss = 3.4726656e-11,1.4897975e-10\n",
      "Iteration 60705: loss = 3.4725056e-11,1.4897908e-10\n",
      "Iteration 60710: loss = 3.4723616e-11,1.4897847e-10\n",
      "Iteration 60715: loss = 3.4721236e-11,1.4897784e-10\n",
      "Iteration 60720: loss = 3.4719855e-11,1.4897714e-10\n",
      "Iteration 60725: loss = 3.4718322e-11,1.4897654e-10\n",
      "Iteration 60730: loss = 3.4717118e-11,1.4897569e-10\n",
      "Iteration 60735: loss = 3.471484e-11,1.489752e-10\n",
      "Iteration 60740: loss = 3.471328e-11,1.4897447e-10\n",
      "Iteration 60745: loss = 3.471196e-11,1.4897385e-10\n",
      "Iteration 60750: loss = 3.4710474e-11,1.4897314e-10\n",
      "Iteration 60755: loss = 3.470825e-11,1.4897245e-10\n",
      "Iteration 60760: loss = 3.4706824e-11,1.489718e-10\n",
      "Iteration 60765: loss = 3.4705357e-11,1.4897107e-10\n",
      "Iteration 60770: loss = 3.4704e-11,1.4897053e-10\n",
      "Iteration 60775: loss = 3.4701794e-11,1.4896957e-10\n",
      "Iteration 60780: loss = 3.4700656e-11,1.4896896e-10\n",
      "Iteration 60785: loss = 3.469925e-11,1.4896807e-10\n",
      "Iteration 60790: loss = 3.4696954e-11,1.4896746e-10\n",
      "Iteration 60795: loss = 3.469561e-11,1.4896662e-10\n",
      "Iteration 60800: loss = 3.4694112e-11,1.4896592e-10\n",
      "Iteration 60805: loss = 3.469279e-11,1.4896522e-10\n",
      "Iteration 60810: loss = 3.4690566e-11,1.489644e-10\n",
      "Iteration 60815: loss = 3.468942e-11,1.4896358e-10\n",
      "Iteration 60820: loss = 3.4688603e-11,1.489623e-10\n",
      "Iteration 60825: loss = 3.4687152e-11,1.489616e-10\n",
      "Iteration 60830: loss = 3.4684998e-11,1.489608e-10\n",
      "Iteration 60835: loss = 3.4683523e-11,1.4896022e-10\n",
      "Iteration 60840: loss = 3.4682993e-11,1.4895868e-10\n",
      "Iteration 60845: loss = 3.4680772e-11,1.4895771e-10\n",
      "Iteration 60850: loss = 3.467943e-11,1.4895712e-10\n",
      "Iteration 60855: loss = 3.4678007e-11,1.4895624e-10\n",
      "Iteration 60860: loss = 3.46776e-11,1.4895482e-10\n",
      "Iteration 60865: loss = 3.4675426e-11,1.4895393e-10\n",
      "Iteration 60870: loss = 3.4674257e-11,1.4895309e-10\n",
      "Iteration 60875: loss = 3.4673302e-11,1.4895205e-10\n",
      "Iteration 60880: loss = 3.4671956e-11,1.4895127e-10\n",
      "Iteration 60885: loss = 3.4670114e-11,1.4895028e-10\n",
      "Iteration 60890: loss = 3.4669465e-11,1.4894906e-10\n",
      "Iteration 60895: loss = 3.466805e-11,1.4894824e-10\n",
      "Iteration 60900: loss = 3.4666877e-11,1.489474e-10\n",
      "Iteration 60905: loss = 3.4664733e-11,1.489466e-10\n",
      "Iteration 60910: loss = 3.4663734e-11,1.4894551e-10\n",
      "Iteration 60915: loss = 3.4663033e-11,1.4894402e-10\n",
      "Iteration 60920: loss = 3.466129e-11,1.489431e-10\n",
      "Iteration 60925: loss = 3.4659917e-11,1.489423e-10\n",
      "Iteration 60930: loss = 3.4658658e-11,1.4894137e-10\n",
      "Iteration 60935: loss = 3.4657447e-11,1.4894068e-10\n",
      "Iteration 60940: loss = 3.4655615e-11,1.489395e-10\n",
      "Iteration 60945: loss = 3.4654286e-11,1.489386e-10\n",
      "Iteration 60950: loss = 3.4653072e-11,1.4893793e-10\n",
      "Iteration 60955: loss = 3.4651785e-11,1.4893682e-10\n",
      "Iteration 60960: loss = 3.464995e-11,1.4893586e-10\n",
      "Iteration 60965: loss = 3.4648753e-11,1.4893514e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60970: loss = 3.464803e-11,1.489337e-10\n",
      "Iteration 60975: loss = 3.464699e-11,1.4893264e-10\n",
      "Iteration 60980: loss = 3.4645117e-11,1.4893178e-10\n",
      "Iteration 60985: loss = 3.4643965e-11,1.4893084e-10\n",
      "Iteration 60990: loss = 3.464275e-11,1.4893009e-10\n",
      "Iteration 60995: loss = 3.464081e-11,1.4892891e-10\n",
      "Iteration 61000: loss = 3.4639635e-11,1.4892809e-10\n",
      "Iteration 61005: loss = 3.4638504e-11,1.4892719e-10\n",
      "Iteration 61010: loss = 3.463718e-11,1.4892637e-10\n",
      "Iteration 61015: loss = 3.463523e-11,1.4892551e-10\n",
      "Iteration 61020: loss = 3.4634e-11,1.489246e-10\n",
      "Iteration 61025: loss = 3.4633168e-11,1.4892337e-10\n",
      "Iteration 61030: loss = 3.4632026e-11,1.4892249e-10\n",
      "Iteration 61035: loss = 3.4629948e-11,1.4892142e-10\n",
      "Iteration 61040: loss = 3.462906e-11,1.4892063e-10\n",
      "Iteration 61045: loss = 3.4628227e-11,1.4891918e-10\n",
      "Iteration 61050: loss = 3.4626257e-11,1.4891831e-10\n",
      "Iteration 61055: loss = 3.4625327e-11,1.4891724e-10\n",
      "Iteration 61060: loss = 3.462425e-11,1.4891637e-10\n",
      "Iteration 61065: loss = 3.462308e-11,1.4891532e-10\n",
      "Iteration 61070: loss = 3.4621146e-11,1.4891446e-10\n",
      "Iteration 61075: loss = 3.4620067e-11,1.4891344e-10\n",
      "Iteration 61080: loss = 3.4618836e-11,1.4891241e-10\n",
      "Iteration 61085: loss = 3.461811e-11,1.4891116e-10\n",
      "Iteration 61090: loss = 3.461608e-11,1.4891026e-10\n",
      "Iteration 61095: loss = 3.4614995e-11,1.4890938e-10\n",
      "Iteration 61100: loss = 3.4613836e-11,1.4890839e-10\n",
      "Iteration 61105: loss = 3.4612674e-11,1.489073e-10\n",
      "Iteration 61110: loss = 3.4610984e-11,1.4890632e-10\n",
      "Iteration 61115: loss = 3.4609822e-11,1.4890549e-10\n",
      "Iteration 61120: loss = 3.4609e-11,1.4890417e-10\n",
      "Iteration 61125: loss = 3.4607095e-11,1.4890325e-10\n",
      "Iteration 61130: loss = 3.4605943e-11,1.489021e-10\n",
      "Iteration 61135: loss = 3.4605062e-11,1.4890131e-10\n",
      "Iteration 61140: loss = 3.4604e-11,1.4890024e-10\n",
      "Iteration 61145: loss = 3.4602394e-11,1.4889909e-10\n",
      "Iteration 61150: loss = 3.4601527e-11,1.488978e-10\n",
      "Iteration 61155: loss = 3.4600427e-11,1.4889678e-10\n",
      "Iteration 61160: loss = 3.4599334e-11,1.4889578e-10\n",
      "Iteration 61165: loss = 3.4597363e-11,1.4889479e-10\n",
      "Iteration 61170: loss = 3.4596312e-11,1.4889406e-10\n",
      "Iteration 61175: loss = 3.4595143e-11,1.4889293e-10\n",
      "Iteration 61180: loss = 3.4594085e-11,1.4889186e-10\n",
      "Iteration 61185: loss = 3.4592152e-11,1.4889108e-10\n",
      "Iteration 61190: loss = 3.459132e-11,1.4888994e-10\n",
      "Iteration 61195: loss = 3.4590133e-11,1.4888908e-10\n",
      "Iteration 61200: loss = 3.3929737e-11,1.4888814e-10\n",
      "Iteration 61205: loss = 3.3926775e-11,1.4888901e-10\n",
      "Iteration 61210: loss = 3.392668e-11,1.4888697e-10\n",
      "Iteration 61215: loss = 3.392747e-11,1.488836e-10\n",
      "Iteration 61220: loss = 3.3928613e-11,1.4888066e-10\n",
      "Iteration 61225: loss = 3.3929384e-11,1.4887785e-10\n",
      "Iteration 61230: loss = 3.4588988e-11,1.4887476e-10\n",
      "Iteration 61235: loss = 3.458919e-11,1.4887296e-10\n",
      "Iteration 61240: loss = 3.3929037e-11,1.4887279e-10\n",
      "Iteration 61245: loss = 3.392991e-11,1.488697e-10\n",
      "Iteration 61250: loss = 3.3929345e-11,1.4886768e-10\n",
      "Iteration 61255: loss = 3.3929866e-11,1.4886488e-10\n",
      "Iteration 61260: loss = 3.3930053e-11,1.4886263e-10\n",
      "Iteration 61265: loss = 3.4589e-11,1.4886058e-10\n",
      "Iteration 61270: loss = 3.3928433e-11,1.4885998e-10\n",
      "Iteration 61275: loss = 3.3929266e-11,1.488569e-10\n",
      "Iteration 61280: loss = 3.4589092e-11,1.4885367e-10\n",
      "Iteration 61285: loss = 3.3929443e-11,1.4885282e-10\n",
      "Iteration 61290: loss = 3.3928253e-11,1.4885214e-10\n",
      "Iteration 61295: loss = 3.392926e-11,1.4884899e-10\n",
      "Iteration 61300: loss = 3.458899e-11,1.4884582e-10\n",
      "Iteration 61305: loss = 3.3929907e-11,1.4884396e-10\n",
      "Iteration 61310: loss = 3.3929377e-11,1.4884197e-10\n",
      "Iteration 61315: loss = 3.458896e-11,1.4883854e-10\n",
      "Iteration 61320: loss = 3.392938e-11,1.48838e-10\n",
      "Iteration 61325: loss = 3.3927562e-11,1.4883772e-10\n",
      "Iteration 61330: loss = 3.3927923e-11,1.4883522e-10\n",
      "Iteration 61335: loss = 3.3929227e-11,1.4883192e-10\n",
      "Iteration 61340: loss = 3.393025e-11,1.4882895e-10\n",
      "Iteration 61345: loss = 3.392972e-11,1.4882698e-10\n",
      "Iteration 61350: loss = 3.392956e-11,1.4882542e-10\n",
      "Iteration 61355: loss = 3.4589234e-11,1.4882191e-10\n",
      "Iteration 61360: loss = 3.3929803e-11,1.488208e-10\n",
      "Iteration 61365: loss = 3.4589432e-11,1.4881789e-10\n",
      "Iteration 61370: loss = 3.3928277e-11,1.4881785e-10\n",
      "Iteration 61375: loss = 3.392372e-11,1.4881983e-10\n",
      "Iteration 61380: loss = 3.3920748e-11,1.4882043e-10\n",
      "Iteration 61385: loss = 3.3918104e-11,1.4882086e-10\n",
      "Iteration 61390: loss = 3.3915152e-11,1.4882112e-10\n",
      "Iteration 61395: loss = 3.39126e-11,1.4882151e-10\n",
      "Iteration 61400: loss = 3.390963e-11,1.4882193e-10\n",
      "Iteration 61405: loss = 3.3906978e-11,1.488224e-10\n",
      "Iteration 61410: loss = 3.3904008e-11,1.4882287e-10\n",
      "Iteration 61415: loss = 3.3901878e-11,1.4882325e-10\n",
      "Iteration 61420: loss = 3.3895952e-11,1.4882627e-10\n",
      "Iteration 61425: loss = 3.388972e-11,1.4883009e-10\n",
      "Iteration 61430: loss = 3.3883837e-11,1.488334e-10\n",
      "Iteration 61435: loss = 3.387682e-11,1.4883692e-10\n",
      "Iteration 61440: loss = 3.3871173e-11,1.4884033e-10\n",
      "Iteration 61445: loss = 3.386528e-11,1.4884355e-10\n",
      "Iteration 61450: loss = 3.3859294e-11,1.4884713e-10\n",
      "Iteration 61455: loss = 3.385282e-11,1.488501e-10\n",
      "Iteration 61460: loss = 3.3847022e-11,1.4885343e-10\n",
      "Iteration 61465: loss = 3.3841582e-11,1.4885641e-10\n",
      "Iteration 61470: loss = 3.383525e-11,1.4885955e-10\n",
      "Iteration 61475: loss = 3.382992e-11,1.4886234e-10\n",
      "Iteration 61480: loss = 3.382446e-11,1.4886573e-10\n",
      "Iteration 61485: loss = 3.3818923e-11,1.4886858e-10\n",
      "Iteration 61490: loss = 3.3165335e-11,1.4887054e-10\n",
      "Iteration 61495: loss = 3.3162414e-11,1.4887118e-10\n",
      "Iteration 61500: loss = 3.315963e-11,1.4887162e-10\n",
      "Iteration 61505: loss = 3.31563e-11,1.4887247e-10\n",
      "Iteration 61510: loss = 3.3153202e-11,1.4887329e-10\n",
      "Iteration 61515: loss = 3.3149865e-11,1.4887397e-10\n",
      "Iteration 61520: loss = 3.3146656e-11,1.4887469e-10\n",
      "Iteration 61525: loss = 3.3144574e-11,1.4887537e-10\n",
      "Iteration 61530: loss = 3.3141462e-11,1.488761e-10\n",
      "Iteration 61535: loss = 3.313851e-11,1.4887669e-10\n",
      "Iteration 61540: loss = 3.3135304e-11,1.4887723e-10\n",
      "Iteration 61545: loss = 3.3132466e-11,1.48878e-10\n",
      "Iteration 61550: loss = 3.312925e-11,1.4887863e-10\n",
      "Iteration 61555: loss = 3.312611e-11,1.4887916e-10\n",
      "Iteration 61560: loss = 3.3123244e-11,1.488799e-10\n",
      "Iteration 61565: loss = 3.3120104e-11,1.4888052e-10\n",
      "Iteration 61570: loss = 3.3117373e-11,1.4888091e-10\n",
      "Iteration 61575: loss = 3.3114237e-11,1.4888182e-10\n",
      "Iteration 61580: loss = 3.3111493e-11,1.4888231e-10\n",
      "Iteration 61585: loss = 3.3108485e-11,1.4888274e-10\n",
      "Iteration 61590: loss = 3.310557e-11,1.488832e-10\n",
      "Iteration 61595: loss = 3.310296e-11,1.4888346e-10\n",
      "Iteration 61600: loss = 3.3100175e-11,1.4888374e-10\n",
      "Iteration 61605: loss = 3.3098402e-11,1.488842e-10\n",
      "Iteration 61610: loss = 3.309564e-11,1.4888463e-10\n",
      "Iteration 61615: loss = 3.309308e-11,1.4888497e-10\n",
      "Iteration 61620: loss = 3.3090284e-11,1.4888527e-10\n",
      "Iteration 61625: loss = 3.3087477e-11,1.4888553e-10\n",
      "Iteration 61630: loss = 3.308494e-11,1.4888574e-10\n",
      "Iteration 61635: loss = 3.3082138e-11,1.4888618e-10\n",
      "Iteration 61640: loss = 3.30796e-11,1.4888663e-10\n",
      "Iteration 61645: loss = 3.3076906e-11,1.488869e-10\n",
      "Iteration 61650: loss = 3.3074456e-11,1.488873e-10\n",
      "Iteration 61655: loss = 3.3071854e-11,1.4888751e-10\n",
      "Iteration 61660: loss = 3.3069134e-11,1.4888756e-10\n",
      "Iteration 61665: loss = 3.3066952e-11,1.4888768e-10\n",
      "Iteration 61670: loss = 3.3064565e-11,1.4888765e-10\n",
      "Iteration 61675: loss = 3.3062057e-11,1.4888808e-10\n",
      "Iteration 61680: loss = 3.3059114e-11,1.4888862e-10\n",
      "Iteration 61685: loss = 3.3056325e-11,1.4888908e-10\n",
      "Iteration 61690: loss = 3.305402e-11,1.4888966e-10\n",
      "Iteration 61695: loss = 3.3050934e-11,1.4889058e-10\n",
      "Iteration 61700: loss = 3.3048037e-11,1.48891e-10\n",
      "Iteration 61705: loss = 3.3044904e-11,1.4889165e-10\n",
      "Iteration 61710: loss = 3.3042097e-11,1.4889227e-10\n",
      "Iteration 61715: loss = 3.30391e-11,1.4889277e-10\n",
      "Iteration 61720: loss = 3.303625e-11,1.4889323e-10\n",
      "Iteration 61725: loss = 3.3033465e-11,1.4889372e-10\n",
      "Iteration 61730: loss = 3.3030582e-11,1.4889431e-10\n",
      "Iteration 61735: loss = 3.3028e-11,1.4889481e-10\n",
      "Iteration 61740: loss = 3.3025038e-11,1.4889512e-10\n",
      "Iteration 61745: loss = 3.3022547e-11,1.4889533e-10\n",
      "Iteration 61750: loss = 3.3019757e-11,1.4889578e-10\n",
      "Iteration 61755: loss = 3.301683e-11,1.4889619e-10\n",
      "Iteration 61760: loss = 3.3013942e-11,1.4889671e-10\n",
      "Iteration 61765: loss = 3.3010678e-11,1.4889762e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61770: loss = 3.3008495e-11,1.4889867e-10\n",
      "Iteration 61775: loss = 3.3005206e-11,1.4889939e-10\n",
      "Iteration 61780: loss = 3.300216e-11,1.4890023e-10\n",
      "Iteration 61785: loss = 3.299881e-11,1.4890074e-10\n",
      "Iteration 61790: loss = 3.2995565e-11,1.4890178e-10\n",
      "Iteration 61795: loss = 3.2992657e-11,1.4890245e-10\n",
      "Iteration 61800: loss = 3.2989406e-11,1.489033e-10\n",
      "Iteration 61805: loss = 3.2986454e-11,1.4890403e-10\n",
      "Iteration 61810: loss = 3.2983352e-11,1.4890457e-10\n",
      "Iteration 61815: loss = 3.2980608e-11,1.4890514e-10\n",
      "Iteration 61820: loss = 3.2977534e-11,1.4890565e-10\n",
      "Iteration 61825: loss = 3.2974488e-11,1.4890633e-10\n",
      "Iteration 61830: loss = 3.2971775e-11,1.489071e-10\n",
      "Iteration 61835: loss = 3.2968753e-11,1.4890764e-10\n",
      "Iteration 61840: loss = 3.2966015e-11,1.489082e-10\n",
      "Iteration 61845: loss = 3.296297e-11,1.4890866e-10\n",
      "Iteration 61850: loss = 3.2960266e-11,1.489092e-10\n",
      "Iteration 61855: loss = 3.2958178e-11,1.4890966e-10\n",
      "Iteration 61860: loss = 3.2955267e-11,1.4891009e-10\n",
      "Iteration 61865: loss = 3.295264e-11,1.4891063e-10\n",
      "Iteration 61870: loss = 3.294975e-11,1.4891112e-10\n",
      "Iteration 61875: loss = 3.2947152e-11,1.4891141e-10\n",
      "Iteration 61880: loss = 3.2944248e-11,1.4891205e-10\n",
      "Iteration 61885: loss = 3.294173e-11,1.4891241e-10\n",
      "Iteration 61890: loss = 3.2938832e-11,1.4891252e-10\n",
      "Iteration 61895: loss = 3.2936282e-11,1.4891277e-10\n",
      "Iteration 61900: loss = 3.2933888e-11,1.48913e-10\n",
      "Iteration 61905: loss = 3.293134e-11,1.4891335e-10\n",
      "Iteration 61910: loss = 3.2928944e-11,1.489135e-10\n",
      "Iteration 61915: loss = 3.292638e-11,1.4891369e-10\n",
      "Iteration 61920: loss = 3.2924146e-11,1.4891385e-10\n",
      "Iteration 61925: loss = 3.292177e-11,1.4891394e-10\n",
      "Iteration 61930: loss = 3.291933e-11,1.4891388e-10\n",
      "Iteration 61935: loss = 3.2917984e-11,1.4891388e-10\n",
      "Iteration 61940: loss = 3.2915518e-11,1.4891392e-10\n",
      "Iteration 61945: loss = 3.2913533e-11,1.489141e-10\n",
      "Iteration 61950: loss = 3.291109e-11,1.4891402e-10\n",
      "Iteration 61955: loss = 3.2909182e-11,1.4891385e-10\n",
      "Iteration 61960: loss = 3.2906813e-11,1.4891369e-10\n",
      "Iteration 61965: loss = 3.2904537e-11,1.4891358e-10\n",
      "Iteration 61970: loss = 3.290249e-11,1.4891338e-10\n",
      "Iteration 61975: loss = 3.2900224e-11,1.4891341e-10\n",
      "Iteration 61980: loss = 3.2898236e-11,1.4891334e-10\n",
      "Iteration 61985: loss = 3.289598e-11,1.489133e-10\n",
      "Iteration 61990: loss = 3.289392e-11,1.4891323e-10\n",
      "Iteration 61995: loss = 3.2891658e-11,1.4891323e-10\n",
      "Iteration 62000: loss = 3.2889375e-11,1.4891312e-10\n",
      "Iteration 62005: loss = 3.2887342e-11,1.4891302e-10\n",
      "Iteration 62010: loss = 3.2885156e-11,1.4891284e-10\n",
      "Iteration 62015: loss = 3.2883987e-11,1.4891274e-10\n",
      "Iteration 62020: loss = 3.288159e-11,1.4891255e-10\n",
      "Iteration 62025: loss = 3.287973e-11,1.4891255e-10\n",
      "Iteration 62030: loss = 3.2878016e-11,1.4891186e-10\n",
      "Iteration 62035: loss = 3.287582e-11,1.4891155e-10\n",
      "Iteration 62040: loss = 3.2874085e-11,1.4891142e-10\n",
      "Iteration 62045: loss = 3.287194e-11,1.489112e-10\n",
      "Iteration 62050: loss = 3.287026e-11,1.4891079e-10\n",
      "Iteration 62055: loss = 3.2868406e-11,1.489105e-10\n",
      "Iteration 62060: loss = 3.286638e-11,1.4891016e-10\n",
      "Iteration 62065: loss = 3.2864565e-11,1.489098e-10\n",
      "Iteration 62070: loss = 3.286259e-11,1.4890944e-10\n",
      "Iteration 62075: loss = 3.28606e-11,1.4890919e-10\n",
      "Iteration 62080: loss = 3.285878e-11,1.4890884e-10\n",
      "Iteration 62085: loss = 3.2856846e-11,1.489084e-10\n",
      "Iteration 62090: loss = 3.285501e-11,1.4890814e-10\n",
      "Iteration 62095: loss = 3.285296e-11,1.489077e-10\n",
      "Iteration 62100: loss = 3.2851968e-11,1.4890754e-10\n",
      "Iteration 62105: loss = 3.2849903e-11,1.4890715e-10\n",
      "Iteration 62110: loss = 3.2847943e-11,1.489068e-10\n",
      "Iteration 62115: loss = 3.284618e-11,1.4890637e-10\n",
      "Iteration 62120: loss = 3.284417e-11,1.4890619e-10\n",
      "Iteration 62125: loss = 3.2842326e-11,1.489059e-10\n",
      "Iteration 62130: loss = 3.2840317e-11,1.4890576e-10\n",
      "Iteration 62135: loss = 3.2838638e-11,1.4890542e-10\n",
      "Iteration 62140: loss = 3.2836848e-11,1.4890486e-10\n",
      "Iteration 62145: loss = 3.2835044e-11,1.489044e-10\n",
      "Iteration 62150: loss = 3.283332e-11,1.4890414e-10\n",
      "Iteration 62155: loss = 3.2831602e-11,1.4890353e-10\n",
      "Iteration 62160: loss = 3.282993e-11,1.4890307e-10\n",
      "Iteration 62165: loss = 3.2828063e-11,1.4890264e-10\n",
      "Iteration 62170: loss = 3.2826495e-11,1.4890217e-10\n",
      "Iteration 62175: loss = 3.2824476e-11,1.4890181e-10\n",
      "Iteration 62180: loss = 3.2823366e-11,1.4890132e-10\n",
      "Iteration 62185: loss = 3.282159e-11,1.4890099e-10\n",
      "Iteration 62190: loss = 3.281976e-11,1.4890059e-10\n",
      "Iteration 62195: loss = 3.281812e-11,1.4890017e-10\n",
      "Iteration 62200: loss = 3.281633e-11,1.4889964e-10\n",
      "Iteration 62205: loss = 3.2814668e-11,1.488992e-10\n",
      "Iteration 62210: loss = 3.281284e-11,1.4889871e-10\n",
      "Iteration 62215: loss = 3.281103e-11,1.488982e-10\n",
      "Iteration 62220: loss = 3.2809436e-11,1.488977e-10\n",
      "Iteration 62225: loss = 3.2807566e-11,1.4889709e-10\n",
      "Iteration 62230: loss = 3.280607e-11,1.4889658e-10\n",
      "Iteration 62235: loss = 3.2804152e-11,1.4889612e-10\n",
      "Iteration 62240: loss = 3.2802653e-11,1.488957e-10\n",
      "Iteration 62245: loss = 3.280079e-11,1.4889513e-10\n",
      "Iteration 62250: loss = 3.2798975e-11,1.4889463e-10\n",
      "Iteration 62255: loss = 3.2797338e-11,1.488941e-10\n",
      "Iteration 62260: loss = 3.279632e-11,1.4889358e-10\n",
      "Iteration 62265: loss = 3.279479e-11,1.4889313e-10\n",
      "Iteration 62270: loss = 3.2792883e-11,1.4889258e-10\n",
      "Iteration 62275: loss = 3.279111e-11,1.4889195e-10\n",
      "Iteration 62280: loss = 3.27894e-11,1.4889173e-10\n",
      "Iteration 62285: loss = 3.2787554e-11,1.4889132e-10\n",
      "Iteration 62290: loss = 3.278596e-11,1.4889076e-10\n",
      "Iteration 62295: loss = 3.2784105e-11,1.488902e-10\n",
      "Iteration 62300: loss = 3.278261e-11,1.488899e-10\n",
      "Iteration 62305: loss = 3.2780726e-11,1.4888948e-10\n",
      "Iteration 62310: loss = 3.27789e-11,1.4888908e-10\n",
      "Iteration 62315: loss = 3.277734e-11,1.4888846e-10\n",
      "Iteration 62320: loss = 3.277553e-11,1.4888805e-10\n",
      "Iteration 62325: loss = 3.2773933e-11,1.4888779e-10\n",
      "Iteration 62330: loss = 3.2772087e-11,1.4888735e-10\n",
      "Iteration 62335: loss = 3.2770505e-11,1.4888671e-10\n",
      "Iteration 62340: loss = 3.2768642e-11,1.4888642e-10\n",
      "Iteration 62345: loss = 3.2767546e-11,1.4888588e-10\n",
      "Iteration 62350: loss = 3.276605e-11,1.4888553e-10\n",
      "Iteration 62355: loss = 3.2764135e-11,1.4888493e-10\n",
      "Iteration 62360: loss = 3.2762713e-11,1.488845e-10\n",
      "Iteration 62365: loss = 3.2760777e-11,1.4888395e-10\n",
      "Iteration 62370: loss = 3.2759198e-11,1.488835e-10\n",
      "Iteration 62375: loss = 3.275734e-11,1.4888304e-10\n",
      "Iteration 62380: loss = 3.275549e-11,1.4888252e-10\n",
      "Iteration 62385: loss = 3.2753956e-11,1.488819e-10\n",
      "Iteration 62390: loss = 3.2752127e-11,1.4888142e-10\n",
      "Iteration 62395: loss = 3.2750528e-11,1.4888102e-10\n",
      "Iteration 62400: loss = 3.2748786e-11,1.4888046e-10\n",
      "Iteration 62405: loss = 3.274726e-11,1.4888e-10\n",
      "Iteration 62410: loss = 3.2745404e-11,1.4887949e-10\n",
      "Iteration 62415: loss = 3.274366e-11,1.4887891e-10\n",
      "Iteration 62420: loss = 3.2742153e-11,1.4887834e-10\n",
      "Iteration 62425: loss = 3.2741157e-11,1.4887783e-10\n",
      "Iteration 62430: loss = 3.2739613e-11,1.4887724e-10\n",
      "Iteration 62435: loss = 3.273782e-11,1.4887666e-10\n",
      "Iteration 62440: loss = 3.2736282e-11,1.4887609e-10\n",
      "Iteration 62445: loss = 3.273445e-11,1.4887568e-10\n",
      "Iteration 62450: loss = 3.2732688e-11,1.4887505e-10\n",
      "Iteration 62455: loss = 3.27312e-11,1.4887463e-10\n",
      "Iteration 62460: loss = 3.2729392e-11,1.4887398e-10\n",
      "Iteration 62465: loss = 3.2727918e-11,1.488733e-10\n",
      "Iteration 62470: loss = 3.2726127e-11,1.4887311e-10\n",
      "Iteration 62475: loss = 3.272453e-11,1.4887258e-10\n",
      "Iteration 62480: loss = 3.2722765e-11,1.4887194e-10\n",
      "Iteration 62485: loss = 3.272104e-11,1.4887146e-10\n",
      "Iteration 62490: loss = 3.2719487e-11,1.4887117e-10\n",
      "Iteration 62495: loss = 3.2717707e-11,1.4887053e-10\n",
      "Iteration 62500: loss = 3.2716194e-11,1.4887015e-10\n",
      "Iteration 62505: loss = 3.2715202e-11,1.4886953e-10\n",
      "Iteration 62510: loss = 3.271364e-11,1.4886904e-10\n",
      "Iteration 62515: loss = 3.2711934e-11,1.488686e-10\n",
      "Iteration 62520: loss = 3.2710133e-11,1.4886795e-10\n",
      "Iteration 62525: loss = 3.2708582e-11,1.4886742e-10\n",
      "Iteration 62530: loss = 3.2706844e-11,1.4886697e-10\n",
      "Iteration 62535: loss = 3.2705304e-11,1.4886654e-10\n",
      "Iteration 62540: loss = 3.270365e-11,1.4886595e-10\n",
      "Iteration 62545: loss = 3.2702046e-11,1.4886531e-10\n",
      "Iteration 62550: loss = 3.270034e-11,1.4886477e-10\n",
      "Iteration 62555: loss = 3.2698583e-11,1.488642e-10\n",
      "Iteration 62560: loss = 3.26972e-11,1.488635e-10\n",
      "Iteration 62565: loss = 3.2695433e-11,1.4886298e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62570: loss = 3.2693917e-11,1.4886242e-10\n",
      "Iteration 62575: loss = 3.2692116e-11,1.4886203e-10\n",
      "Iteration 62580: loss = 3.2690687e-11,1.4886134e-10\n",
      "Iteration 62585: loss = 3.2688973e-11,1.4886076e-10\n",
      "Iteration 62590: loss = 3.2687988e-11,1.4886015e-10\n",
      "Iteration 62595: loss = 3.2686492e-11,1.4885959e-10\n",
      "Iteration 62600: loss = 3.268476e-11,1.4885893e-10\n",
      "Iteration 62605: loss = 3.268323e-11,1.4885855e-10\n",
      "Iteration 62610: loss = 3.268152e-11,1.4885795e-10\n",
      "Iteration 62615: loss = 3.268005e-11,1.488573e-10\n",
      "Iteration 62620: loss = 3.2678315e-11,1.4885668e-10\n",
      "Iteration 62625: loss = 3.267659e-11,1.4885608e-10\n",
      "Iteration 62630: loss = 3.2675123e-11,1.4885553e-10\n",
      "Iteration 62635: loss = 3.2673392e-11,1.4885505e-10\n",
      "Iteration 62640: loss = 3.267198e-11,1.4885455e-10\n",
      "Iteration 62645: loss = 3.2670106e-11,1.4885407e-10\n",
      "Iteration 62650: loss = 3.2668746e-11,1.488533e-10\n",
      "Iteration 62655: loss = 3.266692e-11,1.4885271e-10\n",
      "Iteration 62660: loss = 3.266529e-11,1.4885226e-10\n",
      "Iteration 62665: loss = 3.2663882e-11,1.4885164e-10\n",
      "Iteration 62670: loss = 3.2662983e-11,1.4885085e-10\n",
      "Iteration 62675: loss = 3.2661648e-11,1.4885022e-10\n",
      "Iteration 62680: loss = 3.266001e-11,1.4884952e-10\n",
      "Iteration 62685: loss = 3.2658504e-11,1.4884885e-10\n",
      "Iteration 62690: loss = 3.265688e-11,1.488483e-10\n",
      "Iteration 62695: loss = 3.2655212e-11,1.488476e-10\n",
      "Iteration 62700: loss = 3.265361e-11,1.4884727e-10\n",
      "Iteration 62705: loss = 3.2651454e-11,1.4884714e-10\n",
      "Iteration 62710: loss = 3.264945e-11,1.4884699e-10\n",
      "Iteration 62715: loss = 3.2647236e-11,1.4884695e-10\n",
      "Iteration 62720: loss = 3.264492e-11,1.4884684e-10\n",
      "Iteration 62725: loss = 3.264299e-11,1.4884692e-10\n",
      "Iteration 62730: loss = 3.2640727e-11,1.4884674e-10\n",
      "Iteration 62735: loss = 3.2638742e-11,1.4884693e-10\n",
      "Iteration 62740: loss = 3.2636543e-11,1.4884667e-10\n",
      "Iteration 62745: loss = 3.2634527e-11,1.4884656e-10\n",
      "Iteration 62750: loss = 3.2632314e-11,1.4884664e-10\n",
      "Iteration 62755: loss = 3.2630797e-11,1.4884666e-10\n",
      "Iteration 62760: loss = 3.262883e-11,1.488466e-10\n",
      "Iteration 62765: loss = 3.2626565e-11,1.4884677e-10\n",
      "Iteration 62770: loss = 3.26246e-11,1.4884667e-10\n",
      "Iteration 62775: loss = 3.2622387e-11,1.4884666e-10\n",
      "Iteration 62780: loss = 3.262049e-11,1.4884645e-10\n",
      "Iteration 62785: loss = 3.2618283e-11,1.488463e-10\n",
      "Iteration 62790: loss = 3.2616094e-11,1.4884627e-10\n",
      "Iteration 62795: loss = 3.261411e-11,1.48846e-10\n",
      "Iteration 62800: loss = 3.2611955e-11,1.4884599e-10\n",
      "Iteration 62805: loss = 3.261008e-11,1.4884574e-10\n",
      "Iteration 62810: loss = 3.2607913e-11,1.4884571e-10\n",
      "Iteration 62815: loss = 3.260651e-11,1.4884484e-10\n",
      "Iteration 62820: loss = 3.2605762e-11,1.4884362e-10\n",
      "Iteration 62825: loss = 3.2604103e-11,1.4884297e-10\n",
      "Iteration 62830: loss = 3.2602254e-11,1.4884279e-10\n",
      "Iteration 62835: loss = 3.2601515e-11,1.4884205e-10\n",
      "Iteration 62840: loss = 3.2599794e-11,1.4884188e-10\n",
      "Iteration 62845: loss = 3.259834e-11,1.4884102e-10\n",
      "Iteration 62850: loss = 3.2596537e-11,1.4884075e-10\n",
      "Iteration 62855: loss = 3.1956292e-11,1.4883947e-10\n",
      "Iteration 62860: loss = 3.1953867e-11,1.488397e-10\n",
      "Iteration 62865: loss = 3.1951504e-11,1.4883986e-10\n",
      "Iteration 62870: loss = 3.19488e-11,1.4884018e-10\n",
      "Iteration 62875: loss = 3.1946334e-11,1.4884027e-10\n",
      "Iteration 62880: loss = 3.194393e-11,1.4884075e-10\n",
      "Iteration 62885: loss = 3.1941446e-11,1.4884105e-10\n",
      "Iteration 62890: loss = 3.1938747e-11,1.4884158e-10\n",
      "Iteration 62895: loss = 3.1936374e-11,1.4884186e-10\n",
      "Iteration 62900: loss = 3.1934136e-11,1.4884188e-10\n",
      "Iteration 62905: loss = 3.1931725e-11,1.4884197e-10\n",
      "Iteration 62910: loss = 3.1929497e-11,1.4884215e-10\n",
      "Iteration 62915: loss = 3.1927783e-11,1.4884227e-10\n",
      "Iteration 62920: loss = 3.192549e-11,1.488425e-10\n",
      "Iteration 62925: loss = 3.1923197e-11,1.488428e-10\n",
      "Iteration 62930: loss = 3.1920997e-11,1.4884292e-10\n",
      "Iteration 62935: loss = 3.1918464e-11,1.48843e-10\n",
      "Iteration 62940: loss = 3.191622e-11,1.4884326e-10\n",
      "Iteration 62945: loss = 3.1283833e-11,1.488436e-10\n",
      "Iteration 62950: loss = 3.1281037e-11,1.4884427e-10\n",
      "Iteration 62955: loss = 3.1280242e-11,1.4884247e-10\n",
      "Iteration 62960: loss = 3.1281002e-11,1.4884027e-10\n",
      "Iteration 62965: loss = 3.1280784e-11,1.4883822e-10\n",
      "Iteration 62970: loss = 3.1281425e-11,1.4883622e-10\n",
      "Iteration 62975: loss = 3.128087e-11,1.488342e-10\n",
      "Iteration 62980: loss = 3.128074e-11,1.4883209e-10\n",
      "Iteration 62985: loss = 3.1281245e-11,1.4883e-10\n",
      "Iteration 62990: loss = 3.128106e-11,1.4882803e-10\n",
      "Iteration 62995: loss = 3.1281453e-11,1.4882592e-10\n",
      "Iteration 63000: loss = 3.1281252e-11,1.4882387e-10\n",
      "Iteration 63005: loss = 3.1281075e-11,1.4882182e-10\n",
      "Iteration 63010: loss = 3.1281717e-11,1.4881993e-10\n",
      "Iteration 63015: loss = 3.128136e-11,1.4881785e-10\n",
      "Iteration 63020: loss = 3.128202e-11,1.4881572e-10\n",
      "Iteration 63025: loss = 3.1280676e-11,1.4881464e-10\n",
      "Iteration 63030: loss = 3.127773e-11,1.4881543e-10\n",
      "Iteration 63035: loss = 3.1274632e-11,1.4881596e-10\n",
      "Iteration 63040: loss = 3.1271912e-11,1.4881656e-10\n",
      "Iteration 63045: loss = 3.1269348e-11,1.4881704e-10\n",
      "Iteration 63050: loss = 3.1267457e-11,1.4881743e-10\n",
      "Iteration 63055: loss = 3.1264588e-11,1.4881808e-10\n",
      "Iteration 63060: loss = 3.1262035e-11,1.4881868e-10\n",
      "Iteration 63065: loss = 3.125961e-11,1.4881897e-10\n",
      "Iteration 63070: loss = 3.1257025e-11,1.488194e-10\n",
      "Iteration 63075: loss = 3.1254183e-11,1.4881979e-10\n",
      "Iteration 63080: loss = 3.1251807e-11,1.4881998e-10\n",
      "Iteration 63085: loss = 3.1249326e-11,1.4882051e-10\n",
      "Iteration 63090: loss = 3.1246786e-11,1.4882087e-10\n",
      "Iteration 63095: loss = 3.1244392e-11,1.4882114e-10\n",
      "Iteration 63100: loss = 3.124156e-11,1.4882148e-10\n",
      "Iteration 63105: loss = 3.1239147e-11,1.4882201e-10\n",
      "Iteration 63110: loss = 3.1236746e-11,1.4882248e-10\n",
      "Iteration 63115: loss = 3.1234904e-11,1.4882263e-10\n",
      "Iteration 63120: loss = 3.1235393e-11,1.4882025e-10\n",
      "Iteration 63125: loss = 3.1235337e-11,1.4881793e-10\n",
      "Iteration 63130: loss = 3.1235427e-11,1.4881577e-10\n",
      "Iteration 63135: loss = 3.1235865e-11,1.4881385e-10\n",
      "Iteration 63140: loss = 3.1235483e-11,1.4881166e-10\n",
      "Iteration 63145: loss = 3.1236264e-11,1.4880934e-10\n",
      "Iteration 63150: loss = 3.123632e-11,1.4880723e-10\n",
      "Iteration 63155: loss = 3.1236246e-11,1.4880498e-10\n",
      "Iteration 63160: loss = 3.1236822e-11,1.488028e-10\n",
      "Iteration 63165: loss = 3.123685e-11,1.4880058e-10\n",
      "Iteration 63170: loss = 3.1236954e-11,1.487984e-10\n",
      "Iteration 63175: loss = 3.123764e-11,1.4879598e-10\n",
      "Iteration 63180: loss = 3.1236704e-11,1.4879503e-10\n",
      "Iteration 63185: loss = 3.1236746e-11,1.4879291e-10\n",
      "Iteration 63190: loss = 3.123361e-11,1.4879364e-10\n",
      "Iteration 63195: loss = 3.1230556e-11,1.4879482e-10\n",
      "Iteration 63200: loss = 3.1227274e-11,1.4879603e-10\n",
      "Iteration 63205: loss = 3.12241e-11,1.4879703e-10\n",
      "Iteration 63210: loss = 3.1221293e-11,1.4879777e-10\n",
      "Iteration 63215: loss = 3.1218573e-11,1.4879825e-10\n",
      "Iteration 63220: loss = 3.1215582e-11,1.4879876e-10\n",
      "Iteration 63225: loss = 3.121302e-11,1.487995e-10\n",
      "Iteration 63230: loss = 3.1210423e-11,1.4879983e-10\n",
      "Iteration 63235: loss = 3.120881e-11,1.4880022e-10\n",
      "Iteration 63240: loss = 3.1206097e-11,1.4880067e-10\n",
      "Iteration 63245: loss = 3.1203665e-11,1.4880132e-10\n",
      "Iteration 63250: loss = 3.1201337e-11,1.4880133e-10\n",
      "Iteration 63255: loss = 3.119903e-11,1.4880149e-10\n",
      "Iteration 63260: loss = 3.1196365e-11,1.4880179e-10\n",
      "Iteration 63265: loss = 3.1193947e-11,1.4880205e-10\n",
      "Iteration 63270: loss = 3.119156e-11,1.4880228e-10\n",
      "Iteration 63275: loss = 3.1189197e-11,1.4880258e-10\n",
      "Iteration 63280: loss = 3.118671e-11,1.4880283e-10\n",
      "Iteration 63285: loss = 3.1184385e-11,1.4880303e-10\n",
      "Iteration 63290: loss = 3.1181988e-11,1.4880332e-10\n",
      "Iteration 63295: loss = 3.1179757e-11,1.488037e-10\n",
      "Iteration 63300: loss = 3.117737e-11,1.4880389e-10\n",
      "Iteration 63305: loss = 3.1174816e-11,1.4880415e-10\n",
      "Iteration 63310: loss = 3.1172575e-11,1.4880434e-10\n",
      "Iteration 63315: loss = 3.117026e-11,1.4880439e-10\n",
      "Iteration 63320: loss = 3.1168748e-11,1.4880475e-10\n",
      "Iteration 63325: loss = 3.116628e-11,1.4880497e-10\n",
      "Iteration 63330: loss = 3.1164012e-11,1.4880511e-10\n",
      "Iteration 63335: loss = 3.1161723e-11,1.4880551e-10\n",
      "Iteration 63340: loss = 3.1159474e-11,1.4880566e-10\n",
      "Iteration 63345: loss = 3.1156924e-11,1.4880568e-10\n",
      "Iteration 63350: loss = 3.1154697e-11,1.4880584e-10\n",
      "Iteration 63355: loss = 3.1152518e-11,1.4880597e-10\n",
      "Iteration 63360: loss = 3.1150273e-11,1.4880612e-10\n",
      "Iteration 63365: loss = 3.1147765e-11,1.4880623e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 63370: loss = 3.114556e-11,1.488066e-10\n",
      "Iteration 63375: loss = 3.114334e-11,1.488068e-10\n",
      "Iteration 63380: loss = 3.1141218e-11,1.4880684e-10\n",
      "Iteration 63385: loss = 3.1138776e-11,1.4880683e-10\n",
      "Iteration 63390: loss = 3.1136715e-11,1.4880683e-10\n",
      "Iteration 63395: loss = 3.1134678e-11,1.4880679e-10\n",
      "Iteration 63400: loss = 3.1133377e-11,1.4880679e-10\n",
      "Iteration 63405: loss = 3.1131216e-11,1.4880688e-10\n",
      "Iteration 63410: loss = 3.1129113e-11,1.4880674e-10\n",
      "Iteration 63415: loss = 3.112714e-11,1.4880686e-10\n",
      "Iteration 63420: loss = 3.1125175e-11,1.4880673e-10\n",
      "Iteration 63425: loss = 3.1122917e-11,1.4880683e-10\n",
      "Iteration 63430: loss = 3.1120922e-11,1.4880656e-10\n",
      "Iteration 63435: loss = 3.1119027e-11,1.488067e-10\n",
      "Iteration 63440: loss = 3.111714e-11,1.4880644e-10\n",
      "Iteration 63445: loss = 3.111493e-11,1.4880613e-10\n",
      "Iteration 63450: loss = 3.111307e-11,1.4880604e-10\n",
      "Iteration 63455: loss = 3.1111055e-11,1.4880593e-10\n",
      "Iteration 63460: loss = 3.1109167e-11,1.4880601e-10\n",
      "Iteration 63465: loss = 3.110693e-11,1.4880594e-10\n",
      "Iteration 63470: loss = 3.110499e-11,1.4880586e-10\n",
      "Iteration 63475: loss = 3.1103127e-11,1.4880581e-10\n",
      "Iteration 63480: loss = 3.110116e-11,1.4880575e-10\n",
      "Iteration 63485: loss = 3.1099786e-11,1.488056e-10\n",
      "Iteration 63490: loss = 3.1097843e-11,1.4880545e-10\n",
      "Iteration 63495: loss = 3.1095997e-11,1.4880529e-10\n",
      "Iteration 63500: loss = 3.1094034e-11,1.4880508e-10\n",
      "Iteration 63505: loss = 3.1092177e-11,1.48805e-10\n",
      "Iteration 63510: loss = 3.1090037e-11,1.4880494e-10\n",
      "Iteration 63515: loss = 3.108819e-11,1.4880466e-10\n",
      "Iteration 63520: loss = 3.1086352e-11,1.4880462e-10\n",
      "Iteration 63525: loss = 3.108441e-11,1.4880441e-10\n",
      "Iteration 63530: loss = 3.1082244e-11,1.4880425e-10\n",
      "Iteration 63535: loss = 3.1080385e-11,1.4880394e-10\n",
      "Iteration 63540: loss = 3.107846e-11,1.488038e-10\n",
      "Iteration 63545: loss = 3.107664e-11,1.4880373e-10\n",
      "Iteration 63550: loss = 3.10745e-11,1.4880358e-10\n",
      "Iteration 63555: loss = 3.1072648e-11,1.4880344e-10\n",
      "Iteration 63560: loss = 3.1070802e-11,1.4880322e-10\n",
      "Iteration 63565: loss = 3.106977e-11,1.4880314e-10\n",
      "Iteration 63570: loss = 3.106762e-11,1.4880286e-10\n",
      "Iteration 63575: loss = 3.106583e-11,1.4880255e-10\n",
      "Iteration 63580: loss = 3.1064103e-11,1.4880241e-10\n",
      "Iteration 63585: loss = 3.1062337e-11,1.4880208e-10\n",
      "Iteration 63590: loss = 3.1060408e-11,1.4880165e-10\n",
      "Iteration 63595: loss = 3.105857e-11,1.4880137e-10\n",
      "Iteration 63600: loss = 3.1056883e-11,1.4880112e-10\n",
      "Iteration 63605: loss = 3.1055162e-11,1.4880092e-10\n",
      "Iteration 63610: loss = 3.1053157e-11,1.488006e-10\n",
      "Iteration 63615: loss = 3.105136e-11,1.4880029e-10\n",
      "Iteration 63620: loss = 3.1049645e-11,1.4879997e-10\n",
      "Iteration 63625: loss = 3.1048088e-11,1.4879958e-10\n",
      "Iteration 63630: loss = 3.104621e-11,1.4879929e-10\n",
      "Iteration 63635: loss = 3.104465e-11,1.4879871e-10\n",
      "Iteration 63640: loss = 3.104307e-11,1.4879825e-10\n",
      "Iteration 63645: loss = 3.1042228e-11,1.4879788e-10\n",
      "Iteration 63650: loss = 3.1040337e-11,1.4879738e-10\n",
      "Iteration 63655: loss = 3.1038852e-11,1.4879692e-10\n",
      "Iteration 63660: loss = 3.1037225e-11,1.487966e-10\n",
      "Iteration 63665: loss = 3.1035653e-11,1.4879631e-10\n",
      "Iteration 63670: loss = 3.1033797e-11,1.48796e-10\n",
      "Iteration 63675: loss = 3.1032163e-11,1.4879542e-10\n",
      "Iteration 63680: loss = 3.103061e-11,1.4879513e-10\n",
      "Iteration 63685: loss = 3.1029068e-11,1.4879474e-10\n",
      "Iteration 63690: loss = 3.1027476e-11,1.4879423e-10\n",
      "Iteration 63695: loss = 3.102553e-11,1.4879373e-10\n",
      "Iteration 63700: loss = 3.1024013e-11,1.4879335e-10\n",
      "Iteration 63705: loss = 3.102248e-11,1.4879294e-10\n",
      "Iteration 63710: loss = 3.1020936e-11,1.4879249e-10\n",
      "Iteration 63715: loss = 3.1019114e-11,1.48792e-10\n",
      "Iteration 63720: loss = 3.1017564e-11,1.4879145e-10\n",
      "Iteration 63725: loss = 3.1015985e-11,1.4879098e-10\n",
      "Iteration 63730: loss = 3.1015215e-11,1.4879058e-10\n",
      "Iteration 63735: loss = 3.1013473e-11,1.4879009e-10\n",
      "Iteration 63740: loss = 3.101185e-11,1.4878974e-10\n",
      "Iteration 63745: loss = 3.1010354e-11,1.4878922e-10\n",
      "Iteration 63750: loss = 3.100879e-11,1.487889e-10\n",
      "Iteration 63755: loss = 3.1007006e-11,1.4878834e-10\n",
      "Iteration 63760: loss = 3.1005573e-11,1.4878786e-10\n",
      "Iteration 63765: loss = 3.1003932e-11,1.4878734e-10\n",
      "Iteration 63770: loss = 3.10024e-11,1.4878691e-10\n",
      "Iteration 63775: loss = 3.100057e-11,1.4878632e-10\n",
      "Iteration 63780: loss = 3.0999064e-11,1.4878578e-10\n",
      "Iteration 63785: loss = 3.0997503e-11,1.487855e-10\n",
      "Iteration 63790: loss = 3.0996063e-11,1.4878485e-10\n",
      "Iteration 63795: loss = 3.099416e-11,1.487845e-10\n",
      "Iteration 63800: loss = 3.0992632e-11,1.4878405e-10\n",
      "Iteration 63805: loss = 3.0991133e-11,1.487835e-10\n",
      "Iteration 63810: loss = 3.0990308e-11,1.48783e-10\n",
      "Iteration 63815: loss = 3.098855e-11,1.4878258e-10\n",
      "Iteration 63820: loss = 3.0987036e-11,1.4878236e-10\n",
      "Iteration 63825: loss = 3.098552e-11,1.487817e-10\n",
      "Iteration 63830: loss = 3.0983962e-11,1.4878138e-10\n",
      "Iteration 63835: loss = 3.0982234e-11,1.4878088e-10\n",
      "Iteration 63840: loss = 3.0980715e-11,1.4878039e-10\n",
      "Iteration 63845: loss = 3.0979167e-11,1.4877989e-10\n",
      "Iteration 63850: loss = 3.0977505e-11,1.4877957e-10\n",
      "Iteration 63855: loss = 3.0975392e-11,1.4877956e-10\n",
      "Iteration 63860: loss = 3.0973266e-11,1.4877959e-10\n",
      "Iteration 63865: loss = 3.097102e-11,1.4877985e-10\n",
      "Iteration 63870: loss = 3.096873e-11,1.4878028e-10\n",
      "Iteration 63875: loss = 3.0966188e-11,1.4878063e-10\n",
      "Iteration 63880: loss = 3.0963822e-11,1.4878088e-10\n",
      "Iteration 63885: loss = 3.0961525e-11,1.487811e-10\n",
      "Iteration 63890: loss = 3.0960158e-11,1.4878122e-10\n",
      "Iteration 63895: loss = 3.0957913e-11,1.4878138e-10\n",
      "Iteration 63900: loss = 3.095564e-11,1.4878133e-10\n",
      "Iteration 63905: loss = 3.0953504e-11,1.4878161e-10\n",
      "Iteration 63910: loss = 3.095146e-11,1.4878168e-10\n",
      "Iteration 63915: loss = 3.094937e-11,1.4878168e-10\n",
      "Iteration 63920: loss = 3.0947002e-11,1.487818e-10\n",
      "Iteration 63925: loss = 3.094491e-11,1.4878185e-10\n",
      "Iteration 63930: loss = 3.0942856e-11,1.4878186e-10\n",
      "Iteration 63935: loss = 3.094082e-11,1.4878179e-10\n",
      "Iteration 63940: loss = 3.0938623e-11,1.4878176e-10\n",
      "Iteration 63945: loss = 3.0936736e-11,1.4878161e-10\n",
      "Iteration 63950: loss = 3.093496e-11,1.4878157e-10\n",
      "Iteration 63955: loss = 3.0933006e-11,1.4878138e-10\n",
      "Iteration 63960: loss = 3.0930987e-11,1.4878118e-10\n",
      "Iteration 63965: loss = 3.092906e-11,1.4878103e-10\n",
      "Iteration 63970: loss = 3.0927143e-11,1.4878103e-10\n",
      "Iteration 63975: loss = 3.0926105e-11,1.4878089e-10\n",
      "Iteration 63980: loss = 3.0923968e-11,1.4878067e-10\n",
      "Iteration 63985: loss = 3.0922095e-11,1.487806e-10\n",
      "Iteration 63990: loss = 3.0920353e-11,1.4878049e-10\n",
      "Iteration 63995: loss = 3.091848e-11,1.4878021e-10\n",
      "Iteration 64000: loss = 3.0916342e-11,1.4878017e-10\n",
      "Iteration 64005: loss = 3.0914514e-11,1.4877999e-10\n",
      "Iteration 64010: loss = 3.0912762e-11,1.4877974e-10\n",
      "Iteration 64015: loss = 3.0911124e-11,1.487793e-10\n",
      "Iteration 64020: loss = 3.090908e-11,1.4877911e-10\n",
      "Iteration 64025: loss = 3.0907457e-11,1.4877892e-10\n",
      "Iteration 64030: loss = 3.090577e-11,1.4877849e-10\n",
      "Iteration 64035: loss = 3.0904092e-11,1.4877818e-10\n",
      "Iteration 64040: loss = 3.090214e-11,1.4877784e-10\n",
      "Iteration 64045: loss = 3.0900456e-11,1.4877752e-10\n",
      "Iteration 64050: loss = 3.0898745e-11,1.4877724e-10\n",
      "Iteration 64055: loss = 3.089776e-11,1.4877682e-10\n",
      "Iteration 64060: loss = 3.089592e-11,1.487766e-10\n",
      "Iteration 64065: loss = 3.089418e-11,1.487763e-10\n",
      "Iteration 64070: loss = 3.0893288e-11,1.4877535e-10\n",
      "Iteration 64075: loss = 3.0891352e-11,1.4877496e-10\n",
      "Iteration 64080: loss = 3.0889652e-11,1.4877467e-10\n",
      "Iteration 64085: loss = 3.0888066e-11,1.4877435e-10\n",
      "Iteration 64090: loss = 3.0887484e-11,1.4877294e-10\n",
      "Iteration 64095: loss = 3.088579e-11,1.4877266e-10\n",
      "Iteration 64100: loss = 3.088387e-11,1.4877233e-10\n",
      "Iteration 64105: loss = 3.0882182e-11,1.4877194e-10\n",
      "Iteration 64110: loss = 3.0880465e-11,1.487717e-10\n",
      "Iteration 64115: loss = 3.087885e-11,1.4877129e-10\n",
      "Iteration 64120: loss = 3.0877755e-11,1.4877026e-10\n",
      "Iteration 64125: loss = 3.0876118e-11,1.4876989e-10\n",
      "Iteration 64130: loss = 3.0874452e-11,1.4876955e-10\n",
      "Iteration 64135: loss = 3.0872475e-11,1.4876902e-10\n",
      "Iteration 64140: loss = 3.0871715e-11,1.4876884e-10\n",
      "Iteration 64145: loss = 3.0870112e-11,1.4876841e-10\n",
      "Iteration 64150: loss = 3.0868634e-11,1.4876789e-10\n",
      "Iteration 64155: loss = 3.0866806e-11,1.4876739e-10\n",
      "Iteration 64160: loss = 3.0865383e-11,1.4876696e-10\n",
      "Iteration 64165: loss = 3.0863912e-11,1.4876654e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64170: loss = 3.0862445e-11,1.4876597e-10\n",
      "Iteration 64175: loss = 3.0860765e-11,1.487654e-10\n",
      "Iteration 64180: loss = 3.0859426e-11,1.4876494e-10\n",
      "Iteration 64185: loss = 3.085789e-11,1.4876436e-10\n",
      "Iteration 64190: loss = 3.0856418e-11,1.4876379e-10\n",
      "Iteration 64195: loss = 3.085487e-11,1.4876328e-10\n",
      "Iteration 64200: loss = 3.085343e-11,1.4876278e-10\n",
      "Iteration 64205: loss = 3.0852043e-11,1.4876202e-10\n",
      "Iteration 64210: loss = 3.0850652e-11,1.4876149e-10\n",
      "Iteration 64215: loss = 3.0848945e-11,1.4876113e-10\n",
      "Iteration 64220: loss = 3.0848577e-11,1.4876028e-10\n",
      "Iteration 64225: loss = 3.084725e-11,1.4875969e-10\n",
      "Iteration 64230: loss = 3.084577e-11,1.4875912e-10\n",
      "Iteration 64235: loss = 3.0844088e-11,1.4875862e-10\n",
      "Iteration 64240: loss = 3.0842672e-11,1.4875799e-10\n",
      "Iteration 64245: loss = 3.0841264e-11,1.4875737e-10\n",
      "Iteration 64250: loss = 3.0839855e-11,1.4875678e-10\n",
      "Iteration 64255: loss = 3.0838204e-11,1.4875627e-10\n",
      "Iteration 64260: loss = 3.0834543e-11,1.48758e-10\n",
      "Iteration 64265: loss = 3.0829325e-11,1.4876117e-10\n",
      "Iteration 64270: loss = 3.082427e-11,1.4876417e-10\n",
      "Iteration 64275: loss = 3.0818858e-11,1.4876751e-10\n",
      "Iteration 64280: loss = 3.081376e-11,1.487705e-10\n",
      "Iteration 64285: loss = 3.080862e-11,1.4877388e-10\n",
      "Iteration 64290: loss = 3.08037e-11,1.48777e-10\n",
      "Iteration 64295: loss = 3.0798496e-11,1.487798e-10\n",
      "Iteration 64300: loss = 3.079433e-11,1.4878287e-10\n",
      "Iteration 64305: loss = 3.078945e-11,1.4878568e-10\n",
      "Iteration 64310: loss = 3.07846e-11,1.4878866e-10\n",
      "Iteration 64315: loss = 3.07795e-11,1.4879126e-10\n",
      "Iteration 64320: loss = 3.0774636e-11,1.4879442e-10\n",
      "Iteration 64325: loss = 3.0769873e-11,1.4879699e-10\n",
      "Iteration 64330: loss = 3.076508e-11,1.487998e-10\n",
      "Iteration 64335: loss = 3.0760068e-11,1.488027e-10\n",
      "Iteration 64340: loss = 3.0755308e-11,1.4880552e-10\n",
      "Iteration 64345: loss = 3.075059e-11,1.4880824e-10\n",
      "Iteration 64350: loss = 3.074575e-11,1.4881099e-10\n",
      "Iteration 64355: loss = 3.0740837e-11,1.4881386e-10\n",
      "Iteration 64360: loss = 3.0736164e-11,1.4881676e-10\n",
      "Iteration 64365: loss = 3.073145e-11,1.4881943e-10\n",
      "Iteration 64370: loss = 3.0726886e-11,1.4882219e-10\n",
      "Iteration 64375: loss = 3.0721953e-11,1.488249e-10\n",
      "Iteration 64380: loss = 3.071764e-11,1.4882724e-10\n",
      "Iteration 64385: loss = 3.0715923e-11,1.488276e-10\n",
      "Iteration 64390: loss = 3.0713484e-11,1.4882781e-10\n",
      "Iteration 64395: loss = 3.071167e-11,1.4882812e-10\n",
      "Iteration 64400: loss = 3.07089e-11,1.4882849e-10\n",
      "Iteration 64405: loss = 3.0706424e-11,1.4882866e-10\n",
      "Iteration 64410: loss = 3.070458e-11,1.4882923e-10\n",
      "Iteration 64415: loss = 3.0702038e-11,1.4882967e-10\n",
      "Iteration 64420: loss = 3.069925e-11,1.4883006e-10\n",
      "Iteration 64425: loss = 3.0697497e-11,1.4883045e-10\n",
      "Iteration 64430: loss = 3.069493e-11,1.4883092e-10\n",
      "Iteration 64435: loss = 3.0693226e-11,1.4883117e-10\n",
      "Iteration 64440: loss = 3.0069922e-11,1.4883175e-10\n",
      "Iteration 64445: loss = 3.006695e-11,1.4883236e-10\n",
      "Iteration 64450: loss = 3.0063712e-11,1.4883424e-10\n",
      "Iteration 64455: loss = 3.005899e-11,1.4883668e-10\n",
      "Iteration 64460: loss = 3.0054886e-11,1.4883919e-10\n",
      "Iteration 64465: loss = 3.0050292e-11,1.488416e-10\n",
      "Iteration 64470: loss = 3.004567e-11,1.4884419e-10\n",
      "Iteration 64475: loss = 3.0041813e-11,1.4884657e-10\n",
      "Iteration 64480: loss = 2.9425767e-11,1.4884914e-10\n",
      "Iteration 64485: loss = 2.9422475e-11,1.4885147e-10\n",
      "Iteration 64490: loss = 2.9418426e-11,1.4885342e-10\n",
      "Iteration 64495: loss = 2.941411e-11,1.4885573e-10\n",
      "Iteration 64500: loss = 2.9410505e-11,1.4885783e-10\n",
      "Iteration 64505: loss = 2.9406085e-11,1.4886004e-10\n",
      "Iteration 64510: loss = 2.9402668e-11,1.4886196e-10\n",
      "Iteration 64515: loss = 2.939853e-11,1.4886395e-10\n",
      "Iteration 64520: loss = 2.939431e-11,1.4886617e-10\n",
      "Iteration 64525: loss = 2.9390903e-11,1.4886808e-10\n",
      "Iteration 64530: loss = 2.938707e-11,1.488699e-10\n",
      "Iteration 64535: loss = 2.938311e-11,1.4887189e-10\n",
      "Iteration 64540: loss = 2.937968e-11,1.4887373e-10\n",
      "Iteration 64545: loss = 2.937576e-11,1.4887551e-10\n",
      "Iteration 64550: loss = 2.9372765e-11,1.4887713e-10\n",
      "Iteration 64555: loss = 2.9369108e-11,1.4887834e-10\n",
      "Iteration 64560: loss = 2.936541e-11,1.4887999e-10\n",
      "Iteration 64565: loss = 2.936258e-11,1.4888135e-10\n",
      "Iteration 64570: loss = 2.935886e-11,1.4888304e-10\n",
      "Iteration 64575: loss = 2.9355976e-11,1.4888454e-10\n",
      "Iteration 64580: loss = 2.935245e-11,1.4888595e-10\n",
      "Iteration 64585: loss = 2.9348825e-11,1.4888733e-10\n",
      "Iteration 64590: loss = 2.9346303e-11,1.488887e-10\n",
      "Iteration 64595: loss = 2.934275e-11,1.4889028e-10\n",
      "Iteration 64600: loss = 2.9340027e-11,1.4889147e-10\n",
      "Iteration 64605: loss = 2.933626e-11,1.4889312e-10\n",
      "Iteration 64610: loss = 2.93324e-11,1.488949e-10\n",
      "Iteration 64615: loss = 2.9329195e-11,1.4889662e-10\n",
      "Iteration 64620: loss = 2.9325254e-11,1.4889828e-10\n",
      "Iteration 64625: loss = 2.932127e-11,1.4890006e-10\n",
      "Iteration 64630: loss = 2.931812e-11,1.4890189e-10\n",
      "Iteration 64635: loss = 2.9314263e-11,1.4890375e-10\n",
      "Iteration 64640: loss = 2.931105e-11,1.489055e-10\n",
      "Iteration 64645: loss = 2.930721e-11,1.4890707e-10\n",
      "Iteration 64650: loss = 2.930351e-11,1.4890889e-10\n",
      "Iteration 64655: loss = 2.9300375e-11,1.489105e-10\n",
      "Iteration 64660: loss = 2.9296735e-11,1.4891206e-10\n",
      "Iteration 64665: loss = 2.9294026e-11,1.4891341e-10\n",
      "Iteration 64670: loss = 2.929051e-11,1.4891476e-10\n",
      "Iteration 64675: loss = 2.928707e-11,1.489162e-10\n",
      "Iteration 64680: loss = 2.9284398e-11,1.4891768e-10\n",
      "Iteration 64685: loss = 2.9280977e-11,1.4891886e-10\n",
      "Iteration 64690: loss = 2.92782e-11,1.4892004e-10\n",
      "Iteration 64695: loss = 2.927479e-11,1.489212e-10\n",
      "Iteration 64700: loss = 2.9271374e-11,1.4892254e-10\n",
      "Iteration 64705: loss = 2.9268872e-11,1.4892382e-10\n",
      "Iteration 64710: loss = 2.92659e-11,1.4892501e-10\n",
      "Iteration 64715: loss = 2.9263248e-11,1.4892618e-10\n",
      "Iteration 64720: loss = 2.9259886e-11,1.4892741e-10\n",
      "Iteration 64725: loss = 2.9256698e-11,1.4892838e-10\n",
      "Iteration 64730: loss = 2.925413e-11,1.4892955e-10\n",
      "Iteration 64735: loss = 2.92509e-11,1.4893085e-10\n",
      "Iteration 64740: loss = 2.9247778e-11,1.489318e-10\n",
      "Iteration 64745: loss = 2.9245394e-11,1.4893276e-10\n",
      "Iteration 64750: loss = 2.9242348e-11,1.4893382e-10\n",
      "Iteration 64755: loss = 2.923989e-11,1.4893478e-10\n",
      "Iteration 64760: loss = 2.923685e-11,1.4893584e-10\n",
      "Iteration 64765: loss = 2.9233903e-11,1.4893664e-10\n",
      "Iteration 64770: loss = 2.9231652e-11,1.489378e-10\n",
      "Iteration 64775: loss = 2.9228557e-11,1.4893854e-10\n",
      "Iteration 64780: loss = 2.922625e-11,1.4893972e-10\n",
      "Iteration 64785: loss = 2.922317e-11,1.489405e-10\n",
      "Iteration 64790: loss = 2.9220137e-11,1.489414e-10\n",
      "Iteration 64795: loss = 2.9217892e-11,1.4894216e-10\n",
      "Iteration 64800: loss = 2.9215044e-11,1.4894283e-10\n",
      "Iteration 64805: loss = 2.9212816e-11,1.4894362e-10\n",
      "Iteration 64810: loss = 2.9209926e-11,1.4894447e-10\n",
      "Iteration 64815: loss = 2.920707e-11,1.4894558e-10\n",
      "Iteration 64820: loss = 2.9204885e-11,1.4894602e-10\n",
      "Iteration 64825: loss = 2.9202304e-11,1.4894681e-10\n",
      "Iteration 64830: loss = 2.9199462e-11,1.489475e-10\n",
      "Iteration 64835: loss = 2.9197342e-11,1.489482e-10\n",
      "Iteration 64840: loss = 2.9194643e-11,1.4894865e-10\n",
      "Iteration 64845: loss = 2.919275e-11,1.4894906e-10\n",
      "Iteration 64850: loss = 2.919012e-11,1.489498e-10\n",
      "Iteration 64855: loss = 2.9187455e-11,1.489503e-10\n",
      "Iteration 64860: loss = 2.918565e-11,1.4895059e-10\n",
      "Iteration 64865: loss = 2.918294e-11,1.4895118e-10\n",
      "Iteration 64870: loss = 2.9181e-11,1.4895174e-10\n",
      "Iteration 64875: loss = 2.9178427e-11,1.4895214e-10\n",
      "Iteration 64880: loss = 2.9175964e-11,1.489527e-10\n",
      "Iteration 64885: loss = 2.917444e-11,1.48953e-10\n",
      "Iteration 64890: loss = 2.9171995e-11,1.4895331e-10\n",
      "Iteration 64895: loss = 2.9170343e-11,1.4895371e-10\n",
      "Iteration 64900: loss = 2.916781e-11,1.489539e-10\n",
      "Iteration 64905: loss = 2.916541e-11,1.489544e-10\n",
      "Iteration 64910: loss = 2.916369e-11,1.4895458e-10\n",
      "Iteration 64915: loss = 2.9161316e-11,1.4895485e-10\n",
      "Iteration 64920: loss = 2.9159532e-11,1.4895508e-10\n",
      "Iteration 64925: loss = 2.915714e-11,1.4895557e-10\n",
      "Iteration 64930: loss = 2.9154696e-11,1.4895558e-10\n",
      "Iteration 64935: loss = 2.9152968e-11,1.4895588e-10\n",
      "Iteration 64940: loss = 2.915087e-11,1.4895643e-10\n",
      "Iteration 64945: loss = 2.9148413e-11,1.4895674e-10\n",
      "Iteration 64950: loss = 2.9146723e-11,1.4895711e-10\n",
      "Iteration 64955: loss = 2.9144243e-11,1.4895736e-10\n",
      "Iteration 64960: loss = 2.9142546e-11,1.4895746e-10\n",
      "Iteration 64965: loss = 2.9140093e-11,1.489578e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64970: loss = 2.9137737e-11,1.4895825e-10\n",
      "Iteration 64975: loss = 2.9136003e-11,1.4895857e-10\n",
      "Iteration 64980: loss = 2.913356e-11,1.4895879e-10\n",
      "Iteration 64985: loss = 2.9131877e-11,1.4895896e-10\n",
      "Iteration 64990: loss = 2.912941e-11,1.489594e-10\n",
      "Iteration 64995: loss = 2.9127083e-11,1.4895962e-10\n",
      "Iteration 65000: loss = 2.9125622e-11,1.4895998e-10\n",
      "Iteration 65005: loss = 2.9123228e-11,1.489602e-10\n",
      "Iteration 65010: loss = 2.9121525e-11,1.4896072e-10\n",
      "Iteration 65015: loss = 2.9119127e-11,1.4896118e-10\n",
      "Iteration 65020: loss = 2.911671e-11,1.4896158e-10\n",
      "Iteration 65025: loss = 2.9115e-11,1.4896176e-10\n",
      "Iteration 65030: loss = 2.9112657e-11,1.4896206e-10\n",
      "Iteration 65035: loss = 2.9110343e-11,1.4896223e-10\n",
      "Iteration 65040: loss = 2.9108587e-11,1.4896256e-10\n",
      "Iteration 65045: loss = 2.9106145e-11,1.4896288e-10\n",
      "Iteration 65050: loss = 2.9104497e-11,1.4896315e-10\n",
      "Iteration 65055: loss = 2.9102068e-11,1.4896342e-10\n",
      "Iteration 65060: loss = 2.9100033e-11,1.4896365e-10\n",
      "Iteration 65065: loss = 2.9098222e-11,1.4896384e-10\n",
      "Iteration 65070: loss = 2.9095948e-11,1.4896413e-10\n",
      "Iteration 65075: loss = 2.9094284e-11,1.4896445e-10\n",
      "Iteration 65080: loss = 2.9091828e-11,1.4896495e-10\n",
      "Iteration 65085: loss = 2.9089425e-11,1.4896526e-10\n",
      "Iteration 65090: loss = 2.9087786e-11,1.4896545e-10\n",
      "Iteration 65095: loss = 2.9085385e-11,1.4896584e-10\n",
      "Iteration 65100: loss = 2.908377e-11,1.4896603e-10\n",
      "Iteration 65105: loss = 2.9081425e-11,1.4896626e-10\n",
      "Iteration 65110: loss = 2.9079109e-11,1.4896656e-10\n",
      "Iteration 65115: loss = 2.9077839e-11,1.4896664e-10\n",
      "Iteration 65120: loss = 2.9075608e-11,1.4896673e-10\n",
      "Iteration 65125: loss = 2.9074153e-11,1.4896671e-10\n",
      "Iteration 65130: loss = 2.9071903e-11,1.489669e-10\n",
      "Iteration 65135: loss = 2.9069728e-11,1.4896712e-10\n",
      "Iteration 65140: loss = 2.906818e-11,1.4896724e-10\n",
      "Iteration 65145: loss = 2.9066055e-11,1.4896728e-10\n",
      "Iteration 65150: loss = 2.9063854e-11,1.4896721e-10\n",
      "Iteration 65155: loss = 2.906226e-11,1.489673e-10\n",
      "Iteration 65160: loss = 2.906019e-11,1.4896737e-10\n",
      "Iteration 65165: loss = 2.9058755e-11,1.489672e-10\n",
      "Iteration 65170: loss = 2.905677e-11,1.4896717e-10\n",
      "Iteration 65175: loss = 2.9054899e-11,1.489672e-10\n",
      "Iteration 65180: loss = 2.9053532e-11,1.4896706e-10\n",
      "Iteration 65185: loss = 2.9051527e-11,1.4896695e-10\n",
      "Iteration 65190: loss = 2.9050177e-11,1.4896695e-10\n",
      "Iteration 65195: loss = 2.9048054e-11,1.4896687e-10\n",
      "Iteration 65200: loss = 2.9045932e-11,1.4896695e-10\n",
      "Iteration 65205: loss = 2.90446e-11,1.4896678e-10\n",
      "Iteration 65210: loss = 2.9042553e-11,1.4896681e-10\n",
      "Iteration 65215: loss = 2.9041205e-11,1.4896685e-10\n",
      "Iteration 65220: loss = 2.9039236e-11,1.489667e-10\n",
      "Iteration 65225: loss = 2.9037169e-11,1.4896666e-10\n",
      "Iteration 65230: loss = 2.9035793e-11,1.4896664e-10\n",
      "Iteration 65235: loss = 2.9034034e-11,1.4896669e-10\n",
      "Iteration 65240: loss = 2.9031916e-11,1.4896656e-10\n",
      "Iteration 65245: loss = 2.9030672e-11,1.4896658e-10\n",
      "Iteration 65250: loss = 2.902857e-11,1.489664e-10\n",
      "Iteration 65255: loss = 2.9027189e-11,1.4896631e-10\n",
      "Iteration 65260: loss = 2.9025187e-11,1.4896631e-10\n",
      "Iteration 65265: loss = 2.902308e-11,1.4896641e-10\n",
      "Iteration 65270: loss = 2.9021875e-11,1.489662e-10\n",
      "Iteration 65275: loss = 2.9019781e-11,1.489662e-10\n",
      "Iteration 65280: loss = 2.9018543e-11,1.48966e-10\n",
      "Iteration 65285: loss = 2.9016506e-11,1.4896594e-10\n",
      "Iteration 65290: loss = 2.9014657e-11,1.4896596e-10\n",
      "Iteration 65295: loss = 2.9013467e-11,1.4896578e-10\n",
      "Iteration 65300: loss = 2.9011437e-11,1.489657e-10\n",
      "Iteration 65305: loss = 2.9010142e-11,1.4896542e-10\n",
      "Iteration 65310: loss = 2.90082e-11,1.4896515e-10\n",
      "Iteration 65315: loss = 2.9006233e-11,1.4896522e-10\n",
      "Iteration 65320: loss = 2.9004946e-11,1.4896506e-10\n",
      "Iteration 65325: loss = 2.9002932e-11,1.4896517e-10\n",
      "Iteration 65330: loss = 2.900172e-11,1.48965e-10\n",
      "Iteration 65335: loss = 2.8999672e-11,1.4896483e-10\n",
      "Iteration 65340: loss = 2.8997688e-11,1.4896467e-10\n",
      "Iteration 65345: loss = 2.8996456e-11,1.4896465e-10\n",
      "Iteration 65350: loss = 2.8994675e-11,1.4896435e-10\n",
      "Iteration 65355: loss = 2.8992702e-11,1.4896426e-10\n",
      "Iteration 65360: loss = 2.899136e-11,1.4896441e-10\n",
      "Iteration 65365: loss = 2.8989332e-11,1.4896423e-10\n",
      "Iteration 65370: loss = 2.898809e-11,1.4896434e-10\n",
      "Iteration 65375: loss = 2.898603e-11,1.4896429e-10\n",
      "Iteration 65380: loss = 2.8984074e-11,1.4896412e-10\n",
      "Iteration 65385: loss = 2.89828e-11,1.489639e-10\n",
      "Iteration 65390: loss = 2.8980837e-11,1.4896394e-10\n",
      "Iteration 65395: loss = 2.8979562e-11,1.4896384e-10\n",
      "Iteration 65400: loss = 2.89775e-11,1.4896345e-10\n",
      "Iteration 65405: loss = 2.897555e-11,1.4896372e-10\n",
      "Iteration 65410: loss = 2.8974559e-11,1.4896348e-10\n",
      "Iteration 65415: loss = 2.8972443e-11,1.4896345e-10\n",
      "Iteration 65420: loss = 2.8970958e-11,1.489636e-10\n",
      "Iteration 65425: loss = 2.8968614e-11,1.48964e-10\n",
      "Iteration 65430: loss = 2.896604e-11,1.4896426e-10\n",
      "Iteration 65435: loss = 2.8964125e-11,1.4896478e-10\n",
      "Iteration 65440: loss = 2.8961626e-11,1.4896546e-10\n",
      "Iteration 65445: loss = 2.8959815e-11,1.48966e-10\n",
      "Iteration 65450: loss = 2.8957262e-11,1.4896637e-10\n",
      "Iteration 65455: loss = 2.8954714e-11,1.4896674e-10\n",
      "Iteration 65460: loss = 2.8952925e-11,1.4896737e-10\n",
      "Iteration 65465: loss = 2.895033e-11,1.4896784e-10\n",
      "Iteration 65470: loss = 2.8948066e-11,1.4896825e-10\n",
      "Iteration 65475: loss = 2.8946151e-11,1.4896877e-10\n",
      "Iteration 65480: loss = 2.8943662e-11,1.4896957e-10\n",
      "Iteration 65485: loss = 2.8941786e-11,1.4896984e-10\n",
      "Iteration 65490: loss = 2.8939384e-11,1.4897003e-10\n",
      "Iteration 65495: loss = 2.8936903e-11,1.4897039e-10\n",
      "Iteration 65500: loss = 2.8935175e-11,1.4897078e-10\n",
      "Iteration 65505: loss = 2.893276e-11,1.4897134e-10\n",
      "Iteration 65510: loss = 2.8930724e-11,1.4897202e-10\n",
      "Iteration 65515: loss = 2.8927832e-11,1.489728e-10\n",
      "Iteration 65520: loss = 2.8924979e-11,1.4897351e-10\n",
      "Iteration 65525: loss = 2.8923018e-11,1.4897474e-10\n",
      "Iteration 65530: loss = 2.8920075e-11,1.4897557e-10\n",
      "Iteration 65535: loss = 2.8917849e-11,1.4897648e-10\n",
      "Iteration 65540: loss = 2.8914955e-11,1.4897727e-10\n",
      "Iteration 65545: loss = 2.8912026e-11,1.4897825e-10\n",
      "Iteration 65550: loss = 2.8909755e-11,1.4897913e-10\n",
      "Iteration 65555: loss = 2.8907e-11,1.4898023e-10\n",
      "Iteration 65560: loss = 2.8904044e-11,1.4898101e-10\n",
      "Iteration 65565: loss = 2.890189e-11,1.4898197e-10\n",
      "Iteration 65570: loss = 2.8899078e-11,1.4898278e-10\n",
      "Iteration 65575: loss = 2.8897032e-11,1.4898338e-10\n",
      "Iteration 65580: loss = 2.8894326e-11,1.4898399e-10\n",
      "Iteration 65585: loss = 2.889189e-11,1.489847e-10\n",
      "Iteration 65590: loss = 2.889058e-11,1.4898466e-10\n",
      "Iteration 65595: loss = 2.8889016e-11,1.4898421e-10\n",
      "Iteration 65600: loss = 2.8887998e-11,1.489838e-10\n",
      "Iteration 65605: loss = 2.8886282e-11,1.4898344e-10\n",
      "Iteration 65610: loss = 2.8283114e-11,1.4898299e-10\n",
      "Iteration 65615: loss = 2.8281905e-11,1.489828e-10\n",
      "Iteration 65620: loss = 2.8280822e-11,1.4898208e-10\n",
      "Iteration 65625: loss = 2.8282238e-11,1.4897865e-10\n",
      "Iteration 65630: loss = 2.8283036e-11,1.4897553e-10\n",
      "Iteration 65635: loss = 2.8886274e-11,1.4897236e-10\n",
      "Iteration 65640: loss = 2.828405e-11,1.4897121e-10\n",
      "Iteration 65645: loss = 2.828298e-11,1.4897097e-10\n",
      "Iteration 65650: loss = 2.8283806e-11,1.4896837e-10\n",
      "Iteration 65655: loss = 2.8283842e-11,1.4896565e-10\n",
      "Iteration 65660: loss = 2.8284139e-11,1.4896422e-10\n",
      "Iteration 65665: loss = 2.8885921e-11,1.4896212e-10\n",
      "Iteration 65670: loss = 2.888619e-11,1.4896004e-10\n",
      "Iteration 65675: loss = 2.8284021e-11,1.4895868e-10\n",
      "Iteration 65680: loss = 2.888597e-11,1.4895651e-10\n",
      "Iteration 65685: loss = 2.8283043e-11,1.4895593e-10\n",
      "Iteration 65690: loss = 2.8283834e-11,1.4895313e-10\n",
      "Iteration 65695: loss = 2.8885755e-11,1.4895113e-10\n",
      "Iteration 65700: loss = 2.8282156e-11,1.4895099e-10\n",
      "Iteration 65705: loss = 2.8283218e-11,1.4894835e-10\n",
      "Iteration 65710: loss = 2.8283974e-11,1.4894573e-10\n",
      "Iteration 65715: loss = 2.828278e-11,1.4894481e-10\n",
      "Iteration 65720: loss = 2.8283745e-11,1.4894218e-10\n",
      "Iteration 65725: loss = 2.8283946e-11,1.489401e-10\n",
      "Iteration 65730: loss = 2.8282938e-11,1.4893912e-10\n",
      "Iteration 65735: loss = 2.8281757e-11,1.4893833e-10\n",
      "Iteration 65740: loss = 2.8283507e-11,1.4893503e-10\n",
      "Iteration 65745: loss = 2.8282885e-11,1.4893363e-10\n",
      "Iteration 65750: loss = 2.8282508e-11,1.4893273e-10\n",
      "Iteration 65755: loss = 2.8283584e-11,1.489291e-10\n",
      "Iteration 65760: loss = 2.828382e-11,1.4892713e-10\n",
      "Iteration 65765: loss = 2.828287e-11,1.4892623e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65770: loss = 2.8281724e-11,1.4892584e-10\n",
      "Iteration 65775: loss = 2.8281983e-11,1.489237e-10\n",
      "Iteration 65780: loss = 2.8885403e-11,1.4892018e-10\n",
      "Iteration 65785: loss = 2.888569e-11,1.4891809e-10\n",
      "Iteration 65790: loss = 2.8283079e-11,1.4891727e-10\n",
      "Iteration 65795: loss = 2.8283357e-11,1.489149e-10\n",
      "Iteration 65800: loss = 2.8885403e-11,1.4891267e-10\n",
      "Iteration 65805: loss = 2.8281995e-11,1.4891255e-10\n",
      "Iteration 65810: loss = 2.8282108e-11,1.4891025e-10\n",
      "Iteration 65815: loss = 2.8885602e-11,1.4890672e-10\n",
      "Iteration 65820: loss = 2.8282848e-11,1.4890582e-10\n",
      "Iteration 65825: loss = 2.8283246e-11,1.4890375e-10\n",
      "Iteration 65830: loss = 2.8282864e-11,1.4890233e-10\n",
      "Iteration 65835: loss = 2.8282378e-11,1.489008e-10\n",
      "Iteration 65840: loss = 2.828148e-11,1.4890049e-10\n",
      "Iteration 65845: loss = 2.8281653e-11,1.4889825e-10\n",
      "Iteration 65850: loss = 2.828286e-11,1.488948e-10\n",
      "Iteration 65855: loss = 2.82831e-11,1.488929e-10\n",
      "Iteration 65860: loss = 2.8281906e-11,1.4889245e-10\n",
      "Iteration 65865: loss = 2.8283096e-11,1.4888969e-10\n",
      "Iteration 65870: loss = 2.8884964e-11,1.4888747e-10\n",
      "Iteration 65875: loss = 2.8282423e-11,1.488867e-10\n",
      "Iteration 65880: loss = 2.88849e-11,1.4888375e-10\n",
      "Iteration 65885: loss = 2.8281752e-11,1.4888359e-10\n",
      "Iteration 65890: loss = 2.8884894e-11,1.4888005e-10\n",
      "Iteration 65895: loss = 2.8281476e-11,1.488798e-10\n",
      "Iteration 65900: loss = 2.8282023e-11,1.4887754e-10\n",
      "Iteration 65905: loss = 2.82828e-11,1.4887484e-10\n",
      "Iteration 65910: loss = 2.8282003e-11,1.4887386e-10\n",
      "Iteration 65915: loss = 2.828217e-11,1.4887236e-10\n",
      "Iteration 65920: loss = 2.8282385e-11,1.4886957e-10\n",
      "Iteration 65925: loss = 2.8282895e-11,1.488681e-10\n",
      "Iteration 65930: loss = 2.8281079e-11,1.488677e-10\n",
      "Iteration 65935: loss = 2.8282847e-11,1.4886448e-10\n",
      "Iteration 65940: loss = 2.8282335e-11,1.4886285e-10\n",
      "Iteration 65945: loss = 2.828216e-11,1.4886131e-10\n",
      "Iteration 65950: loss = 2.8282425e-11,1.4885909e-10\n",
      "Iteration 65955: loss = 2.8281278e-11,1.4885808e-10\n",
      "Iteration 65960: loss = 2.8884709e-11,1.488546e-10\n",
      "Iteration 65965: loss = 2.8282716e-11,1.488533e-10\n",
      "Iteration 65970: loss = 2.828108e-11,1.4885307e-10\n",
      "Iteration 65975: loss = 2.8282631e-11,1.4884964e-10\n",
      "Iteration 65980: loss = 2.8281502e-11,1.4884864e-10\n",
      "Iteration 65985: loss = 2.828064e-11,1.4884842e-10\n",
      "Iteration 65990: loss = 2.8281528e-11,1.4884512e-10\n",
      "Iteration 65995: loss = 2.8884279e-11,1.4884215e-10\n",
      "Iteration 66000: loss = 2.8281516e-11,1.48842e-10\n",
      "Iteration 66005: loss = 2.8884261e-11,1.4883891e-10\n",
      "Iteration 66010: loss = 2.8884603e-11,1.4883675e-10\n",
      "Iteration 66015: loss = 2.8282507e-11,1.4883511e-10\n",
      "Iteration 66020: loss = 2.8884542e-11,1.48833e-10\n",
      "Iteration 66025: loss = 2.8281065e-11,1.4883281e-10\n",
      "Iteration 66030: loss = 2.8282253e-11,1.4882995e-10\n",
      "Iteration 66035: loss = 2.8884048e-11,1.488277e-10\n",
      "Iteration 66040: loss = 2.8280697e-11,1.4882752e-10\n",
      "Iteration 66045: loss = 2.8884133e-11,1.4882393e-10\n",
      "Iteration 66050: loss = 2.8280671e-11,1.4882369e-10\n",
      "Iteration 66055: loss = 2.8884168e-11,1.4882023e-10\n",
      "Iteration 66060: loss = 2.8281358e-11,1.4881946e-10\n",
      "Iteration 66065: loss = 2.8280982e-11,1.488178e-10\n",
      "Iteration 66070: loss = 2.8282121e-11,1.4881527e-10\n",
      "Iteration 66075: loss = 2.8281641e-11,1.4881421e-10\n",
      "Iteration 66080: loss = 2.8282099e-11,1.4881214e-10\n",
      "Iteration 66085: loss = 2.8883928e-11,1.4880976e-10\n",
      "Iteration 66090: loss = 2.8280758e-11,1.4880948e-10\n",
      "Iteration 66095: loss = 2.8281015e-11,1.4880733e-10\n",
      "Iteration 66100: loss = 2.8884154e-11,1.4880394e-10\n",
      "Iteration 66105: loss = 2.8280956e-11,1.4880375e-10\n",
      "Iteration 66110: loss = 2.828189e-11,1.4880094e-10\n",
      "Iteration 66115: loss = 2.8281688e-11,1.4879936e-10\n",
      "Iteration 66120: loss = 2.828191e-11,1.4879722e-10\n",
      "Iteration 66125: loss = 2.8281386e-11,1.487956e-10\n",
      "Iteration 66130: loss = 2.8883595e-11,1.4879338e-10\n",
      "Iteration 66135: loss = 2.8280095e-11,1.4879309e-10\n",
      "Iteration 66140: loss = 2.8883533e-11,1.487897e-10\n",
      "Iteration 66145: loss = 2.8280841e-11,1.487889e-10\n",
      "Iteration 66150: loss = 2.8280574e-11,1.4878795e-10\n",
      "Iteration 66155: loss = 2.8281525e-11,1.4878429e-10\n",
      "Iteration 66160: loss = 2.8281747e-11,1.48783e-10\n",
      "Iteration 66165: loss = 2.828013e-11,1.4878261e-10\n",
      "Iteration 66170: loss = 2.8281743e-11,1.4877927e-10\n",
      "Iteration 66175: loss = 2.888388e-11,1.4877707e-10\n",
      "Iteration 66180: loss = 2.8280414e-11,1.4877684e-10\n",
      "Iteration 66185: loss = 2.8280614e-11,1.4877476e-10\n",
      "Iteration 66190: loss = 2.888339e-11,1.4877174e-10\n",
      "Iteration 66195: loss = 2.8279969e-11,1.4877145e-10\n",
      "Iteration 66200: loss = 2.828115e-11,1.4876865e-10\n",
      "Iteration 66205: loss = 2.8281344e-11,1.4876654e-10\n",
      "Iteration 66210: loss = 2.88835e-11,1.4876433e-10\n",
      "Iteration 66215: loss = 2.8280725e-11,1.4876361e-10\n",
      "Iteration 66220: loss = 2.8280966e-11,1.4876156e-10\n",
      "Iteration 66225: loss = 2.8280734e-11,1.4875964e-10\n",
      "Iteration 66230: loss = 2.8280314e-11,1.487589e-10\n",
      "Iteration 66235: loss = 2.888309e-11,1.4875516e-10\n",
      "Iteration 66240: loss = 2.8281016e-11,1.4875431e-10\n",
      "Iteration 66245: loss = 2.8281261e-11,1.4875214e-10\n",
      "Iteration 66250: loss = 2.8883318e-11,1.4874997e-10\n",
      "Iteration 66255: loss = 2.8280612e-11,1.487492e-10\n",
      "Iteration 66260: loss = 2.8883415e-11,1.4874614e-10\n",
      "Iteration 66265: loss = 2.8279946e-11,1.4874611e-10\n",
      "Iteration 66270: loss = 2.8281056e-11,1.4874298e-10\n",
      "Iteration 66275: loss = 2.8882919e-11,1.4874085e-10\n",
      "Iteration 66280: loss = 2.8280175e-11,1.4874009e-10\n",
      "Iteration 66285: loss = 2.888296e-11,1.4873694e-10\n",
      "Iteration 66290: loss = 2.8280184e-11,1.48736e-10\n",
      "Iteration 66295: loss = 2.828073e-11,1.4873378e-10\n",
      "Iteration 66300: loss = 2.8280904e-11,1.4873167e-10\n",
      "Iteration 66305: loss = 2.8280517e-11,1.487309e-10\n",
      "Iteration 66310: loss = 2.8281003e-11,1.4872852e-10\n",
      "Iteration 66315: loss = 2.8882785e-11,1.4872643e-10\n",
      "Iteration 66320: loss = 2.8882606e-11,1.4872485e-10\n",
      "Iteration 66325: loss = 2.8279157e-11,1.4872456e-10\n",
      "Iteration 66330: loss = 2.8280817e-11,1.4872116e-10\n",
      "Iteration 66335: loss = 2.8280578e-11,1.4871968e-10\n",
      "Iteration 66340: loss = 2.8883097e-11,1.4871676e-10\n",
      "Iteration 66345: loss = 2.8279948e-11,1.4871647e-10\n",
      "Iteration 66350: loss = 2.8882466e-11,1.487137e-10\n",
      "Iteration 66355: loss = 2.8279962e-11,1.4871276e-10\n",
      "Iteration 66360: loss = 2.8280248e-11,1.4871056e-10\n",
      "Iteration 66365: loss = 2.8279852e-11,1.4870907e-10\n",
      "Iteration 66370: loss = 2.8280295e-11,1.4870748e-10\n",
      "Iteration 66375: loss = 2.8280522e-11,1.4870465e-10\n",
      "Iteration 66380: loss = 2.8280345e-11,1.4870373e-10\n",
      "Iteration 66385: loss = 2.8882875e-11,1.4870091e-10\n",
      "Iteration 66390: loss = 2.8882424e-11,1.486994e-10\n",
      "Iteration 66395: loss = 2.8279936e-11,1.486986e-10\n",
      "Iteration 66400: loss = 2.8882475e-11,1.4869556e-10\n",
      "Iteration 66405: loss = 2.8279228e-11,1.4869539e-10\n",
      "Iteration 66410: loss = 2.8882488e-11,1.4869186e-10\n",
      "Iteration 66415: loss = 2.8882355e-11,1.4869019e-10\n",
      "Iteration 66420: loss = 2.8279601e-11,1.4868937e-10\n",
      "Iteration 66425: loss = 2.8882749e-11,1.486859e-10\n",
      "Iteration 66430: loss = 2.8279679e-11,1.4868548e-10\n",
      "Iteration 66435: loss = 2.8279901e-11,1.486834e-10\n",
      "Iteration 66440: loss = 2.8279698e-11,1.4868184e-10\n",
      "Iteration 66445: loss = 2.8279901e-11,1.4868033e-10\n",
      "Iteration 66450: loss = 2.8882445e-11,1.4867661e-10\n",
      "Iteration 66455: loss = 2.827997e-11,1.4867635e-10\n",
      "Iteration 66460: loss = 2.8882513e-11,1.486736e-10\n",
      "Iteration 66465: loss = 2.8882327e-11,1.4867191e-10\n",
      "Iteration 66470: loss = 2.8881878e-11,1.486703e-10\n",
      "Iteration 66475: loss = 2.8280073e-11,1.4866872e-10\n",
      "Iteration 66480: loss = 2.8881923e-11,1.4866657e-10\n",
      "Iteration 66485: loss = 2.82799e-11,1.4866496e-10\n",
      "Iteration 66490: loss = 2.888192e-11,1.4866298e-10\n",
      "Iteration 66495: loss = 2.8279878e-11,1.4866142e-10\n",
      "Iteration 66500: loss = 2.8881961e-11,1.4865913e-10\n",
      "Iteration 66505: loss = 2.8279051e-11,1.4865843e-10\n",
      "Iteration 66510: loss = 2.8881779e-11,1.4865578e-10\n",
      "Iteration 66515: loss = 2.8278773e-11,1.4865519e-10\n",
      "Iteration 66520: loss = 2.827999e-11,1.4865235e-10\n",
      "Iteration 66525: loss = 2.8279495e-11,1.486514e-10\n",
      "Iteration 66530: loss = 2.8279767e-11,1.4864847e-10\n",
      "Iteration 66535: loss = 2.8881614e-11,1.4864712e-10\n",
      "Iteration 66540: loss = 2.8278721e-11,1.4864664e-10\n",
      "Iteration 66545: loss = 2.8881909e-11,1.4864313e-10\n",
      "Iteration 66550: loss = 2.827987e-11,1.4864175e-10\n",
      "Iteration 66555: loss = 2.8881883e-11,1.4863936e-10\n",
      "Iteration 66560: loss = 2.8279207e-11,1.4863855e-10\n",
      "Iteration 66565: loss = 2.8881961e-11,1.4863565e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66570: loss = 2.8278954e-11,1.4863506e-10\n",
      "Iteration 66575: loss = 2.8882409e-11,1.4863138e-10\n",
      "Iteration 66580: loss = 2.8279457e-11,1.4863066e-10\n",
      "Iteration 66585: loss = 2.8279296e-11,1.4862904e-10\n",
      "Iteration 66590: loss = 2.888183e-11,1.4862647e-10\n",
      "Iteration 66595: loss = 2.8278416e-11,1.4862633e-10\n",
      "Iteration 66600: loss = 2.8881569e-11,1.4862306e-10\n",
      "Iteration 66605: loss = 2.8279518e-11,1.4862168e-10\n",
      "Iteration 66610: loss = 2.8279342e-11,1.4861988e-10\n",
      "Iteration 66615: loss = 2.8278676e-11,1.4861849e-10\n",
      "Iteration 66620: loss = 2.8279157e-11,1.486164e-10\n",
      "Iteration 66625: loss = 2.8881569e-11,1.4861358e-10\n",
      "Iteration 66630: loss = 2.8279329e-11,1.4861326e-10\n",
      "Iteration 66635: loss = 2.8881146e-11,1.4861089e-10\n",
      "Iteration 66640: loss = 2.827917e-11,1.4860961e-10\n",
      "Iteration 66645: loss = 2.8881625e-11,1.4860625e-10\n",
      "Iteration 66650: loss = 2.8278978e-11,1.4860613e-10\n",
      "Iteration 66655: loss = 2.8881503e-11,1.4860324e-10\n",
      "Iteration 66660: loss = 2.8881264e-11,1.4860173e-10\n",
      "Iteration 66665: loss = 2.8881548e-11,1.4859934e-10\n",
      "Iteration 66670: loss = 2.8278784e-11,1.4859883e-10\n",
      "Iteration 66675: loss = 2.8881982e-11,1.485955e-10\n",
      "Iteration 66680: loss = 2.8880844e-11,1.4859473e-10\n",
      "Iteration 66685: loss = 2.8278602e-11,1.4859361e-10\n",
      "Iteration 66690: loss = 2.8881184e-11,1.4859063e-10\n",
      "Iteration 66695: loss = 2.8279074e-11,1.4858931e-10\n",
      "Iteration 66700: loss = 2.8278607e-11,1.485879e-10\n",
      "Iteration 66705: loss = 2.8881168e-11,1.4858526e-10\n",
      "Iteration 66710: loss = 2.8279093e-11,1.4858395e-10\n",
      "Iteration 66715: loss = 2.8278489e-11,1.4858252e-10\n",
      "Iteration 66720: loss = 2.8278236e-11,1.48581e-10\n",
      "Iteration 66725: loss = 2.827818e-11,1.4857914e-10\n",
      "Iteration 66730: loss = 2.8880806e-11,1.4857607e-10\n",
      "Iteration 66735: loss = 2.8278074e-11,1.4857542e-10\n",
      "Iteration 66740: loss = 2.8880573e-11,1.4857246e-10\n",
      "Iteration 66745: loss = 2.8278754e-11,1.4857165e-10\n",
      "Iteration 66750: loss = 2.88806e-11,1.4856928e-10\n",
      "Iteration 66755: loss = 2.8880785e-11,1.4856734e-10\n",
      "Iteration 66760: loss = 2.8278546e-11,1.4856608e-10\n",
      "Iteration 66765: loss = 2.8881323e-11,1.4856336e-10\n",
      "Iteration 66770: loss = 2.8278345e-11,1.4856265e-10\n",
      "Iteration 66775: loss = 2.8880407e-11,1.4856036e-10\n",
      "Iteration 66780: loss = 2.8880393e-11,1.4855836e-10\n",
      "Iteration 66785: loss = 2.8880847e-11,1.4855625e-10\n",
      "Iteration 66790: loss = 2.8278638e-11,1.4855504e-10\n",
      "Iteration 66795: loss = 2.8880412e-11,1.4855296e-10\n",
      "Iteration 66800: loss = 2.8278565e-11,1.4855132e-10\n",
      "Iteration 66805: loss = 2.8880428e-11,1.4854931e-10\n",
      "Iteration 66810: loss = 2.8278107e-11,1.4854806e-10\n",
      "Iteration 66815: loss = 2.8881439e-11,1.485447e-10\n",
      "Iteration 66820: loss = 2.8277916e-11,1.485447e-10\n",
      "Iteration 66825: loss = 2.8880467e-11,1.485418e-10\n",
      "Iteration 66830: loss = 2.8880292e-11,1.4854028e-10\n",
      "Iteration 66835: loss = 2.8277327e-11,1.4853967e-10\n",
      "Iteration 66840: loss = 2.827818e-11,1.4853693e-10\n",
      "Iteration 66845: loss = 2.8277805e-11,1.4853571e-10\n",
      "Iteration 66850: loss = 2.8880386e-11,1.4853269e-10\n",
      "Iteration 66855: loss = 2.8277334e-11,1.4853219e-10\n",
      "Iteration 66860: loss = 2.8880185e-11,1.485295e-10\n",
      "Iteration 66865: loss = 2.8277889e-11,1.4852808e-10\n",
      "Iteration 66870: loss = 2.8277604e-11,1.4852722e-10\n",
      "Iteration 66875: loss = 2.8880167e-11,1.4852354e-10\n",
      "Iteration 66880: loss = 2.8277177e-11,1.485232e-10\n",
      "Iteration 66885: loss = 2.8278121e-11,1.4852033e-10\n",
      "Iteration 66890: loss = 2.8277656e-11,1.4851895e-10\n",
      "Iteration 66895: loss = 2.8277956e-11,1.4851662e-10\n",
      "Iteration 66900: loss = 2.8879966e-11,1.4851514e-10\n",
      "Iteration 66905: loss = 2.8880232e-11,1.4851297e-10\n",
      "Iteration 66910: loss = 2.8880467e-11,1.4851101e-10\n",
      "Iteration 66915: loss = 2.8277977e-11,1.4851002e-10\n",
      "Iteration 66920: loss = 2.8880745e-11,1.4850685e-10\n",
      "Iteration 66925: loss = 2.8277956e-11,1.4850599e-10\n",
      "Iteration 66930: loss = 2.8880726e-11,1.4850335e-10\n",
      "Iteration 66935: loss = 2.8879824e-11,1.4850254e-10\n",
      "Iteration 66940: loss = 2.8277542e-11,1.4850132e-10\n",
      "Iteration 66945: loss = 2.8880074e-11,1.4849842e-10\n",
      "Iteration 66950: loss = 2.8879616e-11,1.484971e-10\n",
      "Iteration 66955: loss = 2.8879921e-11,1.4849497e-10\n",
      "Iteration 66960: loss = 2.8879898e-11,1.4849315e-10\n",
      "Iteration 66965: loss = 2.8277894e-11,1.4849169e-10\n",
      "Iteration 66970: loss = 2.827738e-11,1.4849022e-10\n",
      "Iteration 66975: loss = 2.8880195e-11,1.4848744e-10\n",
      "Iteration 66980: loss = 2.8277611e-11,1.4848626e-10\n",
      "Iteration 66985: loss = 2.8879713e-11,1.4848425e-10\n",
      "Iteration 66990: loss = 2.8277455e-11,1.4848293e-10\n",
      "Iteration 66995: loss = 2.8276987e-11,1.4848155e-10\n",
      "Iteration 67000: loss = 2.8277707e-11,1.4847899e-10\n",
      "Iteration 67005: loss = 2.8879336e-11,1.4847731e-10\n",
      "Iteration 67010: loss = 2.8879338e-11,1.4847495e-10\n",
      "Iteration 67015: loss = 2.827665e-11,1.4847447e-10\n",
      "Iteration 67020: loss = 2.8277528e-11,1.484717e-10\n",
      "Iteration 67025: loss = 2.8277288e-11,1.4847014e-10\n",
      "Iteration 67030: loss = 2.8276659e-11,1.4846904e-10\n",
      "Iteration 67035: loss = 2.8276654e-11,1.4846702e-10\n",
      "Iteration 67040: loss = 2.8880153e-11,1.484635e-10\n",
      "Iteration 67045: loss = 2.8276678e-11,1.4846324e-10\n",
      "Iteration 67050: loss = 2.8276903e-11,1.4846134e-10\n",
      "Iteration 67055: loss = 2.8879397e-11,1.4845856e-10\n",
      "Iteration 67060: loss = 2.8277403e-11,1.4845791e-10\n",
      "Iteration 67065: loss = 2.8879052e-11,1.4845565e-10\n",
      "Iteration 67070: loss = 2.8276964e-11,1.4845451e-10\n",
      "Iteration 67075: loss = 2.887951e-11,1.4845133e-10\n",
      "Iteration 67080: loss = 2.8277271e-11,1.4845078e-10\n",
      "Iteration 67085: loss = 2.8879135e-11,1.4844763e-10\n",
      "Iteration 67090: loss = 2.8276817e-11,1.4844734e-10\n",
      "Iteration 67095: loss = 2.887964e-11,1.4844413e-10\n",
      "Iteration 67100: loss = 2.8878955e-11,1.4844301e-10\n",
      "Iteration 67105: loss = 2.8879475e-11,1.484408e-10\n",
      "Iteration 67110: loss = 2.8879442e-11,1.484387e-10\n",
      "Iteration 67115: loss = 2.8277165e-11,1.4843773e-10\n",
      "Iteration 67120: loss = 2.8276942e-11,1.4843608e-10\n",
      "Iteration 67125: loss = 2.8879779e-11,1.4843325e-10\n",
      "Iteration 67130: loss = 2.8879113e-11,1.484318e-10\n",
      "Iteration 67135: loss = 2.8277e-11,1.4843067e-10\n",
      "Iteration 67140: loss = 2.8277101e-11,1.4842888e-10\n",
      "Iteration 67145: loss = 2.887895e-11,1.4842659e-10\n",
      "Iteration 67150: loss = 2.8879205e-11,1.4842463e-10\n",
      "Iteration 67155: loss = 2.8276414e-11,1.4842372e-10\n",
      "Iteration 67160: loss = 2.827668e-11,1.4842172e-10\n",
      "Iteration 67165: loss = 2.8879907e-11,1.4841825e-10\n",
      "Iteration 67170: loss = 2.8276262e-11,1.4841842e-10\n",
      "Iteration 67175: loss = 2.8878832e-11,1.4841552e-10\n",
      "Iteration 67180: loss = 2.8276706e-11,1.4841431e-10\n",
      "Iteration 67185: loss = 2.8878677e-11,1.4841194e-10\n",
      "Iteration 67190: loss = 2.8275444e-11,1.4841181e-10\n",
      "Iteration 67195: loss = 2.827664e-11,1.4840902e-10\n",
      "Iteration 67200: loss = 2.887921e-11,1.4840622e-10\n",
      "Iteration 67205: loss = 2.8275117e-11,1.4840673e-10\n",
      "Iteration 67210: loss = 2.8276709e-11,1.484033e-10\n",
      "Iteration 67215: loss = 2.8879461e-11,1.4840032e-10\n",
      "Iteration 67220: loss = 2.8275533e-11,1.4840089e-10\n",
      "Iteration 67225: loss = 2.8275783e-11,1.4839853e-10\n",
      "Iteration 67230: loss = 2.8879047e-11,1.483951e-10\n",
      "Iteration 67235: loss = 2.8275453e-11,1.4839525e-10\n",
      "Iteration 67240: loss = 2.887826e-11,1.4839249e-10\n",
      "Iteration 67245: loss = 2.827589e-11,1.4839113e-10\n",
      "Iteration 67250: loss = 2.887871e-11,1.4838825e-10\n",
      "Iteration 67255: loss = 2.8276478e-11,1.48387e-10\n",
      "Iteration 67260: loss = 2.827652e-11,1.4838597e-10\n",
      "Iteration 67265: loss = 2.8276414e-11,1.483831e-10\n",
      "Iteration 67270: loss = 2.8275784e-11,1.4838206e-10\n",
      "Iteration 67275: loss = 2.827616e-11,1.4837986e-10\n",
      "Iteration 67280: loss = 2.8276397e-11,1.4837848e-10\n",
      "Iteration 67285: loss = 2.8878273e-11,1.4837567e-10\n",
      "Iteration 67290: loss = 2.8878474e-11,1.4837433e-10\n",
      "Iteration 67295: loss = 2.8878766e-11,1.4837183e-10\n",
      "Iteration 67300: loss = 2.8275415e-11,1.4837197e-10\n",
      "Iteration 67305: loss = 2.887888e-11,1.4836837e-10\n",
      "Iteration 67310: loss = 2.8275627e-11,1.483679e-10\n",
      "Iteration 67315: loss = 2.8879059e-11,1.4836428e-10\n",
      "Iteration 67320: loss = 2.8276215e-11,1.483636e-10\n",
      "Iteration 67325: loss = 2.8878289e-11,1.4836157e-10\n",
      "Iteration 67330: loss = 2.8878384e-11,1.4835955e-10\n",
      "Iteration 67335: loss = 2.827578e-11,1.4835858e-10\n",
      "Iteration 67340: loss = 2.827611e-11,1.4835679e-10\n",
      "Iteration 67345: loss = 2.8877975e-11,1.483545e-10\n",
      "Iteration 67350: loss = 2.887813e-11,1.483526e-10\n",
      "Iteration 67355: loss = 2.8275706e-11,1.483516e-10\n",
      "Iteration 67360: loss = 2.88783e-11,1.4834875e-10\n",
      "Iteration 67365: loss = 2.887769e-11,1.4834753e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67370: loss = 2.8878145e-11,1.4834514e-10\n",
      "Iteration 67375: loss = 2.8275859e-11,1.4834396e-10\n",
      "Iteration 67380: loss = 2.827552e-11,1.4834237e-10\n",
      "Iteration 67385: loss = 2.8275798e-11,1.4834033e-10\n",
      "Iteration 67390: loss = 2.8274822e-11,1.4833926e-10\n",
      "Iteration 67395: loss = 2.8877673e-11,1.4833647e-10\n",
      "Iteration 67400: loss = 2.887767e-11,1.483343e-10\n",
      "Iteration 67405: loss = 2.8274766e-11,1.4833397e-10\n",
      "Iteration 67410: loss = 2.887799e-11,1.4833046e-10\n",
      "Iteration 67415: loss = 2.8275047e-11,1.4833e-10\n",
      "Iteration 67420: loss = 2.8877652e-11,1.4832724e-10\n",
      "Iteration 67425: loss = 2.8274683e-11,1.4832681e-10\n",
      "Iteration 67430: loss = 2.8877923e-11,1.4832316e-10\n",
      "Iteration 67435: loss = 2.8275033e-11,1.4832265e-10\n",
      "Iteration 67440: loss = 2.8275318e-11,1.4832055e-10\n",
      "Iteration 67445: loss = 2.8275072e-11,1.4831898e-10\n",
      "Iteration 67450: loss = 2.8877291e-11,1.4831664e-10\n",
      "Iteration 67455: loss = 2.8877243e-11,1.4831453e-10\n",
      "Iteration 67460: loss = 2.8877586e-11,1.4831317e-10\n",
      "Iteration 67465: loss = 2.827527e-11,1.4831132e-10\n",
      "Iteration 67470: loss = 2.827532e-11,1.4830942e-10\n",
      "Iteration 67475: loss = 2.8275403e-11,1.483082e-10\n",
      "Iteration 67480: loss = 2.8877276e-11,1.4830591e-10\n",
      "Iteration 67485: loss = 2.8877562e-11,1.4830387e-10\n",
      "Iteration 67490: loss = 2.8274933e-11,1.4830281e-10\n",
      "Iteration 67495: loss = 2.8877914e-11,1.4829984e-10\n",
      "Iteration 67500: loss = 2.8877206e-11,1.4829851e-10\n",
      "Iteration 67505: loss = 2.8877166e-11,1.482968e-10\n",
      "Iteration 67510: loss = 2.8877505e-11,1.482947e-10\n",
      "Iteration 67515: loss = 2.8274985e-11,1.4829368e-10\n",
      "Iteration 67520: loss = 2.887777e-11,1.4829085e-10\n",
      "Iteration 67525: loss = 2.827529e-11,1.4828977e-10\n",
      "Iteration 67530: loss = 2.8878159e-11,1.4828683e-10\n",
      "Iteration 67535: loss = 2.8275243e-11,1.4828638e-10\n",
      "Iteration 67540: loss = 2.8877109e-11,1.4828402e-10\n",
      "Iteration 67545: loss = 2.8877109e-11,1.4828212e-10\n",
      "Iteration 67550: loss = 2.8877123e-11,1.4828044e-10\n",
      "Iteration 67555: loss = 2.8877368e-11,1.4827825e-10\n",
      "Iteration 67560: loss = 2.8274959e-11,1.4827722e-10\n",
      "Iteration 67565: loss = 2.8876871e-11,1.4827513e-10\n",
      "Iteration 67570: loss = 2.8876805e-11,1.482737e-10\n",
      "Iteration 67575: loss = 2.8877083e-11,1.4827131e-10\n",
      "Iteration 67580: loss = 2.8876719e-11,1.4826992e-10\n",
      "Iteration 67585: loss = 2.887677e-11,1.482679e-10\n",
      "Iteration 67590: loss = 2.8876769e-11,1.4826612e-10\n",
      "Iteration 67595: loss = 2.88769e-11,1.4826407e-10\n",
      "Iteration 67600: loss = 2.8274827e-11,1.4826274e-10\n",
      "Iteration 67605: loss = 2.8274924e-11,1.4826104e-10\n",
      "Iteration 67610: loss = 2.8876528e-11,1.4825902e-10\n",
      "Iteration 67615: loss = 2.8274508e-11,1.4825768e-10\n",
      "Iteration 67620: loss = 2.8877059e-11,1.4825476e-10\n",
      "Iteration 67625: loss = 2.8274836e-11,1.4825383e-10\n",
      "Iteration 67630: loss = 2.8876469e-11,1.4825173e-10\n",
      "Iteration 67635: loss = 2.8274503e-11,1.4825047e-10\n",
      "Iteration 67640: loss = 2.8274506e-11,1.4824839e-10\n",
      "Iteration 67645: loss = 2.8273809e-11,1.4824719e-10\n",
      "Iteration 67650: loss = 2.8876651e-11,1.4824426e-10\n",
      "Iteration 67655: loss = 2.8274117e-11,1.4824314e-10\n",
      "Iteration 67660: loss = 2.8273502e-11,1.482421e-10\n",
      "Iteration 67665: loss = 2.8876708e-11,1.4823859e-10\n",
      "Iteration 67670: loss = 2.8273562e-11,1.4823835e-10\n",
      "Iteration 67675: loss = 2.827412e-11,1.4823613e-10\n",
      "Iteration 67680: loss = 2.8274187e-11,1.482344e-10\n",
      "Iteration 67685: loss = 2.8274489e-11,1.4823273e-10\n",
      "Iteration 67690: loss = 2.8274475e-11,1.4823082e-10\n",
      "Iteration 67695: loss = 2.887642e-11,1.4822862e-10\n",
      "Iteration 67700: loss = 2.8876195e-11,1.48227e-10\n",
      "Iteration 67705: loss = 2.887665e-11,1.482242e-10\n",
      "Iteration 67710: loss = 2.8876753e-11,1.4822318e-10\n",
      "Iteration 67715: loss = 2.8876151e-11,1.4822095e-10\n",
      "Iteration 67720: loss = 2.8876346e-11,1.4821959e-10\n",
      "Iteration 67725: loss = 2.8876401e-11,1.4821751e-10\n",
      "Iteration 67730: loss = 2.887674e-11,1.482153e-10\n",
      "Iteration 67735: loss = 2.827423e-11,1.4821429e-10\n",
      "Iteration 67740: loss = 2.8876795e-11,1.4821157e-10\n",
      "Iteration 67745: loss = 2.887609e-11,1.4821051e-10\n",
      "Iteration 67750: loss = 2.8876844e-11,1.4820781e-10\n",
      "Iteration 67755: loss = 2.827389e-11,1.4820736e-10\n",
      "Iteration 67760: loss = 2.8876278e-11,1.4820478e-10\n",
      "Iteration 67765: loss = 2.8274022e-11,1.4820356e-10\n",
      "Iteration 67770: loss = 2.8876755e-11,1.4820002e-10\n",
      "Iteration 67775: loss = 2.8274013e-11,1.481999e-10\n",
      "Iteration 67780: loss = 2.8876618e-11,1.4819751e-10\n",
      "Iteration 67785: loss = 2.827371e-11,1.4819679e-10\n",
      "Iteration 67790: loss = 2.88758e-11,1.4819435e-10\n",
      "Iteration 67795: loss = 2.8876517e-11,1.4819182e-10\n",
      "Iteration 67800: loss = 2.8273573e-11,1.4819135e-10\n",
      "Iteration 67805: loss = 2.8876151e-11,1.4818856e-10\n",
      "Iteration 67810: loss = 2.8876268e-11,1.4818664e-10\n",
      "Iteration 67815: loss = 2.8273675e-11,1.4818576e-10\n",
      "Iteration 67820: loss = 2.8876922e-11,1.4818237e-10\n",
      "Iteration 67825: loss = 2.8273753e-11,1.4818215e-10\n",
      "Iteration 67830: loss = 2.8876323e-11,1.4817944e-10\n",
      "Iteration 67835: loss = 2.8273264e-11,1.4817887e-10\n",
      "Iteration 67840: loss = 2.8876799e-11,1.4817529e-10\n",
      "Iteration 67845: loss = 2.8273633e-11,1.4817479e-10\n",
      "Iteration 67850: loss = 2.8875796e-11,1.4817268e-10\n",
      "Iteration 67855: loss = 2.8273509e-11,1.4817175e-10\n",
      "Iteration 67860: loss = 2.8876063e-11,1.4816852e-10\n",
      "Iteration 67865: loss = 2.8273507e-11,1.4816773e-10\n",
      "Iteration 67870: loss = 2.8875834e-11,1.4816497e-10\n",
      "Iteration 67875: loss = 2.8273632e-11,1.4816398e-10\n",
      "Iteration 67880: loss = 2.8875522e-11,1.4816176e-10\n",
      "Iteration 67885: loss = 2.8273486e-11,1.4816057e-10\n",
      "Iteration 67890: loss = 2.8273536e-11,1.4815872e-10\n",
      "Iteration 67895: loss = 2.8273378e-11,1.4815679e-10\n",
      "Iteration 67900: loss = 2.8875896e-11,1.4815427e-10\n",
      "Iteration 67905: loss = 2.8273368e-11,1.4815332e-10\n",
      "Iteration 67910: loss = 2.827348e-11,1.4815144e-10\n",
      "Iteration 67915: loss = 2.887599e-11,1.4814883e-10\n",
      "Iteration 67920: loss = 2.8875803e-11,1.481471e-10\n",
      "Iteration 67925: loss = 2.8272875e-11,1.4814658e-10\n",
      "Iteration 67930: loss = 2.8876334e-11,1.4814305e-10\n",
      "Iteration 67935: loss = 2.8272168e-11,1.4814347e-10\n",
      "Iteration 67940: loss = 2.8875667e-11,1.4813992e-10\n",
      "Iteration 67945: loss = 2.8272473e-11,1.4813965e-10\n",
      "Iteration 67950: loss = 2.8875314e-11,1.481365e-10\n",
      "Iteration 67955: loss = 2.827306e-11,1.4813545e-10\n",
      "Iteration 67960: loss = 2.8876332e-11,1.481321e-10\n",
      "Iteration 67965: loss = 2.8272893e-11,1.4813203e-10\n",
      "Iteration 67970: loss = 2.8273097e-11,1.4812976e-10\n",
      "Iteration 67975: loss = 2.8874951e-11,1.4812765e-10\n",
      "Iteration 67980: loss = 2.8875078e-11,1.4812591e-10\n",
      "Iteration 67985: loss = 2.8272815e-11,1.4812455e-10\n",
      "Iteration 67990: loss = 2.8875359e-11,1.4812204e-10\n",
      "Iteration 67995: loss = 2.827219e-11,1.4812188e-10\n",
      "Iteration 68000: loss = 2.8875362e-11,1.4811824e-10\n",
      "Iteration 68005: loss = 2.8272199e-11,1.4811793e-10\n",
      "Iteration 68010: loss = 2.8273111e-11,1.4811505e-10\n",
      "Iteration 68015: loss = 2.8875962e-11,1.48112e-10\n",
      "Iteration 68020: loss = 2.8272473e-11,1.4811184e-10\n",
      "Iteration 68025: loss = 2.887597e-11,1.4810828e-10\n",
      "Iteration 68030: loss = 2.8271838e-11,1.4810905e-10\n",
      "Iteration 68035: loss = 2.8874986e-11,1.4810542e-10\n",
      "Iteration 68040: loss = 2.8875282e-11,1.4810357e-10\n",
      "Iteration 68045: loss = 2.8272806e-11,1.4810246e-10\n",
      "Iteration 68050: loss = 2.8272384e-11,1.4810117e-10\n",
      "Iteration 68055: loss = 2.8272606e-11,1.4809892e-10\n",
      "Iteration 68060: loss = 2.8272764e-11,1.4809712e-10\n",
      "Iteration 68065: loss = 2.8272459e-11,1.4809558e-10\n",
      "Iteration 68070: loss = 2.8272504e-11,1.4809375e-10\n",
      "Iteration 68075: loss = 2.8875088e-11,1.4809079e-10\n",
      "Iteration 68080: loss = 2.8272593e-11,1.4808986e-10\n",
      "Iteration 68085: loss = 2.8272629e-11,1.48088e-10\n",
      "Iteration 68090: loss = 2.827249e-11,1.4808636e-10\n",
      "Iteration 68095: loss = 2.8875714e-11,1.4808288e-10\n",
      "Iteration 68100: loss = 2.8272001e-11,1.4808381e-10\n",
      "Iteration 68105: loss = 2.8874833e-11,1.480803e-10\n",
      "Iteration 68110: loss = 2.8272279e-11,1.4807987e-10\n",
      "Iteration 68115: loss = 2.8874436e-11,1.4807701e-10\n",
      "Iteration 68120: loss = 2.8272553e-11,1.4807507e-10\n",
      "Iteration 68125: loss = 2.8271954e-11,1.4807461e-10\n",
      "Iteration 68130: loss = 2.8874686e-11,1.480713e-10\n",
      "Iteration 68135: loss = 2.8272157e-11,1.4807021e-10\n",
      "Iteration 68140: loss = 2.8272449e-11,1.4806882e-10\n",
      "Iteration 68145: loss = 2.8874146e-11,1.4806638e-10\n",
      "Iteration 68150: loss = 2.8874347e-11,1.4806409e-10\n",
      "Iteration 68155: loss = 2.887436e-11,1.4806285e-10\n",
      "Iteration 68160: loss = 2.887436e-11,1.4806104e-10\n",
      "Iteration 68165: loss = 2.8874654e-11,1.4805838e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68170: loss = 2.887423e-11,1.4805744e-10\n",
      "Iteration 68175: loss = 2.8874424e-11,1.4805479e-10\n",
      "Iteration 68180: loss = 2.8874481e-11,1.4805375e-10\n",
      "Iteration 68185: loss = 2.8272244e-11,1.4805236e-10\n",
      "Iteration 68190: loss = 2.8875005e-11,1.480491e-10\n",
      "Iteration 68195: loss = 2.8272247e-11,1.4804875e-10\n",
      "Iteration 68200: loss = 2.8874325e-11,1.4804596e-10\n",
      "Iteration 68205: loss = 2.887437e-11,1.4804488e-10\n",
      "Iteration 68210: loss = 2.8874375e-11,1.480431e-10\n",
      "Iteration 68215: loss = 2.8272046e-11,1.4804181e-10\n",
      "Iteration 68220: loss = 2.8874389e-11,1.4803916e-10\n",
      "Iteration 68225: loss = 2.887377e-11,1.4803803e-10\n",
      "Iteration 68230: loss = 2.8874595e-11,1.4803525e-10\n",
      "Iteration 68235: loss = 2.8874897e-11,1.4803322e-10\n",
      "Iteration 68240: loss = 2.8874238e-11,1.4803203e-10\n",
      "Iteration 68245: loss = 2.8874228e-11,1.4803016e-10\n",
      "Iteration 68250: loss = 2.827205e-11,1.4802919e-10\n",
      "Iteration 68255: loss = 2.8874542e-11,1.4802579e-10\n",
      "Iteration 68260: loss = 2.8874542e-11,1.4802462e-10\n",
      "Iteration 68265: loss = 2.8874103e-11,1.4802337e-10\n",
      "Iteration 68270: loss = 2.887413e-11,1.4802133e-10\n",
      "Iteration 68275: loss = 2.8874292e-11,1.4801942e-10\n",
      "Iteration 68280: loss = 2.8271782e-11,1.4801838e-10\n",
      "Iteration 68285: loss = 2.887527e-11,1.4801471e-10\n",
      "Iteration 68290: loss = 2.8271385e-11,1.4801532e-10\n",
      "Iteration 68295: loss = 2.8874637e-11,1.4801163e-10\n",
      "Iteration 68300: loss = 2.8873926e-11,1.4801066e-10\n",
      "Iteration 68305: loss = 2.8873947e-11,1.4800859e-10\n",
      "Iteration 68310: loss = 2.82717e-11,1.4800755e-10\n",
      "Iteration 68315: loss = 2.8874217e-11,1.4800475e-10\n",
      "Iteration 68320: loss = 2.8873577e-11,1.4800375e-10\n",
      "Iteration 68325: loss = 2.8873825e-11,1.4800183e-10\n",
      "Iteration 68330: loss = 2.8874734e-11,1.4799913e-10\n",
      "Iteration 68335: loss = 2.8271543e-11,1.4799878e-10\n",
      "Iteration 68340: loss = 2.8874727e-11,1.4799514e-10\n",
      "Iteration 68345: loss = 2.8271531e-11,1.4799496e-10\n",
      "Iteration 68350: loss = 2.8873343e-11,1.479929e-10\n",
      "Iteration 68355: loss = 2.8271501e-11,1.4799138e-10\n",
      "Iteration 68360: loss = 2.8875005e-11,1.4798777e-10\n",
      "Iteration 68365: loss = 2.8270872e-11,1.4798843e-10\n",
      "Iteration 68370: loss = 2.8874287e-11,1.4798492e-10\n",
      "Iteration 68375: loss = 2.887362e-11,1.4798338e-10\n",
      "Iteration 68380: loss = 2.8271408e-11,1.4798233e-10\n",
      "Iteration 68385: loss = 2.887481e-11,1.4797903e-10\n",
      "Iteration 68390: loss = 2.827138e-11,1.4797882e-10\n",
      "Iteration 68395: loss = 2.8873298e-11,1.4797664e-10\n",
      "Iteration 68400: loss = 2.8873265e-11,1.4797474e-10\n",
      "Iteration 68405: loss = 2.8873784e-11,1.4797262e-10\n",
      "Iteration 68410: loss = 2.8270506e-11,1.4797233e-10\n",
      "Iteration 68415: loss = 2.8873742e-11,1.4796893e-10\n",
      "Iteration 68420: loss = 2.8874661e-11,1.4796625e-10\n",
      "Iteration 68425: loss = 2.8271246e-11,1.4796586e-10\n",
      "Iteration 68430: loss = 2.8873954e-11,1.4796335e-10\n",
      "Iteration 68435: loss = 2.8270802e-11,1.4796298e-10\n",
      "Iteration 68440: loss = 2.887396e-11,1.479595e-10\n",
      "Iteration 68445: loss = 2.8873548e-11,1.4795813e-10\n",
      "Iteration 68450: loss = 2.8271057e-11,1.4795723e-10\n",
      "Iteration 68455: loss = 2.887452e-11,1.4795362e-10\n",
      "Iteration 68460: loss = 2.827103e-11,1.4795362e-10\n",
      "Iteration 68465: loss = 2.887298e-11,1.4795144e-10\n",
      "Iteration 68470: loss = 2.8270672e-11,1.4795042e-10\n",
      "Iteration 68475: loss = 2.8873856e-11,1.4794682e-10\n",
      "Iteration 68480: loss = 2.8270839e-11,1.4794654e-10\n",
      "Iteration 68485: loss = 2.8270844e-11,1.4794468e-10\n",
      "Iteration 68490: loss = 2.8874408e-11,1.4794102e-10\n",
      "Iteration 68495: loss = 2.8872765e-11,1.4794073e-10\n",
      "Iteration 68500: loss = 2.887296e-11,1.4793859e-10\n",
      "Iteration 68505: loss = 2.8270728e-11,1.4793748e-10\n",
      "Iteration 68510: loss = 2.8270714e-11,1.4793562e-10\n",
      "Iteration 68515: loss = 2.8874216e-11,1.4793217e-10\n",
      "Iteration 68520: loss = 2.8270738e-11,1.4793211e-10\n",
      "Iteration 68525: loss = 2.887255e-11,1.4792993e-10\n",
      "Iteration 68530: loss = 2.8872809e-11,1.4792806e-10\n",
      "Iteration 68535: loss = 2.8270289e-11,1.4792703e-10\n",
      "Iteration 68540: loss = 2.8872895e-11,1.4792442e-10\n",
      "Iteration 68545: loss = 2.8270566e-11,1.4792345e-10\n",
      "Iteration 68550: loss = 2.8873112e-11,1.4792066e-10\n",
      "Iteration 68555: loss = 2.8872404e-11,1.4791945e-10\n",
      "Iteration 68560: loss = 2.8872718e-11,1.4791726e-10\n",
      "Iteration 68565: loss = 2.8269673e-11,1.4791687e-10\n",
      "Iteration 68570: loss = 2.8872939e-11,1.4791326e-10\n",
      "Iteration 68575: loss = 2.827044e-11,1.4791246e-10\n",
      "Iteration 68580: loss = 2.8270377e-11,1.479108e-10\n",
      "Iteration 68585: loss = 2.887361e-11,1.4790752e-10\n",
      "Iteration 68590: loss = 2.8270457e-11,1.479072e-10\n",
      "Iteration 68595: loss = 2.8270428e-11,1.4790516e-10\n",
      "Iteration 68600: loss = 2.8872509e-11,1.4790293e-10\n",
      "Iteration 68605: loss = 2.8270225e-11,1.4790215e-10\n",
      "Iteration 68610: loss = 2.8872987e-11,1.4789876e-10\n",
      "Iteration 68615: loss = 2.8872293e-11,1.478977e-10\n",
      "Iteration 68620: loss = 2.8872087e-11,1.4789583e-10\n",
      "Iteration 68625: loss = 2.8269867e-11,1.4789474e-10\n",
      "Iteration 68630: loss = 2.8269812e-11,1.4789275e-10\n",
      "Iteration 68635: loss = 2.8872632e-11,1.4789006e-10\n",
      "Iteration 68640: loss = 2.88719e-11,1.4788877e-10\n",
      "Iteration 68645: loss = 2.8871886e-11,1.4788773e-10\n",
      "Iteration 68650: loss = 2.8872224e-11,1.4788476e-10\n",
      "Iteration 68655: loss = 2.8872854e-11,1.4788223e-10\n",
      "Iteration 68660: loss = 2.8871983e-11,1.4788196e-10\n",
      "Iteration 68665: loss = 2.8872118e-11,1.4787936e-10\n",
      "Iteration 68670: loss = 2.8269878e-11,1.478782e-10\n",
      "Iteration 68675: loss = 2.887269e-11,1.4787527e-10\n",
      "Iteration 68680: loss = 2.8871782e-11,1.478747e-10\n",
      "Iteration 68685: loss = 2.887203e-11,1.478721e-10\n",
      "Iteration 68690: loss = 2.8872982e-11,1.4786931e-10\n",
      "Iteration 68695: loss = 2.8871738e-11,1.4786924e-10\n",
      "Iteration 68700: loss = 2.8872266e-11,1.478665e-10\n",
      "Iteration 68705: loss = 2.887228e-11,1.4786512e-10\n",
      "Iteration 68710: loss = 2.887258e-11,1.4786314e-10\n",
      "Iteration 68715: loss = 2.8871579e-11,1.4786197e-10\n",
      "Iteration 68720: loss = 2.8872757e-11,1.478587e-10\n",
      "Iteration 68725: loss = 2.8871891e-11,1.4785786e-10\n",
      "Iteration 68730: loss = 2.8871914e-11,1.4785576e-10\n",
      "Iteration 68735: loss = 2.887189e-11,1.4785417e-10\n",
      "Iteration 68740: loss = 2.8872148e-11,1.4785259e-10\n",
      "Iteration 68745: loss = 2.887244e-11,1.4785047e-10\n",
      "Iteration 68750: loss = 2.8871456e-11,1.4784937e-10\n",
      "Iteration 68755: loss = 2.8872646e-11,1.4784596e-10\n",
      "Iteration 68760: loss = 2.8872384e-11,1.4784497e-10\n",
      "Iteration 68765: loss = 2.8871785e-11,1.4784382e-10\n",
      "Iteration 68770: loss = 2.8871752e-11,1.478421e-10\n",
      "Iteration 68775: loss = 2.8872684e-11,1.478388e-10\n",
      "Iteration 68780: loss = 2.8269479e-11,1.4783938e-10\n",
      "Iteration 68785: loss = 2.887224e-11,1.478357e-10\n",
      "Iteration 68790: loss = 2.8269465e-11,1.4783541e-10\n",
      "Iteration 68795: loss = 2.8872254e-11,1.4783202e-10\n",
      "Iteration 68800: loss = 2.887224e-11,1.4783078e-10\n",
      "Iteration 68805: loss = 2.8871577e-11,1.4782968e-10\n",
      "Iteration 68810: loss = 2.8269382e-11,1.4782855e-10\n",
      "Iteration 68815: loss = 2.8872538e-11,1.4782518e-10\n",
      "Iteration 68820: loss = 2.8269253e-11,1.4782496e-10\n",
      "Iteration 68825: loss = 2.8871858e-11,1.4782184e-10\n",
      "Iteration 68830: loss = 2.887113e-11,1.4782073e-10\n",
      "Iteration 68835: loss = 2.8872136e-11,1.4781841e-10\n",
      "Iteration 68840: loss = 2.887214e-11,1.4781633e-10\n",
      "Iteration 68845: loss = 2.8872146e-11,1.4781461e-10\n",
      "Iteration 68850: loss = 2.8872146e-11,1.4781276e-10\n",
      "Iteration 68855: loss = 2.8871489e-11,1.4781168e-10\n",
      "Iteration 68860: loss = 2.8871773e-11,1.4780963e-10\n",
      "Iteration 68865: loss = 2.887172e-11,1.4780766e-10\n",
      "Iteration 68870: loss = 2.8871724e-11,1.4780577e-10\n",
      "Iteration 68875: loss = 2.8872056e-11,1.4780382e-10\n",
      "Iteration 68880: loss = 2.887128e-11,1.4780266e-10\n",
      "Iteration 68885: loss = 2.887123e-11,1.478008e-10\n",
      "Iteration 68890: loss = 2.8871577e-11,1.4779881e-10\n",
      "Iteration 68895: loss = 2.826904e-11,1.4779801e-10\n",
      "Iteration 68900: loss = 2.8872254e-11,1.4779448e-10\n",
      "Iteration 68905: loss = 2.8870855e-11,1.4779405e-10\n",
      "Iteration 68910: loss = 2.8871636e-11,1.4779135e-10\n",
      "Iteration 68915: loss = 2.8268924e-11,1.4779082e-10\n",
      "Iteration 68920: loss = 2.887073e-11,1.4778845e-10\n",
      "Iteration 68925: loss = 2.8870904e-11,1.4778667e-10\n",
      "Iteration 68930: loss = 2.8870961e-11,1.4778484e-10\n",
      "Iteration 68935: loss = 2.8871454e-11,1.4778216e-10\n",
      "Iteration 68940: loss = 2.8268688e-11,1.4778176e-10\n",
      "Iteration 68945: loss = 2.8871438e-11,1.4777901e-10\n",
      "Iteration 68950: loss = 2.887104e-11,1.4777736e-10\n",
      "Iteration 68955: loss = 2.8871077e-11,1.4777558e-10\n",
      "Iteration 68960: loss = 2.887135e-11,1.4777357e-10\n",
      "Iteration 68965: loss = 2.8268768e-11,1.4777259e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68970: loss = 2.887198e-11,1.477693e-10\n",
      "Iteration 68975: loss = 2.8871313e-11,1.4776802e-10\n",
      "Iteration 68980: loss = 2.8870325e-11,1.4776716e-10\n",
      "Iteration 68985: loss = 2.8870678e-11,1.4776513e-10\n",
      "Iteration 68990: loss = 2.8870267e-11,1.4776369e-10\n",
      "Iteration 68995: loss = 2.8871155e-11,1.4776075e-10\n",
      "Iteration 69000: loss = 2.8870255e-11,1.4775994e-10\n",
      "Iteration 69005: loss = 2.8871454e-11,1.477572e-10\n",
      "Iteration 69010: loss = 2.8870234e-11,1.4775631e-10\n",
      "Iteration 69015: loss = 2.8871456e-11,1.4775335e-10\n",
      "Iteration 69020: loss = 2.8870503e-11,1.4775232e-10\n",
      "Iteration 69025: loss = 2.826848e-11,1.477511e-10\n",
      "Iteration 69030: loss = 2.8871669e-11,1.4774779e-10\n",
      "Iteration 69035: loss = 2.8871744e-11,1.4774579e-10\n",
      "Iteration 69040: loss = 2.8870697e-11,1.4774491e-10\n",
      "Iteration 69045: loss = 2.8870078e-11,1.4774362e-10\n",
      "Iteration 69050: loss = 2.8871986e-11,1.4774035e-10\n",
      "Iteration 69055: loss = 2.826805e-11,1.477408e-10\n",
      "Iteration 69060: loss = 2.8871246e-11,1.4773724e-10\n",
      "Iteration 69065: loss = 2.8870135e-11,1.477366e-10\n",
      "Iteration 69070: loss = 2.8870165e-11,1.4773438e-10\n",
      "Iteration 69075: loss = 2.8870404e-11,1.4773258e-10\n",
      "Iteration 69080: loss = 2.887017e-11,1.47731e-10\n",
      "Iteration 69085: loss = 2.887116e-11,1.4772825e-10\n",
      "Iteration 69090: loss = 2.8267928e-11,1.4772805e-10\n",
      "Iteration 69095: loss = 2.8871175e-11,1.477247e-10\n",
      "Iteration 69100: loss = 2.8869832e-11,1.4772417e-10\n",
      "Iteration 69105: loss = 2.8870522e-11,1.4772171e-10\n",
      "Iteration 69110: loss = 2.8870746e-11,1.4771973e-10\n",
      "Iteration 69115: loss = 2.8869842e-11,1.4771884e-10\n",
      "Iteration 69120: loss = 2.887147e-11,1.477153e-10\n",
      "Iteration 69125: loss = 2.8870073e-11,1.4771478e-10\n",
      "Iteration 69130: loss = 2.886969e-11,1.4771333e-10\n",
      "Iteration 69135: loss = 2.8870378e-11,1.4771084e-10\n",
      "Iteration 69140: loss = 2.8870609e-11,1.4770876e-10\n",
      "Iteration 69145: loss = 2.8869664e-11,1.4770798e-10\n",
      "Iteration 69150: loss = 2.8871593e-11,1.4770418e-10\n",
      "Iteration 69155: loss = 2.8870012e-11,1.4770404e-10\n",
      "Iteration 69160: loss = 2.8869998e-11,1.4770202e-10\n",
      "Iteration 69165: loss = 2.8870281e-11,1.4769992e-10\n",
      "Iteration 69170: loss = 2.8267735e-11,1.4769916e-10\n",
      "Iteration 69175: loss = 2.8871254e-11,1.4769573e-10\n",
      "Iteration 69180: loss = 2.8869596e-11,1.4769516e-10\n",
      "Iteration 69185: loss = 2.886987e-11,1.4769327e-10\n",
      "Iteration 69190: loss = 2.8869964e-11,1.4769128e-10\n",
      "Iteration 69195: loss = 2.8267633e-11,1.4769033e-10\n",
      "Iteration 69200: loss = 2.8871091e-11,1.4768674e-10\n",
      "Iteration 69205: loss = 2.8869886e-11,1.4768586e-10\n",
      "Iteration 69210: loss = 2.8869948e-11,1.4768406e-10\n",
      "Iteration 69215: loss = 2.8267203e-11,1.4768334e-10\n",
      "Iteration 69220: loss = 2.8870434e-11,1.4768005e-10\n",
      "Iteration 69225: loss = 2.8872077e-11,1.4767658e-10\n",
      "Iteration 69230: loss = 2.8869428e-11,1.4767725e-10\n",
      "Iteration 69235: loss = 2.8869693e-11,1.4767519e-10\n",
      "Iteration 69240: loss = 2.886931e-11,1.4767389e-10\n",
      "Iteration 69245: loss = 2.8869523e-11,1.4767243e-10\n",
      "Iteration 69250: loss = 2.886965e-11,1.476699e-10\n",
      "Iteration 69255: loss = 2.8870342e-11,1.4766742e-10\n",
      "Iteration 69260: loss = 2.8266655e-11,1.476676e-10\n",
      "Iteration 69265: loss = 2.8870085e-11,1.4766402e-10\n",
      "Iteration 69270: loss = 2.8870325e-11,1.47662e-10\n",
      "Iteration 69275: loss = 2.8267161e-11,1.476617e-10\n",
      "Iteration 69280: loss = 2.88704e-11,1.4765836e-10\n",
      "Iteration 69285: loss = 2.8870184e-11,1.476568e-10\n",
      "Iteration 69290: loss = 2.8869249e-11,1.4765644e-10\n",
      "Iteration 69295: loss = 2.8870179e-11,1.4765295e-10\n",
      "Iteration 69300: loss = 2.8870207e-11,1.4765109e-10\n",
      "Iteration 69305: loss = 2.8266786e-11,1.4765124e-10\n",
      "Iteration 69310: loss = 2.8869953e-11,1.4764784e-10\n",
      "Iteration 69315: loss = 2.8871553e-11,1.4764462e-10\n",
      "Iteration 69320: loss = 2.8870727e-11,1.4764343e-10\n",
      "Iteration 69325: loss = 2.8868853e-11,1.4764426e-10\n",
      "Iteration 69330: loss = 2.8868808e-11,1.4764155e-10\n",
      "Iteration 69335: loss = 2.8870732e-11,1.4763835e-10\n",
      "Iteration 69340: loss = 2.8870984e-11,1.476359e-10\n",
      "Iteration 69345: loss = 2.8869343e-11,1.4763638e-10\n",
      "Iteration 69350: loss = 2.8869537e-11,1.4763386e-10\n",
      "Iteration 69355: loss = 2.8871266e-11,1.476304e-10\n",
      "Iteration 69360: loss = 2.82668e-11,1.4763132e-10\n",
      "Iteration 69365: loss = 2.8869393e-11,1.476284e-10\n",
      "Iteration 69370: loss = 2.8868935e-11,1.4762777e-10\n",
      "Iteration 69375: loss = 2.8869166e-11,1.4762552e-10\n",
      "Iteration 69380: loss = 2.886942e-11,1.4762302e-10\n",
      "Iteration 69385: loss = 2.8869025e-11,1.4762233e-10\n",
      "Iteration 69390: loss = 2.8869055e-11,1.4761971e-10\n",
      "Iteration 69395: loss = 2.887087e-11,1.4761617e-10\n",
      "Iteration 69400: loss = 2.8869488e-11,1.4761575e-10\n",
      "Iteration 69405: loss = 2.8868965e-11,1.4761488e-10\n",
      "Iteration 69410: loss = 2.8868867e-11,1.4761325e-10\n",
      "Iteration 69415: loss = 2.8869082e-11,1.47611e-10\n",
      "Iteration 69420: loss = 2.887026e-11,1.4760776e-10\n",
      "Iteration 69425: loss = 2.8869865e-11,1.4760648e-10\n",
      "Iteration 69430: loss = 2.8868652e-11,1.476064e-10\n",
      "Iteration 69435: loss = 2.8868958e-11,1.4760403e-10\n",
      "Iteration 69440: loss = 2.8869166e-11,1.476013e-10\n",
      "Iteration 69445: loss = 2.8869677e-11,1.4759993e-10\n",
      "Iteration 69450: loss = 2.8869055e-11,1.4759868e-10\n",
      "Iteration 69455: loss = 2.8869079e-11,1.4759703e-10\n",
      "Iteration 69460: loss = 2.8869055e-11,1.4759506e-10\n",
      "Iteration 69465: loss = 2.8870017e-11,1.4759163e-10\n",
      "Iteration 69470: loss = 2.8870498e-11,1.4758919e-10\n",
      "Iteration 69475: loss = 2.8868371e-11,1.4759041e-10\n",
      "Iteration 69480: loss = 2.8868569e-11,1.4758733e-10\n",
      "Iteration 69485: loss = 2.8870498e-11,1.4758382e-10\n",
      "Iteration 69490: loss = 2.886938e-11,1.4758295e-10\n",
      "Iteration 69495: loss = 2.886807e-11,1.4758306e-10\n",
      "Iteration 69500: loss = 2.8868666e-11,1.4758075e-10\n",
      "Iteration 69505: loss = 2.8869782e-11,1.4757728e-10\n",
      "Iteration 69510: loss = 2.8871195e-11,1.4757429e-10\n",
      "Iteration 69515: loss = 2.8870012e-11,1.475738e-10\n",
      "Iteration 69520: loss = 2.8868694e-11,1.4757362e-10\n",
      "Iteration 69525: loss = 2.8868954e-11,1.475716e-10\n",
      "Iteration 69530: loss = 2.8868803e-11,1.4757011e-10\n",
      "Iteration 69535: loss = 2.8868416e-11,1.4756846e-10\n",
      "Iteration 69540: loss = 2.887003e-11,1.4756522e-10\n",
      "Iteration 69545: loss = 2.8867927e-11,1.475654e-10\n",
      "Iteration 69550: loss = 2.886878e-11,1.4756277e-10\n",
      "Iteration 69555: loss = 2.8869832e-11,1.4755946e-10\n",
      "Iteration 69560: loss = 2.887073e-11,1.4755641e-10\n",
      "Iteration 69565: loss = 2.886917e-11,1.4755705e-10\n",
      "Iteration 69570: loss = 2.8868264e-11,1.4755613e-10\n",
      "Iteration 69575: loss = 2.8869176e-11,1.4755325e-10\n",
      "Iteration 69580: loss = 2.8867653e-11,1.4755278e-10\n",
      "Iteration 69585: loss = 2.8868635e-11,1.475499e-10\n",
      "Iteration 69590: loss = 2.8869088e-11,1.475479e-10\n",
      "Iteration 69595: loss = 2.8868137e-11,1.4754693e-10\n",
      "Iteration 69600: loss = 2.8869906e-11,1.4754357e-10\n",
      "Iteration 69605: loss = 2.8868019e-11,1.475435e-10\n",
      "Iteration 69610: loss = 2.8867816e-11,1.47542e-10\n",
      "Iteration 69615: loss = 2.88694e-11,1.4753831e-10\n",
      "Iteration 69620: loss = 2.887035e-11,1.4753498e-10\n",
      "Iteration 69625: loss = 2.8869802e-11,1.4753364e-10\n",
      "Iteration 69630: loss = 2.886825e-11,1.4753422e-10\n",
      "Iteration 69635: loss = 2.8868116e-11,1.4753256e-10\n",
      "Iteration 69640: loss = 2.8869804e-11,1.4752886e-10\n",
      "Iteration 69645: loss = 2.8868706e-11,1.4752831e-10\n",
      "Iteration 69650: loss = 2.826519e-11,1.4752838e-10\n",
      "Iteration 69655: loss = 2.8868312e-11,1.4752471e-10\n",
      "Iteration 69660: loss = 2.8869832e-11,1.475217e-10\n",
      "Iteration 69665: loss = 2.8870876e-11,1.4751844e-10\n",
      "Iteration 69670: loss = 2.8870859e-11,1.4751657e-10\n",
      "Iteration 69675: loss = 2.886969e-11,1.4751661e-10\n",
      "Iteration 69680: loss = 2.8867913e-11,1.4751621e-10\n",
      "Iteration 69685: loss = 2.8265121e-11,1.475153e-10\n",
      "Iteration 69690: loss = 2.88685e-11,1.47512e-10\n",
      "Iteration 69695: loss = 2.8869898e-11,1.4750877e-10\n",
      "Iteration 69700: loss = 2.8869983e-11,1.4750637e-10\n",
      "Iteration 69705: loss = 2.8869471e-11,1.4750563e-10\n",
      "Iteration 69710: loss = 2.8867705e-11,1.4750531e-10\n",
      "Iteration 69715: loss = 2.8265215e-11,1.4750441e-10\n",
      "Iteration 69720: loss = 2.886827e-11,1.4750137e-10\n",
      "Iteration 69725: loss = 2.8870253e-11,1.4749799e-10\n",
      "Iteration 69730: loss = 2.8870253e-11,1.4749535e-10\n",
      "Iteration 69735: loss = 2.887051e-11,1.4749357e-10\n",
      "Iteration 69740: loss = 2.8869357e-11,1.4749318e-10\n",
      "Iteration 69745: loss = 2.8867781e-11,1.4749274e-10\n",
      "Iteration 69750: loss = 2.8265043e-11,1.474921e-10\n",
      "Iteration 69755: loss = 2.8868132e-11,1.4748866e-10\n",
      "Iteration 69760: loss = 2.8869745e-11,1.4748541e-10\n",
      "Iteration 69765: loss = 2.8869735e-11,1.4748312e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69770: loss = 2.887119e-11,1.4748021e-10\n",
      "Iteration 69775: loss = 2.8870082e-11,1.474799e-10\n",
      "Iteration 69780: loss = 2.886818e-11,1.4747956e-10\n",
      "Iteration 69785: loss = 2.8264946e-11,1.4747939e-10\n",
      "Iteration 69790: loss = 2.886834e-11,1.4747614e-10\n",
      "Iteration 69795: loss = 2.8869906e-11,1.4747259e-10\n",
      "Iteration 69800: loss = 2.8870876e-11,1.4746934e-10\n",
      "Iteration 69805: loss = 2.8871546e-11,1.4746693e-10\n",
      "Iteration 69810: loss = 2.8873232e-11,1.4746337e-10\n",
      "Iteration 69815: loss = 2.887375e-11,1.474613e-10\n",
      "Iteration 69820: loss = 2.8871157e-11,1.4746174e-10\n",
      "Iteration 69825: loss = 2.8869894e-11,1.4746182e-10\n",
      "Iteration 69830: loss = 2.8868337e-11,1.4746149e-10\n",
      "Iteration 69835: loss = 2.826477e-11,1.4746133e-10\n",
      "Iteration 69840: loss = 2.8868151e-11,1.4745816e-10\n",
      "Iteration 69845: loss = 2.8869665e-11,1.4745477e-10\n",
      "Iteration 69850: loss = 2.88708e-11,1.4745137e-10\n",
      "Iteration 69855: loss = 2.887184e-11,1.4744872e-10\n",
      "Iteration 69860: loss = 2.8872973e-11,1.4744576e-10\n",
      "Iteration 69865: loss = 2.8874847e-11,1.4744235e-10\n",
      "Iteration 69870: loss = 2.88757e-11,1.4743895e-10\n",
      "Iteration 69875: loss = 2.8876943e-11,1.4743601e-10\n",
      "Iteration 69880: loss = 2.8878545e-11,1.4743272e-10\n",
      "Iteration 69885: loss = 2.8879643e-11,1.4742946e-10\n",
      "Iteration 69890: loss = 2.8881167e-11,1.4742635e-10\n",
      "Iteration 69895: loss = 2.8882638e-11,1.4742299e-10\n",
      "Iteration 69900: loss = 2.888445e-11,1.4741969e-10\n",
      "Iteration 69905: loss = 2.8884744e-11,1.4741672e-10\n",
      "Iteration 69910: loss = 2.8886579e-11,1.4741347e-10\n",
      "Iteration 69915: loss = 2.8887781e-11,1.4741054e-10\n",
      "Iteration 69920: loss = 2.8888844e-11,1.4740714e-10\n",
      "Iteration 69925: loss = 2.8890173e-11,1.4740432e-10\n",
      "Iteration 69930: loss = 2.889166e-11,1.4740124e-10\n",
      "Iteration 69935: loss = 2.8893143e-11,1.4739761e-10\n",
      "Iteration 69940: loss = 2.889393e-11,1.4739482e-10\n",
      "Iteration 69945: loss = 2.8895122e-11,1.4739217e-10\n",
      "Iteration 69950: loss = 2.8896696e-11,1.4738863e-10\n",
      "Iteration 69955: loss = 2.8897384e-11,1.473856e-10\n",
      "Iteration 69960: loss = 2.8899104e-11,1.4738243e-10\n",
      "Iteration 69965: loss = 2.8900327e-11,1.4737955e-10\n",
      "Iteration 69970: loss = 2.8901792e-11,1.4737633e-10\n",
      "Iteration 69975: loss = 2.8902568e-11,1.4737331e-10\n",
      "Iteration 69980: loss = 2.8904152e-11,1.4737012e-10\n",
      "Iteration 69985: loss = 2.8905487e-11,1.4736683e-10\n",
      "Iteration 69990: loss = 2.8907115e-11,1.4736388e-10\n",
      "Iteration 69995: loss = 2.890777e-11,1.4736082e-10\n",
      "Iteration 70000: loss = 2.890909e-11,1.4735772e-10\n",
      "Iteration 70005: loss = 2.8910754e-11,1.4735466e-10\n",
      "Iteration 70010: loss = 2.8911337e-11,1.4735141e-10\n",
      "Iteration 70015: loss = 2.8912865e-11,1.4734851e-10\n",
      "Iteration 70020: loss = 2.8914137e-11,1.4734546e-10\n",
      "Iteration 70025: loss = 2.8913075e-11,1.4734465e-10\n",
      "Iteration 70030: loss = 2.8913873e-11,1.4734176e-10\n",
      "Iteration 70035: loss = 2.8913297e-11,1.4734039e-10\n",
      "Iteration 70040: loss = 2.8912909e-11,1.473391e-10\n",
      "Iteration 70045: loss = 2.8913675e-11,1.4733648e-10\n",
      "Iteration 70050: loss = 2.8914102e-11,1.4733481e-10\n",
      "Iteration 70055: loss = 2.8913405e-11,1.4733315e-10\n",
      "Iteration 70060: loss = 2.8912836e-11,1.4733187e-10\n",
      "Iteration 70065: loss = 2.8912428e-11,1.4733052e-10\n",
      "Iteration 70070: loss = 2.8913434e-11,1.4732783e-10\n",
      "Iteration 70075: loss = 2.8913706e-11,1.4732562e-10\n",
      "Iteration 70080: loss = 2.891197e-11,1.4732555e-10\n",
      "Iteration 70085: loss = 2.8912045e-11,1.4732357e-10\n",
      "Iteration 70090: loss = 2.8912575e-11,1.4732125e-10\n",
      "Iteration 70095: loss = 2.891301e-11,1.4731923e-10\n",
      "Iteration 70100: loss = 2.8913328e-11,1.4731737e-10\n",
      "Iteration 70105: loss = 2.8913634e-11,1.473154e-10\n",
      "Iteration 70110: loss = 2.8911767e-11,1.4731517e-10\n",
      "Iteration 70115: loss = 2.890996e-11,1.4731505e-10\n",
      "Iteration 70120: loss = 2.8910756e-11,1.4731254e-10\n",
      "Iteration 70125: loss = 2.8912284e-11,1.4730915e-10\n",
      "Iteration 70130: loss = 2.891389e-11,1.4730603e-10\n",
      "Iteration 70135: loss = 2.8911975e-11,1.4730585e-10\n",
      "Iteration 70140: loss = 2.8913746e-11,1.4730248e-10\n",
      "Iteration 70145: loss = 2.8913143e-11,1.4730114e-10\n",
      "Iteration 70150: loss = 2.8912739e-11,1.472996e-10\n",
      "Iteration 70155: loss = 2.891329e-11,1.4729759e-10\n",
      "Iteration 70160: loss = 2.8913386e-11,1.4729575e-10\n",
      "Iteration 70165: loss = 2.8911366e-11,1.4729565e-10\n",
      "Iteration 70170: loss = 2.8912234e-11,1.4729323e-10\n",
      "Iteration 70175: loss = 2.8912725e-11,1.4729112e-10\n",
      "Iteration 70180: loss = 2.8912544e-11,1.4728886e-10\n",
      "Iteration 70185: loss = 2.8913481e-11,1.4728656e-10\n",
      "Iteration 70190: loss = 2.8911505e-11,1.4728624e-10\n",
      "Iteration 70195: loss = 2.8911075e-11,1.4728552e-10\n",
      "Iteration 70200: loss = 2.8911878e-11,1.4728244e-10\n",
      "Iteration 70205: loss = 2.8912232e-11,1.4728008e-10\n",
      "Iteration 70210: loss = 2.8911701e-11,1.4727954e-10\n",
      "Iteration 70215: loss = 2.8912693e-11,1.4727611e-10\n",
      "Iteration 70220: loss = 2.8912083e-11,1.4727469e-10\n",
      "Iteration 70225: loss = 2.891222e-11,1.4727329e-10\n",
      "Iteration 70230: loss = 2.8912256e-11,1.4727114e-10\n",
      "Iteration 70235: loss = 2.8913259e-11,1.4726845e-10\n",
      "Iteration 70240: loss = 2.8911559e-11,1.4726839e-10\n",
      "Iteration 70245: loss = 2.8911698e-11,1.4726685e-10\n",
      "Iteration 70250: loss = 2.8912478e-11,1.4726369e-10\n",
      "Iteration 70255: loss = 2.8911581e-11,1.4726276e-10\n",
      "Iteration 70260: loss = 2.8911767e-11,1.472613e-10\n",
      "Iteration 70265: loss = 2.891202e-11,1.4725873e-10\n",
      "Iteration 70270: loss = 2.8912461e-11,1.4725655e-10\n",
      "Iteration 70275: loss = 2.8913259e-11,1.4725418e-10\n",
      "Iteration 70280: loss = 2.8911568e-11,1.4725399e-10\n",
      "Iteration 70285: loss = 2.89103e-11,1.4725327e-10\n",
      "Iteration 70290: loss = 2.891202e-11,1.472501e-10\n",
      "Iteration 70295: loss = 2.8912739e-11,1.4724742e-10\n",
      "Iteration 70300: loss = 2.8911464e-11,1.4724685e-10\n",
      "Iteration 70305: loss = 2.8912437e-11,1.4724394e-10\n",
      "Iteration 70310: loss = 2.891263e-11,1.4724193e-10\n",
      "Iteration 70315: loss = 2.8913011e-11,1.4724003e-10\n",
      "Iteration 70320: loss = 2.8911084e-11,1.4723978e-10\n",
      "Iteration 70325: loss = 2.8911843e-11,1.4723753e-10\n",
      "Iteration 70330: loss = 2.8912414e-11,1.472355e-10\n",
      "Iteration 70335: loss = 2.8913108e-11,1.4723306e-10\n",
      "Iteration 70340: loss = 2.8911212e-11,1.4723288e-10\n",
      "Iteration 70345: loss = 2.8910659e-11,1.4723157e-10\n",
      "Iteration 70350: loss = 2.8912372e-11,1.4722809e-10\n",
      "Iteration 70355: loss = 2.8912229e-11,1.4722656e-10\n",
      "Iteration 70360: loss = 2.8912607e-11,1.4722422e-10\n",
      "Iteration 70365: loss = 2.8910653e-11,1.4722407e-10\n",
      "Iteration 70370: loss = 2.8910515e-11,1.4722254e-10\n",
      "Iteration 70375: loss = 2.8911054e-11,1.4722032e-10\n",
      "Iteration 70380: loss = 2.8911503e-11,1.4721832e-10\n",
      "Iteration 70385: loss = 2.8911299e-11,1.4721618e-10\n",
      "Iteration 70390: loss = 2.8912804e-11,1.4721316e-10\n",
      "Iteration 70395: loss = 2.89116e-11,1.4721302e-10\n",
      "Iteration 70400: loss = 2.891059e-11,1.4721215e-10\n",
      "Iteration 70405: loss = 2.891201e-11,1.4720905e-10\n",
      "Iteration 70410: loss = 2.8911465e-11,1.4720695e-10\n",
      "Iteration 70415: loss = 2.8911104e-11,1.4720633e-10\n",
      "Iteration 70420: loss = 2.8911864e-11,1.4720347e-10\n",
      "Iteration 70425: loss = 2.891224e-11,1.4720142e-10\n",
      "Iteration 70430: loss = 2.891038e-11,1.4720136e-10\n",
      "Iteration 70435: loss = 2.8911184e-11,1.471988e-10\n",
      "Iteration 70440: loss = 2.8911928e-11,1.4719662e-10\n",
      "Iteration 70445: loss = 2.8911956e-11,1.4719424e-10\n",
      "Iteration 70450: loss = 2.8911387e-11,1.4719351e-10\n",
      "Iteration 70455: loss = 2.8911531e-11,1.4719084e-10\n",
      "Iteration 70460: loss = 2.8911875e-11,1.4718898e-10\n",
      "Iteration 70465: loss = 2.891135e-11,1.4718747e-10\n",
      "Iteration 70470: loss = 2.8911644e-11,1.471862e-10\n",
      "Iteration 70475: loss = 2.8911838e-11,1.4718396e-10\n",
      "Iteration 70480: loss = 2.8911906e-11,1.4718174e-10\n",
      "Iteration 70485: loss = 2.8910893e-11,1.4718095e-10\n",
      "Iteration 70490: loss = 2.8911729e-11,1.4717852e-10\n",
      "Iteration 70495: loss = 2.8911547e-11,1.4717655e-10\n",
      "Iteration 70500: loss = 2.8911882e-11,1.4717427e-10\n",
      "Iteration 70505: loss = 2.8911188e-11,1.4717295e-10\n",
      "Iteration 70510: loss = 2.8911012e-11,1.4717161e-10\n",
      "Iteration 70515: loss = 2.8910402e-11,1.4717087e-10\n",
      "Iteration 70520: loss = 2.8911198e-11,1.4716761e-10\n",
      "Iteration 70525: loss = 2.8911625e-11,1.4716539e-10\n",
      "Iteration 70530: loss = 2.8911049e-11,1.4716406e-10\n",
      "Iteration 70535: loss = 2.8911443e-11,1.4716205e-10\n",
      "Iteration 70540: loss = 2.8910771e-11,1.4716073e-10\n",
      "Iteration 70545: loss = 2.891123e-11,1.471592e-10\n",
      "Iteration 70550: loss = 2.891129e-11,1.4715675e-10\n",
      "Iteration 70555: loss = 2.8911649e-11,1.4715476e-10\n",
      "Iteration 70560: loss = 2.8911118e-11,1.4715335e-10\n",
      "Iteration 70565: loss = 2.8910582e-11,1.4715194e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70570: loss = 2.8910931e-11,1.4715058e-10\n",
      "Iteration 70575: loss = 2.8911262e-11,1.471481e-10\n",
      "Iteration 70580: loss = 2.8911784e-11,1.4714593e-10\n",
      "Iteration 70585: loss = 2.8910608e-11,1.4714513e-10\n",
      "Iteration 70590: loss = 2.8910313e-11,1.4714355e-10\n",
      "Iteration 70595: loss = 2.8910353e-11,1.4714158e-10\n",
      "Iteration 70600: loss = 2.8910591e-11,1.4713952e-10\n",
      "Iteration 70605: loss = 2.8911665e-11,1.4713697e-10\n",
      "Iteration 70610: loss = 2.8910345e-11,1.4713603e-10\n",
      "Iteration 70615: loss = 2.8910756e-11,1.4713394e-10\n",
      "Iteration 70620: loss = 2.8910886e-11,1.4713188e-10\n",
      "Iteration 70625: loss = 2.8911087e-11,1.471303e-10\n",
      "Iteration 70630: loss = 2.8910235e-11,1.4712953e-10\n",
      "Iteration 70635: loss = 2.890998e-11,1.4712749e-10\n",
      "Iteration 70640: loss = 2.891052e-11,1.4712526e-10\n",
      "Iteration 70645: loss = 2.8911038e-11,1.4712309e-10\n",
      "Iteration 70650: loss = 2.891152e-11,1.4712104e-10\n",
      "Iteration 70655: loss = 2.891125e-11,1.4711973e-10\n",
      "Iteration 70660: loss = 2.8909973e-11,1.4711891e-10\n",
      "Iteration 70665: loss = 2.8911547e-11,1.4711565e-10\n",
      "Iteration 70670: loss = 2.8909783e-11,1.4711547e-10\n",
      "Iteration 70675: loss = 2.8910685e-11,1.4711278e-10\n",
      "Iteration 70680: loss = 2.8911701e-11,1.4711016e-10\n",
      "Iteration 70685: loss = 2.8910525e-11,1.4710934e-10\n",
      "Iteration 70690: loss = 2.8910917e-11,1.4710752e-10\n",
      "Iteration 70695: loss = 2.8911049e-11,1.4710529e-10\n",
      "Iteration 70700: loss = 2.8910463e-11,1.4710375e-10\n",
      "Iteration 70705: loss = 2.891025e-11,1.4710227e-10\n",
      "Iteration 70710: loss = 2.890973e-11,1.471008e-10\n",
      "Iteration 70715: loss = 2.891024e-11,1.470988e-10\n",
      "Iteration 70720: loss = 2.8910804e-11,1.4709654e-10\n",
      "Iteration 70725: loss = 2.8911547e-11,1.4709428e-10\n",
      "Iteration 70730: loss = 2.8911281e-11,1.470929e-10\n",
      "Iteration 70735: loss = 2.8910032e-11,1.4709206e-10\n",
      "Iteration 70740: loss = 2.8911123e-11,1.4708933e-10\n",
      "Iteration 70745: loss = 2.8909236e-11,1.4708898e-10\n",
      "Iteration 70750: loss = 2.8910693e-11,1.4708582e-10\n",
      "Iteration 70755: loss = 2.8909842e-11,1.470851e-10\n",
      "Iteration 70760: loss = 2.8910645e-11,1.4708229e-10\n",
      "Iteration 70765: loss = 2.8910978e-11,1.4708032e-10\n",
      "Iteration 70770: loss = 2.8910941e-11,1.4707842e-10\n",
      "Iteration 70775: loss = 2.890972e-11,1.4707785e-10\n",
      "Iteration 70780: loss = 2.8910064e-11,1.4707555e-10\n",
      "Iteration 70785: loss = 2.8910818e-11,1.470733e-10\n",
      "Iteration 70790: loss = 2.8911354e-11,1.4707102e-10\n",
      "Iteration 70795: loss = 2.8911129e-11,1.470696e-10\n",
      "Iteration 70800: loss = 2.8909227e-11,1.4706956e-10\n",
      "Iteration 70805: loss = 2.8909533e-11,1.4706718e-10\n",
      "Iteration 70810: loss = 2.8910875e-11,1.4706412e-10\n",
      "Iteration 70815: loss = 2.891029e-11,1.4706283e-10\n",
      "Iteration 70820: loss = 2.891049e-11,1.4706095e-10\n",
      "Iteration 70825: loss = 2.8909567e-11,1.4706004e-10\n",
      "Iteration 70830: loss = 2.8910813e-11,1.4705684e-10\n",
      "Iteration 70835: loss = 2.8909921e-11,1.4705598e-10\n",
      "Iteration 70840: loss = 2.8910055e-11,1.4705406e-10\n",
      "Iteration 70845: loss = 2.8910686e-11,1.4705187e-10\n",
      "Iteration 70850: loss = 2.8910799e-11,1.4704989e-10\n",
      "Iteration 70855: loss = 2.8908992e-11,1.4704976e-10\n",
      "Iteration 70860: loss = 2.8909855e-11,1.470471e-10\n",
      "Iteration 70865: loss = 2.891061e-11,1.4704442e-10\n",
      "Iteration 70870: loss = 2.8910777e-11,1.4704266e-10\n",
      "Iteration 70875: loss = 2.8910254e-11,1.4704125e-10\n",
      "Iteration 70880: loss = 2.8909306e-11,1.470404e-10\n",
      "Iteration 70885: loss = 2.8909555e-11,1.470383e-10\n",
      "Iteration 70890: loss = 2.8910289e-11,1.4703586e-10\n",
      "Iteration 70895: loss = 2.8910437e-11,1.4703391e-10\n",
      "Iteration 70900: loss = 2.8910005e-11,1.4703262e-10\n",
      "Iteration 70905: loss = 2.8911082e-11,1.4702983e-10\n",
      "Iteration 70910: loss = 2.890922e-11,1.4702958e-10\n",
      "Iteration 70915: loss = 2.890877e-11,1.4702814e-10\n",
      "Iteration 70920: loss = 2.8909519e-11,1.4702574e-10\n",
      "Iteration 70925: loss = 2.891009e-11,1.4702359e-10\n",
      "Iteration 70930: loss = 2.8910785e-11,1.4702127e-10\n",
      "Iteration 70935: loss = 2.8909887e-11,1.4702031e-10\n",
      "Iteration 70940: loss = 2.891073e-11,1.4701773e-10\n",
      "Iteration 70945: loss = 2.8909845e-11,1.4701701e-10\n",
      "Iteration 70950: loss = 2.8910013e-11,1.4701489e-10\n",
      "Iteration 70955: loss = 2.890946e-11,1.4701312e-10\n",
      "Iteration 70960: loss = 2.8911271e-11,1.4700986e-10\n",
      "Iteration 70965: loss = 2.8909541e-11,1.4700963e-10\n",
      "Iteration 70970: loss = 2.8910513e-11,1.4700684e-10\n",
      "Iteration 70975: loss = 2.8910098e-11,1.4700531e-10\n",
      "Iteration 70980: loss = 2.8909103e-11,1.4700445e-10\n",
      "Iteration 70985: loss = 2.890953e-11,1.4700222e-10\n",
      "Iteration 70990: loss = 2.8910112e-11,1.470001e-10\n",
      "Iteration 70995: loss = 2.8910352e-11,1.4699819e-10\n",
      "Iteration 71000: loss = 2.8908502e-11,1.4699791e-10\n",
      "Iteration 71005: loss = 2.8910057e-11,1.4699496e-10\n",
      "Iteration 71010: loss = 2.8909979e-11,1.4699336e-10\n",
      "Iteration 71015: loss = 2.8909682e-11,1.4699171e-10\n",
      "Iteration 71020: loss = 2.8910284e-11,1.4698923e-10\n",
      "Iteration 71025: loss = 2.8908688e-11,1.4698912e-10\n",
      "Iteration 71030: loss = 2.8909817e-11,1.469863e-10\n",
      "Iteration 71035: loss = 2.8910081e-11,1.4698397e-10\n",
      "Iteration 71040: loss = 2.8909241e-11,1.4698313e-10\n",
      "Iteration 71045: loss = 2.8909423e-11,1.4698098e-10\n",
      "Iteration 71050: loss = 2.8909876e-11,1.4697897e-10\n",
      "Iteration 71055: loss = 2.8910116e-11,1.4697711e-10\n",
      "Iteration 71060: loss = 2.890827e-11,1.4697675e-10\n",
      "Iteration 71065: loss = 2.8910032e-11,1.4697359e-10\n",
      "Iteration 71070: loss = 2.890894e-11,1.4697263e-10\n",
      "Iteration 71075: loss = 2.890953e-11,1.4697028e-10\n",
      "Iteration 71080: loss = 2.8909439e-11,1.4696847e-10\n",
      "Iteration 71085: loss = 2.8908625e-11,1.469675e-10\n",
      "Iteration 71090: loss = 2.8908959e-11,1.4696519e-10\n",
      "Iteration 71095: loss = 2.890952e-11,1.4696323e-10\n",
      "Iteration 71100: loss = 2.89103e-11,1.4696087e-10\n",
      "Iteration 71105: loss = 2.8909389e-11,1.4695999e-10\n",
      "Iteration 71110: loss = 2.8909562e-11,1.4695789e-10\n",
      "Iteration 71115: loss = 2.8909911e-11,1.4695578e-10\n",
      "Iteration 71120: loss = 2.8908783e-11,1.46955e-10\n",
      "Iteration 71125: loss = 2.8910025e-11,1.4695187e-10\n",
      "Iteration 71130: loss = 2.8908478e-11,1.4695165e-10\n",
      "Iteration 71135: loss = 2.8908756e-11,1.469495e-10\n",
      "Iteration 71140: loss = 2.8909255e-11,1.4694715e-10\n",
      "Iteration 71145: loss = 2.8909762e-11,1.4694512e-10\n",
      "Iteration 71150: loss = 2.8910166e-11,1.4694314e-10\n",
      "Iteration 71155: loss = 2.8908398e-11,1.4694276e-10\n",
      "Iteration 71160: loss = 2.8908174e-11,1.4694121e-10\n",
      "Iteration 71165: loss = 2.890972e-11,1.469379e-10\n",
      "Iteration 71170: loss = 2.8908818e-11,1.4693721e-10\n",
      "Iteration 71175: loss = 2.8909035e-11,1.4693495e-10\n",
      "Iteration 71180: loss = 2.8909255e-11,1.469329e-10\n",
      "Iteration 71185: loss = 2.890907e-11,1.4693141e-10\n",
      "Iteration 71190: loss = 2.8908464e-11,1.4692983e-10\n",
      "Iteration 71195: loss = 2.890912e-11,1.469276e-10\n",
      "Iteration 71200: loss = 2.8908468e-11,1.4692633e-10\n",
      "Iteration 71205: loss = 2.8908972e-11,1.4692401e-10\n",
      "Iteration 71210: loss = 2.8908638e-11,1.4692245e-10\n",
      "Iteration 71215: loss = 2.8908964e-11,1.4692045e-10\n",
      "Iteration 71220: loss = 2.8909293e-11,1.4691837e-10\n",
      "Iteration 71225: loss = 2.8908613e-11,1.469169e-10\n",
      "Iteration 71230: loss = 2.8908412e-11,1.4691556e-10\n",
      "Iteration 71235: loss = 2.8908884e-11,1.4691345e-10\n",
      "Iteration 71240: loss = 2.8909418e-11,1.4691143e-10\n",
      "Iteration 71245: loss = 2.8909375e-11,1.4690935e-10\n",
      "Iteration 71250: loss = 2.8907828e-11,1.4690921e-10\n",
      "Iteration 71255: loss = 2.890882e-11,1.4690613e-10\n",
      "Iteration 71260: loss = 2.8910123e-11,1.4690338e-10\n",
      "Iteration 71265: loss = 2.89086e-11,1.4690313e-10\n",
      "Iteration 71270: loss = 2.8908773e-11,1.4690095e-10\n",
      "Iteration 71275: loss = 2.8908287e-11,1.4689953e-10\n",
      "Iteration 71280: loss = 2.8908499e-11,1.468978e-10\n",
      "Iteration 71285: loss = 2.8909314e-11,1.4689534e-10\n",
      "Iteration 71290: loss = 2.8908187e-11,1.4689445e-10\n",
      "Iteration 71295: loss = 2.8909113e-11,1.4689198e-10\n",
      "Iteration 71300: loss = 2.8909023e-11,1.4688992e-10\n",
      "Iteration 71305: loss = 2.8908863e-11,1.4688847e-10\n",
      "Iteration 71310: loss = 2.8908433e-11,1.4688686e-10\n",
      "Iteration 71315: loss = 2.8908246e-11,1.4688535e-10\n",
      "Iteration 71320: loss = 2.8908258e-11,1.46884e-10\n",
      "Iteration 71325: loss = 2.8908103e-11,1.4688188e-10\n",
      "Iteration 71330: loss = 2.8908565e-11,1.4687958e-10\n",
      "Iteration 71335: loss = 2.8909411e-11,1.4687734e-10\n",
      "Iteration 71340: loss = 2.89085e-11,1.4687637e-10\n",
      "Iteration 71345: loss = 2.8909338e-11,1.4687378e-10\n",
      "Iteration 71350: loss = 2.8907725e-11,1.4687339e-10\n",
      "Iteration 71355: loss = 2.8908709e-11,1.4687102e-10\n",
      "Iteration 71360: loss = 2.8908912e-11,1.4686877e-10\n",
      "Iteration 71365: loss = 2.8908024e-11,1.4686785e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71370: loss = 2.8908943e-11,1.4686515e-10\n",
      "Iteration 71375: loss = 2.8908037e-11,1.4686434e-10\n",
      "Iteration 71380: loss = 2.8908006e-11,1.4686241e-10\n",
      "Iteration 71385: loss = 2.8908598e-11,1.4686018e-10\n",
      "Iteration 71390: loss = 2.890925e-11,1.4685785e-10\n",
      "Iteration 71395: loss = 2.8908076e-11,1.4685678e-10\n",
      "Iteration 71400: loss = 2.8908551e-11,1.4685497e-10\n",
      "Iteration 71405: loss = 2.8908794e-11,1.4685275e-10\n",
      "Iteration 71410: loss = 2.890796e-11,1.468517e-10\n",
      "Iteration 71415: loss = 2.8907923e-11,1.4685045e-10\n",
      "Iteration 71420: loss = 2.8907807e-11,1.4684823e-10\n",
      "Iteration 71425: loss = 2.890828e-11,1.4684616e-10\n",
      "Iteration 71430: loss = 2.8909097e-11,1.4684387e-10\n",
      "Iteration 71435: loss = 2.8908187e-11,1.4684302e-10\n",
      "Iteration 71440: loss = 2.8909186e-11,1.4684007e-10\n",
      "Iteration 71445: loss = 2.8908249e-11,1.4683912e-10\n",
      "Iteration 71450: loss = 2.8908447e-11,1.4683683e-10\n",
      "Iteration 71455: loss = 2.8907375e-11,1.4683613e-10\n",
      "Iteration 71460: loss = 2.8908447e-11,1.4683343e-10\n",
      "Iteration 71465: loss = 2.8907416e-11,1.4683244e-10\n",
      "Iteration 71470: loss = 2.8908079e-11,1.4683e-10\n",
      "Iteration 71475: loss = 2.890857e-11,1.4682787e-10\n",
      "Iteration 71480: loss = 2.8907475e-11,1.4682702e-10\n",
      "Iteration 71485: loss = 2.8907704e-11,1.4682532e-10\n",
      "Iteration 71490: loss = 2.890816e-11,1.468231e-10\n",
      "Iteration 71495: loss = 2.8909533e-11,1.4682013e-10\n",
      "Iteration 71500: loss = 2.890766e-11,1.4681964e-10\n",
      "Iteration 71505: loss = 2.890798e-11,1.4681777e-10\n",
      "Iteration 71510: loss = 2.8908598e-11,1.4681556e-10\n",
      "Iteration 71515: loss = 2.890732e-11,1.4681475e-10\n",
      "Iteration 71520: loss = 2.890853e-11,1.4681167e-10\n",
      "Iteration 71525: loss = 2.8907418e-11,1.4681104e-10\n",
      "Iteration 71530: loss = 2.8907945e-11,1.4680865e-10\n",
      "Iteration 71535: loss = 2.8907975e-11,1.4680684e-10\n",
      "Iteration 71540: loss = 2.890794e-11,1.4680519e-10\n",
      "Iteration 71545: loss = 2.8907926e-11,1.4680386e-10\n",
      "Iteration 71550: loss = 2.8907687e-11,1.4680172e-10\n",
      "Iteration 71555: loss = 2.8908757e-11,1.4679913e-10\n",
      "Iteration 71560: loss = 2.8907618e-11,1.4679881e-10\n",
      "Iteration 71565: loss = 2.8907933e-11,1.4679626e-10\n",
      "Iteration 71570: loss = 2.8908221e-11,1.467948e-10\n",
      "Iteration 71575: loss = 2.8908024e-11,1.4679329e-10\n",
      "Iteration 71580: loss = 2.890778e-11,1.4679089e-10\n",
      "Iteration 71585: loss = 2.8907656e-11,1.4678997e-10\n",
      "Iteration 71590: loss = 2.8908041e-11,1.4678787e-10\n",
      "Iteration 71595: loss = 2.890789e-11,1.4678558e-10\n",
      "Iteration 71600: loss = 2.89076e-11,1.4678406e-10\n",
      "Iteration 71605: loss = 2.890742e-11,1.4678253e-10\n",
      "Iteration 71610: loss = 2.8907625e-11,1.4678056e-10\n",
      "Iteration 71615: loss = 2.8908173e-11,1.4677837e-10\n",
      "Iteration 71620: loss = 2.8908445e-11,1.4677676e-10\n",
      "Iteration 71625: loss = 2.8908211e-11,1.4677513e-10\n",
      "Iteration 71630: loss = 2.890741e-11,1.4677408e-10\n",
      "Iteration 71635: loss = 2.8907139e-11,1.4677183e-10\n",
      "Iteration 71640: loss = 2.8907656e-11,1.4676976e-10\n",
      "Iteration 71645: loss = 2.8908383e-11,1.467673e-10\n",
      "Iteration 71650: loss = 2.8906539e-11,1.4676707e-10\n",
      "Iteration 71655: loss = 2.890837e-11,1.4676343e-10\n",
      "Iteration 71660: loss = 2.8907205e-11,1.4676267e-10\n",
      "Iteration 71665: loss = 2.8907815e-11,1.4676092e-10\n",
      "Iteration 71670: loss = 2.890762e-11,1.4675866e-10\n",
      "Iteration 71675: loss = 2.8907666e-11,1.4675702e-10\n",
      "Iteration 71680: loss = 2.8908114e-11,1.4675479e-10\n",
      "Iteration 71685: loss = 2.8906983e-11,1.4675382e-10\n",
      "Iteration 71690: loss = 2.8907548e-11,1.4675251e-10\n",
      "Iteration 71695: loss = 2.890743e-11,1.4675024e-10\n",
      "Iteration 71700: loss = 2.8907767e-11,1.4674821e-10\n",
      "Iteration 71705: loss = 2.8906738e-11,1.4674725e-10\n",
      "Iteration 71710: loss = 2.8907357e-11,1.4674495e-10\n",
      "Iteration 71715: loss = 2.890719e-11,1.4674398e-10\n",
      "Iteration 71720: loss = 2.8907168e-11,1.4674154e-10\n",
      "Iteration 71725: loss = 2.8907682e-11,1.4673936e-10\n",
      "Iteration 71730: loss = 2.890724e-11,1.4673786e-10\n",
      "Iteration 71735: loss = 2.8907496e-11,1.4673576e-10\n",
      "Iteration 71740: loss = 2.8907328e-11,1.4673428e-10\n",
      "Iteration 71745: loss = 2.8906827e-11,1.4673246e-10\n",
      "Iteration 71750: loss = 2.8907418e-11,1.467309e-10\n",
      "Iteration 71755: loss = 2.8907139e-11,1.4672898e-10\n",
      "Iteration 71760: loss = 2.890801e-11,1.4672635e-10\n",
      "Iteration 71765: loss = 2.8907108e-11,1.4672619e-10\n",
      "Iteration 71770: loss = 2.8907128e-11,1.467237e-10\n",
      "Iteration 71775: loss = 2.8907621e-11,1.467218e-10\n",
      "Iteration 71780: loss = 2.890795e-11,1.467198e-10\n",
      "Iteration 71785: loss = 2.8908426e-11,1.4671767e-10\n",
      "Iteration 71790: loss = 2.8907375e-11,1.4671668e-10\n",
      "Iteration 71795: loss = 2.8907063e-11,1.4671518e-10\n",
      "Iteration 71800: loss = 2.8907016e-11,1.4671275e-10\n",
      "Iteration 71805: loss = 2.8907425e-11,1.4671064e-10\n",
      "Iteration 71810: loss = 2.8907066e-11,1.4670914e-10\n",
      "Iteration 71815: loss = 2.8907527e-11,1.4670756e-10\n",
      "Iteration 71820: loss = 2.890721e-11,1.467061e-10\n",
      "Iteration 71825: loss = 2.8907021e-11,1.4670387e-10\n",
      "Iteration 71830: loss = 2.890705e-11,1.4670223e-10\n",
      "Iteration 71835: loss = 2.8907344e-11,1.4670071e-10\n",
      "Iteration 71840: loss = 2.8907895e-11,1.4669843e-10\n",
      "Iteration 71845: loss = 2.8906724e-11,1.466976e-10\n",
      "Iteration 71850: loss = 2.8907736e-11,1.4669481e-10\n",
      "Iteration 71855: loss = 2.8907057e-11,1.4669337e-10\n",
      "Iteration 71860: loss = 2.8907541e-11,1.4669135e-10\n",
      "Iteration 71865: loss = 2.8907099e-11,1.4668984e-10\n",
      "Iteration 71870: loss = 2.8907108e-11,1.4668818e-10\n",
      "Iteration 71875: loss = 2.8907139e-11,1.466862e-10\n",
      "Iteration 71880: loss = 2.890699e-11,1.4668472e-10\n",
      "Iteration 71885: loss = 2.890681e-11,1.4668244e-10\n",
      "Iteration 71890: loss = 2.8907045e-11,1.4668064e-10\n",
      "Iteration 71895: loss = 2.8906606e-11,1.4667974e-10\n",
      "Iteration 71900: loss = 2.8907198e-11,1.4667753e-10\n",
      "Iteration 71905: loss = 2.890751e-11,1.4667527e-10\n",
      "Iteration 71910: loss = 2.8907142e-11,1.4667378e-10\n",
      "Iteration 71915: loss = 2.8906719e-11,1.466724e-10\n",
      "Iteration 71920: loss = 2.8906504e-11,1.4667079e-10\n",
      "Iteration 71925: loss = 2.8907014e-11,1.466688e-10\n",
      "Iteration 71930: loss = 2.8907215e-11,1.4666639e-10\n",
      "Iteration 71935: loss = 2.8906757e-11,1.4666539e-10\n",
      "Iteration 71940: loss = 2.8907376e-11,1.4666326e-10\n",
      "Iteration 71945: loss = 2.8907626e-11,1.4666109e-10\n",
      "Iteration 71950: loss = 2.8906754e-11,1.466601e-10\n",
      "Iteration 71955: loss = 2.8907175e-11,1.4665769e-10\n",
      "Iteration 71960: loss = 2.8907118e-11,1.4665635e-10\n",
      "Iteration 71965: loss = 2.8907163e-11,1.4665416e-10\n",
      "Iteration 71970: loss = 2.8906282e-11,1.4665322e-10\n",
      "Iteration 71975: loss = 2.8907087e-11,1.4665097e-10\n",
      "Iteration 71980: loss = 2.8906608e-11,1.4664937e-10\n",
      "Iteration 71985: loss = 2.8906794e-11,1.4664744e-10\n",
      "Iteration 71990: loss = 2.8907243e-11,1.4664536e-10\n",
      "Iteration 71995: loss = 2.8906872e-11,1.4664386e-10\n",
      "Iteration 72000: loss = 2.8906688e-11,1.4664225e-10\n",
      "Iteration 72005: loss = 2.8906317e-11,1.4664084e-10\n",
      "Iteration 72010: loss = 2.89065e-11,1.4663892e-10\n",
      "Iteration 72015: loss = 2.8907335e-11,1.4663659e-10\n",
      "Iteration 72020: loss = 2.8906542e-11,1.4663556e-10\n",
      "Iteration 72025: loss = 2.8907035e-11,1.4663316e-10\n",
      "Iteration 72030: loss = 2.8906872e-11,1.4663157e-10\n",
      "Iteration 72035: loss = 2.8906863e-11,1.4662971e-10\n",
      "Iteration 72040: loss = 2.89065e-11,1.4662847e-10\n",
      "Iteration 72045: loss = 2.8906122e-11,1.466268e-10\n",
      "Iteration 72050: loss = 2.8906197e-11,1.4662503e-10\n",
      "Iteration 72055: loss = 2.8906799e-11,1.4662291e-10\n",
      "Iteration 72060: loss = 2.8906365e-11,1.4662134e-10\n",
      "Iteration 72065: loss = 2.890569e-11,1.4661991e-10\n",
      "Iteration 72070: loss = 2.8906285e-11,1.4661787e-10\n",
      "Iteration 72075: loss = 2.8906063e-11,1.4661572e-10\n",
      "Iteration 72080: loss = 2.8906736e-11,1.466134e-10\n",
      "Iteration 72085: loss = 2.8905706e-11,1.4661301e-10\n",
      "Iteration 72090: loss = 2.8906927e-11,1.4661006e-10\n",
      "Iteration 72095: loss = 2.8905782e-11,1.4660943e-10\n",
      "Iteration 72100: loss = 2.890642e-11,1.4660723e-10\n",
      "Iteration 72105: loss = 2.8906625e-11,1.4660478e-10\n",
      "Iteration 72110: loss = 2.8906174e-11,1.4660374e-10\n",
      "Iteration 72115: loss = 2.890607e-11,1.4660217e-10\n",
      "Iteration 72120: loss = 2.8905905e-11,1.4659989e-10\n",
      "Iteration 72125: loss = 2.8906336e-11,1.4659776e-10\n",
      "Iteration 72130: loss = 2.8905512e-11,1.4659743e-10\n",
      "Iteration 72135: loss = 2.8906414e-11,1.4659375e-10\n",
      "Iteration 72140: loss = 2.8906337e-11,1.4659282e-10\n",
      "Iteration 72145: loss = 2.8906667e-11,1.4659005e-10\n",
      "Iteration 72150: loss = 2.8905529e-11,1.4658993e-10\n",
      "Iteration 72155: loss = 2.8905945e-11,1.4658749e-10\n",
      "Iteration 72160: loss = 2.8905848e-11,1.4658524e-10\n",
      "Iteration 72165: loss = 2.890615e-11,1.4658336e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72170: loss = 2.8905753e-11,1.4658233e-10\n",
      "Iteration 72175: loss = 2.8906216e-11,1.4658014e-10\n",
      "Iteration 72180: loss = 2.8905933e-11,1.4657868e-10\n",
      "Iteration 72185: loss = 2.8906509e-11,1.4657653e-10\n",
      "Iteration 72190: loss = 2.8905356e-11,1.4657567e-10\n",
      "Iteration 72195: loss = 2.8906391e-11,1.4657273e-10\n",
      "Iteration 72200: loss = 2.8904762e-11,1.4657255e-10\n",
      "Iteration 72205: loss = 2.8905734e-11,1.4656995e-10\n",
      "Iteration 72210: loss = 2.8906327e-11,1.465678e-10\n",
      "Iteration 72215: loss = 2.8905593e-11,1.4656637e-10\n",
      "Iteration 72220: loss = 2.8906641e-11,1.4656393e-10\n",
      "Iteration 72225: loss = 2.8905552e-11,1.4656311e-10\n",
      "Iteration 72230: loss = 2.8905415e-11,1.4656154e-10\n",
      "Iteration 72235: loss = 2.8905931e-11,1.4655904e-10\n",
      "Iteration 72240: loss = 2.890572e-11,1.4655749e-10\n",
      "Iteration 72245: loss = 2.8905094e-11,1.4655632e-10\n",
      "Iteration 72250: loss = 2.890573e-11,1.465541e-10\n",
      "Iteration 72255: loss = 2.8906232e-11,1.4655215e-10\n",
      "Iteration 72260: loss = 2.8905989e-11,1.4655041e-10\n",
      "Iteration 72265: loss = 2.89066e-11,1.4654808e-10\n",
      "Iteration 72270: loss = 2.8905453e-11,1.4654716e-10\n",
      "Iteration 72275: loss = 2.8906036e-11,1.4654503e-10\n",
      "Iteration 72280: loss = 2.8904922e-11,1.4654421e-10\n",
      "Iteration 72285: loss = 2.8905463e-11,1.465418e-10\n",
      "Iteration 72290: loss = 2.8905501e-11,1.4653989e-10\n",
      "Iteration 72295: loss = 2.8905135e-11,1.4653824e-10\n",
      "Iteration 72300: loss = 2.8905364e-11,1.4653628e-10\n",
      "Iteration 72305: loss = 2.8905565e-11,1.4653481e-10\n",
      "Iteration 72310: loss = 2.8904356e-11,1.4653381e-10\n",
      "Iteration 72315: loss = 2.8905071e-11,1.4653166e-10\n",
      "Iteration 72320: loss = 2.8905975e-11,1.465292e-10\n",
      "Iteration 72325: loss = 2.8905123e-11,1.4652812e-10\n",
      "Iteration 72330: loss = 2.890585e-11,1.4652576e-10\n",
      "Iteration 72335: loss = 2.8905418e-11,1.4652411e-10\n",
      "Iteration 72340: loss = 2.8904995e-11,1.4652266e-10\n",
      "Iteration 72345: loss = 2.8905676e-11,1.4652032e-10\n",
      "Iteration 72350: loss = 2.8904849e-11,1.4651941e-10\n",
      "Iteration 72355: loss = 2.8905557e-11,1.4651705e-10\n",
      "Iteration 72360: loss = 2.8905133e-11,1.4651538e-10\n",
      "Iteration 72365: loss = 2.8904793e-11,1.4651373e-10\n",
      "Iteration 72370: loss = 2.8905349e-11,1.465114e-10\n",
      "Iteration 72375: loss = 2.89047e-11,1.4651005e-10\n",
      "Iteration 72380: loss = 2.8905373e-11,1.4650754e-10\n",
      "Iteration 72385: loss = 2.8904568e-11,1.4650646e-10\n",
      "Iteration 72390: loss = 2.8905234e-11,1.4650445e-10\n",
      "Iteration 72395: loss = 2.8905397e-11,1.4650237e-10\n",
      "Iteration 72400: loss = 2.8903926e-11,1.4650207e-10\n",
      "Iteration 72405: loss = 2.890477e-11,1.464996e-10\n",
      "Iteration 72410: loss = 2.8905314e-11,1.4649737e-10\n",
      "Iteration 72415: loss = 2.8905442e-11,1.4649539e-10\n",
      "Iteration 72420: loss = 2.8904578e-11,1.464944e-10\n",
      "Iteration 72425: loss = 2.8906271e-11,1.4649099e-10\n",
      "Iteration 72430: loss = 2.8904728e-11,1.464906e-10\n",
      "Iteration 72435: loss = 2.8905005e-11,1.464885e-10\n",
      "Iteration 72440: loss = 2.8904844e-11,1.4648696e-10\n",
      "Iteration 72445: loss = 2.890511e-11,1.4648482e-10\n",
      "Iteration 72450: loss = 2.8904074e-11,1.4648399e-10\n",
      "Iteration 72455: loss = 2.8905092e-11,1.4648137e-10\n",
      "Iteration 72460: loss = 2.8904712e-11,1.4648055e-10\n",
      "Iteration 72465: loss = 2.8904166e-11,1.4647863e-10\n",
      "Iteration 72470: loss = 2.890479e-11,1.4647644e-10\n",
      "Iteration 72475: loss = 2.8904892e-11,1.4647447e-10\n",
      "Iteration 72480: loss = 2.890418e-11,1.4647346e-10\n",
      "Iteration 72485: loss = 2.8904714e-11,1.4647135e-10\n",
      "Iteration 72490: loss = 2.8905331e-11,1.4646913e-10\n",
      "Iteration 72495: loss = 2.890567e-11,1.4646685e-10\n",
      "Iteration 72500: loss = 2.8904741e-11,1.4646602e-10\n",
      "Iteration 72505: loss = 2.89044e-11,1.4646453e-10\n",
      "Iteration 72510: loss = 2.8905633e-11,1.4646154e-10\n",
      "Iteration 72515: loss = 2.890456e-11,1.464605e-10\n",
      "Iteration 72520: loss = 2.8904129e-11,1.4645923e-10\n",
      "Iteration 72525: loss = 2.890399e-11,1.4645758e-10\n",
      "Iteration 72530: loss = 2.8904545e-11,1.464551e-10\n",
      "Iteration 72535: loss = 2.8904374e-11,1.4645417e-10\n",
      "Iteration 72540: loss = 2.8903926e-11,1.4645245e-10\n",
      "Iteration 72545: loss = 2.8904433e-11,1.4645009e-10\n",
      "Iteration 72550: loss = 2.8905114e-11,1.4644795e-10\n",
      "Iteration 72555: loss = 2.8904693e-11,1.4644634e-10\n",
      "Iteration 72560: loss = 2.8903826e-11,1.4644538e-10\n",
      "Iteration 72565: loss = 2.890486e-11,1.464426e-10\n",
      "Iteration 72570: loss = 2.8904011e-11,1.4644155e-10\n",
      "Iteration 72575: loss = 2.8905276e-11,1.4643856e-10\n",
      "Iteration 72580: loss = 2.8903505e-11,1.4643817e-10\n",
      "Iteration 72585: loss = 2.8903832e-11,1.4643642e-10\n",
      "Iteration 72590: loss = 2.890454e-11,1.46434e-10\n",
      "Iteration 72595: loss = 2.890523e-11,1.4643128e-10\n",
      "Iteration 72600: loss = 2.8904457e-11,1.4643034e-10\n",
      "Iteration 72605: loss = 2.8904061e-11,1.464288e-10\n",
      "Iteration 72610: loss = 2.8903921e-11,1.4642726e-10\n",
      "Iteration 72615: loss = 2.8904873e-11,1.4642469e-10\n",
      "Iteration 72620: loss = 2.8904495e-11,1.4642325e-10\n",
      "Iteration 72625: loss = 2.8904061e-11,1.4642175e-10\n",
      "Iteration 72630: loss = 2.8903631e-11,1.4642026e-10\n",
      "Iteration 72635: loss = 2.890419e-11,1.46418e-10\n",
      "Iteration 72640: loss = 2.8904245e-11,1.4641612e-10\n",
      "Iteration 72645: loss = 2.8904086e-11,1.4641524e-10\n",
      "Iteration 72650: loss = 2.8903635e-11,1.4641333e-10\n",
      "Iteration 72655: loss = 2.8904226e-11,1.4641129e-10\n",
      "Iteration 72660: loss = 2.8904762e-11,1.4640862e-10\n",
      "Iteration 72665: loss = 2.8903241e-11,1.4640847e-10\n",
      "Iteration 72670: loss = 2.8904698e-11,1.4640537e-10\n",
      "Iteration 72675: loss = 2.8904329e-11,1.4640383e-10\n",
      "Iteration 72680: loss = 2.8903236e-11,1.4640283e-10\n",
      "Iteration 72685: loss = 2.8904679e-11,1.4639985e-10\n",
      "Iteration 72690: loss = 2.890358e-11,1.463996e-10\n",
      "Iteration 72695: loss = 2.8904074e-11,1.4639664e-10\n",
      "Iteration 72700: loss = 2.8903937e-11,1.4639569e-10\n",
      "Iteration 72705: loss = 2.8903503e-11,1.4639388e-10\n",
      "Iteration 72710: loss = 2.8904079e-11,1.4639161e-10\n",
      "Iteration 72715: loss = 2.8904679e-11,1.4638923e-10\n",
      "Iteration 72720: loss = 2.8903801e-11,1.4638835e-10\n",
      "Iteration 72725: loss = 2.8904058e-11,1.4638633e-10\n",
      "Iteration 72730: loss = 2.8903999e-11,1.4638445e-10\n",
      "Iteration 72735: loss = 2.8903558e-11,1.463834e-10\n",
      "Iteration 72740: loss = 2.8903631e-11,1.4638102e-10\n",
      "Iteration 72745: loss = 2.8903427e-11,1.4638013e-10\n",
      "Iteration 72750: loss = 2.8903567e-11,1.4637799e-10\n",
      "Iteration 72755: loss = 2.8904004e-11,1.4637554e-10\n",
      "Iteration 72760: loss = 2.8903907e-11,1.4637407e-10\n",
      "Iteration 72765: loss = 2.8903727e-11,1.4637265e-10\n",
      "Iteration 72770: loss = 2.8904221e-11,1.4637015e-10\n",
      "Iteration 72775: loss = 2.8902719e-11,1.4636997e-10\n",
      "Iteration 72780: loss = 2.8903645e-11,1.4636714e-10\n",
      "Iteration 72785: loss = 2.890492e-11,1.4636424e-10\n",
      "Iteration 72790: loss = 2.8903298e-11,1.4636385e-10\n",
      "Iteration 72795: loss = 2.8902965e-11,1.4636231e-10\n",
      "Iteration 72800: loss = 2.8903713e-11,1.4635994e-10\n",
      "Iteration 72805: loss = 2.890358e-11,1.4635854e-10\n",
      "Iteration 72810: loss = 2.890392e-11,1.4635623e-10\n",
      "Iteration 72815: loss = 2.8903472e-11,1.4635554e-10\n",
      "Iteration 72820: loss = 2.8903808e-11,1.4635274e-10\n",
      "Iteration 72825: loss = 2.890341e-11,1.4635126e-10\n",
      "Iteration 72830: loss = 2.890375e-11,1.4634938e-10\n",
      "Iteration 72835: loss = 2.890283e-11,1.4634824e-10\n",
      "Iteration 72840: loss = 2.8903524e-11,1.4634594e-10\n",
      "Iteration 72845: loss = 2.8903433e-11,1.4634424e-10\n",
      "Iteration 72850: loss = 2.890349e-11,1.4634226e-10\n",
      "Iteration 72855: loss = 2.8902832e-11,1.4634115e-10\n",
      "Iteration 72860: loss = 2.8903567e-11,1.463388e-10\n",
      "Iteration 72865: loss = 2.8903227e-11,1.4633722e-10\n",
      "Iteration 72870: loss = 2.8903555e-11,1.4633526e-10\n",
      "Iteration 72875: loss = 2.890318e-11,1.463343e-10\n",
      "Iteration 72880: loss = 2.8902727e-11,1.4633239e-10\n",
      "Iteration 72885: loss = 2.89035e-11,1.4632998e-10\n",
      "Iteration 72890: loss = 2.8903983e-11,1.4632795e-10\n",
      "Iteration 72895: loss = 2.8902174e-11,1.4632759e-10\n",
      "Iteration 72900: loss = 2.8903156e-11,1.4632505e-10\n",
      "Iteration 72905: loss = 2.8904415e-11,1.4632223e-10\n",
      "Iteration 72910: loss = 2.8904013e-11,1.4632054e-10\n",
      "Iteration 72915: loss = 2.8902486e-11,1.4632041e-10\n",
      "Iteration 72920: loss = 2.8903496e-11,1.463176e-10\n",
      "Iteration 72925: loss = 2.8902701e-11,1.4631726e-10\n",
      "Iteration 72930: loss = 2.8902594e-11,1.4631482e-10\n",
      "Iteration 72935: loss = 2.8903229e-11,1.4631257e-10\n",
      "Iteration 72940: loss = 2.890422e-11,1.4630983e-10\n",
      "Iteration 72945: loss = 2.890456e-11,1.4630785e-10\n",
      "Iteration 72950: loss = 2.8902977e-11,1.463076e-10\n",
      "Iteration 72955: loss = 2.890404e-11,1.463046e-10\n",
      "Iteration 72960: loss = 2.8902304e-11,1.4630441e-10\n",
      "Iteration 72965: loss = 2.8903241e-11,1.4630197e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 72970: loss = 2.8904433e-11,1.4629892e-10\n",
      "Iteration 72975: loss = 2.890362e-11,1.4629807e-10\n",
      "Iteration 72980: loss = 2.8902459e-11,1.4629774e-10\n",
      "Iteration 72985: loss = 2.890249e-11,1.4629542e-10\n",
      "Iteration 72990: loss = 2.8903097e-11,1.4629313e-10\n",
      "Iteration 72995: loss = 2.8904087e-11,1.4629084e-10\n",
      "Iteration 73000: loss = 2.8905336e-11,1.4628788e-10\n",
      "Iteration 73005: loss = 2.8903525e-11,1.4628754e-10\n",
      "Iteration 73010: loss = 2.8902505e-11,1.462866e-10\n",
      "Iteration 73015: loss = 2.8904447e-11,1.4628318e-10\n",
      "Iteration 73020: loss = 2.890263e-11,1.4628303e-10\n",
      "Iteration 73025: loss = 2.890199e-11,1.4628153e-10\n",
      "Iteration 73030: loss = 2.890276e-11,1.4627943e-10\n",
      "Iteration 73035: loss = 2.890347e-11,1.4627705e-10\n",
      "Iteration 73040: loss = 2.8905179e-11,1.4627369e-10\n",
      "Iteration 73045: loss = 2.8904013e-11,1.462728e-10\n",
      "Iteration 73050: loss = 2.8902547e-11,1.4627256e-10\n",
      "Iteration 73055: loss = 2.890285e-11,1.4627032e-10\n",
      "Iteration 73060: loss = 2.8904846e-11,1.4626679e-10\n",
      "Iteration 73065: loss = 2.8904476e-11,1.4626514e-10\n",
      "Iteration 73070: loss = 2.8902868e-11,1.4626482e-10\n",
      "Iteration 73075: loss = 2.8902228e-11,1.4626375e-10\n",
      "Iteration 73080: loss = 2.890301e-11,1.4626149e-10\n",
      "Iteration 73085: loss = 2.8904289e-11,1.4625842e-10\n",
      "Iteration 73090: loss = 2.8903399e-11,1.4625749e-10\n",
      "Iteration 73095: loss = 2.8901659e-11,1.4625712e-10\n",
      "Iteration 73100: loss = 2.8902367e-11,1.4625465e-10\n",
      "Iteration 73105: loss = 2.890306e-11,1.4625227e-10\n",
      "Iteration 73110: loss = 2.8904849e-11,1.462491e-10\n",
      "Iteration 73115: loss = 2.8905931e-11,1.462459e-10\n",
      "Iteration 73120: loss = 2.8907833e-11,1.4624238e-10\n",
      "Iteration 73125: loss = 2.8909597e-11,1.4623903e-10\n",
      "Iteration 73130: loss = 2.8910922e-11,1.4623577e-10\n",
      "Iteration 73135: loss = 2.8912107e-11,1.462325e-10\n",
      "Iteration 73140: loss = 2.8912907e-11,1.4623007e-10\n",
      "Iteration 73145: loss = 2.8914635e-11,1.46227e-10\n",
      "Iteration 73150: loss = 2.8915648e-11,1.4622349e-10\n",
      "Iteration 73155: loss = 2.8917651e-11,1.4622013e-10\n",
      "Iteration 73160: loss = 2.891845e-11,1.4621743e-10\n",
      "Iteration 73165: loss = 2.8919443e-11,1.4621424e-10\n",
      "Iteration 73170: loss = 2.8920968e-11,1.4621103e-10\n",
      "Iteration 73175: loss = 2.8922593e-11,1.4620773e-10\n",
      "Iteration 73180: loss = 2.8924462e-11,1.4620422e-10\n",
      "Iteration 73185: loss = 2.8924696e-11,1.4620187e-10\n",
      "Iteration 73190: loss = 2.8926292e-11,1.4619853e-10\n",
      "Iteration 73195: loss = 2.8927907e-11,1.4619533e-10\n",
      "Iteration 73200: loss = 2.8928809e-11,1.4619211e-10\n",
      "Iteration 73205: loss = 2.8929992e-11,1.461894e-10\n",
      "Iteration 73210: loss = 2.8931321e-11,1.4618634e-10\n",
      "Iteration 73215: loss = 2.8933222e-11,1.4618305e-10\n",
      "Iteration 73220: loss = 2.8933812e-11,1.4618014e-10\n",
      "Iteration 73225: loss = 2.893497e-11,1.461773e-10\n",
      "Iteration 73230: loss = 2.8936803e-11,1.4617411e-10\n",
      "Iteration 73235: loss = 2.89384e-11,1.4617071e-10\n",
      "Iteration 73240: loss = 2.8939094e-11,1.461678e-10\n",
      "Iteration 73245: loss = 2.8940144e-11,1.4616497e-10\n",
      "Iteration 73250: loss = 2.8941979e-11,1.4616161e-10\n",
      "Iteration 73255: loss = 2.8942826e-11,1.461584e-10\n",
      "Iteration 73260: loss = 2.8944467e-11,1.4615535e-10\n",
      "Iteration 73265: loss = 2.8945546e-11,1.4615259e-10\n",
      "Iteration 73270: loss = 2.894715e-11,1.4614934e-10\n",
      "Iteration 73275: loss = 2.8947974e-11,1.461461e-10\n",
      "Iteration 73280: loss = 2.8949334e-11,1.461433e-10\n",
      "Iteration 73285: loss = 2.8951208e-11,1.4614002e-10\n",
      "Iteration 73290: loss = 2.8951763e-11,1.4613694e-10\n",
      "Iteration 73295: loss = 2.895307e-11,1.461341e-10\n",
      "Iteration 73300: loss = 2.8954032e-11,1.4613148e-10\n",
      "Iteration 73305: loss = 2.8955565e-11,1.4612817e-10\n",
      "Iteration 73310: loss = 2.8956152e-11,1.4612567e-10\n",
      "Iteration 73315: loss = 2.8957621e-11,1.4612223e-10\n",
      "Iteration 73320: loss = 2.8958636e-11,1.4611982e-10\n",
      "Iteration 73325: loss = 2.8959396e-11,1.4611652e-10\n",
      "Iteration 73330: loss = 2.8960952e-11,1.4611334e-10\n",
      "Iteration 73335: loss = 2.896221e-11,1.4611068e-10\n",
      "Iteration 73340: loss = 2.8963693e-11,1.4610736e-10\n",
      "Iteration 73345: loss = 2.8964463e-11,1.4610438e-10\n",
      "Iteration 73350: loss = 2.8965422e-11,1.4610166e-10\n",
      "Iteration 73355: loss = 2.8967261e-11,1.4609838e-10\n",
      "Iteration 73360: loss = 2.8968265e-11,1.4609572e-10\n",
      "Iteration 73365: loss = 2.896898e-11,1.4609276e-10\n",
      "Iteration 73370: loss = 2.8970498e-11,1.4608952e-10\n",
      "Iteration 73375: loss = 2.8972227e-11,1.4608643e-10\n",
      "Iteration 73380: loss = 2.8972477e-11,1.4608378e-10\n",
      "Iteration 73385: loss = 2.8974032e-11,1.4608069e-10\n",
      "Iteration 73390: loss = 2.8975477e-11,1.4607775e-10\n",
      "Iteration 73395: loss = 2.8976502e-11,1.4607511e-10\n",
      "Iteration 73400: loss = 2.8977437e-11,1.4607196e-10\n",
      "Iteration 73405: loss = 2.8978882e-11,1.4606877e-10\n",
      "Iteration 73410: loss = 2.8980165e-11,1.4606599e-10\n",
      "Iteration 73415: loss = 2.8980414e-11,1.4606327e-10\n",
      "Iteration 73420: loss = 2.8982178e-11,1.4606001e-10\n",
      "Iteration 73425: loss = 2.898315e-11,1.4605714e-10\n",
      "Iteration 73430: loss = 2.8984773e-11,1.4605399e-10\n",
      "Iteration 73435: loss = 2.8985314e-11,1.460512e-10\n",
      "Iteration 73440: loss = 2.8986888e-11,1.4604817e-10\n",
      "Iteration 73445: loss = 2.898797e-11,1.460454e-10\n",
      "Iteration 73450: loss = 2.898883e-11,1.4604255e-10\n",
      "Iteration 73455: loss = 2.8990097e-11,1.460394e-10\n",
      "Iteration 73460: loss = 2.8991592e-11,1.4603638e-10\n",
      "Iteration 73465: loss = 2.8992956e-11,1.4603338e-10\n",
      "Iteration 73470: loss = 2.8993618e-11,1.4603055e-10\n",
      "Iteration 73475: loss = 2.8995209e-11,1.4602757e-10\n",
      "Iteration 73480: loss = 2.8995981e-11,1.46025e-10\n",
      "Iteration 73485: loss = 2.8997305e-11,1.460219e-10\n",
      "Iteration 73490: loss = 2.8998146e-11,1.4601892e-10\n",
      "Iteration 73495: loss = 2.8999489e-11,1.4601598e-10\n",
      "Iteration 73500: loss = 2.900085e-11,1.4601309e-10\n",
      "Iteration 73505: loss = 2.9001615e-11,1.460099e-10\n",
      "Iteration 73510: loss = 2.9002918e-11,1.4600704e-10\n",
      "Iteration 73515: loss = 2.9004436e-11,1.4600389e-10\n",
      "Iteration 73520: loss = 2.900572e-11,1.4600102e-10\n",
      "Iteration 73525: loss = 2.9006006e-11,1.4599857e-10\n",
      "Iteration 73530: loss = 2.9007336e-11,1.459955e-10\n",
      "Iteration 73535: loss = 2.9008934e-11,1.4599255e-10\n",
      "Iteration 73540: loss = 2.900942e-11,1.4598962e-10\n",
      "Iteration 73545: loss = 2.9010724e-11,1.4598679e-10\n",
      "Iteration 73550: loss = 2.9012195e-11,1.4598386e-10\n",
      "Iteration 73555: loss = 2.9012964e-11,1.4598144e-10\n",
      "Iteration 73560: loss = 2.9013509e-11,1.4597867e-10\n",
      "Iteration 73565: loss = 2.901475e-11,1.4597577e-10\n",
      "Iteration 73570: loss = 2.9016145e-11,1.4597284e-10\n",
      "Iteration 73575: loss = 2.9016631e-11,1.4596988e-10\n",
      "Iteration 73580: loss = 2.9018144e-11,1.4596707e-10\n",
      "Iteration 73585: loss = 2.9019386e-11,1.4596424e-10\n",
      "Iteration 73590: loss = 2.9020774e-11,1.4596128e-10\n",
      "Iteration 73595: loss = 2.902096e-11,1.4595947e-10\n",
      "Iteration 73600: loss = 2.9018815e-11,1.4595927e-10\n",
      "Iteration 73605: loss = 2.9019844e-11,1.45957e-10\n",
      "Iteration 73610: loss = 2.9020598e-11,1.4595408e-10\n",
      "Iteration 73615: loss = 2.9019447e-11,1.4595357e-10\n",
      "Iteration 73620: loss = 2.9020212e-11,1.4595097e-10\n",
      "Iteration 73625: loss = 2.9018815e-11,1.4595032e-10\n",
      "Iteration 73630: loss = 2.9020291e-11,1.4594728e-10\n",
      "Iteration 73635: loss = 2.9019712e-11,1.459459e-10\n",
      "Iteration 73640: loss = 2.9019962e-11,1.4594422e-10\n",
      "Iteration 73645: loss = 2.9020448e-11,1.4594162e-10\n",
      "Iteration 73650: loss = 2.9018723e-11,1.4594165e-10\n",
      "Iteration 73655: loss = 2.9019684e-11,1.4593918e-10\n",
      "Iteration 73660: loss = 2.9020097e-11,1.4593708e-10\n",
      "Iteration 73665: loss = 2.9019674e-11,1.4593521e-10\n",
      "Iteration 73670: loss = 2.9020134e-11,1.4593331e-10\n",
      "Iteration 73675: loss = 2.9019924e-11,1.4593143e-10\n",
      "Iteration 73680: loss = 2.9020194e-11,1.4592963e-10\n",
      "Iteration 73685: loss = 2.9019525e-11,1.4592813e-10\n",
      "Iteration 73690: loss = 2.9021271e-11,1.4592497e-10\n",
      "Iteration 73695: loss = 2.901919e-11,1.4592512e-10\n",
      "Iteration 73700: loss = 2.9020624e-11,1.4592175e-10\n",
      "Iteration 73705: loss = 2.901885e-11,1.4592182e-10\n",
      "Iteration 73710: loss = 2.902e-11,1.4591921e-10\n",
      "Iteration 73715: loss = 2.9020529e-11,1.4591714e-10\n",
      "Iteration 73720: loss = 2.9020321e-11,1.4591525e-10\n",
      "Iteration 73725: loss = 2.9020708e-11,1.4591331e-10\n",
      "Iteration 73730: loss = 2.9020737e-11,1.4591145e-10\n",
      "Iteration 73735: loss = 2.9020134e-11,1.4591026e-10\n",
      "Iteration 73740: loss = 2.9020614e-11,1.4590809e-10\n",
      "Iteration 73745: loss = 2.9020217e-11,1.4590679e-10\n",
      "Iteration 73750: loss = 2.9020014e-11,1.4590504e-10\n",
      "Iteration 73755: loss = 2.9020591e-11,1.4590291e-10\n",
      "Iteration 73760: loss = 2.9020555e-11,1.4590053e-10\n",
      "Iteration 73765: loss = 2.902026e-11,1.4589979e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73770: loss = 2.9020215e-11,1.4589738e-10\n",
      "Iteration 73775: loss = 2.9020316e-11,1.4589618e-10\n",
      "Iteration 73780: loss = 2.9020343e-11,1.4589374e-10\n",
      "Iteration 73785: loss = 2.9019537e-11,1.4589265e-10\n",
      "Iteration 73790: loss = 2.9019264e-11,1.4589112e-10\n",
      "Iteration 73795: loss = 2.9019828e-11,1.4588909e-10\n",
      "Iteration 73800: loss = 2.9020257e-11,1.4588722e-10\n",
      "Iteration 73805: loss = 2.9020552e-11,1.4588522e-10\n",
      "Iteration 73810: loss = 2.9020139e-11,1.4588342e-10\n",
      "Iteration 73815: loss = 2.9021471e-11,1.4588059e-10\n",
      "Iteration 73820: loss = 2.9019573e-11,1.4588054e-10\n",
      "Iteration 73825: loss = 2.9019688e-11,1.4587875e-10\n",
      "Iteration 73830: loss = 2.9021155e-11,1.458754e-10\n",
      "Iteration 73835: loss = 2.902007e-11,1.4587531e-10\n",
      "Iteration 73840: loss = 2.9020106e-11,1.4587277e-10\n",
      "Iteration 73845: loss = 2.9021044e-11,1.4587014e-10\n",
      "Iteration 73850: loss = 2.9020404e-11,1.4586854e-10\n",
      "Iteration 73855: loss = 2.9020545e-11,1.4586748e-10\n",
      "Iteration 73860: loss = 2.9020814e-11,1.458648e-10\n",
      "Iteration 73865: loss = 2.9020293e-11,1.4586347e-10\n",
      "Iteration 73870: loss = 2.9020128e-11,1.458619e-10\n",
      "Iteration 73875: loss = 2.9020624e-11,1.4585988e-10\n",
      "Iteration 73880: loss = 2.9021133e-11,1.458578e-10\n",
      "Iteration 73885: loss = 2.9020722e-11,1.4585656e-10\n",
      "Iteration 73890: loss = 2.9020805e-11,1.458545e-10\n",
      "Iteration 73895: loss = 2.9020808e-11,1.4585257e-10\n",
      "Iteration 73900: loss = 2.9020444e-11,1.4585136e-10\n",
      "Iteration 73905: loss = 2.902104e-11,1.458489e-10\n",
      "Iteration 73910: loss = 2.902066e-11,1.4584756e-10\n",
      "Iteration 73915: loss = 2.9021625e-11,1.4584511e-10\n",
      "Iteration 73920: loss = 2.9019674e-11,1.4584497e-10\n",
      "Iteration 73925: loss = 2.9020397e-11,1.458428e-10\n",
      "Iteration 73930: loss = 2.9020947e-11,1.4584073e-10\n",
      "Iteration 73935: loss = 2.9020583e-11,1.4583881e-10\n",
      "Iteration 73940: loss = 2.9020985e-11,1.4583655e-10\n",
      "Iteration 73945: loss = 2.9021063e-11,1.458349e-10\n",
      "Iteration 73950: loss = 2.9020418e-11,1.4583354e-10\n",
      "Iteration 73955: loss = 2.9022095e-11,1.4583057e-10\n",
      "Iteration 73960: loss = 2.9020189e-11,1.4583046e-10\n",
      "Iteration 73965: loss = 2.9020238e-11,1.4582832e-10\n",
      "Iteration 73970: loss = 2.9021294e-11,1.4582578e-10\n",
      "Iteration 73975: loss = 2.9019903e-11,1.4582512e-10\n",
      "Iteration 73980: loss = 2.902095e-11,1.4582249e-10\n",
      "Iteration 73985: loss = 2.9022172e-11,1.4582006e-10\n",
      "Iteration 73990: loss = 2.9020203e-11,1.4581991e-10\n",
      "Iteration 73995: loss = 2.9021239e-11,1.458172e-10\n",
      "Iteration 74000: loss = 2.9021759e-11,1.4581489e-10\n",
      "Iteration 74005: loss = 2.9021431e-11,1.458137e-10\n",
      "Iteration 74010: loss = 2.9020175e-11,1.4581297e-10\n",
      "Iteration 74015: loss = 2.902092e-11,1.4581056e-10\n",
      "Iteration 74020: loss = 2.9021405e-11,1.4580831e-10\n",
      "Iteration 74025: loss = 2.9021473e-11,1.4580676e-10\n",
      "Iteration 74030: loss = 2.9021292e-11,1.4580513e-10\n",
      "Iteration 74035: loss = 2.9021292e-11,1.4580283e-10\n",
      "Iteration 74040: loss = 2.902144e-11,1.4580138e-10\n",
      "Iteration 74045: loss = 2.9020434e-11,1.4580054e-10\n",
      "Iteration 74050: loss = 2.9021107e-11,1.4579812e-10\n",
      "Iteration 74055: loss = 2.9021016e-11,1.4579607e-10\n",
      "Iteration 74060: loss = 2.9021787e-11,1.4579365e-10\n",
      "Iteration 74065: loss = 2.9021075e-11,1.4579234e-10\n",
      "Iteration 74070: loss = 2.9021424e-11,1.457903e-10\n",
      "Iteration 74075: loss = 2.9021481e-11,1.457884e-10\n",
      "Iteration 74080: loss = 2.902044e-11,1.4578763e-10\n",
      "Iteration 74085: loss = 2.9021863e-11,1.457846e-10\n",
      "Iteration 74090: loss = 2.90206e-11,1.457838e-10\n",
      "Iteration 74095: loss = 2.9021209e-11,1.4578182e-10\n",
      "Iteration 74100: loss = 2.9021667e-11,1.457798e-10\n",
      "Iteration 74105: loss = 2.9021724e-11,1.4577793e-10\n",
      "Iteration 74110: loss = 2.9021882e-11,1.4577596e-10\n",
      "Iteration 74115: loss = 2.9022292e-11,1.4577407e-10\n",
      "Iteration 74120: loss = 2.9021577e-11,1.4577274e-10\n",
      "Iteration 74125: loss = 2.9022194e-11,1.4577049e-10\n",
      "Iteration 74130: loss = 2.9020416e-11,1.4577026e-10\n",
      "Iteration 74135: loss = 2.9020739e-11,1.4576844e-10\n",
      "Iteration 74140: loss = 2.9021263e-11,1.4576626e-10\n",
      "Iteration 74145: loss = 2.9021771e-11,1.4576425e-10\n",
      "Iteration 74150: loss = 2.9021485e-11,1.4576215e-10\n",
      "Iteration 74155: loss = 2.9022293e-11,1.4575978e-10\n",
      "Iteration 74160: loss = 2.902166e-11,1.4575842e-10\n",
      "Iteration 74165: loss = 2.9021277e-11,1.4575717e-10\n",
      "Iteration 74170: loss = 2.9022028e-11,1.4575469e-10\n",
      "Iteration 74175: loss = 2.9021618e-11,1.4575338e-10\n",
      "Iteration 74180: loss = 2.9022453e-11,1.4575083e-10\n",
      "Iteration 74185: loss = 2.9021776e-11,1.4574951e-10\n",
      "Iteration 74190: loss = 2.9021422e-11,1.4574816e-10\n",
      "Iteration 74195: loss = 2.902087e-11,1.4574675e-10\n",
      "Iteration 74200: loss = 2.902184e-11,1.4574418e-10\n",
      "Iteration 74205: loss = 2.9021957e-11,1.4574202e-10\n",
      "Iteration 74210: loss = 2.9021452e-11,1.4574053e-10\n",
      "Iteration 74215: loss = 2.9022595e-11,1.45738e-10\n",
      "Iteration 74220: loss = 2.9021311e-11,1.4573717e-10\n",
      "Iteration 74225: loss = 2.9021487e-11,1.4573542e-10\n",
      "Iteration 74230: loss = 2.9021544e-11,1.4573388e-10\n",
      "Iteration 74235: loss = 2.9022037e-11,1.4573169e-10\n",
      "Iteration 74240: loss = 2.9022425e-11,1.4572966e-10\n",
      "Iteration 74245: loss = 2.9021953e-11,1.4572829e-10\n",
      "Iteration 74250: loss = 2.9022973e-11,1.4572582e-10\n",
      "Iteration 74255: loss = 2.9022397e-11,1.4572463e-10\n",
      "Iteration 74260: loss = 2.902274e-11,1.4572242e-10\n",
      "Iteration 74265: loss = 2.9022222e-11,1.4572105e-10\n",
      "Iteration 74270: loss = 2.9022803e-11,1.4571906e-10\n",
      "Iteration 74275: loss = 2.9021485e-11,1.4571833e-10\n",
      "Iteration 74280: loss = 2.9021884e-11,1.457162e-10\n",
      "Iteration 74285: loss = 2.9022474e-11,1.45714e-10\n",
      "Iteration 74290: loss = 2.902227e-11,1.4571183e-10\n",
      "Iteration 74295: loss = 2.9022312e-11,1.4571014e-10\n",
      "Iteration 74300: loss = 2.902234e-11,1.4570825e-10\n",
      "Iteration 74305: loss = 2.902205e-11,1.4570689e-10\n",
      "Iteration 74310: loss = 2.9022175e-11,1.4570492e-10\n",
      "Iteration 74315: loss = 2.9021863e-11,1.4570356e-10\n",
      "Iteration 74320: loss = 2.9021952e-11,1.457022e-10\n",
      "Iteration 74325: loss = 2.9022493e-11,1.4569948e-10\n",
      "Iteration 74330: loss = 2.902239e-11,1.4569758e-10\n",
      "Iteration 74335: loss = 2.9022089e-11,1.4569629e-10\n",
      "Iteration 74340: loss = 2.9022227e-11,1.4569414e-10\n",
      "Iteration 74345: loss = 2.902256e-11,1.456921e-10\n",
      "Iteration 74350: loss = 2.902304e-11,1.4569007e-10\n",
      "Iteration 74355: loss = 2.9022307e-11,1.4568902e-10\n",
      "Iteration 74360: loss = 2.9022784e-11,1.4568681e-10\n",
      "Iteration 74365: loss = 2.9022841e-11,1.4568494e-10\n",
      "Iteration 74370: loss = 2.9022743e-11,1.4568327e-10\n",
      "Iteration 74375: loss = 2.9023152e-11,1.4568124e-10\n",
      "Iteration 74380: loss = 2.9023256e-11,1.456793e-10\n",
      "Iteration 74385: loss = 2.9022984e-11,1.4567789e-10\n",
      "Iteration 74390: loss = 2.9021745e-11,1.4567716e-10\n",
      "Iteration 74395: loss = 2.9022422e-11,1.4567493e-10\n",
      "Iteration 74400: loss = 2.9022972e-11,1.4567288e-10\n",
      "Iteration 74405: loss = 2.9022722e-11,1.4567063e-10\n",
      "Iteration 74410: loss = 2.9023192e-11,1.4566853e-10\n",
      "Iteration 74415: loss = 2.9022689e-11,1.4566783e-10\n",
      "Iteration 74420: loss = 2.9023013e-11,1.456654e-10\n",
      "Iteration 74425: loss = 2.9022446e-11,1.4566462e-10\n",
      "Iteration 74430: loss = 2.9022451e-11,1.4566214e-10\n",
      "Iteration 74435: loss = 2.9022874e-11,1.4566023e-10\n",
      "Iteration 74440: loss = 2.9023025e-11,1.4565876e-10\n",
      "Iteration 74445: loss = 2.9023119e-11,1.4565654e-10\n",
      "Iteration 74450: loss = 2.9022597e-11,1.4565572e-10\n",
      "Iteration 74455: loss = 2.9022574e-11,1.4565307e-10\n",
      "Iteration 74460: loss = 2.9023133e-11,1.4565114e-10\n",
      "Iteration 74465: loss = 2.9022817e-11,1.4564974e-10\n",
      "Iteration 74470: loss = 2.9022965e-11,1.456476e-10\n",
      "Iteration 74475: loss = 2.9023334e-11,1.4564579e-10\n",
      "Iteration 74480: loss = 2.9022845e-11,1.4564447e-10\n",
      "Iteration 74485: loss = 2.9023306e-11,1.4564228e-10\n",
      "Iteration 74490: loss = 2.9022791e-11,1.4564072e-10\n",
      "Iteration 74495: loss = 2.9023338e-11,1.4563868e-10\n",
      "Iteration 74500: loss = 2.9022791e-11,1.4563727e-10\n",
      "Iteration 74505: loss = 2.9023561e-11,1.4563489e-10\n",
      "Iteration 74510: loss = 2.9022623e-11,1.4563406e-10\n",
      "Iteration 74515: loss = 2.9022602e-11,1.4563238e-10\n",
      "Iteration 74520: loss = 2.9023164e-11,1.4563023e-10\n",
      "Iteration 74525: loss = 2.9023603e-11,1.4562791e-10\n",
      "Iteration 74530: loss = 2.902348e-11,1.4562579e-10\n",
      "Iteration 74535: loss = 2.9023896e-11,1.4562455e-10\n",
      "Iteration 74540: loss = 2.9023952e-11,1.456226e-10\n",
      "Iteration 74545: loss = 2.9024083e-11,1.4562063e-10\n",
      "Iteration 74550: loss = 2.9023811e-11,1.456193e-10\n",
      "Iteration 74555: loss = 2.9023265e-11,1.4561773e-10\n",
      "Iteration 74560: loss = 2.9023596e-11,1.4561569e-10\n",
      "Iteration 74565: loss = 2.9023783e-11,1.4561395e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74570: loss = 2.9023318e-11,1.4561244e-10\n",
      "Iteration 74575: loss = 2.902402e-11,1.4561026e-10\n",
      "Iteration 74580: loss = 2.9023348e-11,1.4560853e-10\n",
      "Iteration 74585: loss = 2.9023756e-11,1.45607e-10\n",
      "Iteration 74590: loss = 2.9023638e-11,1.4560475e-10\n",
      "Iteration 74595: loss = 2.9023539e-11,1.4560296e-10\n",
      "Iteration 74600: loss = 2.9023707e-11,1.4560156e-10\n",
      "Iteration 74605: loss = 2.9023823e-11,1.4559964e-10\n",
      "Iteration 74610: loss = 2.9023964e-11,1.4559794e-10\n",
      "Iteration 74615: loss = 2.9023865e-11,1.4559567e-10\n",
      "Iteration 74620: loss = 2.9023575e-11,1.4559429e-10\n",
      "Iteration 74625: loss = 2.9023684e-11,1.4559229e-10\n",
      "Iteration 74630: loss = 2.9023296e-11,1.4559098e-10\n",
      "Iteration 74635: loss = 2.9023372e-11,1.4558911e-10\n",
      "Iteration 74640: loss = 2.9023584e-11,1.4558738e-10\n",
      "Iteration 74645: loss = 2.9024677e-11,1.455848e-10\n",
      "Iteration 74650: loss = 2.9022723e-11,1.4558466e-10\n",
      "Iteration 74655: loss = 2.9023096e-11,1.4558256e-10\n",
      "Iteration 74660: loss = 2.9023598e-11,1.4558049e-10\n",
      "Iteration 74665: loss = 2.9024075e-11,1.4557848e-10\n",
      "Iteration 74670: loss = 2.902407e-11,1.4557618e-10\n",
      "Iteration 74675: loss = 2.9024436e-11,1.4557483e-10\n",
      "Iteration 74680: loss = 2.9023825e-11,1.4557358e-10\n",
      "Iteration 74685: loss = 2.9024885e-11,1.4557038e-10\n",
      "Iteration 74690: loss = 2.9024365e-11,1.4556968e-10\n",
      "Iteration 74695: loss = 2.9022432e-11,1.4556939e-10\n",
      "Iteration 74700: loss = 2.9023945e-11,1.455665e-10\n",
      "Iteration 74705: loss = 2.9024214e-11,1.45564e-10\n",
      "Iteration 74710: loss = 2.902444e-11,1.4556248e-10\n",
      "Iteration 74715: loss = 2.902413e-11,1.4556117e-10\n",
      "Iteration 74720: loss = 2.9024344e-11,1.4555829e-10\n",
      "Iteration 74725: loss = 2.9024116e-11,1.4555748e-10\n",
      "Iteration 74730: loss = 2.9024158e-11,1.4555548e-10\n",
      "Iteration 74735: loss = 2.902506e-11,1.4555299e-10\n",
      "Iteration 74740: loss = 2.902374e-11,1.4555229e-10\n",
      "Iteration 74745: loss = 2.902486e-11,1.4554973e-10\n",
      "Iteration 74750: loss = 2.9024309e-11,1.4554831e-10\n",
      "Iteration 74755: loss = 2.9024448e-11,1.4554591e-10\n",
      "Iteration 74760: loss = 2.9023936e-11,1.4554512e-10\n",
      "Iteration 74765: loss = 2.9025012e-11,1.4554236e-10\n",
      "Iteration 74770: loss = 2.9024486e-11,1.4554097e-10\n",
      "Iteration 74775: loss = 2.9024706e-11,1.4553919e-10\n",
      "Iteration 74780: loss = 2.9024056e-11,1.4553792e-10\n",
      "Iteration 74785: loss = 2.9024056e-11,1.4553583e-10\n",
      "Iteration 74790: loss = 2.90246e-11,1.4553361e-10\n",
      "Iteration 74795: loss = 2.9025692e-11,1.4553092e-10\n",
      "Iteration 74800: loss = 2.9024604e-11,1.4553016e-10\n",
      "Iteration 74805: loss = 2.9024763e-11,1.4552828e-10\n",
      "Iteration 74810: loss = 2.9025218e-11,1.4552606e-10\n",
      "Iteration 74815: loss = 2.9023945e-11,1.455254e-10\n",
      "Iteration 74820: loss = 2.9024763e-11,1.4552301e-10\n",
      "Iteration 74825: loss = 2.9025173e-11,1.4552105e-10\n",
      "Iteration 74830: loss = 2.9023955e-11,1.4552028e-10\n",
      "Iteration 74835: loss = 2.9024352e-11,1.4551818e-10\n",
      "Iteration 74840: loss = 2.9024236e-11,1.455161e-10\n",
      "Iteration 74845: loss = 2.9024378e-11,1.4551449e-10\n",
      "Iteration 74850: loss = 2.9024297e-11,1.455125e-10\n",
      "Iteration 74855: loss = 2.9024748e-11,1.4551049e-10\n",
      "Iteration 74860: loss = 2.902579e-11,1.4550772e-10\n",
      "Iteration 74865: loss = 2.9023861e-11,1.4550768e-10\n",
      "Iteration 74870: loss = 2.9024628e-11,1.455053e-10\n",
      "Iteration 74875: loss = 2.902512e-11,1.4550318e-10\n",
      "Iteration 74880: loss = 2.9026179e-11,1.455005e-10\n",
      "Iteration 74885: loss = 2.9025183e-11,1.454997e-10\n",
      "Iteration 74890: loss = 2.9025995e-11,1.4549718e-10\n",
      "Iteration 74895: loss = 2.9025393e-11,1.454958e-10\n",
      "Iteration 74900: loss = 2.9024503e-11,1.4549505e-10\n",
      "Iteration 74905: loss = 2.9025204e-11,1.4549284e-10\n",
      "Iteration 74910: loss = 2.902499e-11,1.4549063e-10\n",
      "Iteration 74915: loss = 2.9026087e-11,1.4548812e-10\n",
      "Iteration 74920: loss = 2.9024899e-11,1.4548784e-10\n",
      "Iteration 74925: loss = 2.9025652e-11,1.4548485e-10\n",
      "Iteration 74930: loss = 2.902468e-11,1.4548461e-10\n",
      "Iteration 74935: loss = 2.9025574e-11,1.4548122e-10\n",
      "Iteration 74940: loss = 2.9025303e-11,1.4548049e-10\n",
      "Iteration 74945: loss = 2.9025187e-11,1.4547867e-10\n",
      "Iteration 74950: loss = 2.9024975e-11,1.454765e-10\n",
      "Iteration 74955: loss = 2.902572e-11,1.454741e-10\n",
      "Iteration 74960: loss = 2.9025393e-11,1.4547283e-10\n",
      "Iteration 74965: loss = 2.9024921e-11,1.4547141e-10\n",
      "Iteration 74970: loss = 2.902461e-11,1.4547018e-10\n",
      "Iteration 74975: loss = 2.9024927e-11,1.4546797e-10\n",
      "Iteration 74980: loss = 2.902565e-11,1.4546575e-10\n",
      "Iteration 74985: loss = 2.902604e-11,1.4546367e-10\n",
      "Iteration 74990: loss = 2.9024129e-11,1.4546346e-10\n",
      "Iteration 74995: loss = 2.9025156e-11,1.454609e-10\n",
      "Iteration 75000: loss = 2.9025697e-11,1.4545887e-10\n",
      "Iteration 75005: loss = 2.9026974e-11,1.4545605e-10\n",
      "Iteration 75010: loss = 2.9025242e-11,1.4545576e-10\n",
      "Iteration 75015: loss = 2.9026155e-11,1.4545315e-10\n",
      "Iteration 75020: loss = 2.9025766e-11,1.4545185e-10\n",
      "Iteration 75025: loss = 2.9026061e-11,1.4544971e-10\n",
      "Iteration 75030: loss = 2.902567e-11,1.4544825e-10\n",
      "Iteration 75035: loss = 2.9026594e-11,1.4544589e-10\n",
      "Iteration 75040: loss = 2.902463e-11,1.4544574e-10\n",
      "Iteration 75045: loss = 2.9025936e-11,1.4544266e-10\n",
      "Iteration 75050: loss = 2.9026096e-11,1.4544049e-10\n",
      "Iteration 75055: loss = 2.9025152e-11,1.4543966e-10\n",
      "Iteration 75060: loss = 2.902597e-11,1.454375e-10\n",
      "Iteration 75065: loss = 2.9026802e-11,1.4543508e-10\n",
      "Iteration 75070: loss = 2.9025527e-11,1.454343e-10\n",
      "Iteration 75075: loss = 2.9026025e-11,1.4543232e-10\n",
      "Iteration 75080: loss = 2.9026741e-11,1.4542982e-10\n",
      "Iteration 75085: loss = 2.9026344e-11,1.4542846e-10\n",
      "Iteration 75090: loss = 2.9025907e-11,1.4542698e-10\n",
      "Iteration 75095: loss = 2.9026956e-11,1.4542435e-10\n",
      "Iteration 75100: loss = 2.9025782e-11,1.4542358e-10\n",
      "Iteration 75105: loss = 2.9027341e-11,1.4542055e-10\n",
      "Iteration 75110: loss = 2.9026156e-11,1.4541975e-10\n",
      "Iteration 75115: loss = 2.9026316e-11,1.4541797e-10\n",
      "Iteration 75120: loss = 2.9026717e-11,1.4541583e-10\n",
      "Iteration 75125: loss = 2.9025449e-11,1.4541536e-10\n",
      "Iteration 75130: loss = 2.9025919e-11,1.4541306e-10\n",
      "Iteration 75135: loss = 2.9026413e-11,1.45411e-10\n",
      "Iteration 75140: loss = 2.902624e-11,1.4540905e-10\n",
      "Iteration 75145: loss = 2.9027067e-11,1.454066e-10\n",
      "Iteration 75150: loss = 2.9025107e-11,1.4540641e-10\n",
      "Iteration 75155: loss = 2.9026668e-11,1.45403e-10\n",
      "Iteration 75160: loss = 2.9025666e-11,1.4540236e-10\n",
      "Iteration 75165: loss = 2.9026464e-11,1.4540008e-10\n",
      "Iteration 75170: loss = 2.9027288e-11,1.4539721e-10\n",
      "Iteration 75175: loss = 2.902571e-11,1.4539712e-10\n",
      "Iteration 75180: loss = 2.9025558e-11,1.453952e-10\n",
      "Iteration 75185: loss = 2.9026044e-11,1.4539325e-10\n",
      "Iteration 75190: loss = 2.9026578e-11,1.4539109e-10\n",
      "Iteration 75195: loss = 2.9027634e-11,1.4538842e-10\n",
      "Iteration 75200: loss = 2.9027095e-11,1.4538708e-10\n",
      "Iteration 75205: loss = 2.9026101e-11,1.4538633e-10\n",
      "Iteration 75210: loss = 2.9027692e-11,1.4538312e-10\n",
      "Iteration 75215: loss = 2.9026353e-11,1.453824e-10\n",
      "Iteration 75220: loss = 2.90268e-11,1.4538042e-10\n",
      "Iteration 75225: loss = 2.9026304e-11,1.4537886e-10\n",
      "Iteration 75230: loss = 2.9027019e-11,1.4537661e-10\n",
      "Iteration 75235: loss = 2.9027412e-11,1.453747e-10\n",
      "Iteration 75240: loss = 2.9026274e-11,1.4537384e-10\n",
      "Iteration 75245: loss = 2.9026693e-11,1.4537163e-10\n",
      "Iteration 75250: loss = 2.9026184e-11,1.4537022e-10\n",
      "Iteration 75255: loss = 2.9027003e-11,1.4536788e-10\n",
      "Iteration 75260: loss = 2.9028039e-11,1.4536514e-10\n",
      "Iteration 75265: loss = 2.9026684e-11,1.4536448e-10\n",
      "Iteration 75270: loss = 2.9027223e-11,1.4536239e-10\n",
      "Iteration 75275: loss = 2.9027327e-11,1.4536053e-10\n",
      "Iteration 75280: loss = 2.9027503e-11,1.4535848e-10\n",
      "Iteration 75285: loss = 2.9027942e-11,1.4535652e-10\n",
      "Iteration 75290: loss = 2.9026012e-11,1.453563e-10\n",
      "Iteration 75295: loss = 2.9026842e-11,1.4535399e-10\n",
      "Iteration 75300: loss = 2.9027327e-11,1.4535181e-10\n",
      "Iteration 75305: loss = 2.9027317e-11,1.4534964e-10\n",
      "Iteration 75310: loss = 2.9027747e-11,1.4534829e-10\n",
      "Iteration 75315: loss = 2.902728e-11,1.4534686e-10\n",
      "Iteration 75320: loss = 2.9026732e-11,1.453456e-10\n",
      "Iteration 75325: loss = 2.9027239e-11,1.4534346e-10\n",
      "Iteration 75330: loss = 2.9027544e-11,1.4534085e-10\n",
      "Iteration 75335: loss = 2.902728e-11,1.4533931e-10\n",
      "Iteration 75340: loss = 2.9026751e-11,1.4533787e-10\n",
      "Iteration 75345: loss = 2.9027883e-11,1.453353e-10\n",
      "Iteration 75350: loss = 2.9026712e-11,1.4533508e-10\n",
      "Iteration 75355: loss = 2.9027551e-11,1.4533184e-10\n",
      "Iteration 75360: loss = 2.9027248e-11,1.4533104e-10\n",
      "Iteration 75365: loss = 2.9027584e-11,1.4532912e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75370: loss = 2.902814e-11,1.4532614e-10\n",
      "Iteration 75375: loss = 2.902803e-11,1.453252e-10\n",
      "Iteration 75380: loss = 2.9027683e-11,1.4532336e-10\n",
      "Iteration 75385: loss = 2.9027935e-11,1.4532078e-10\n",
      "Iteration 75390: loss = 2.9026713e-11,1.4531999e-10\n",
      "Iteration 75395: loss = 2.9027643e-11,1.4531754e-10\n",
      "Iteration 75400: loss = 2.9027105e-11,1.4531684e-10\n",
      "Iteration 75405: loss = 2.9026732e-11,1.453148e-10\n",
      "Iteration 75410: loss = 2.9027239e-11,1.4531291e-10\n",
      "Iteration 75415: loss = 2.9027733e-11,1.4531082e-10\n",
      "Iteration 75420: loss = 2.902935e-11,1.4530781e-10\n",
      "Iteration 75425: loss = 2.9027473e-11,1.4530767e-10\n",
      "Iteration 75430: loss = 2.9027829e-11,1.4530557e-10\n",
      "Iteration 75435: loss = 2.9028018e-11,1.4530352e-10\n",
      "Iteration 75440: loss = 2.9027003e-11,1.453028e-10\n",
      "Iteration 75445: loss = 2.9028599e-11,1.452996e-10\n",
      "Iteration 75450: loss = 2.902742e-11,1.452988e-10\n",
      "Iteration 75455: loss = 2.9027879e-11,1.4529679e-10\n",
      "Iteration 75460: loss = 2.9027334e-11,1.4529533e-10\n",
      "Iteration 75465: loss = 2.9027052e-11,1.4529386e-10\n",
      "Iteration 75470: loss = 2.9027877e-11,1.4529145e-10\n",
      "Iteration 75475: loss = 2.9028231e-11,1.4528934e-10\n",
      "Iteration 75480: loss = 2.9028014e-11,1.4528784e-10\n",
      "Iteration 75485: loss = 2.902814e-11,1.4528591e-10\n",
      "Iteration 75490: loss = 2.9027886e-11,1.4528455e-10\n",
      "Iteration 75495: loss = 2.902851e-11,1.452821e-10\n",
      "Iteration 75500: loss = 2.9028073e-11,1.4528087e-10\n",
      "Iteration 75505: loss = 2.9028e-11,1.4527886e-10\n",
      "Iteration 75510: loss = 2.902774e-11,1.4527744e-10\n",
      "Iteration 75515: loss = 2.9028313e-11,1.4527547e-10\n",
      "Iteration 75520: loss = 2.9028708e-11,1.4527328e-10\n",
      "Iteration 75525: loss = 2.9028177e-11,1.4527192e-10\n",
      "Iteration 75530: loss = 2.902832e-11,1.4527003e-10\n",
      "Iteration 75535: loss = 2.9028101e-11,1.4526869e-10\n",
      "Iteration 75540: loss = 2.9028684e-11,1.452663e-10\n",
      "Iteration 75545: loss = 2.902839e-11,1.4526476e-10\n",
      "Iteration 75550: loss = 2.902969e-11,1.4526214e-10\n",
      "Iteration 75555: loss = 2.902782e-11,1.4526197e-10\n",
      "Iteration 75560: loss = 2.9028233e-11,1.4525983e-10\n",
      "Iteration 75565: loss = 2.902864e-11,1.452577e-10\n",
      "Iteration 75570: loss = 2.9029e-11,1.4525502e-10\n",
      "Iteration 75575: loss = 2.9028523e-11,1.4525367e-10\n",
      "Iteration 75580: loss = 2.9028996e-11,1.4525163e-10\n",
      "Iteration 75585: loss = 2.902843e-11,1.4525092e-10\n",
      "Iteration 75590: loss = 2.9029001e-11,1.4524856e-10\n",
      "Iteration 75595: loss = 2.9029218e-11,1.4524609e-10\n",
      "Iteration 75600: loss = 2.9028266e-11,1.4524594e-10\n",
      "Iteration 75605: loss = 2.9029812e-11,1.4524251e-10\n",
      "Iteration 75610: loss = 2.9028833e-11,1.4524179e-10\n",
      "Iteration 75615: loss = 2.9028118e-11,1.4524039e-10\n",
      "Iteration 75620: loss = 2.9028642e-11,1.4523842e-10\n",
      "Iteration 75625: loss = 2.9028386e-11,1.4523609e-10\n",
      "Iteration 75630: loss = 2.9029132e-11,1.4523398e-10\n",
      "Iteration 75635: loss = 2.9029603e-11,1.452318e-10\n",
      "Iteration 75640: loss = 2.9027641e-11,1.4523162e-10\n",
      "Iteration 75645: loss = 2.9029461e-11,1.4522852e-10\n",
      "Iteration 75650: loss = 2.9029624e-11,1.4522655e-10\n",
      "Iteration 75655: loss = 2.9028453e-11,1.4522625e-10\n",
      "Iteration 75660: loss = 2.9028618e-11,1.4522392e-10\n",
      "Iteration 75665: loss = 2.902996e-11,1.4522117e-10\n",
      "Iteration 75670: loss = 2.9029404e-11,1.4521975e-10\n",
      "Iteration 75675: loss = 2.9029001e-11,1.4521895e-10\n",
      "Iteration 75680: loss = 2.903014e-11,1.4521577e-10\n",
      "Iteration 75685: loss = 2.9029605e-11,1.4521494e-10\n",
      "Iteration 75690: loss = 2.9029385e-11,1.4521269e-10\n",
      "Iteration 75695: loss = 2.902883e-11,1.4521184e-10\n",
      "Iteration 75700: loss = 2.9028925e-11,1.452097e-10\n",
      "Iteration 75705: loss = 2.9029728e-11,1.4520721e-10\n",
      "Iteration 75710: loss = 2.9028552e-11,1.452064e-10\n",
      "Iteration 75715: loss = 2.9028972e-11,1.4520507e-10\n",
      "Iteration 75720: loss = 2.9029057e-11,1.4520266e-10\n",
      "Iteration 75725: loss = 2.902926e-11,1.4520066e-10\n",
      "Iteration 75730: loss = 2.9029007e-11,1.4519919e-10\n",
      "Iteration 75735: loss = 2.9029196e-11,1.451971e-10\n",
      "Iteration 75740: loss = 2.9029328e-11,1.4519586e-10\n",
      "Iteration 75745: loss = 2.902937e-11,1.4519352e-10\n",
      "Iteration 75750: loss = 2.9028816e-11,1.4519229e-10\n",
      "Iteration 75755: loss = 2.9029288e-11,1.4519008e-10\n",
      "Iteration 75760: loss = 2.9029392e-11,1.4518806e-10\n",
      "Iteration 75765: loss = 2.902984e-11,1.451857e-10\n",
      "Iteration 75770: loss = 2.9030027e-11,1.4518423e-10\n",
      "Iteration 75775: loss = 2.9029524e-11,1.4518318e-10\n",
      "Iteration 75780: loss = 2.9029489e-11,1.4518108e-10\n",
      "Iteration 75785: loss = 2.902982e-11,1.4517923e-10\n",
      "Iteration 75790: loss = 2.9030696e-11,1.451765e-10\n",
      "Iteration 75795: loss = 2.9029543e-11,1.451764e-10\n",
      "Iteration 75800: loss = 2.90295e-11,1.4517418e-10\n",
      "Iteration 75805: loss = 2.9029253e-11,1.4517267e-10\n",
      "Iteration 75810: loss = 2.902942e-11,1.451707e-10\n",
      "Iteration 75815: loss = 2.9030035e-11,1.4516802e-10\n",
      "Iteration 75820: loss = 2.9029909e-11,1.4516716e-10\n",
      "Iteration 75825: loss = 2.9029362e-11,1.4516578e-10\n",
      "Iteration 75830: loss = 2.9029267e-11,1.4516364e-10\n",
      "Iteration 75835: loss = 2.9029765e-11,1.4516163e-10\n",
      "Iteration 75840: loss = 2.9031139e-11,1.4515887e-10\n",
      "Iteration 75845: loss = 2.9030086e-11,1.45158e-10\n",
      "Iteration 75850: loss = 2.9029551e-11,1.4515655e-10\n",
      "Iteration 75855: loss = 2.9031196e-11,1.4515357e-10\n",
      "Iteration 75860: loss = 2.902991e-11,1.4515272e-10\n",
      "Iteration 75865: loss = 2.9029446e-11,1.4515134e-10\n",
      "Iteration 75870: loss = 2.9030668e-11,1.4514852e-10\n",
      "Iteration 75875: loss = 2.9029404e-11,1.4514774e-10\n",
      "Iteration 75880: loss = 2.9029978e-11,1.4514559e-10\n",
      "Iteration 75885: loss = 2.903112e-11,1.45143e-10\n",
      "Iteration 75890: loss = 2.90315e-11,1.4514095e-10\n",
      "Iteration 75895: loss = 2.9030346e-11,1.4514023e-10\n",
      "Iteration 75900: loss = 2.9029163e-11,1.4513939e-10\n",
      "Iteration 75905: loss = 2.9029898e-11,1.4513682e-10\n",
      "Iteration 75910: loss = 2.903043e-11,1.4513499e-10\n",
      "Iteration 75915: loss = 2.903231e-11,1.4513146e-10\n",
      "Iteration 75920: loss = 2.9030368e-11,1.4513124e-10\n",
      "Iteration 75925: loss = 2.903013e-11,1.4512981e-10\n",
      "Iteration 75930: loss = 2.9031796e-11,1.4512642e-10\n",
      "Iteration 75935: loss = 2.9029881e-11,1.4512622e-10\n",
      "Iteration 75940: loss = 2.9030292e-11,1.4512438e-10\n",
      "Iteration 75945: loss = 2.9031165e-11,1.4512168e-10\n",
      "Iteration 75950: loss = 2.9030276e-11,1.4512087e-10\n",
      "Iteration 75955: loss = 2.90312e-11,1.4511828e-10\n",
      "Iteration 75960: loss = 2.9030632e-11,1.4511699e-10\n",
      "Iteration 75965: loss = 2.903202e-11,1.4511392e-10\n",
      "Iteration 75970: loss = 2.9030325e-11,1.4511382e-10\n",
      "Iteration 75975: loss = 2.9031458e-11,1.4511096e-10\n",
      "Iteration 75980: loss = 2.9029825e-11,1.4511055e-10\n",
      "Iteration 75985: loss = 2.9030387e-11,1.4510876e-10\n",
      "Iteration 75990: loss = 2.9030866e-11,1.4510637e-10\n",
      "Iteration 75995: loss = 2.903149e-11,1.4510441e-10\n",
      "Iteration 76000: loss = 2.9032375e-11,1.4510115e-10\n",
      "Iteration 76005: loss = 2.903137e-11,1.4510096e-10\n",
      "Iteration 76010: loss = 2.9031666e-11,1.4509892e-10\n",
      "Iteration 76015: loss = 2.9031359e-11,1.4509739e-10\n",
      "Iteration 76020: loss = 2.903159e-11,1.4509516e-10\n",
      "Iteration 76025: loss = 2.9031043e-11,1.4509381e-10\n",
      "Iteration 76030: loss = 2.903145e-11,1.4509174e-10\n",
      "Iteration 76035: loss = 2.9030285e-11,1.4509091e-10\n",
      "Iteration 76040: loss = 2.9030769e-11,1.4508891e-10\n",
      "Iteration 76045: loss = 2.9031381e-11,1.4508669e-10\n",
      "Iteration 76050: loss = 2.9032185e-11,1.4508358e-10\n",
      "Iteration 76055: loss = 2.903241e-11,1.4508164e-10\n",
      "Iteration 76060: loss = 2.9031421e-11,1.4508136e-10\n",
      "Iteration 76065: loss = 2.9031772e-11,1.4507938e-10\n",
      "Iteration 76070: loss = 2.9032153e-11,1.4507731e-10\n",
      "Iteration 76075: loss = 2.9033087e-11,1.4507454e-10\n",
      "Iteration 76080: loss = 2.903137e-11,1.4507444e-10\n",
      "Iteration 76085: loss = 2.9031106e-11,1.450729e-10\n",
      "Iteration 76090: loss = 2.9031657e-11,1.4507073e-10\n",
      "Iteration 76095: loss = 2.903282e-11,1.4506762e-10\n",
      "Iteration 76100: loss = 2.90323e-11,1.4506683e-10\n",
      "Iteration 76105: loss = 2.9032771e-11,1.4506459e-10\n",
      "Iteration 76110: loss = 2.9032929e-11,1.450627e-10\n",
      "Iteration 76115: loss = 2.9030918e-11,1.450625e-10\n",
      "Iteration 76120: loss = 2.9032818e-11,1.4505924e-10\n",
      "Iteration 76125: loss = 2.9033059e-11,1.4505733e-10\n",
      "Iteration 76130: loss = 2.9031317e-11,1.4505716e-10\n",
      "Iteration 76135: loss = 2.9031453e-11,1.450553e-10\n",
      "Iteration 76140: loss = 2.903202e-11,1.4505301e-10\n",
      "Iteration 76145: loss = 2.9032178e-11,1.4505076e-10\n",
      "Iteration 76150: loss = 2.9033817e-11,1.4504745e-10\n",
      "Iteration 76155: loss = 2.9032539e-11,1.4504672e-10\n",
      "Iteration 76160: loss = 2.903155e-11,1.4504645e-10\n",
      "Iteration 76165: loss = 2.9031536e-11,1.4504487e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76170: loss = 2.9031286e-11,1.4504277e-10\n",
      "Iteration 76175: loss = 2.9031751e-11,1.4504076e-10\n",
      "Iteration 76180: loss = 2.9033172e-11,1.4503765e-10\n",
      "Iteration 76185: loss = 2.9035026e-11,1.4503437e-10\n",
      "Iteration 76190: loss = 2.903379e-11,1.4503374e-10\n",
      "Iteration 76195: loss = 2.9031813e-11,1.4503351e-10\n",
      "Iteration 76200: loss = 2.9032214e-11,1.4503164e-10\n",
      "Iteration 76205: loss = 2.9031647e-11,1.4503011e-10\n",
      "Iteration 76210: loss = 2.9032117e-11,1.4502813e-10\n",
      "Iteration 76215: loss = 2.9033678e-11,1.4502467e-10\n",
      "Iteration 76220: loss = 2.9032889e-11,1.45024e-10\n",
      "Iteration 76225: loss = 2.9031513e-11,1.4502388e-10\n",
      "Iteration 76230: loss = 2.903163e-11,1.4502129e-10\n",
      "Iteration 76235: loss = 2.9032146e-11,1.4501932e-10\n",
      "Iteration 76240: loss = 2.9033742e-11,1.4501594e-10\n",
      "Iteration 76245: loss = 2.9035661e-11,1.45013e-10\n",
      "Iteration 76250: loss = 2.903513e-11,1.4501145e-10\n",
      "Iteration 76255: loss = 2.903335e-11,1.4501139e-10\n",
      "Iteration 76260: loss = 2.903223e-11,1.450111e-10\n",
      "Iteration 76265: loss = 2.9032122e-11,1.45009e-10\n",
      "Iteration 76270: loss = 2.903372e-11,1.4500563e-10\n",
      "Iteration 76275: loss = 2.9035621e-11,1.4500227e-10\n",
      "Iteration 76280: loss = 2.903719e-11,1.4499903e-10\n",
      "Iteration 76285: loss = 2.9038077e-11,1.4499554e-10\n",
      "Iteration 76290: loss = 2.9040092e-11,1.4499256e-10\n",
      "Iteration 76295: loss = 2.9041713e-11,1.4498913e-10\n",
      "Iteration 76300: loss = 2.9042843e-11,1.4498576e-10\n",
      "Iteration 76305: loss = 2.9044461e-11,1.4498239e-10\n",
      "Iteration 76310: loss = 2.9046465e-11,1.449791e-10\n",
      "Iteration 76315: loss = 2.9047978e-11,1.4497598e-10\n",
      "Iteration 76320: loss = 2.9048644e-11,1.4497292e-10\n",
      "Iteration 76325: loss = 2.9049757e-11,1.4496998e-10\n",
      "Iteration 76330: loss = 2.9051607e-11,1.4496691e-10\n",
      "Iteration 76335: loss = 2.9052547e-11,1.4496364e-10\n",
      "Iteration 76340: loss = 2.9054389e-11,1.449604e-10\n",
      "Iteration 76345: loss = 2.9055465e-11,1.449576e-10\n",
      "Iteration 76350: loss = 2.9057312e-11,1.4495442e-10\n",
      "Iteration 76355: loss = 2.9057883e-11,1.4495151e-10\n",
      "Iteration 76360: loss = 2.905945e-11,1.4494841e-10\n",
      "Iteration 76365: loss = 2.9060759e-11,1.4494547e-10\n",
      "Iteration 76370: loss = 2.9061524e-11,1.4494242e-10\n",
      "Iteration 76375: loss = 2.9063177e-11,1.4493934e-10\n",
      "Iteration 76380: loss = 2.9064626e-11,1.4493631e-10\n",
      "Iteration 76385: loss = 2.9065922e-11,1.4493333e-10\n",
      "Iteration 76390: loss = 2.9066837e-11,1.4493037e-10\n",
      "Iteration 76395: loss = 2.9068093e-11,1.4492779e-10\n",
      "Iteration 76400: loss = 2.906965e-11,1.4492463e-10\n",
      "Iteration 76405: loss = 2.907121e-11,1.4492127e-10\n",
      "Iteration 76410: loss = 2.9071507e-11,1.449186e-10\n",
      "Iteration 76415: loss = 2.9073235e-11,1.4491558e-10\n",
      "Iteration 76420: loss = 2.9074847e-11,1.4491212e-10\n",
      "Iteration 76425: loss = 2.9075603e-11,1.4490911e-10\n",
      "Iteration 76430: loss = 2.9076706e-11,1.4490668e-10\n",
      "Iteration 76435: loss = 2.9078398e-11,1.449032e-10\n",
      "Iteration 76440: loss = 2.9079966e-11,1.4490009e-10\n",
      "Iteration 76445: loss = 2.908076e-11,1.4489698e-10\n",
      "Iteration 76450: loss = 2.9081805e-11,1.4489435e-10\n",
      "Iteration 76455: loss = 2.9083591e-11,1.4489127e-10\n",
      "Iteration 76460: loss = 2.908425e-11,1.4488799e-10\n",
      "Iteration 76465: loss = 2.9085534e-11,1.448857e-10\n",
      "Iteration 76470: loss = 2.9086974e-11,1.448823e-10\n",
      "Iteration 76475: loss = 2.9088803e-11,1.4487911e-10\n",
      "Iteration 76480: loss = 2.9089009e-11,1.4487644e-10\n",
      "Iteration 76485: loss = 2.9090815e-11,1.4487322e-10\n",
      "Iteration 76490: loss = 2.90924e-11,1.448699e-10\n",
      "Iteration 76495: loss = 2.9092825e-11,1.4486712e-10\n",
      "Iteration 76500: loss = 2.9094102e-11,1.4486431e-10\n",
      "Iteration 76505: loss = 2.9095846e-11,1.4486111e-10\n",
      "Iteration 76510: loss = 2.9096925e-11,1.4485885e-10\n",
      "Iteration 76515: loss = 2.9097598e-11,1.448556e-10\n",
      "Iteration 76520: loss = 2.9099162e-11,1.4485252e-10\n",
      "Iteration 76525: loss = 2.9100118e-11,1.4485009e-10\n",
      "Iteration 76530: loss = 2.9101938e-11,1.448471e-10\n",
      "Iteration 76535: loss = 2.910265e-11,1.4484389e-10\n",
      "Iteration 76540: loss = 2.91039e-11,1.4484114e-10\n",
      "Iteration 76545: loss = 2.910545e-11,1.4483793e-10\n",
      "Iteration 76550: loss = 2.9105943e-11,1.4483537e-10\n",
      "Iteration 76555: loss = 2.9107213e-11,1.4483233e-10\n",
      "Iteration 76560: loss = 2.9108955e-11,1.4482897e-10\n",
      "Iteration 76565: loss = 2.911021e-11,1.4482629e-10\n",
      "Iteration 76570: loss = 2.9110544e-11,1.4482376e-10\n",
      "Iteration 76575: loss = 2.911204e-11,1.4482068e-10\n",
      "Iteration 76580: loss = 2.911354e-11,1.4481742e-10\n",
      "Iteration 76585: loss = 2.9114582e-11,1.4481442e-10\n",
      "Iteration 76590: loss = 2.91161e-11,1.448112e-10\n",
      "Iteration 76595: loss = 2.9116837e-11,1.4480904e-10\n",
      "Iteration 76600: loss = 2.9118364e-11,1.4480582e-10\n",
      "Iteration 76605: loss = 2.911936e-11,1.4480268e-10\n",
      "Iteration 76610: loss = 2.912064e-11,1.4480002e-10\n",
      "Iteration 76615: loss = 2.9121903e-11,1.4479713e-10\n",
      "Iteration 76620: loss = 2.9122704e-11,1.447942e-10\n",
      "Iteration 76625: loss = 2.912394e-11,1.4479123e-10\n",
      "Iteration 76630: loss = 2.9125192e-11,1.4478865e-10\n",
      "Iteration 76635: loss = 2.912669e-11,1.4478543e-10\n",
      "Iteration 76640: loss = 2.912724e-11,1.4478278e-10\n",
      "Iteration 76645: loss = 2.912871e-11,1.447798e-10\n",
      "Iteration 76650: loss = 2.9129785e-11,1.447772e-10\n",
      "Iteration 76655: loss = 2.913027e-11,1.447743e-10\n",
      "Iteration 76660: loss = 2.9131753e-11,1.4477103e-10\n",
      "Iteration 76665: loss = 2.9132755e-11,1.4476845e-10\n",
      "Iteration 76670: loss = 2.9134205e-11,1.4476569e-10\n",
      "Iteration 76675: loss = 2.9134868e-11,1.4476255e-10\n",
      "Iteration 76680: loss = 2.913589e-11,1.447602e-10\n",
      "Iteration 76685: loss = 2.9137272e-11,1.4475728e-10\n",
      "Iteration 76690: loss = 2.913905e-11,1.4475425e-10\n",
      "Iteration 76695: loss = 2.913973e-11,1.4475106e-10\n",
      "Iteration 76700: loss = 2.914115e-11,1.4474791e-10\n",
      "Iteration 76705: loss = 2.914228e-11,1.4474524e-10\n",
      "Iteration 76710: loss = 2.9143184e-11,1.4474219e-10\n",
      "Iteration 76715: loss = 2.9144607e-11,1.4473926e-10\n",
      "Iteration 76720: loss = 2.9145727e-11,1.4473671e-10\n",
      "Iteration 76725: loss = 2.914702e-11,1.4473375e-10\n",
      "Iteration 76730: loss = 2.9147962e-11,1.4473085e-10\n",
      "Iteration 76735: loss = 2.9149284e-11,1.4472785e-10\n",
      "Iteration 76740: loss = 2.9150703e-11,1.4472509e-10\n",
      "Iteration 76745: loss = 2.9151286e-11,1.447221e-10\n",
      "Iteration 76750: loss = 2.915216e-11,1.4471951e-10\n",
      "Iteration 76755: loss = 2.9153055e-11,1.4471703e-10\n",
      "Iteration 76760: loss = 2.9154654e-11,1.4471413e-10\n",
      "Iteration 76765: loss = 2.9155258e-11,1.4471113e-10\n",
      "Iteration 76770: loss = 2.9156802e-11,1.4470802e-10\n",
      "Iteration 76775: loss = 2.9157895e-11,1.4470554e-10\n",
      "Iteration 76780: loss = 2.9158242e-11,1.447029e-10\n",
      "Iteration 76785: loss = 2.9159588e-11,1.4469999e-10\n",
      "Iteration 76790: loss = 2.9161073e-11,1.4469702e-10\n",
      "Iteration 76795: loss = 2.916182e-11,1.4469469e-10\n",
      "Iteration 76800: loss = 2.9162568e-11,1.446919e-10\n",
      "Iteration 76805: loss = 2.9163668e-11,1.4468905e-10\n",
      "Iteration 76810: loss = 2.9165313e-11,1.4468618e-10\n",
      "Iteration 76815: loss = 2.9166468e-11,1.4468343e-10\n",
      "Iteration 76820: loss = 2.916697e-11,1.4468043e-10\n",
      "Iteration 76825: loss = 2.916839e-11,1.4467758e-10\n",
      "Iteration 76830: loss = 2.9169597e-11,1.4467458e-10\n",
      "Iteration 76835: loss = 2.917032e-11,1.4467166e-10\n",
      "Iteration 76840: loss = 2.917152e-11,1.446688e-10\n",
      "Iteration 76845: loss = 2.9173018e-11,1.4466596e-10\n",
      "Iteration 76850: loss = 2.917421e-11,1.4466324e-10\n",
      "Iteration 76855: loss = 2.9174687e-11,1.4466026e-10\n",
      "Iteration 76860: loss = 2.9176075e-11,1.446578e-10\n",
      "Iteration 76865: loss = 2.9177286e-11,1.4465458e-10\n",
      "Iteration 76870: loss = 2.9178018e-11,1.4465193e-10\n",
      "Iteration 76875: loss = 2.9179156e-11,1.4464943e-10\n",
      "Iteration 76880: loss = 2.9180353e-11,1.446463e-10\n",
      "Iteration 76885: loss = 2.9181758e-11,1.4464341e-10\n",
      "Iteration 76890: loss = 2.9182257e-11,1.4464077e-10\n",
      "Iteration 76895: loss = 2.918374e-11,1.4463795e-10\n",
      "Iteration 76900: loss = 2.9184936e-11,1.4463525e-10\n",
      "Iteration 76905: loss = 2.9185616e-11,1.4463233e-10\n",
      "Iteration 76910: loss = 2.9186823e-11,1.446295e-10\n",
      "Iteration 76915: loss = 2.918809e-11,1.4462673e-10\n",
      "Iteration 76920: loss = 2.9189557e-11,1.4462391e-10\n",
      "Iteration 76925: loss = 2.9189953e-11,1.4462108e-10\n",
      "Iteration 76930: loss = 2.9191392e-11,1.4461808e-10\n",
      "Iteration 76935: loss = 2.919257e-11,1.4461536e-10\n",
      "Iteration 76940: loss = 2.9193824e-11,1.4461238e-10\n",
      "Iteration 76945: loss = 2.9194508e-11,1.4460962e-10\n",
      "Iteration 76950: loss = 2.9195684e-11,1.4460688e-10\n",
      "Iteration 76955: loss = 2.9197197e-11,1.4460405e-10\n",
      "Iteration 76960: loss = 2.9197644e-11,1.446012e-10\n",
      "Iteration 76965: loss = 2.9199126e-11,1.4459836e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 76970: loss = 2.920023e-11,1.4459553e-10\n",
      "Iteration 76975: loss = 2.9201468e-11,1.4459274e-10\n",
      "Iteration 76980: loss = 2.920222e-11,1.4458978e-10\n",
      "Iteration 76985: loss = 2.920349e-11,1.4458701e-10\n",
      "Iteration 76990: loss = 2.9204916e-11,1.4458418e-10\n",
      "Iteration 76995: loss = 2.9205267e-11,1.4458137e-10\n",
      "Iteration 77000: loss = 2.9206526e-11,1.4457852e-10\n",
      "Iteration 77005: loss = 2.9208008e-11,1.4457566e-10\n",
      "Iteration 77010: loss = 2.920916e-11,1.4457276e-10\n",
      "Iteration 77015: loss = 2.9209826e-11,1.4457005e-10\n",
      "Iteration 77020: loss = 2.9210977e-11,1.4456698e-10\n",
      "Iteration 77025: loss = 2.9212185e-11,1.4456435e-10\n",
      "Iteration 77030: loss = 2.9212955e-11,1.4456146e-10\n",
      "Iteration 77035: loss = 2.9214152e-11,1.4455859e-10\n",
      "Iteration 77040: loss = 2.921555e-11,1.4455587e-10\n",
      "Iteration 77045: loss = 2.9216806e-11,1.4455326e-10\n",
      "Iteration 77050: loss = 2.921759e-11,1.4455033e-10\n",
      "Iteration 77055: loss = 2.9218697e-11,1.4454753e-10\n",
      "Iteration 77060: loss = 2.9219932e-11,1.445446e-10\n",
      "Iteration 77065: loss = 2.922135e-11,1.4454174e-10\n",
      "Iteration 77070: loss = 2.9221812e-11,1.4453912e-10\n",
      "Iteration 77075: loss = 2.922329e-11,1.4453627e-10\n",
      "Iteration 77080: loss = 2.9224474e-11,1.4453343e-10\n",
      "Iteration 77085: loss = 2.9224925e-11,1.4453076e-10\n",
      "Iteration 77090: loss = 2.922636e-11,1.445278e-10\n",
      "Iteration 77095: loss = 2.922758e-11,1.4452486e-10\n",
      "Iteration 77100: loss = 2.9229102e-11,1.4452232e-10\n",
      "Iteration 77105: loss = 2.9229476e-11,1.4451941e-10\n",
      "Iteration 77110: loss = 2.9231017e-11,1.4451638e-10\n",
      "Iteration 77115: loss = 2.9232172e-11,1.445135e-10\n",
      "Iteration 77120: loss = 2.9232578e-11,1.4451075e-10\n",
      "Iteration 77125: loss = 2.923405e-11,1.4450789e-10\n",
      "Iteration 77130: loss = 2.923535e-11,1.4450517e-10\n",
      "Iteration 77135: loss = 2.923666e-11,1.4450255e-10\n",
      "Iteration 77140: loss = 2.9237137e-11,1.4449958e-10\n",
      "Iteration 77145: loss = 2.9238355e-11,1.44497e-10\n",
      "Iteration 77150: loss = 2.923984e-11,1.4449411e-10\n",
      "Iteration 77155: loss = 2.9240253e-11,1.4449125e-10\n",
      "Iteration 77160: loss = 2.9241692e-11,1.4448837e-10\n",
      "Iteration 77165: loss = 2.9242882e-11,1.4448565e-10\n",
      "Iteration 77170: loss = 2.9244242e-11,1.444829e-10\n",
      "Iteration 77175: loss = 2.9244586e-11,1.4448014e-10\n",
      "Iteration 77180: loss = 2.9245766e-11,1.4447743e-10\n",
      "Iteration 77185: loss = 2.924719e-11,1.4447483e-10\n",
      "Iteration 77190: loss = 2.9247632e-11,1.444718e-10\n",
      "Iteration 77195: loss = 2.9248996e-11,1.4446924e-10\n",
      "Iteration 77200: loss = 2.9250078e-11,1.4446644e-10\n",
      "Iteration 77205: loss = 2.925121e-11,1.4446347e-10\n",
      "Iteration 77210: loss = 2.9251934e-11,1.444607e-10\n",
      "Iteration 77215: loss = 2.9253006e-11,1.4445793e-10\n",
      "Iteration 77220: loss = 2.925438e-11,1.4445521e-10\n",
      "Iteration 77225: loss = 2.9255546e-11,1.4445242e-10\n",
      "Iteration 77230: loss = 2.9256198e-11,1.4444984e-10\n",
      "Iteration 77235: loss = 2.9257458e-11,1.44447e-10\n",
      "Iteration 77240: loss = 2.925851e-11,1.444443e-10\n",
      "Iteration 77245: loss = 2.9259154e-11,1.4444164e-10\n",
      "Iteration 77250: loss = 2.9260254e-11,1.4443892e-10\n",
      "Iteration 77255: loss = 2.926176e-11,1.4443619e-10\n",
      "Iteration 77260: loss = 2.9262835e-11,1.4443369e-10\n",
      "Iteration 77265: loss = 2.926319e-11,1.4443045e-10\n",
      "Iteration 77270: loss = 2.9264594e-11,1.4442794e-10\n",
      "Iteration 77275: loss = 2.926578e-11,1.4442511e-10\n",
      "Iteration 77280: loss = 2.9266357e-11,1.4442247e-10\n",
      "Iteration 77285: loss = 2.926744e-11,1.4441963e-10\n",
      "Iteration 77290: loss = 2.9268588e-11,1.4441685e-10\n",
      "Iteration 77295: loss = 2.9270062e-11,1.444141e-10\n",
      "Iteration 77300: loss = 2.927036e-11,1.4441133e-10\n",
      "Iteration 77305: loss = 2.9271766e-11,1.4440843e-10\n",
      "Iteration 77310: loss = 2.927288e-11,1.4440613e-10\n",
      "Iteration 77315: loss = 2.9273504e-11,1.4440339e-10\n",
      "Iteration 77320: loss = 2.9274593e-11,1.4440066e-10\n",
      "Iteration 77325: loss = 2.927578e-11,1.4439767e-10\n",
      "Iteration 77330: loss = 2.927705e-11,1.4439501e-10\n",
      "Iteration 77335: loss = 2.927746e-11,1.4439229e-10\n",
      "Iteration 77340: loss = 2.927884e-11,1.4438946e-10\n",
      "Iteration 77345: loss = 2.927993e-11,1.4438682e-10\n",
      "Iteration 77350: loss = 2.9281053e-11,1.443841e-10\n",
      "Iteration 77355: loss = 2.9281657e-11,1.4438142e-10\n",
      "Iteration 77360: loss = 2.9282746e-11,1.4437883e-10\n",
      "Iteration 77365: loss = 2.9284162e-11,1.4437619e-10\n",
      "Iteration 77370: loss = 2.928455e-11,1.443735e-10\n",
      "Iteration 77375: loss = 2.928582e-11,1.4437085e-10\n",
      "Iteration 77380: loss = 2.9286955e-11,1.4436792e-10\n",
      "Iteration 77385: loss = 2.9288103e-11,1.443654e-10\n",
      "Iteration 77390: loss = 2.9288814e-11,1.4436258e-10\n",
      "Iteration 77395: loss = 2.928979e-11,1.4435982e-10\n",
      "Iteration 77400: loss = 2.929134e-11,1.4435689e-10\n",
      "Iteration 77405: loss = 2.9290653e-11,1.4435572e-10\n",
      "Iteration 77410: loss = 2.928838e-11,1.4435564e-10\n",
      "Iteration 77415: loss = 2.9289383e-11,1.4435333e-10\n",
      "Iteration 77420: loss = 2.9290046e-11,1.4435098e-10\n",
      "Iteration 77425: loss = 2.9289512e-11,1.443499e-10\n",
      "Iteration 77430: loss = 2.929092e-11,1.4434684e-10\n",
      "Iteration 77435: loss = 2.928963e-11,1.4434619e-10\n",
      "Iteration 77440: loss = 2.929034e-11,1.4434365e-10\n",
      "Iteration 77445: loss = 2.9288853e-11,1.4434298e-10\n",
      "Iteration 77450: loss = 2.9290567e-11,1.4434005e-10\n",
      "Iteration 77455: loss = 2.9289803e-11,1.4433885e-10\n",
      "Iteration 77460: loss = 2.929003e-11,1.4433696e-10\n",
      "Iteration 77465: loss = 2.929002e-11,1.4433517e-10\n",
      "Iteration 77470: loss = 2.9289234e-11,1.4433388e-10\n",
      "Iteration 77475: loss = 2.9290306e-11,1.4433163e-10\n",
      "Iteration 77480: loss = 2.928879e-11,1.4433113e-10\n",
      "Iteration 77485: loss = 2.9289363e-11,1.4432895e-10\n",
      "Iteration 77490: loss = 2.9289796e-11,1.4432706e-10\n",
      "Iteration 77495: loss = 2.9289654e-11,1.4432489e-10\n",
      "Iteration 77500: loss = 2.928992e-11,1.4432289e-10\n",
      "Iteration 77505: loss = 2.9289196e-11,1.4432167e-10\n",
      "Iteration 77510: loss = 2.92892e-11,1.4432062e-10\n",
      "Iteration 77515: loss = 2.9288863e-11,1.4431863e-10\n",
      "Iteration 77520: loss = 2.9289255e-11,1.4431664e-10\n",
      "Iteration 77525: loss = 2.928958e-11,1.4431445e-10\n",
      "Iteration 77530: loss = 2.9291083e-11,1.4431188e-10\n",
      "Iteration 77535: loss = 2.9289588e-11,1.4431126e-10\n",
      "Iteration 77540: loss = 2.929053e-11,1.443089e-10\n",
      "Iteration 77545: loss = 2.928905e-11,1.4430833e-10\n",
      "Iteration 77550: loss = 2.9290896e-11,1.443053e-10\n",
      "Iteration 77555: loss = 2.9289415e-11,1.4430471e-10\n",
      "Iteration 77560: loss = 2.928909e-11,1.4430351e-10\n",
      "Iteration 77565: loss = 2.9289463e-11,1.4430142e-10\n",
      "Iteration 77570: loss = 2.928899e-11,1.4429939e-10\n",
      "Iteration 77575: loss = 2.928945e-11,1.4429757e-10\n",
      "Iteration 77580: loss = 2.9290584e-11,1.4429505e-10\n",
      "Iteration 77585: loss = 2.9289075e-11,1.4429452e-10\n",
      "Iteration 77590: loss = 2.928986e-11,1.4429208e-10\n",
      "Iteration 77595: loss = 2.9289404e-11,1.4429086e-10\n",
      "Iteration 77600: loss = 2.928936e-11,1.44289e-10\n",
      "Iteration 77605: loss = 2.929041e-11,1.4428625e-10\n",
      "Iteration 77610: loss = 2.9288145e-11,1.4428649e-10\n",
      "Iteration 77615: loss = 2.928874e-11,1.4428425e-10\n",
      "Iteration 77620: loss = 2.9289137e-11,1.4428231e-10\n",
      "Iteration 77625: loss = 2.9288648e-11,1.4428062e-10\n",
      "Iteration 77630: loss = 2.928875e-11,1.4427896e-10\n",
      "Iteration 77635: loss = 2.9288832e-11,1.4427724e-10\n",
      "Iteration 77640: loss = 2.9288905e-11,1.4427573e-10\n",
      "Iteration 77645: loss = 2.9288526e-11,1.442741e-10\n",
      "Iteration 77650: loss = 2.9288648e-11,1.4427237e-10\n",
      "Iteration 77655: loss = 2.928913e-11,1.4427025e-10\n",
      "Iteration 77660: loss = 2.9289855e-11,1.4426738e-10\n",
      "Iteration 77665: loss = 2.9289904e-11,1.4426559e-10\n",
      "Iteration 77670: loss = 2.9288627e-11,1.4426561e-10\n",
      "Iteration 77675: loss = 2.9289324e-11,1.442634e-10\n",
      "Iteration 77680: loss = 2.9290317e-11,1.4426088e-10\n",
      "Iteration 77685: loss = 2.9289546e-11,1.4425965e-10\n",
      "Iteration 77690: loss = 2.928876e-11,1.4425863e-10\n",
      "Iteration 77695: loss = 2.929047e-11,1.4425576e-10\n",
      "Iteration 77700: loss = 2.9288922e-11,1.4425507e-10\n",
      "Iteration 77705: loss = 2.928999e-11,1.4425248e-10\n",
      "Iteration 77710: loss = 2.928928e-11,1.4425132e-10\n",
      "Iteration 77715: loss = 2.928927e-11,1.4424947e-10\n",
      "Iteration 77720: loss = 2.928947e-11,1.4424767e-10\n",
      "Iteration 77725: loss = 2.9288728e-11,1.4424656e-10\n",
      "Iteration 77730: loss = 2.928977e-11,1.4424409e-10\n",
      "Iteration 77735: loss = 2.928981e-11,1.4424223e-10\n",
      "Iteration 77740: loss = 2.9289033e-11,1.4424116e-10\n",
      "Iteration 77745: loss = 2.928928e-11,1.4423929e-10\n",
      "Iteration 77750: loss = 2.928861e-11,1.4423798e-10\n",
      "Iteration 77755: loss = 2.928897e-11,1.4423601e-10\n",
      "Iteration 77760: loss = 2.9288568e-11,1.4423418e-10\n",
      "Iteration 77765: loss = 2.928923e-11,1.4423207e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77770: loss = 2.9289696e-11,1.442298e-10\n",
      "Iteration 77775: loss = 2.9287763e-11,1.4422999e-10\n",
      "Iteration 77780: loss = 2.928928e-11,1.4422666e-10\n",
      "Iteration 77785: loss = 2.928926e-11,1.442249e-10\n",
      "Iteration 77790: loss = 2.928881e-11,1.4422369e-10\n",
      "Iteration 77795: loss = 2.9288776e-11,1.4422209e-10\n",
      "Iteration 77800: loss = 2.9289116e-11,1.442201e-10\n",
      "Iteration 77805: loss = 2.9289095e-11,1.4421818e-10\n",
      "Iteration 77810: loss = 2.9289102e-11,1.4421606e-10\n",
      "Iteration 77815: loss = 2.9287933e-11,1.4421533e-10\n",
      "Iteration 77820: loss = 2.9288315e-11,1.4421347e-10\n",
      "Iteration 77825: loss = 2.928871e-11,1.4421161e-10\n",
      "Iteration 77830: loss = 2.9289123e-11,1.4420973e-10\n",
      "Iteration 77835: loss = 2.928874e-11,1.4420799e-10\n",
      "Iteration 77840: loss = 2.928909e-11,1.4420615e-10\n",
      "Iteration 77845: loss = 2.9289734e-11,1.4420382e-10\n",
      "Iteration 77850: loss = 2.9287597e-11,1.4420386e-10\n",
      "Iteration 77855: loss = 2.928913e-11,1.4420101e-10\n",
      "Iteration 77860: loss = 2.9289095e-11,1.4419921e-10\n",
      "Iteration 77865: loss = 2.9287194e-11,1.4419924e-10\n",
      "Iteration 77870: loss = 2.928811e-11,1.4419654e-10\n",
      "Iteration 77875: loss = 2.9288474e-11,1.4419475e-10\n",
      "Iteration 77880: loss = 2.9288173e-11,1.4419288e-10\n",
      "Iteration 77885: loss = 2.9288728e-11,1.4419083e-10\n",
      "Iteration 77890: loss = 2.928936e-11,1.4418848e-10\n",
      "Iteration 77895: loss = 2.928963e-11,1.4418684e-10\n",
      "Iteration 77900: loss = 2.92876e-11,1.4418663e-10\n",
      "Iteration 77905: loss = 2.9287853e-11,1.4418472e-10\n",
      "Iteration 77910: loss = 2.9289276e-11,1.4418176e-10\n",
      "Iteration 77915: loss = 2.9289262e-11,1.4417986e-10\n",
      "Iteration 77920: loss = 2.928751e-11,1.4417983e-10\n",
      "Iteration 77925: loss = 2.9288887e-11,1.441766e-10\n",
      "Iteration 77930: loss = 2.9288495e-11,1.4417531e-10\n",
      "Iteration 77935: loss = 2.9288585e-11,1.4417359e-10\n",
      "Iteration 77940: loss = 2.9288173e-11,1.4417241e-10\n",
      "Iteration 77945: loss = 2.9288995e-11,1.441697e-10\n",
      "Iteration 77950: loss = 2.9287583e-11,1.4416915e-10\n",
      "Iteration 77955: loss = 2.928829e-11,1.4416693e-10\n",
      "Iteration 77960: loss = 2.9288735e-11,1.441648e-10\n",
      "Iteration 77965: loss = 2.9289068e-11,1.4416313e-10\n",
      "Iteration 77970: loss = 2.928808e-11,1.4416199e-10\n",
      "Iteration 77975: loss = 2.92885e-11,1.4415996e-10\n",
      "Iteration 77980: loss = 2.9288374e-11,1.441581e-10\n",
      "Iteration 77985: loss = 2.9289054e-11,1.4415595e-10\n",
      "Iteration 77990: loss = 2.928802e-11,1.441551e-10\n",
      "Iteration 77995: loss = 2.928802e-11,1.4415319e-10\n",
      "Iteration 78000: loss = 2.928893e-11,1.4415097e-10\n",
      "Iteration 78005: loss = 2.9288315e-11,1.4414958e-10\n",
      "Iteration 78010: loss = 2.9288603e-11,1.4414758e-10\n",
      "Iteration 78015: loss = 2.9287947e-11,1.4414625e-10\n",
      "Iteration 78020: loss = 2.9287517e-11,1.4414492e-10\n",
      "Iteration 78025: loss = 2.9287853e-11,1.4414282e-10\n",
      "Iteration 78030: loss = 2.9288544e-11,1.4414074e-10\n",
      "Iteration 78035: loss = 2.9288925e-11,1.4413876e-10\n",
      "Iteration 78040: loss = 2.9288332e-11,1.4413756e-10\n",
      "Iteration 78045: loss = 2.9289e-11,1.4413513e-10\n",
      "Iteration 78050: loss = 2.928767e-11,1.4413441e-10\n",
      "Iteration 78055: loss = 2.9288096e-11,1.4413248e-10\n",
      "Iteration 78060: loss = 2.9288447e-11,1.4413065e-10\n",
      "Iteration 78065: loss = 2.9288034e-11,1.4412882e-10\n",
      "Iteration 78070: loss = 2.928853e-11,1.4412664e-10\n",
      "Iteration 78075: loss = 2.928793e-11,1.4412595e-10\n",
      "Iteration 78080: loss = 2.92882e-11,1.4412353e-10\n",
      "Iteration 78085: loss = 2.928766e-11,1.4412203e-10\n",
      "Iteration 78090: loss = 2.928805e-11,1.4412035e-10\n",
      "Iteration 78095: loss = 2.9288055e-11,1.4411826e-10\n",
      "Iteration 78100: loss = 2.9288436e-11,1.4411707e-10\n",
      "Iteration 78105: loss = 2.9288308e-11,1.4411529e-10\n",
      "Iteration 78110: loss = 2.928793e-11,1.4411407e-10\n",
      "Iteration 78115: loss = 2.928757e-11,1.4411222e-10\n",
      "Iteration 78120: loss = 2.9287812e-11,1.441103e-10\n",
      "Iteration 78125: loss = 2.9288228e-11,1.4410842e-10\n",
      "Iteration 78130: loss = 2.9287794e-11,1.4410632e-10\n",
      "Iteration 78135: loss = 2.928874e-11,1.4410395e-10\n",
      "Iteration 78140: loss = 2.9288884e-11,1.4410226e-10\n",
      "Iteration 78145: loss = 2.9287933e-11,1.4410201e-10\n",
      "Iteration 78150: loss = 2.9287947e-11,1.4410013e-10\n",
      "Iteration 78155: loss = 2.9288166e-11,1.4409766e-10\n",
      "Iteration 78160: loss = 2.9288533e-11,1.4409585e-10\n",
      "Iteration 78165: loss = 2.92872e-11,1.440951e-10\n",
      "Iteration 78170: loss = 2.9287277e-11,1.4409318e-10\n",
      "Iteration 78175: loss = 2.9288027e-11,1.4409092e-10\n",
      "Iteration 78180: loss = 2.9287555e-11,1.440893e-10\n",
      "Iteration 78185: loss = 2.9287964e-11,1.4408755e-10\n",
      "Iteration 78190: loss = 2.9288075e-11,1.4408623e-10\n",
      "Iteration 78195: loss = 2.9288e-11,1.4408463e-10\n",
      "Iteration 78200: loss = 2.9287697e-11,1.4408263e-10\n",
      "Iteration 78205: loss = 2.928817e-11,1.4408058e-10\n",
      "Iteration 78210: loss = 2.928826e-11,1.4407878e-10\n",
      "Iteration 78215: loss = 2.928792e-11,1.440772e-10\n",
      "Iteration 78220: loss = 2.9288932e-11,1.4407486e-10\n",
      "Iteration 78225: loss = 2.9287673e-11,1.4407399e-10\n",
      "Iteration 78230: loss = 2.928861e-11,1.4407159e-10\n",
      "Iteration 78235: loss = 2.928654e-11,1.4407157e-10\n",
      "Iteration 78240: loss = 2.9287347e-11,1.4406924e-10\n",
      "Iteration 78245: loss = 2.928766e-11,1.4406736e-10\n",
      "Iteration 78250: loss = 2.9287413e-11,1.440655e-10\n",
      "Iteration 78255: loss = 2.9287676e-11,1.4406357e-10\n",
      "Iteration 78260: loss = 2.928898e-11,1.4406082e-10\n",
      "Iteration 78265: loss = 2.928703e-11,1.4406067e-10\n",
      "Iteration 78270: loss = 2.9287798e-11,1.4405831e-10\n",
      "Iteration 78275: loss = 2.9287905e-11,1.4405653e-10\n",
      "Iteration 78280: loss = 2.9286937e-11,1.4405568e-10\n",
      "Iteration 78285: loss = 2.928716e-11,1.4405369e-10\n",
      "Iteration 78290: loss = 2.92875e-11,1.4405216e-10\n",
      "Iteration 78295: loss = 2.92877e-11,1.4405031e-10\n",
      "Iteration 78300: loss = 2.9286875e-11,1.4404868e-10\n",
      "Iteration 78305: loss = 2.928724e-11,1.440466e-10\n",
      "Iteration 78310: loss = 2.928754e-11,1.4404485e-10\n",
      "Iteration 78315: loss = 2.9288003e-11,1.4404303e-10\n",
      "Iteration 78320: loss = 2.9288176e-11,1.440405e-10\n",
      "Iteration 78325: loss = 2.928767e-11,1.4403968e-10\n",
      "Iteration 78330: loss = 2.9287406e-11,1.4403836e-10\n",
      "Iteration 78335: loss = 2.928789e-11,1.4403537e-10\n",
      "Iteration 78340: loss = 2.928743e-11,1.4403448e-10\n",
      "Iteration 78345: loss = 2.9287926e-11,1.4403262e-10\n",
      "Iteration 78350: loss = 2.928732e-11,1.4403131e-10\n",
      "Iteration 78355: loss = 2.928867e-11,1.4402855e-10\n",
      "Iteration 78360: loss = 2.9287395e-11,1.4402772e-10\n",
      "Iteration 78365: loss = 2.9288183e-11,1.4402549e-10\n",
      "Iteration 78370: loss = 2.928698e-11,1.4402457e-10\n",
      "Iteration 78375: loss = 2.928733e-11,1.4402278e-10\n",
      "Iteration 78380: loss = 2.9287055e-11,1.4402073e-10\n",
      "Iteration 78385: loss = 2.9287e-11,1.4401941e-10\n",
      "Iteration 78390: loss = 2.9287166e-11,1.4401788e-10\n",
      "Iteration 78395: loss = 2.9287572e-11,1.4401519e-10\n",
      "Iteration 78400: loss = 2.9287284e-11,1.4401434e-10\n",
      "Iteration 78405: loss = 2.928741e-11,1.4401193e-10\n",
      "Iteration 78410: loss = 2.928711e-11,1.4401061e-10\n",
      "Iteration 78415: loss = 2.9287378e-11,1.4400905e-10\n",
      "Iteration 78420: loss = 2.9287097e-11,1.4400714e-10\n",
      "Iteration 78425: loss = 2.9286695e-11,1.4400568e-10\n",
      "Iteration 78430: loss = 2.928699e-11,1.4400382e-10\n",
      "Iteration 78435: loss = 2.92876e-11,1.4400164e-10\n",
      "Iteration 78440: loss = 2.9287104e-11,1.4400015e-10\n",
      "Iteration 78445: loss = 2.9287534e-11,1.4399823e-10\n",
      "Iteration 78450: loss = 2.928699e-11,1.4399675e-10\n",
      "Iteration 78455: loss = 2.9287208e-11,1.439949e-10\n",
      "Iteration 78460: loss = 2.9287267e-11,1.4399333e-10\n",
      "Iteration 78465: loss = 2.928764e-11,1.4399099e-10\n",
      "Iteration 78470: loss = 2.9286424e-11,1.4399039e-10\n",
      "Iteration 78475: loss = 2.9286917e-11,1.4398857e-10\n",
      "Iteration 78480: loss = 2.9287368e-11,1.439864e-10\n",
      "Iteration 78485: loss = 2.928763e-11,1.4398403e-10\n",
      "Iteration 78490: loss = 2.9287166e-11,1.4398321e-10\n",
      "Iteration 78495: loss = 2.9286972e-11,1.439818e-10\n",
      "Iteration 78500: loss = 2.9286667e-11,1.4397979e-10\n",
      "Iteration 78505: loss = 2.9287055e-11,1.4397775e-10\n",
      "Iteration 78510: loss = 2.928765e-11,1.4397555e-10\n",
      "Iteration 78515: loss = 2.9286785e-11,1.4397467e-10\n",
      "Iteration 78520: loss = 2.928697e-11,1.4397261e-10\n",
      "Iteration 78525: loss = 2.9287486e-11,1.4397052e-10\n",
      "Iteration 78530: loss = 2.9286285e-11,1.4396961e-10\n",
      "Iteration 78535: loss = 2.9286434e-11,1.4396774e-10\n",
      "Iteration 78540: loss = 2.928706e-11,1.4396631e-10\n",
      "Iteration 78545: loss = 2.9286663e-11,1.4396426e-10\n",
      "Iteration 78550: loss = 2.928748e-11,1.4396208e-10\n",
      "Iteration 78555: loss = 2.928633e-11,1.4396125e-10\n",
      "Iteration 78560: loss = 2.9286743e-11,1.4395918e-10\n",
      "Iteration 78565: loss = 2.9287493e-11,1.4395679e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78570: loss = 2.928629e-11,1.4395597e-10\n",
      "Iteration 78575: loss = 2.9286813e-11,1.4395403e-10\n",
      "Iteration 78580: loss = 2.9286993e-11,1.4395204e-10\n",
      "Iteration 78585: loss = 2.928739e-11,1.4395002e-10\n",
      "Iteration 78590: loss = 2.9286244e-11,1.4394931e-10\n",
      "Iteration 78595: loss = 2.9286688e-11,1.4394737e-10\n",
      "Iteration 78600: loss = 2.9287166e-11,1.4394527e-10\n",
      "Iteration 78605: loss = 2.928698e-11,1.4394315e-10\n",
      "Iteration 78610: loss = 2.9287326e-11,1.439415e-10\n",
      "Iteration 78615: loss = 2.9287545e-11,1.4393972e-10\n",
      "Iteration 78620: loss = 2.9287347e-11,1.4393818e-10\n",
      "Iteration 78625: loss = 2.9286643e-11,1.4393706e-10\n",
      "Iteration 78630: loss = 2.9287198e-11,1.4393506e-10\n",
      "Iteration 78635: loss = 2.9285612e-11,1.4393428e-10\n",
      "Iteration 78640: loss = 2.928618e-11,1.4393235e-10\n",
      "Iteration 78645: loss = 2.928682e-11,1.4393045e-10\n",
      "Iteration 78650: loss = 2.9286452e-11,1.4392848e-10\n",
      "Iteration 78655: loss = 2.9286813e-11,1.4392645e-10\n",
      "Iteration 78660: loss = 2.9287177e-11,1.439245e-10\n",
      "Iteration 78665: loss = 2.928819e-11,1.4392207e-10\n",
      "Iteration 78670: loss = 2.9286486e-11,1.4392174e-10\n",
      "Iteration 78675: loss = 2.928678e-11,1.4391976e-10\n",
      "Iteration 78680: loss = 2.9286275e-11,1.4391827e-10\n",
      "Iteration 78685: loss = 2.9286792e-11,1.4391596e-10\n",
      "Iteration 78690: loss = 2.928631e-11,1.4391438e-10\n",
      "Iteration 78695: loss = 2.9286844e-11,1.4391245e-10\n",
      "Iteration 78700: loss = 2.9286337e-11,1.4391079e-10\n",
      "Iteration 78705: loss = 2.92873e-11,1.4390869e-10\n",
      "Iteration 78710: loss = 2.9285328e-11,1.4390836e-10\n",
      "Iteration 78715: loss = 2.9286327e-11,1.4390601e-10\n",
      "Iteration 78720: loss = 2.9286847e-11,1.4390387e-10\n",
      "Iteration 78725: loss = 2.9286632e-11,1.4390247e-10\n",
      "Iteration 78730: loss = 2.9286577e-11,1.439008e-10\n",
      "Iteration 78735: loss = 2.928617e-11,1.438992e-10\n",
      "Iteration 78740: loss = 2.9286153e-11,1.4389762e-10\n",
      "Iteration 78745: loss = 2.9286625e-11,1.4389542e-10\n",
      "Iteration 78750: loss = 2.9285796e-11,1.4389458e-10\n",
      "Iteration 78755: loss = 2.9286393e-11,1.438924e-10\n",
      "Iteration 78760: loss = 2.9286722e-11,1.4389034e-10\n",
      "Iteration 78765: loss = 2.9286743e-11,1.4388829e-10\n",
      "Iteration 78770: loss = 2.928635e-11,1.4388743e-10\n",
      "Iteration 78775: loss = 2.928659e-11,1.4388538e-10\n",
      "Iteration 78780: loss = 2.928612e-11,1.4388415e-10\n",
      "Iteration 78785: loss = 2.9285945e-11,1.43882e-10\n",
      "Iteration 78790: loss = 2.9287444e-11,1.4387916e-10\n",
      "Iteration 78795: loss = 2.9286195e-11,1.4387899e-10\n",
      "Iteration 78800: loss = 2.928676e-11,1.4387687e-10\n",
      "Iteration 78805: loss = 2.9286306e-11,1.4387541e-10\n",
      "Iteration 78810: loss = 2.92863e-11,1.4387377e-10\n",
      "Iteration 78815: loss = 2.92861e-11,1.438716e-10\n",
      "Iteration 78820: loss = 2.9286153e-11,1.4387033e-10\n",
      "Iteration 78825: loss = 2.9286115e-11,1.4386872e-10\n",
      "Iteration 78830: loss = 2.9286278e-11,1.4386638e-10\n",
      "Iteration 78835: loss = 2.9285897e-11,1.4386564e-10\n",
      "Iteration 78840: loss = 2.9285463e-11,1.4386355e-10\n",
      "Iteration 78845: loss = 2.928599e-11,1.438614e-10\n",
      "Iteration 78850: loss = 2.928734e-11,1.4385873e-10\n",
      "Iteration 78855: loss = 2.928543e-11,1.438584e-10\n",
      "Iteration 78860: loss = 2.9286077e-11,1.4385648e-10\n",
      "Iteration 78865: loss = 2.9286285e-11,1.438542e-10\n",
      "Iteration 78870: loss = 2.9285383e-11,1.4385333e-10\n",
      "Iteration 78875: loss = 2.928582e-11,1.4385125e-10\n",
      "Iteration 78880: loss = 2.9285713e-11,1.4385039e-10\n",
      "Iteration 78885: loss = 2.928587e-11,1.4384806e-10\n",
      "Iteration 78890: loss = 2.9285584e-11,1.4384627e-10\n",
      "Iteration 78895: loss = 2.928568e-11,1.4384471e-10\n",
      "Iteration 78900: loss = 2.9285723e-11,1.438431e-10\n",
      "Iteration 78905: loss = 2.928523e-11,1.4384165e-10\n",
      "Iteration 78910: loss = 2.9285855e-11,1.4383923e-10\n",
      "Iteration 78915: loss = 2.9285654e-11,1.4383787e-10\n",
      "Iteration 78920: loss = 2.9286344e-11,1.4383561e-10\n",
      "Iteration 78925: loss = 2.928512e-11,1.438347e-10\n",
      "Iteration 78930: loss = 2.9285782e-11,1.4383267e-10\n",
      "Iteration 78935: loss = 2.928702e-11,1.4382988e-10\n",
      "Iteration 78940: loss = 2.928504e-11,1.4382973e-10\n",
      "Iteration 78945: loss = 2.9285623e-11,1.438276e-10\n",
      "Iteration 78950: loss = 2.928557e-11,1.4382545e-10\n",
      "Iteration 78955: loss = 2.9286507e-11,1.438229e-10\n",
      "Iteration 78960: loss = 2.9285605e-11,1.4382201e-10\n",
      "Iteration 78965: loss = 2.9286004e-11,1.4382052e-10\n",
      "Iteration 78970: loss = 2.9286486e-11,1.438183e-10\n",
      "Iteration 78975: loss = 2.9286105e-11,1.4381672e-10\n",
      "Iteration 78980: loss = 2.9285723e-11,1.4381517e-10\n",
      "Iteration 78985: loss = 2.928557e-11,1.4381377e-10\n",
      "Iteration 78990: loss = 2.928557e-11,1.4381141e-10\n",
      "Iteration 78995: loss = 2.928593e-11,1.4381014e-10\n",
      "Iteration 79000: loss = 2.9285647e-11,1.4380767e-10\n",
      "Iteration 79005: loss = 2.928592e-11,1.4380652e-10\n",
      "Iteration 79010: loss = 2.9285418e-11,1.4380476e-10\n",
      "Iteration 79015: loss = 2.928589e-11,1.4380278e-10\n",
      "Iteration 79020: loss = 2.928541e-11,1.4380194e-10\n",
      "Iteration 79025: loss = 2.9284863e-11,1.4380022e-10\n",
      "Iteration 79030: loss = 2.9285404e-11,1.437982e-10\n",
      "Iteration 79035: loss = 2.9286528e-11,1.4379561e-10\n",
      "Iteration 79040: loss = 2.928532e-11,1.4379475e-10\n",
      "Iteration 79045: loss = 2.9284863e-11,1.4379335e-10\n",
      "Iteration 79050: loss = 2.9285355e-11,1.4379152e-10\n",
      "Iteration 79055: loss = 2.9286566e-11,1.4378886e-10\n",
      "Iteration 79060: loss = 2.928538e-11,1.4378808e-10\n",
      "Iteration 79065: loss = 2.9285126e-11,1.437866e-10\n",
      "Iteration 79070: loss = 2.928665e-11,1.4378337e-10\n",
      "Iteration 79075: loss = 2.9285636e-11,1.4378243e-10\n",
      "Iteration 79080: loss = 2.9284995e-11,1.4378122e-10\n",
      "Iteration 79085: loss = 2.9286195e-11,1.4377854e-10\n",
      "Iteration 79090: loss = 2.9285793e-11,1.4377716e-10\n",
      "Iteration 79095: loss = 2.9284856e-11,1.4377635e-10\n",
      "Iteration 79100: loss = 2.9285442e-11,1.4377406e-10\n",
      "Iteration 79105: loss = 2.9285734e-11,1.4377233e-10\n",
      "Iteration 79110: loss = 2.9285307e-11,1.4377087e-10\n",
      "Iteration 79115: loss = 2.9285675e-11,1.4376883e-10\n",
      "Iteration 79120: loss = 2.9284904e-11,1.4376789e-10\n",
      "Iteration 79125: loss = 2.9285404e-11,1.4376586e-10\n",
      "Iteration 79130: loss = 2.9285244e-11,1.4376333e-10\n",
      "Iteration 79135: loss = 2.9285362e-11,1.4376217e-10\n",
      "Iteration 79140: loss = 2.928591e-11,1.4376006e-10\n",
      "Iteration 79145: loss = 2.928553e-11,1.4375857e-10\n",
      "Iteration 79150: loss = 2.9286126e-11,1.4375645e-10\n",
      "Iteration 79155: loss = 2.928577e-11,1.4375481e-10\n",
      "Iteration 79160: loss = 2.9285154e-11,1.437537e-10\n",
      "Iteration 79165: loss = 2.928544e-11,1.43752e-10\n",
      "Iteration 79170: loss = 2.9285088e-11,1.437505e-10\n",
      "Iteration 79175: loss = 2.9284908e-11,1.4374849e-10\n",
      "Iteration 79180: loss = 2.9285355e-11,1.4374614e-10\n",
      "Iteration 79185: loss = 2.928453e-11,1.437454e-10\n",
      "Iteration 79190: loss = 2.9284842e-11,1.4374324e-10\n",
      "Iteration 79195: loss = 2.9285317e-11,1.4374178e-10\n",
      "Iteration 79200: loss = 2.928483e-11,1.4373996e-10\n",
      "Iteration 79205: loss = 2.9284908e-11,1.4373852e-10\n",
      "Iteration 79210: loss = 2.928437e-11,1.4373683e-10\n",
      "Iteration 79215: loss = 2.928496e-11,1.4373486e-10\n",
      "Iteration 79220: loss = 2.928611e-11,1.4373239e-10\n",
      "Iteration 79225: loss = 2.928498e-11,1.4373149e-10\n",
      "Iteration 79230: loss = 2.9284034e-11,1.4373065e-10\n",
      "Iteration 79235: loss = 2.9284648e-11,1.4372833e-10\n",
      "Iteration 79240: loss = 2.9285154e-11,1.437262e-10\n",
      "Iteration 79245: loss = 2.928623e-11,1.437234e-10\n",
      "Iteration 79250: loss = 2.928451e-11,1.4372313e-10\n",
      "Iteration 79255: loss = 2.928485e-11,1.4372076e-10\n",
      "Iteration 79260: loss = 2.9284408e-11,1.4371965e-10\n",
      "Iteration 79265: loss = 2.9284884e-11,1.437177e-10\n",
      "Iteration 79270: loss = 2.9285723e-11,1.437151e-10\n",
      "Iteration 79275: loss = 2.928479e-11,1.4371429e-10\n",
      "Iteration 79280: loss = 2.9284766e-11,1.4371258e-10\n",
      "Iteration 79285: loss = 2.9285647e-11,1.4371015e-10\n",
      "Iteration 79290: loss = 2.928544e-11,1.4370855e-10\n",
      "Iteration 79295: loss = 2.928402e-11,1.4370799e-10\n",
      "Iteration 79300: loss = 2.9284634e-11,1.4370598e-10\n",
      "Iteration 79305: loss = 2.928521e-11,1.4370377e-10\n",
      "Iteration 79310: loss = 2.9285525e-11,1.4370177e-10\n",
      "Iteration 79315: loss = 2.9285057e-11,1.4370029e-10\n",
      "Iteration 79320: loss = 2.9285636e-11,1.4369819e-10\n",
      "Iteration 79325: loss = 2.9284682e-11,1.4369722e-10\n",
      "Iteration 79330: loss = 2.928482e-11,1.4369532e-10\n",
      "Iteration 79335: loss = 2.9284512e-11,1.4369397e-10\n",
      "Iteration 79340: loss = 2.9285237e-11,1.4369185e-10\n",
      "Iteration 79345: loss = 2.9284516e-11,1.4369055e-10\n",
      "Iteration 79350: loss = 2.928429e-11,1.436891e-10\n",
      "Iteration 79355: loss = 2.92841e-11,1.4368695e-10\n",
      "Iteration 79360: loss = 2.928475e-11,1.4368487e-10\n",
      "Iteration 79365: loss = 2.9285345e-11,1.436827e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 79370: loss = 2.9283347e-11,1.4368258e-10\n",
      "Iteration 79375: loss = 2.9283857e-11,1.436805e-10\n",
      "Iteration 79380: loss = 2.928498e-11,1.4367793e-10\n",
      "Iteration 79385: loss = 2.928529e-11,1.4367572e-10\n",
      "Iteration 79390: loss = 2.9283603e-11,1.4367543e-10\n",
      "Iteration 79395: loss = 2.9285362e-11,1.4367223e-10\n",
      "Iteration 79400: loss = 2.9284495e-11,1.4367134e-10\n",
      "Iteration 79405: loss = 2.9284745e-11,1.4366938e-10\n",
      "Iteration 79410: loss = 2.9283683e-11,1.4366905e-10\n",
      "Iteration 79415: loss = 2.9283496e-11,1.4366695e-10\n",
      "Iteration 79420: loss = 2.928429e-11,1.4366486e-10\n",
      "Iteration 79425: loss = 2.9285043e-11,1.4366272e-10\n",
      "Iteration 79430: loss = 2.9284603e-11,1.4366126e-10\n",
      "Iteration 79435: loss = 2.928408e-11,1.4365999e-10\n",
      "Iteration 79440: loss = 2.9285893e-11,1.4365674e-10\n",
      "Iteration 79445: loss = 2.9283975e-11,1.4365648e-10\n",
      "Iteration 79450: loss = 2.9284495e-11,1.4365432e-10\n",
      "Iteration 79455: loss = 2.9283846e-11,1.4365371e-10\n",
      "Iteration 79460: loss = 2.9283707e-11,1.4365155e-10\n",
      "Iteration 79465: loss = 2.9284537e-11,1.4364938e-10\n",
      "Iteration 79470: loss = 2.928587e-11,1.4364654e-10\n",
      "Iteration 79475: loss = 2.9284165e-11,1.4364625e-10\n",
      "Iteration 79480: loss = 2.9284547e-11,1.43644e-10\n",
      "Iteration 79485: loss = 2.928258e-11,1.436439e-10\n",
      "Iteration 79490: loss = 2.928343e-11,1.4364158e-10\n",
      "Iteration 79495: loss = 2.928392e-11,1.4363957e-10\n",
      "Iteration 79500: loss = 2.928437e-11,1.4363755e-10\n",
      "Iteration 79505: loss = 2.928407e-11,1.4363565e-10\n",
      "Iteration 79510: loss = 2.9285886e-11,1.4363222e-10\n",
      "Iteration 79515: loss = 2.928425e-11,1.4363183e-10\n",
      "Iteration 79520: loss = 2.928378e-11,1.4363118e-10\n",
      "Iteration 79525: loss = 2.9284093e-11,1.4362928e-10\n",
      "Iteration 79530: loss = 2.9285397e-11,1.4362597e-10\n",
      "Iteration 79535: loss = 2.928428e-11,1.4362561e-10\n",
      "Iteration 79540: loss = 2.9283794e-11,1.4362446e-10\n",
      "Iteration 79545: loss = 2.928451e-11,1.436222e-10\n",
      "Iteration 79550: loss = 2.928524e-11,1.4361907e-10\n",
      "Iteration 79555: loss = 2.9284762e-11,1.436182e-10\n",
      "Iteration 79560: loss = 2.9285057e-11,1.4361613e-10\n",
      "Iteration 79565: loss = 2.9284884e-11,1.436147e-10\n",
      "Iteration 79570: loss = 2.9283083e-11,1.4361454e-10\n",
      "Iteration 79575: loss = 2.92837e-11,1.4361225e-10\n",
      "Iteration 79580: loss = 2.9284367e-11,1.436102e-10\n",
      "Iteration 79585: loss = 2.9284186e-11,1.4360801e-10\n",
      "Iteration 79590: loss = 2.9283704e-11,1.4360721e-10\n",
      "Iteration 79595: loss = 2.9284568e-11,1.4360484e-10\n",
      "Iteration 79600: loss = 2.9283843e-11,1.4360363e-10\n",
      "Iteration 79605: loss = 2.9284637e-11,1.4360084e-10\n",
      "Iteration 79610: loss = 2.9283503e-11,1.4360069e-10\n",
      "Iteration 79615: loss = 2.9283218e-11,1.4359942e-10\n",
      "Iteration 79620: loss = 2.9282906e-11,1.4359743e-10\n",
      "Iteration 79625: loss = 2.928342e-11,1.4359522e-10\n",
      "Iteration 79630: loss = 2.9283832e-11,1.4359318e-10\n",
      "Iteration 79635: loss = 2.9285588e-11,1.4359003e-10\n",
      "Iteration 79640: loss = 2.928593e-11,1.435878e-10\n",
      "Iteration 79645: loss = 2.9284283e-11,1.4358771e-10\n",
      "Iteration 79650: loss = 2.9283083e-11,1.4358673e-10\n",
      "Iteration 79655: loss = 2.928485e-11,1.4358335e-10\n",
      "Iteration 79660: loss = 2.9285428e-11,1.4358131e-10\n",
      "Iteration 79665: loss = 2.9283544e-11,1.4358098e-10\n",
      "Iteration 79670: loss = 2.92828e-11,1.4357997e-10\n",
      "Iteration 79675: loss = 2.9283576e-11,1.435775e-10\n",
      "Iteration 79680: loss = 2.9284863e-11,1.435751e-10\n",
      "Iteration 79685: loss = 2.928364e-11,1.4357403e-10\n",
      "Iteration 79690: loss = 2.9282937e-11,1.435731e-10\n",
      "Iteration 79695: loss = 2.9283586e-11,1.4357075e-10\n",
      "Iteration 79700: loss = 2.9285175e-11,1.4356798e-10\n",
      "Iteration 79705: loss = 2.928474e-11,1.4356645e-10\n",
      "Iteration 79710: loss = 2.9283066e-11,1.4356626e-10\n",
      "Iteration 79715: loss = 2.9283333e-11,1.4356431e-10\n",
      "Iteration 79720: loss = 2.9284904e-11,1.4356122e-10\n",
      "Iteration 79725: loss = 2.9282965e-11,1.4356098e-10\n",
      "Iteration 79730: loss = 2.9283402e-11,1.4355926e-10\n",
      "Iteration 79735: loss = 2.9284238e-11,1.4355664e-10\n",
      "Iteration 79740: loss = 2.9283836e-11,1.4355535e-10\n",
      "Iteration 79745: loss = 2.9282386e-11,1.4355475e-10\n",
      "Iteration 79750: loss = 2.928335e-11,1.4355256e-10\n",
      "Iteration 79755: loss = 2.9283985e-11,1.4355034e-10\n",
      "Iteration 79760: loss = 2.9284426e-11,1.4354762e-10\n",
      "Iteration 79765: loss = 2.9286507e-11,1.435441e-10\n",
      "Iteration 79770: loss = 2.928531e-11,1.4354318e-10\n",
      "Iteration 79775: loss = 2.9284488e-11,1.4354294e-10\n",
      "Iteration 79780: loss = 2.9283038e-11,1.4354234e-10\n",
      "Iteration 79785: loss = 2.92837e-11,1.4354004e-10\n",
      "Iteration 79790: loss = 2.9285057e-11,1.4353657e-10\n",
      "Iteration 79795: loss = 2.9286806e-11,1.4353344e-10\n",
      "Iteration 79800: loss = 2.9288728e-11,1.4353013e-10\n",
      "Iteration 79805: loss = 2.9289418e-11,1.43528e-10\n",
      "Iteration 79810: loss = 2.929047e-11,1.4352428e-10\n",
      "Iteration 79815: loss = 2.929231e-11,1.4352125e-10\n",
      "Iteration 79820: loss = 2.9293252e-11,1.4351853e-10\n",
      "Iteration 79825: loss = 2.9294393e-11,1.4351537e-10\n",
      "Iteration 79830: loss = 2.929588e-11,1.4351242e-10\n",
      "Iteration 79835: loss = 2.9297106e-11,1.4350952e-10\n",
      "Iteration 79840: loss = 2.9298956e-11,1.4350642e-10\n",
      "Iteration 79845: loss = 2.9299865e-11,1.4350351e-10\n",
      "Iteration 79850: loss = 2.9300867e-11,1.4350122e-10\n",
      "Iteration 79855: loss = 2.9302612e-11,1.434978e-10\n",
      "Iteration 79860: loss = 2.9303504e-11,1.4349434e-10\n",
      "Iteration 79865: loss = 2.9304646e-11,1.4349155e-10\n",
      "Iteration 79870: loss = 2.9306474e-11,1.4348837e-10\n",
      "Iteration 79875: loss = 2.930811e-11,1.4348535e-10\n",
      "Iteration 79880: loss = 2.9309045e-11,1.4348225e-10\n",
      "Iteration 79885: loss = 2.9310818e-11,1.4347895e-10\n",
      "Iteration 79890: loss = 2.9312216e-11,1.4347637e-10\n",
      "Iteration 79895: loss = 2.9312993e-11,1.4347316e-10\n",
      "Iteration 79900: loss = 2.9314436e-11,1.4347024e-10\n",
      "Iteration 79905: loss = 2.9316105e-11,1.4346688e-10\n",
      "Iteration 79910: loss = 2.9317705e-11,1.4346413e-10\n",
      "Iteration 79915: loss = 2.931834e-11,1.4346116e-10\n",
      "Iteration 79920: loss = 2.9320144e-11,1.4345788e-10\n",
      "Iteration 79925: loss = 2.9321774e-11,1.4345468e-10\n",
      "Iteration 79930: loss = 2.932307e-11,1.4345194e-10\n",
      "Iteration 79935: loss = 2.9323988e-11,1.4344889e-10\n",
      "Iteration 79940: loss = 2.9325608e-11,1.4344576e-10\n",
      "Iteration 79945: loss = 2.9327447e-11,1.4344256e-10\n",
      "Iteration 79950: loss = 2.9328054e-11,1.4343987e-10\n",
      "Iteration 79955: loss = 2.9329716e-11,1.434368e-10\n",
      "Iteration 79960: loss = 2.9331225e-11,1.4343388e-10\n",
      "Iteration 79965: loss = 2.9333026e-11,1.4343032e-10\n",
      "Iteration 79970: loss = 2.9333893e-11,1.434273e-10\n",
      "Iteration 79975: loss = 2.933565e-11,1.4342413e-10\n",
      "Iteration 79980: loss = 2.9336675e-11,1.434215e-10\n",
      "Iteration 79985: loss = 2.933772e-11,1.4341844e-10\n",
      "Iteration 79990: loss = 2.933931e-11,1.4341535e-10\n",
      "Iteration 79995: loss = 2.9341102e-11,1.4341216e-10\n"
     ]
    }
   ],
   "source": [
    "optim = tf.keras.optimizers.Adam(epsilon=1e-08)\n",
    "PINN_solver_pcgrad.train(N=N, optimizer=optim, method = 'PCG_gradient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize layer gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b0208d7d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 6))\n",
    "\n",
    "sns.kdeplot(PINN_solver.gradients_log[0][1][0].numpy().flatten(), linewidth =2, ax = axs[0][0], \n",
    "            label = r'$\\nabla_\\mathcal{\\theta}\\mathcal{L}_u$')\n",
    "axs[0][0].legend()\n",
    "\n",
    "\n",
    "sns.kdeplot(PINN_solver.gradients_log[0][2][0].numpy().flatten(), linewidth =2, ax = axs[0][1], \n",
    "            label = r'$\\nabla_\\mathcal{\\theta}\\mathcal{L}_f$')\n",
    "axs[0][1].legend()\n",
    "\n",
    "\n",
    "sns.kdeplot(PINN_solver.gradients_log[2][1][0].numpy().flatten(), linewidth =2, ax = axs[1][0], \n",
    "            label = r'$\\nabla_\\mathcal{\\theta}\\mathcal{L}_u$')\n",
    "axs[1][0].legend()\n",
    "\n",
    "\n",
    "sns.kdeplot(PINN_solver.gradients_log[2][2][0].numpy().flatten(), linewidth =2, ax = axs[1][1], \n",
    "            label = r'$\\nabla_\\mathcal{\\theta}\\mathcal{L}_f$')\n",
    "axs[1][1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1b0208d7760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "sns.kdeplot(PINN_solver.gradients_log[1][2][0].numpy().flatten(), ax = axs, linewidth =2,\n",
    "            label = r'$\\Delta_\\mathcal{\\theta}\\mathcal{L}_r$')\n",
    "\n",
    "sns.kdeplot(PINN_solver.gradients_log[1][1][0].numpy().flatten(), linewidth =2, ax = axs, \n",
    "            label = r'$\\Delta_\\mathcal{\\theta}\\mathcal{L}_u$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_loss_log = np.vstack(PINN_solver.loss_log)\n",
    "pinn_pcgrad_loss_log = np.vstack(PINN_solver_pcgrad.loss_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss convergence comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations_range = np.array(range(len(pinn_pcgrad_loss_log[50000:52000, 1]))) + 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x720 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(2, 2, figsize=(17, 10))\n",
    "\n",
    "sns.lineplot(x = iterations_range, y = pinn_loss_log[50000:52000, 0], linewidth = 3, color='blue',\n",
    "             label=r'$\\mathcal{L}_u$ without PCGrad', ax = ax[0][0])\n",
    "\n",
    "ax[0][0].set(yscale=\"log\")\n",
    "ax[0][0].set_xlabel(\"Iteration\", fontdict=dict(weight='bold'))\n",
    "ax[0][0].set_ylabel(\"Loss\", fontdict=dict(weight='bold'))\n",
    "\n",
    "\n",
    "sns.lineplot(x = iterations_range, y = pinn_pcgrad_loss_log[50000:52000, 0], linewidth = 3, color='red', \n",
    "             label=r'$\\mathcal{L}_u$ with PCGrad', ax = ax[0][1])\n",
    "\n",
    "ax[0][1].set(yscale=\"log\")\n",
    "ax[0][1].set_xlabel(\"Iteration\", fontdict=dict(weight='bold'))\n",
    "ax[0][1].set_ylabel(\"Loss\", fontdict=dict(weight='bold'))\n",
    "\n",
    "\n",
    "sns.lineplot(x = iterations_range, y = pinn_loss_log[50000:52000, 1], linewidth = 3, color='blue',\n",
    "             label=r'$\\mathcal{L}_f$ without PCGrad', ax = ax[1][0])\n",
    "\n",
    "ax[1][0].set(yscale=\"log\")\n",
    "ax[1][0].set_xlabel(\"Iteration\", fontdict=dict(weight='bold'))\n",
    "ax[1][0].set_ylabel(\"Loss\", fontdict=dict(weight='bold'))\n",
    "\n",
    "\n",
    "sns.lineplot(x = iterations_range, y = pinn_pcgrad_loss_log[50000:52000, 1], linewidth = 3, color='red', \n",
    "             label=r'$\\mathcal{L}_f$ with PCGrad', ax = ax[1][1])\n",
    "\n",
    "ax[1][1].set(yscale=\"log\")\n",
    "ax[1][1].set_xlabel(\"Iteration\", fontdict=dict(weight='bold'))\n",
    "ax[1][1].set_ylabel(\"Loss\", fontdict=dict(weight='bold'))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_convergence_demonstration_small_homo.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "sns.lineplot(x = range(len(pinn_loss_log[:, 0])), y = pinn_loss_log[:, 0],  linewidth = 3, \n",
    "             label = \"$\\mathcal{L}_u$ without PCGrad\", ax = axes[0])\n",
    "sns.lineplot(x = range(len(pinn_pcgrad_loss_log[:, 0])), y = pinn_pcgrad_loss_log[:, 0],  linewidth = 3, \n",
    "             label = \"$\\mathcal{L}_u$ with PCGrad\", ax = axes[0])\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale(\"log\")\n",
    "axes[0].set_xlabel(\"Iteration\", fontdict=dict(weight='bold'), fontsize=20)\n",
    "axes[0].set_ylabel(\"Loss\", fontdict=dict(weight='bold'), fontsize=20)\n",
    "\n",
    "sns.lineplot(x = range(len(pinn_loss_log[:, 1])), y = pinn_loss_log[:, 1],  linewidth = 3, \n",
    "             label = \"$\\mathcal{L}_f$ without PCGrad\", ax = axes[1])\n",
    "\n",
    "sns.lineplot(x = range(len(pinn_pcgrad_loss_log[:, 1])), y = pinn_pcgrad_loss_log[:, 1],  linewidth = 3, \n",
    "             label = \"$\\mathcal{L}_f$ with PCGrad\", ax = axes[1])\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].set_xlabel(\"Iteration\", fontdict=dict(weight='bold'), fontsize=20)\n",
    "axes[1].set_ylabel(\"Loss\", fontdict=dict(weight='bold'), fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"performance_comparison_small_homo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution quality comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_solver_solution_df = pd.read_csv('homo_80000.csv')\n",
    "matlab_solver_solution_df.columns = ['time', 'state_0', 'state_1', 'state_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlab_solver_solution_df.drop(labels='time', inplace=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997940</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.002321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.993850</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>0.003480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.004639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.989770</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>0.005796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.999390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_0   state_1   state_2\n",
       "0     0.997940  0.000894  0.001161\n",
       "1     0.995890  0.001785  0.002321\n",
       "2     0.993850  0.002673  0.003480\n",
       "3     0.991800  0.003557  0.004639\n",
       "4     0.989770  0.004439  0.005796\n",
       "...        ...       ...       ...\n",
       "4995  0.000034  0.000575  0.999390\n",
       "4996  0.000034  0.000574  0.999390\n",
       "4997  0.000034  0.000573  0.999390\n",
       "4998  0.000034  0.000572  0.999390\n",
       "4999  0.000034  0.000572  0.999390\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matlab_solver_solution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = PINN_solver.mu_x, PINN_solver.sigma_x\n",
    "\n",
    "ub = 80000\n",
    "n_star = 5000+1\n",
    "x_star = np.linspace(lb, ub, n_star)[1:] #N_star = x_star.shape[0] \n",
    "x_star_normalized = (x_star-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = PINN_solver.model(x_star_normalized)\n",
    "y_pred_pcgrad = PINN_solver_pcgrad.model(x_star_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame(y_pred.numpy(), columns = ['state_0', 'state_1', 'state_2'])\n",
    "y_pred_pcgrad_df = pd.DataFrame(y_pred_pcgrad.numpy(), columns = ['state_0', 'state_1', 'state_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAAIkCAYAAAA+muvAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOydd3hb5fXHP7K895b3imM7w87eg4SEhASSAGVDW6CMlpbSAaWMtr/SQgeFAi2lZRVaSgfQsrL33ku2k9iJ95T33pJ+f5zIkmzJGSSxnbyf53kf6169uno1bN/vPed8j8ZsNptRKBQKhUKhUCgUiisQl8FegEKhUCgUCoVCoVAMFkoQKRQKhUKhUCgUiisWJYgUCoVCoVAoFArFFYsSRAqFQqFQKBQKheKKxXWwFzBc6OjoICsri7CwMLRa7WAvR6FQKBQKhUKhUJwlRqOR6upqxo4di6enp919ShCdJVlZWdx1112DvQyFQqFQKBQKhUJxnvzjH/9g8uTJdvuUIDpLwsLCAHkTIyIiBnk1CoVCoVAoFAqF4myprKzkrrvu6j2nt0UJorPEkiYXERFBTEzMIK9GoVAoFAqFQqFQnCuOSl+UqYJCoVAoFAqFQqG4YlGCSKFQKBQKhUKhUFyxKEGkUCgUCoVCoVAorliUIFIoFAqFQqFQKBRXLEoQKRQKhUKhUCgUiisWJYgUCoVCoVAoFArFFYsSRAqFQqFQKBQKheKKRQkihUKhUCgUCoVCccWiBJFCoVAoFAqFQqG4YlGCSKFQKBQKhUKhUFyxKEGkUCgUCoVCoVAorliUIFIoFAqFQqFQKBRXLEoQKRQKhUKhUCgUiisWJYgUCoVCoVAoFArFFYsSRAqFQqFQKBQKheKKRQkihUKhUCgUCoVCccWiBJFCoVAoFAqFQqG4YnEd7AUozo2ypjKe3Pgke0v34uPug7vWHZ2PjtiAWDRo0Gg0Tn8CfGvytxgRPKLfcY9WHuXDYx/azR3oeFoXLZ6unni6ejItehoTIif0O6bJbKK6tZogryDcte4X941RKBSKoUBPD7S1gdkMJhNoteDrK9sgPy2jqgpaWuS20QhhYTK377yeHsjMtM4DGD/e/pgWSkuhoMC6PyoKkpLs51hu791rfX6zGaZPBx+f/sesqYH9+62vKSQEpk1zfMz9+2UNln2TJ0NMTP+5XV3w6adyPLMZ3Nzgxhv7Hw8gOxsOH7a+/uRk6+s3mazDbIbPP4emJutjFy0CPz+5z8MDAgJkf0UFrF5tnefuDnPmyHEsz2855r598posa5owQV6TySSPS0iQ+7q74f33rfOam+Hqq+2PZ7mdlwfHj1u34+MhLc06Z/x4cDl9zfrjj6GxUW43NEBGBnh7Wz83yzHq6uDgQet2QABMnGh9jePGQWCg3N68WdYA0Noq372IiP6fU08PbN9u3a/Vwty51u2RI2XtAHo97N4tt7u7Zf3Jyc6/e21t1v1TpshrAoiMlLUClJfDZ59Z59XXy+u3PZ7lmCdOQHW19b6UFAgPl9u+vjBvntzu6oK//tU6r7JS3m/b9Vlul5VBUZF1f1SUfN4Wli2T9wTg3/+2fk7V1fK+eHnZHxPk+3nsmHXbzw9GjbJuz5tnXffGjXDqlPVxXl7y+9cXoxEOHbJuu7jApEnW7fHj5fsF8jnt2SO3Ozvlc4iN7X9MgKws6Oiwbo8ZA56ecjs+HmbNktvl5fDFF9Z55eXy3bAwYwZ85Svy2Q5RlCAaRpQ0lvDO4XfYkL+BipaK8zrGspRlDgVRZlUmz21/7ryO+fzVzzsURHXtdUS8GAGAj5sPQV5BhHqHEuMfQ6x/rIyAWJKCkhgdNppAz8Dzen6FQnEeWE4um5rkBKGz03pSbjTan2iaTHJCWFgoJzrd3fKPLSGh/zyzWU5gGhvlZMpohBUr5J++5X7Lz+JiOSk2GmVER8MNN9jPsfz83//kxNhkkrk33CAnEX3ntrXBc89Z1+PhAU89ZT1xsp27ZQusWmXdnjMHrr3WXoxYxgsvQG2tdfvRR+WE02iUkw93d+uJ7nvvWd/nyEhYsqT/+2Qywc6dUFJinTtlipxwWU7eR4+W/Z2d8OtfW+e5usLs2Y7fp7IyeV8thIXJ+2q5f8UK6wncH/4gJ9EWYmKs99m+9o4OEUUW3NwgKMg674YbZN0gJ+9ZWda5Xl6y3r7vp8kkr8uWZ56x3r9okYgugF27YP36M32jHfP669bbY8fKSRnId9n2cwL5jM+Gf/7TejskBL7zHbnd1QV//rP1Po0GPvjgnJcMwJNPyncKRGTV1lrv++STsz+OrZj4+tetJ/P//a/953Qu7Nhhvb1okZzswpf7nA4csN7u+zl9/LH93E2bzu6Yhw9bb4eEWN/Pri55/RY0Gln72ZCZab8dHGw97uef239Otu/TmbAIFJDfDcvntHZt/+c8W7KzrbcrK61ibfduWLfu/I554oT19tix8rcA5HP69FPrfRqNXByxUFcnz//NbzoWdEMAJYiGEYUNhZgxY+57teEcsER/LiSerp4O99e31/febu1upbW7ldKmUo5UHnE4P8ovitFhoxkbNpY58XO4adRNF3ytCsVFwyIwjEZob5c//m1tMry85ATSIhBsx4YNMrezU8bixXIFznK/5TEFBfJPvLtb/qHHxMiJaN/jGY1ywpaVZRU2N90k4sFyIm40ynrb2+G3v7W+Bk9PeOIJx69vxw65Wmlh5ky45hrHc199Va7k2r43ff8JWsTDf/5j3RcTIz8tryM01HqysWeP/T94s1n+sfd9Tzs7rRESELHyu99ZX/ekSZCaKvfV1clVfAtbt0o0wPZ9stzu6rJf/yuvWG+PGyefhSMqKuCddxzf1xfbE4iQEKsg6ovRKGLubKiutr9qbntFuy+WyM6Z6O6W6JbttjPa28/umCCfpQVLpGSo8yX+HysuIepzGny6u0WYKUGk+LIkByezrWgbJvP5/6PQ4FgQfRmR5VQQddQ73O+M8uZyypvL2ZC/gb1le5UgUnw5jEY5ie3okJMyrVb+IPf0WH/29EjqRmGhzGlrg6lT5aq+5X7L3OZm+OMf5YTbkg7yrW/ZH8vC9u32VzFnzYKFCx2vs6946OiQq459KSgQ8WQhIcH5CXN9vf0xW1rs0x4suPQpIzUa5aTY8npsR3m5/dzCQlizxnr/8uXW4/U97rvvWtNvLMLF0d+c0lJ47TXr9r33Qlyc3O57Mcc2PWQgTCb79JTEROvtvsfs6HD8Pp0JSxqbo2OeL7bvz4W8kGUrNC7GWi8Ul/qYGs3QOmkeSmsZSgy1z0lx9vj4WNMrhyBKEA0jIv0ieXDSgxQ2FLK7dDddxi46jZ2kBqeidZErfmbkD4XZbKaxs5G9ZXsBcHVxJSM8g4TABIfHHhcxjmfnPWv3eEs0qu9PgG5TN509nXT0dDAmfIzDY3b2dBLsFUxDR8M5i7gZMTOc3vfavteYEz+H9PD0ixLxUlxiLCfKXV2Sy97eLifUXV39x3//K1e7W1pEvNx4o0Q1+s7LzYV//Utu9/RIDcVXv+r4+T/80P6EuaoK0tP7z+vstE9dcHOzjy7Y4trnT6utWOpL36v1nZ3yHnR3y32Wmo6+8xobJeXKksJmO2zTsEDSLtavl/uCg+GBBxwfs7sb3n7b+VptKS+3F0lLl1qjOX0FUUvL2R2zL7ZCo+8xz5eLIQhsj2lJn9NoZHR2nv8JnK+vHKOnR9LeLMesrj4/4QYSdbN8p9LSRDhrNGAw2KfFnQvh4dYTnfHj5fgajRzPUv9wPutMSZHbthGojg753mu18l5b3hPL6O6W99vy2Xp4WL/n0dHyu63RyJobGqzztm6V32nLccB62/L7aNnv42P9jMPDpU7K8ty33mpd68GD1u+t7RpBjtnaar3P2xv8/a1zZs2y1mrU1Fjrok6elMdant/22J2d9p+hp6fUBVm46iprKpZWa01JrKyUz99Sv2b7e2EyWf+eWJ7H9qT26qul9gzkuZKS5HZbm/xdtUQD+v6ulZbav6cxMda/H6NHw4IFcttgsH5fNRqJ4Op01uPYvgcGg/VvskYj8/z8ZDsoyBrR7u62rzPLzJTvW99jgnxHLBFWjUZqsMLDrXMWLZLvmOW4lufPz5fXY7nP9phtbda0VstnHx9vnbNsmXxXQb4TRUVyu7pavjOWOjBbjEbIybFuu7jY1yXNmGGtk0pMtN7X3i6fr+33xJaTJ+2j4yNHWl/TiBHWz6mqyppGC5KhYPs9ueoqyVSwfJZDECWIhhlhPmG8ufxNekw9nKw9id6gJ7c2F6PZ2G/u6lPWgtEeUw9uWjc6jZ2YzCZcNPYnFxm6DDJ0GX0P8aWYEz+H2h/VYjKbaO5spr6jnsqWSkoaSyhpKqG0qZSixiJyanLIrc2l22T9pzcj1rEgqm6t5pHVj2DGzKjQUdw+9nbuSr/LYV2U4hJgMlmvqnd2Wm+3tEiKVV2d/ENpb5d/RpZ5nZ1W8bJzpxT4WmpYBkrFevdd+5qHESOkRqIvZrN9wW7fdCdbLDnQFpyJF0cip63NmsLm7W39Y993bmGhFJw6Enm2rwfgjTest21z6fuKl4YG+4jRQFhOvMA+hUmrlX/aLi5yu7NzYPE2ECNGSE2NRiOfX2OjHPPECRGo58P48fJ90GjkRM5gkGNmZ8t34XyYOFEEoYsLfO1r8hlqtZLaZvnnfja4uclwdZVjPv209YTxzTetJzfXXCPvv6urdWi11p/Ohk4Hjz1mfT7bVMa335aI4UCPdzZuvdV6QvPNb1qPmZsrJ16W74KLi3VoNPbbffclJFhPpu+913rM1lb5fjs6xpm2Leu9mDz88IU/prPUyS+Ds+jyl2H+/At/zKuuuvDHBLj55gt/TIvBwoVkhvMLueeNRWxeSKZNk3qyC82yZRf+mJcQJYiGKa4urowKG8WosFG0d7dzvOY4eoOewoZCQBzesquy7R4THxDP+/r38XHzYWz4WDJ0GUT5RaHRaChtKiXGP+airNVF40KAZwABngEkBCYwPab/L3i3sZuTdSfJrsrmQPkBZsfNdnisdXnreqNYx2uO87MtP+NnW37GohGL+Oakb7IsdRmuLuprfc6YTHKi3N4uV7i6uqzbllSy9nYpxMzKkjmtrXIVMzm5//EcFYHHOPl+9RUvfQutbbGcyFlwJnT6ihzLFWZLjYm7u1W09J2blSWRj77Cre9zWQrtLSxcKO5Lrq5yVdJSTK7Vyom8weD8dTkjMFCOq9XKex4RYb3q+ItfnF/kxcMDfvpT68n7z39uve/662HlynM/JogZgeWqpkXEATz7LPzsZ44f4+5ufT2W27Zj0iTrVeypU62PO3LEKuacPbbvfRYBM2qUdZ22JCeL0HJ1tQodZz/PJVp1tqL1XPjGNy78MVNSrBGZC4WPz5C+IqxQKBQW1JnjZYCXmxcTIycyMXIijR2NZFZlojfo+dq4r5FZlUlWVRbt3e2MDBELxNbuVvaW7WVv2V5CvEJIDErkK//5CmmhadyVfhd3jL2D2AAnFowXCTetG6PDRjM6bDS3jLnF6TzbqJct6/LWsS5vHVF+UTw06SG+PeXbhHgPzcK9S4LFbautTVJiSkokfaK7W4RMa6vc19oqhe0ffmjdp9PBffc5Pu6hQ/a1G2Mcp0v2pnNYUoUsdSZ9IyeWubYMFM3pK4hOnpQr+7aRp87O/qls1dXiPGaJfjzxhERfXF0lkhEQIAKmpkaK9/Pzna/BGTNmwE9+0n+/weA8HeFM+PiIm5iFRYust19//fwEkSUl0RFpafIeeHqe3fDwsN622Bn35TvfkUiMZa5FoLi6nn/K2vjx8NZb5/dYZ7i5Oa/JUigUCsVljRJElxkBngHMjpvN7LjZGFoM6A169AY9RY1FDiMnte21rM9fT5exq3fuGwff4OQjJ4dkfc5No26ivqOedXnr6DH1T+0pby7nZ1t+xm92/ob7J9zPD2b8gPjAoVvEd84YjXIS3NQk9rqNjXIi19IiIqC5WW63torTWHGxNUXqoYccn5h3dsqxLNhGa/piyWm34MxBSqMRgWF7rI4OOcHv7raKtba2/pETS9+Jjg65/+c/t1qmtrTICbufn+Tnb93qfK22WJzCLEyZYo1i2KZjbNjgPF3vTNimpdliSSU6W9zdJf3O29s+J7sv990n3wPLXG9vec9ttx0NLy/nx/zd785trWdDcLBjkwiFQqFQKIYIShBdxuh8dVzjew0LkhZQ1FBEZlUm2VXZdBrtU5Iyq+w97mP8Y/hn1j/J0GWQGpKKm7ZPStEgctOom7hp1E3UttXy8fGPeefwO73GEba0dbfx6r5XeW3/a9yZfic/mfuT3gjZkMVkEkHT2Oh4NDdLX4X16+VEuLtbGtTZNjO0xZL2ZsFZNMHSDM/CuQoii3GByWRt2ubpKRGT7m5J+8rJkZ4flp43A1Fba9/LwTYtzzZC8uKL1iaA54ptQa0t/v5nfwytVoSZr68MZyf9Xl6SPmiZ52hYUossaXZng6NolEKhUCgUinNGCaIrABeNC4lBiSQGJbJ05FJya3PRG/ScrD1Jt6kbVxdXNGh6a3PGho8ltzaX3Npc3LXujAodRYZOHOqe2vgU14y4hvkJ83ud7QaDEO8QHpz0IA9OepDDFYf5y8G/8L7+fVq77a/SG81G/q7/Ox8e+5DS75cObhqd0Wi1Q66vtxc7f/ubpH41NMjJ+ve+1198WLAVCwOlTPWNTDhzROsbMejqEhedri5rWp1FqFm6xLe1yb7du62ixM/PXmjYCrWvflWaC54PtvbRtgwUPXGGViupXc56nMTHwy9/KcLIdjgSMbYuTwPh4uK8t49CoVAoFIpBRwmiKwxXF9feWp327naOVR8jMTCR7OpssquyqWytJNTbaj/ZZeziqOEoRw1HaWhv4JV9r/DbXb8lyi+Kr4/7Os9d/dygp9ZNiJzAn6//M7+95re8cfANfr/n95Q32/dM+WrGVy+NGOrqkhP4ujpxbNLrxb2puFjSviwNIfty9Ki1izSIsHAkiPrWaTgTOWAVDBqNPC4uTupR2ttl35gxEpXw9oYlS8R2NDQU5syBb3/73F63ZS2dnf3rfMDe0vRccSaIUlLExjMoSKJQgYEiXgIC7MWM7baX18AiRqcTtzCFQqFQKBRXDEoQXcF4uXkxKWoSk6Im0dDRQKZBzBiq26odzt9Ttqf3dnlzOduKttHQ0UCQ13lcqb8I+Hv489jMx3hk6iO8r3+fF3a9QE5tDu5ad56Z+8yFeyKzWU7+q6slqlNfL3Utfbve79gBGzdatydOdC6IgoLsBVFjo0Rp+mIriNzdRQTMnSvRGT8/ETjt7dJXIixMamUqK0WcffYZ/PnPkt42caLU4FiYOdN625GN9dlSWwtRUf332woid3d5jtBQEYlBQQOPtDTHzzV7tr3hgEKhUCgUCsV5oASRAoBAz0DmxM9hdtxsKlsqyazKJNOQSXOXnOCbzCayq+1tvCP9Inll7yvEBcSRoctgdNhovN2kHqWzpxMPVweRgkuAh6sH35j4De6dcC//yf4PxY3FxAXEOZybachkV8kuvjHxG/1NJ8xmidRUV1uHwSCObGVl0oisvh5+/GPHdR99oyLOIh0gJ/6FhdZtb2/pPxAQYB3+/rL/jjsgNlb2vfcerFolzQ/z8sQdbaAaIAt9G3faci6OaBqNvE6LwLFtoGjLgw/CXXdZm0IOQcMOhUKhUCgUVyZKECns0Gg0RPpFEukXycKkhRQ2FKI36MmuyuaG1BvQV+k5Xn0cF40LKcHSs6K4sZjixmJWn1xNcnAySUFJLPnHEhYkLeDu9LtZMnIJ7lr3MzzzhcdF48LtY293er/ZbOaH637I+vz1/GHPK7w86WkWeI1BU1gogqGlxXFPnF277B3F6uocR3McCSJ/fym+DwqyFzvTp8v9ycmS2hYWJrUnbW1yfNu+KZZu02Yz/OhH/Rt7ng2WTveO0vIiImRNERFnHqGhZ2cCEBb25SJPCoVCoVAoFBcJJYgUTnHRuJAUlERSUBLXjbyu14whuzqbypbKfu5zRrORnNoc/p39b+o76vno2Ed8dOwjJkVO4sCDBwbpVTjAaITKSnbv+xj31eu5vwUmlx0n+Lm7aWhzI7C5B82MGc7tl3U6+z41tbVWQeTiImlsQUEiXNatg6QkSZVLT4f773d8zKQkEWCHDoldtqXfz/Hj4qy22kH/JY1G+qbs2HFurz84WCJMDQ2Oo0EvvQQvv3xux1QoFAqFQqEYpihBpDgr3LRujAkfw5jwMbR1t5FdlY3eoKekqX/qld6gt9sO9AxkXd46MnQZ6Hx0l96EoaUFSksl6lNSIn1uenrIPvQmk0+bogW3w8RKAEn5qj6lx3PudPw8/PofLzzcXhBFREg6WEiIRFa0Nu57N9zgfF3l5SJmdu6Un0ePiljry8GDEg1y9L6NGtVfEPn5wYgRMpKSrD/j4yXSdKbO8c6adioUCoVCoVBchihBpDhnvN28mRI9hSnRU6hvryezSswYatpq6DZ209jZaDc/NSSVXSW72FWyi3CfcDJ0GaSHp+Pv4c+7R95l6cil6Hx1F2ZxZrOkgxUXi/jZuxcyM+V2cbFEaE7bUd825jY2FWziaOVRyvvoHr/aFl7c+wrTk+cxe9JNeETGWNO+Zs+Wmp1x48Sp7Wx719TXw6ZNEjXasMFeVA1EdbWIJ9u0OQsrVoggS0uziqCQEFWjo1AoFAqFQnGWKEGk+FIEeQUxN34uc+LmUNFSgd6gJ9AzkBM1J9BX6alrryPMx1o7UtVaxYb8DWzI3wDAz7f+HK1Gy8Kkhdwz/p4Ba34cYjaLw1thoXXY1vf8/e8iJiyUlEhUBXGluyHtBqbHTGfdqbW0uRbi3SPTDL7w3mgjz4RuJLr1BM/HP8/dGdfjonGROp+FC8/9zZo40d444WxJThY3O0eC6LrrZCgUCoVCoVAozgsliBQXBI1GQ5RfFFF+USwasYiC+gL0Bj3Ha47TZexy+Jh1eesAqT1am7eWjp4Obh59c3+3t750dkJBAZw8CUeOSOTHkVgAMSiwFUTFxSKI/P3lMZGRRERG8lXdbzny1i/5Q8G/+J9PCQ02vUrLmsv4+idf55W9r/DioheZlzDv7N8YW66+Gt55Z+A5KSkwdaqIp4kTpQ6pb+8hhUKhUCgUCsUFQwkixQXHRePCiOARjAgewXXG68ipyUFv0JNXn4fJbALExjuzKtPucaHeofxu1+8YHTaaDF0G8QHxUm9kiQKdPCkjJwf275eGpyUlIhgefdTxYmJjYY+1fxIdHfD97/cTGRpgwg9+y1+MzzH50Jv8bMvPqGmrsZtzqOIQ89+bz3enfpdXlrxi/zzl5fC3v0lK3Jo1jutwFi+2F0RubjB5MsyaJWl4M2cqJzaFQqFQKBSKS4wSRIqLirvWnXRdOum6dFq7WsmuFjOGgvoCpkdPJ7MqE0OrAXetO6khqXT0dHCo4hCHyg8S1QyT6jyJrGxlUccb3EAad5HOKKOvNDw1m+VJGhqkPifodINYd3cxEIiLg+uvlxS6uXNFdEya5Nhq+jRuWjcenvIwd6XfxfPbn+flvS/3i3BdlXCV3DCZYOVKeP11WLtWtgE2b4YFC/offMECGDtW3OsWL4Y5c6SvkEKhUCgUCoVi0FCCSHHJ8HH3YWr0VKZGT6WuvQ69QU+mIZPjNcepbq3GzcUV39pmwguqCC+owqehlWY3Vz72NZAdVk021TzHdlZ4pfJJfLx9PU5zM9x6KyQkQGSkfYRm5cpzXmuAZwC/ueY3fHPyN3ly45P8O/vfAMyJm8ONMdfAH/8Ir7wi5gp9eecdx4IoJEQMHhQKhUKhUCgUQwYliBSDQrBXMPMS5nFV/FVUFmZRvP0LWj7dg19hORF5BsILqqiPCCRndhobfAz2j201UZ+WQJBFEGVkiMnBrFkXfJ2JQYn86+Z/8ei0R/nJp4/yXvYoND+Ik6iUM774wnnTU4VCoVAoFArFkEIJIsXg0NEBmZlojhwhsqyMSMBU44vLp9YGrmHFNehN3WR62tt4B0yezbuT/ElL88W49FqSJywgOTgZrdnM+vz1zEuYh7vW/cKttaWFGe9vYf0Lp9DU73c6TR/pwuZ58Ux5/GVmKjGkUCgUCoVCMSxQgkhxaTEYYN8+0Ouhu9vuLpeERPDygvZ2AFy7jcS4BPLLuHvY4VHF0dps2nvaCdDF0QjsXTEJqOZA1r/wcvXCy9WLR9c+SpBnELeMvoWvjvsqs+Nmn/9aTSZ4/3144gmorMRhZx83N7j9dn49qZ0nGz4CCuCzFVyTdQ2/mP8LpsVMO//nVygUCoVCoVBcdJQgUlx8TCY4cUKEUF6eCJ7TzVHtcHWVZqcWVzhPT8ZMXEz8V+9jVlUWmYZMihqLHD5Fe087n+V8BkB9Rz1vHHqDvPo8Nnxtw/mv+5Zb4L//dXxfUBB8+9vw8MMcc63nmdcz7O5en7+e9fnrWZayjGfnP8v4iPHnvw6FQqFQKBQKxUVDCSLFxaOrCw4dgt27oaoKDhyAvXvF9OCOO+znJiSIGLruOonI3Hsv3HAD+PnhC0yPmc70mOnUttWiN+jRG/TUd9T3PtxsNpNVnWV3SH8Pf/584M9k6DJID0/Hz8Pv3NZ/ww39BVFAAPzwh2Lz7e8PgHtdK4uTF7Pq5Kp+h/g893M+z/2cm0ffzE/n/pR0Xfq5rUGhUCgUCoVCcVHRmM0W72LFQJSWlrJgwQI2btxITEzMYC9naNPeLn2C9uyBtjbpIfT221I3ZOE734HERLHBHjcOAgPP6SnMZjOlTaVkVmWSVZVFfXs9Gwo2kF0laXVuLm48NvOx3loiDRoSgxLJ0GUQ5x/Hnf+9k5tH38zNo28m0NPJc5tMYtW9e7dYef/gB/CjH1ntvfuws3gnP9n8EzYXbna67mUpy3hy9pPMiJ1xTq9XoVAoFAqFQnH+DHQurwTRWaIE0VnQ1SURoJ077cWP2Qx//rNEiSzcdRe89x5otV/6aY0mI3n1eegNeo5VHeNE7QkaOhqYGj3V4fzs6mw+OvYRIH2SHpj4AH9c+kfHBz9wAJ57Dn73Oxgx4qzWs6lgE89seobdpbudzpmXMI8nZz/JNUnXSPNZhUKhUCgUCsVFY6BzeZUyp/jyGI1w+DBs2QItLf3v12hg3jz4z39k29sboqIuiBgC0LpoSQlJISUkhc6UTk7UnEBv0JNfn4+Z/nr/SOWR3ttdxi4q9m6gZHoBMUEJ/cXJ5Mnwv/+d03quTryanfftZPWp1Tyz6RkOVx7uN2dL4Ra2FG7h7zf+nbsz7j6n4ysUCoVCoVAoLhxKECnOH7MZcnNh3TpJiysuhvh4+zmenjB1qqSblZTAtddKulxw8EVZkoerB+MixjEuYhzNnc1kVWWhN+ipaKkAoL27nby6PLvHhB3M4e3ff5Wg624hXZdOhi6DUO9QAI5VH2NU6KhzjuJoNBqWjlzKkuQlfJbzGc/veJ59Zfvsn9c7jJtG3fQlXq1CoVAoFAqF4suiBJHi/KipgdWrxTWurg5WrZLbX/+6GCR4ecHMmTBlirVB6c6dEi26RPh5+DEjdgYzYmdQ3VpNZlUmeoOeb0/5Npk528g06NGaQNcK7NhJfWwc21Lq2Va0jSi/KHzcfLj7f3czOmw0d6XfxZ3pd5IQmHBOa9BoNKxIW8Hy1OVsLtzMr3b8ig354nz3venfw9vN2+HjatpqekWZQqFQKBQKheLioQSR4twwmWDXLti8WVLlDh6ENWugp0fu37AB3nkH5s61CiELg1grE+YTxtWJVzM/YT4lb7yA/o0sskKg1htrf6ED+yElBYDy5vJe4XKs+hhPb3qazYWbWf/V9ef1/BqNhqsTr+bqxKvZX7af3+/5PQ9Pedjh3LbuNlL/mMqo0FE8MPEBbhlzi1PhpFAoFAqFQqH4cihBpDh7qqvhk0+grMy6LyDAKoZA7qur6y+Ghgial18m7gdPEAcs0cCpYNDrIGfJFHqWLO6dZzabyaqyt/EO8Ajg42Mfk65LZ0TQCLQu51cDNSV6Ch985QOn97+vf5+69jp2luxkZ8lOHl3zKHel38UDkx5Q/YwUCoVCoVAoLjBKECnOjNksFtobNkhUyJbkZKkR2ne6PmbePBg//lKv8Ox4/nl4+uneTa0ZUus0pD75Ah3ffZjjp80YChsKae9pJ9wnnKbOJsyYcXVxZWTwSDKrMsmsysTbzZux4WPJ0GUQ5BnE99Z8j1vH3MrViVeft1ACEWKv7n3Vbl9jZyN/OvAn/nTgT0yOmszXx32d28bcRphP2Hk/j0KhUCgUCoVCUIJIMTBtbeKydvJk//u8vWHJEnjwQbjqKvjpT+Huuwc1Nc4pzz4LP/uZ/T5PT/jHP+Cmm/AEJkROYELkBJo6m8iqyiIxKJG8ujyyq7Np627Dw9Wj96Ft3W3sK9vHvrJ9FDYU8t7R93jr8FtE+EbwyNRHeGrOU+e1zLr2ugFrhw6UH+BA+QG+t+Z7LE5ezF3pd7EidQU+7j7n9XwKhUKhUCgUVzpKECmcU1QEH38MTU3Q3Ax+ftb7Ro2C664DX1/ZPnHigtloX3BefbW/GPL2hs8+gwUL+k339/BnZuxMZsbOpKq1Cr1BT6Yhk8bORoeH311i7TdU2VJJVlUWrV2tdiLFbBZH8ro6qK2F+npp1dTZaf3Z2Qn+/iFsuWcLOTU5vHXoLd47+h7VbdWnD54BnX7g3oLRvZVVjYdZlbkdb98HuXHUDdw+9nauG3md6mukUCgUCoVCcQ4oQaRwzIED4hxnMkFmpoiHFSukL8/118OYMfaRoKEqhv7xD3j0Uft9fn6wciXMmXPGh4f7hLMwaSELEhdQ3FiM3qAnuzqbjh5pPNvW3cap+lN2j3HHj2uffZHuqhG0FaRTcyKNGoM73d2AVx20O7ccT06GO++E1NBUXlj0As8teI7Pcj7jzUNvsu4f34WT1/V7TJtLN//wquNDvxamjNAQEgIhIeJsHhYGERHy0QUGnvHlKhQKhUKhUFxxKEGksMdkgvXrYfduCWvs2iW1QwBffCH9hMaOHdw1ni1r1sA999jv8/aGtWthxoxzOpRGoyE+MJ74wHiWjFxCZvlJjpRnUtx2nLvS7yLTkMmxmmP4e/gT6RvBeztNwEkZie7glwZd3nDX9ZC3CPR3Qc4K6PK1ex4XF/vndde6c/Pom7l59M1M/3MHex1kLmJyg1YdXa06dlY6Xv+hYw1McKCIPv9c3qa4OGkhZRkREf3XolAoFAqFQnE5ogSRworRKClyx47JtsFgFUMgOV333ANHj0qfoaFMfj7cdpu9A56bG/z3v+cshizk5kqg7PPPXdm9exRPPDGKp3/WwbHqY+gNek7WnqSxsxEPDw1eXtDefvqB2i7Q6SFxE7gYYeRqGblL4YOVds8xUKDN2Hn+zn0T/xHJtIRxvc1iJ0VNwkXjwrZt8Kc/9Z/v5gaxsVaBZBFMiYkSxYqOVoJJoVAoFArF5YESRAqhuxv+8x9784SICKkTWnn6pN3NDZ566pKKoZ4eKV/q7JQlWkZDA+j10NgowsPVFSZNst6PKR7NvN+j+ewTjjOKXFJwmT0Pl/+OYPQJKYHSaGS4uclwd5fesm5u4OMjKWdmM3z6KXz0kQgiW7ZvB09XTyZGTmRi5EQaOxp7m78GBFZZBREAZtBl2j0+MjiQEXdtJcycjr9bMB4eIjScMW6crK21VUZLCzQ3m2lrG7hmyNu/gza3DvaW7WVv2V5+tuVnhHmHcW3yteQfexZI6PeY7m7RlPn5jo/p4SF9eefPH/CpFQqFQqFQKIY8ShApoKsL/vlPKCiw3+/nB2+8Aa+8An/+s7jNXX31BXtas1n8Gurr5WdTk5QtFRZKcKqmBhYutPdysFBeDm++ad0OCYHvfMd2hhYm3AeaiWz8tIUdzIbNwGZ5CfX1jtf00ksiwM6GbdvEZC80FJKSYMmSAAJ8ZrMkdDZtc6oo6cykM+go7n5NaDxa+ecxN6rbTq9Oo+VrK2LxdJVFxfjHkKHLYEzYGNq7Xfjtzt9y+9jbSQ1N7X2+t95ytAoNHR1WswbLT8ttgwE+yVlFn0+W6rZq/q7/Oxy7B0eC6Ex0dkJUlOP73n1XTP2Sk2WkpkJamozYWBVZUigUCoVCMbRQguhKp7sbPvhAVIgtISHwta9J49Vf/Qq+9S1ISDjvp+npkb6uhYVQXCxRncpKcViz5S9/kRN5C1OmOBZEHh72233bI/UyfjzmknY4dOY1Go391zMQZrPU34Ckktmbu4UTxAJcmq5m9foOCotMuPv8gNiACtwmv4tXfDZthiiM3p14+HRQ2lRKaVMpa06tobatllf3vcr/bf0/JkdN5uHJD3PvhHudrsPTU8SJI4FiNpvZ99eXKChx8uBZv4WUldAQD41x0BgvYwDjB5DX6uzrkJMj2rqgQMrRbPHygpQUEUe2QiklRaJyCoVCoVAoFJcaJYiuZEwmyQXrK4bCw0UMWSy1XVzOWQzV1sKpUxLJ2bsXtmyRp6mqEpGzZInjxwUH2wsiZ9Eazz7lNE4FEWD2tE/x6+tKbTZDdjZs2nQ63c4JGo0In9RUeevWrrXe5+xk3mTSUFXuRWUJgA8QxYygUDxaCtFnm3vnubr34KI1cuDTKbT7Z4oJQ2ABB2a/wPrcbdw19uu4u517aEWj0bDjvh2UN5ez5tQaVp9azbq8dTR1NsmEkWtl9KXTFxrieX/Bdpqqgigqks8vP1/SCv38+otSC3l5ztfT3i4laEeP9r8vNhb+9S+YOfOcX6ZCoVAoFArFeaME0ZWKpTgmJ0e2jx6VmqHx4+GrXxU3tnPAZJLIT26uHLK21nrfiRMiiixUOnFCAxFEtrS0gL+/CCBLnY+bm4iT0aNFiHh6QlCgmWuu0eDmJvVEGo28RLNZ7KYTEmSNJhPMmiX1RmazbP/4x2Kq54yYGJgwQeqOLOVT+/bZzxno7Wprs98eGxtPmC6K6rZqDC0GGjsb6elypbXen/qKYKi4CrgKAgph9guc2DiN69fvJlEXwqjYCBIiAsjM1HDyJCQmmklJ0TB7tpgeOCPKL4r7JtzHfRPuo9vYza6SXaw6uYoNBRs4XHEYM2b7B3i0MDZdw11fCXJ4vCdWPstX/3eSWbGzmBU7i9Fho9G6iCPEqVMOH3JGSkok/dARGzeKH8bYseL4PmaMBDEVCoVCoVAovixKEF2pbNtmvUyfmyviyMNDXObOUgx1dYn4yckRg4HAwP7RF+h/ol5ZKWJEo5GnDA2VzDx/fxE4yclWd7P58yWdyhF33HH6hskEN9wA2dfDAw/0W8SkSQO/jh/+EG6+2X6fVgu33y73TZgg+3p6JKWuvR2uugrmzpU6p+pqiRqNG2c1OxDDA4k4tbbaH9vXF9y0bkT5RRHlF0VHTweGFgPZVe32EwOK8eqKxa8rlS66ySmrJKesEm83b/L2jCI3yw+Q1zrvuipuvS6coCAIChJhaelHtG2biLrERPmM3LRuXJVwFVclXAVAXXsdWwu3sqlgE5sKN3GsWlwGr05wXi/2WeE/OVFzgvf17wPg4+bDpKhJTImawvffmEFY5zTaDNGcOqXhxAn5jhw/7rx2C0TIJiY6vm/Tpv5ueDqdvUCy3A4IcP4cCoVCoVAoFH1RguhKJCcHNm+W23V1cundbJaz/RtugL/+Fe66y+nDKyslQnLggPw8fFhS4R54wHEdS2CgiJ2m01laISFiljB2bH8Rde215/F6/vQnaajz+ecSSnjjjXM6K77pJli+XCy1tVp5HU8+KVbTtri6ipjx9RWhdjYOa11d8pqKi2WUlcl2S4u8H42N0NTkSbxrPA0eZvbbPDbENZ7YmifQYC/w2rrbqK7vstu3xfdBmg2RXG943W5/dzc8/7x1OyBAxIVOJ5+DpycEewVz46gbuXHUjQBUNFewpXCLnaGDLTVtNZyoOWG3r7W7lW1F29hWtK13X6h3KFOipzB16lS+EjWFyVFTcGkPJydHooYWoXTihKTijRgh0T9HZGf332cwyNi40X5/dLQIo/R0yMiQPsJ9I48KhUKhUCgUFq5oQfTmm29SWFjIc889N9hLuXTU1IgAsnDggFiGWTAaRaX0wWyWGpIdO6w1In/7GxQVWeccOWIVRJayo+Rk2ZeQICfgs2df4JPT/Hx44gnr9n/+I2f577131ofQaOC116Qu5qc/dR6ROh/c3WH6dBnOMJslilRYqOHmm08bT5SY0Hj6ow2ZSUFlHW1tLmC2CqPOlj4FPAHFGJsyqNJWEeIV0pu+1tjY//k++8x628dHPpfQUGtaWnJyJKNH34HuK0Bk/8fvKtl1Vq+9pq2G1adWs/rU6t59i0YsYu3da5k9235uZ6dE2pzhSBA5o6xMxrp1sp2V5fg7191tTa9UKBQKhUJx5XJFCqKuri5ee+01/vKXv/CVr3xlsJdz6bA0XrUVQIsWSVHNSy9J6tmLL0rvodOYTHIFf+dOOcm0ZeJEe0GUnQ2PPSZX50eMsDc+uNe5Sdr5YzLBfffZF+n4+8Mvf9lvaksL/P73EvlxdfCtj4mB99+/CGs8CzQaiTqNHStDcAGCgCC6jd0cM+Sy59QJjpVU0NbkQUt3A4cqD2JuioLGWPAvx6d2KsdMx9BqtIT5hKHz0VFXFwg2EaagPiVBlp5GxcXyGZeWytiyRT7vqioICxOfjbAwEU7XJF3Dj71zqHQ5RJF2PfrOL6jtqDqr1xrhG+Fwv4cH/K3geeLr4pkQOYHUkNReUQfytczMlO9YdrZ8J7u6HB7KDnd35wL3F78QN/lx4ySSZPk5apRzwwiFQqFQKBSXH1ekIPrZz35GTU0Nt99+O90D2YpdbmzaBBUV9vuWLoWpU2HxYukz9OijgGiNrCzYutXeIMGWMWMkqtDUJKlm8+ZJBMhZf5oLzuuvywJteeklsSuzoaQEli2TkqnKSvjjH4dXVMBN68a4qDGMixpDW3cb2VXZ6KfpOVXXw7GaNRypPIqvVsfUqbm0N3vS3uRFe3MZBU3eVLUmEJQQSH1zOzQk0hOYRzsReNHfLMHW3Q9E0FpS2yyIePPi1z9KAVKA2/H0NHMsv4jMun3sL9vPvvJ9HCw/SGt3n+IpYELEBIevsbGjkac3Pd277eXqRYYugwkRExgfMZ6MCRl8d/FY/DzEg72nR8wbLALJMnJy5D4LY8Y4T8M7elSiUhs2yLDg6ipW4BaRNG6c1JGFhzs+jkKhUCgUiuHNFSmIvv/97xMeHs4f/vAHKgeyPLucKCyEXX1SncaMEQ9skKKehQsBa1lRaalEDoKC+vcCiosTe+SoKIkiPPSQmNRdMsrKxB7OlkWLJGJkw5EjYvFt+Zj/9Cc52X3kkUuzzAuNt5s3U6KnMCV6CvXt9WRWZaI36DG0GNC6NOEf1mQ337z0EA2ZH1JffwxMGkp7vNjrfzu3eLxNXZ29XXlfQeTIxc1s7m+r7emp4cM3EwgOTmBy2K0sjYfgiUYatLm8889a1n+ioysok3rfXfhPnd3/oMBRg70Pd3tPO3vL9rK3bK/d/sTARDJ0GdYxL4MbbhzRG03q6oKTJ0XM6/US1XKGI+tvEEGVlSXjgw+s+6OjJSo6YYKYbfj7Oz+2QqFQKBSK4cNlJ4g2bNjAt7/97X77v/Od7/DI6bPg8CvtUm9PD3zxhZzNWvD3l2pzm1CJ2SwGCWvWSBbahg1ilz16NNxyi8xJTZUMO4vhQFraJXwdtjz2mOTBWfDzgzfftHs9+/eLRmposH/o3/8ufWYdpc4NJ4K8gpgbP5c5cXOoaKlAb9CTVZVFS5f1fWnvaeNkw+kQj4sZ3NsYMQK84/7K1LAMotxG09bkRW2tpOudOCGRl9LS/ul1FvpGDIODJaJYUyPj+HEALTCKY2uhIhtgJHATueONtM+z2pdbOFxxGKpGQVABuDnvjlvQUEBBQwGf5nzau+/+Cffz5vI3AUmRs7jO3Xab8/eutXXgmiVHWGqTVq+Gp55yPKehQX61XM69ZZRCoVAoFIpBYpifEvZnwYIFZDuowHa5ks9Qdu2SM1VbbrrJ7qy0rU1M2o4fl0L8f//bml137JgUoD/66BBJG9q0STp42vL883a2cDt3SmSob2PXG26QWqHhLoZs0Wg0vRbei0YsoqC+AL1Bz/Ga49S01RAXEEdhQyEAIV4hxPjHUNRYRFFjEVrNKlJCUkgfkc60UX40JK3l0TG34aHxo7paRENVFb23Gxqk3mn8eBFGtbUDm2RU9SktKi7S8pvfiNudTidRRZ0OkrRz4HW5YOESVIwp7Cjc+hXQDtBx9zRjwsc43N/Q0cDtH91uF1FKC03DXeuOj4+kep46JZEivd76s7h44OcbO9Z5jdE3viFmDuPGWaNJEyfKRQVnqXsKhUKhUCgGl8votFDQaDS4Xk5nu1+W+nppRANipvDxx5LflpDQO6WsTMzZLI5knp72vgsA//gH/Oxnl2bJA9LVBd/5jv2+CRMk5HOa/fut1ta2fO97Upx/OWtjF40LI4JHMCJ4BNcZryOnJoeZsTPZX7afI4Yj+Ln7obGJohnNRo7XHOd4zXH2lO5hbd5aHl3zKLeOuZVHpj7CxPET7Y7f2Sna2iKSqqokHbHve22hrw63NF5tbJSRmyvblZUTsfSGNdUlEOYay2+uXktRzz6ONxxBb9CTW5uLyWyC0qliJBGeDcEnydBlOHzuTEMma/PWsjZvbe8+VxdX0kLTRCCFi0iadW0Gt9wS1fu+1NeLgcPRozIOH5ZtS7nhBMdlUAAcOiTvxc6dMiy4u4uQmjhR+mJNniw1Su7uzo+lUCgUCoXi0qCUw+XOxo3WKvMtW6TA4vHHpfr8+ec5VBzKypX2tSQeHtKU9M035SQwPh7efXeInLy9+qolJ8vKa6+JqwPyspYu7X+C/vTT4io2nMwUvizuWnfSdemk69JZkbqC7Ops9AY9pU2l/eaazWb2l0sXpLbuNt498i5erl787KqfofPV9c7z8JBamuho+8dbbLMtIqmqSnoEfeMb1lS62lqHju5Af+EUGKClaMsCYAEzg+CmCAhM7aTF/SS//50HOz8fKRO1negjurj64f7H1Bv0/fb1mHrIqsoiqyqLD7AWCIV4hdhFkualz2Pu3KTe+7u6JFJ66JA4KDqivl5K9RzR1SWPPXQI3npL9rm7iyiaPBmmTYN77nH8WIVCoVAoFBcXJYguZ8rLpTIc5Ax17+kCdbMZ3nyTnNYYPhv5034Pi44W04G0NNFQb73lvJ7kklJTI6rGlvvugxkzAIl0LV7c/+T62WfhJz+5RGscovi4+zA1eipTo6dS115HpkHMGGrbpSCoqLGIunarq4JWo8XX3ZfXD7yOzkdHhi6DdF06/h6OnQQ8PMS6PCbGfn9rq0SQDAbrz+pqqTmyxWQSsWSp97I1Q6ivlwEewFiKj9g80OiBS48HFRWSzqm1OnVzuDQbPvsL6DJBdxR0evBy0JgJqG2vZXPhZjYXSsPilxa9xPdnfL/3fnd3SRPMGGc63Si3v7IuKpL0wb7mFM7o6pI2YAcOSBDXmSAym68sIa9QKBQKxaVmSAqinp4e7r77blJSUnj22Wft7jt8+DAvvfQS2dnZuLm5MXfuXH70ox8RNpCdlBMeGa5WY2eLrZewXm9nqtAaGs9HCY/1e8iMGWI2p9VKZtp3vjOETsaee04KPywEBMCvfw1IDdSyZfZ9kUCM6K50MdSXYK9grkq4irnxcylvLkdv0NNt7GZO3ByOVB6huauZtNA0vN28ATC0Glifv54N+RuID4zHaDaSU5PDg5MeJDk4ecDn8vGRiIptVKWnR0SrrVDy9pZoSXe33OesRsds7l+XlJ8Pf/mLfGd1OoiMFPfDqzRP8/YhayhL61+J8QcOOs06wFka3s7inSz/1/LelLtxEePI0GUwJmwM48f7UFMjNu+HDkmqneVn3x5efZk82fl906ZJud/kyWIKOXmyvJ9D5vdSoVAoFIphzpATRG1tbTz++OMcPnyYlD4dFY8fP869995Leno6v/nNb6itreXll1/m2LFj/O9//8N9SOR0DRGKiuRM0cKCBfC1r2F+7TU0ej2fX/17ut28qaqSK+tubrBihW1j0CF2wlVdLZ7Ztjz9NISFYTZLatbhw/Z333efeC0oHKPRaIj2jybaP5pFIxaRX5/PkcojfJ77OV6uXv3mmzFT2FDIx8c+Jqs6ixd2vcCUqCn89prfMi9h3lk/r6urmCnY2rSbzWKAYRtJqqyUNDtbc0SjUSI1ltS8ri7RxZb7ystlHDwIer19Xt+0sRG8cVc9Bo2eY7V69AYZmVWZtG19CGpTJZIUcZQRvo4Fkd6gp6GjgW1F29hWtM36XqIhOTiZDF0G43TjyEjL4J6rMvi/wAQ0Gg1VVfL9PHjQGhUqKbEe15kgamqSuWaztRQQJJo2aZJVIE2dKtG5IfU7q1AoFArFMGFICaKtW7fyq1/9inrJj+nHK6+8QkBAAG+99RYepy8hjx49mltuuYWPP/6YO+6441Iud2jTt+dQUhLmr36NVZH30/DP1ZwcuZSjR+GTT8SN7Z13LnEfoXMlLEzOCB9/HLZvF0e50xG+F17obzq3bJlEDdQJ4tmhddEyMmQkI0NGsiJtBSdqTqA36Mmry8OMVZG0dbdxvMZaw7W/fD8f6D+gubOZDF0GcQFxdqYNZ4tGI3bV/v4wcqR1f3e31bjBIpJWrLCafnR2Ov+M+7YY02jgw/cDcXGZS3j4XK6LhAfGgS7CxA2fdHL4oFUE6ldAwvL+x3RUlwQiFk/WneRk3Uk+Pv5x734/dz/SdemM043jgYkP8NRiqyODwWAVSAsWOH4Nhw7ZC0ILDQ1SHrhxo3VfRIREkyxj8mTVK0mhUCgUirNhyAiipqYmHnroIRYvXsxTTz3F3Llz7e7v6upi165d3Hzzzb1iCCAjI4OEhAQ2btyoBJGFmhpxF7DlqqvYsVPD/oMaSLmOggL47DO5a/VqeOYZeP31IW4NPG0abN0qPZU0GrHDo3/Nxpgx4oqnzAbPD3ete6+5QEtXC9lVYsZQ1lxGZlUmRrPVgSPIM4hIv0gOVhzkYMVBAjwCeuuNwn2+vEe7m1t/EwezWQSBRSBVVkpUyDabEiTa6eMj8wwGSaMDqVeyPO7wYTCbXTimt4+IRUXJvL6OhEePt8L7KyHykIyoAxBYgjOau5rZVbKLXSW7WJG6wu4+nU4MQOYubMFsNgN+/R5/5MgZ3iAbKivh009lAHzlK/DRR2f/eIVCoVAorlSGzCmjp6cnK1euZIQTC6eSkhI6Ozsd3p+YmMiJEycu9hKHD32jQ1FR5HbG915N7uyEDz+0L2x//32pFxo//pKt8vzQaCT8Y8Ovfw3p6XD//VJr8emn0qdV8eXxdfdlWsw0psVMo7atlqmlUwn0CGRnyU5KmkoYHzHeLiLU2NnI9uLtbC/ezqm6UzR2NvLDGT/kqvirzity5AiNRkw+goLsGwO3toowqqiQERBgFUED0dBgbzPv6Sk9udatk8dbBFl0NDwS/Tf2nnKBU0sBCE8/SsADt3Cq7pRdJM0REyId+3X/Q/8PvrXyW4wMGcmEiAlMjJzIhIgJTIicwKOPhrJihTXNzjL6ij9HTJvmeH9Hh9TVTZ0qIy5ORVIVCoVCcWUzZASRu7u7UzEE0Hy6w6avr2+/+3x8fHrvv+Lp6JCmKTY0jZ3J/z6xnvF4eMDNN0uEyNJ76L33hoEYGoC77pKT48ZG57bIii9HiHcIS1OWsmTkEsqay1iZu5KyZsduAWazmXV566huq+aL3C+ID4jn+QXP85VRX8HD1YljwpfEx0fS7WxT7lpbrQLJIpYsTna2j7vzTmtanqurCISuLrHRtrXS3rLFPmR09zXjePGRXFq7WsmuzuZo5VGpTarSs/et2+js6YLIQ+hGlhPi6ThidrjyMGbM5Nbmkluby7+z/917X6x/bK9Amn3fRB55egKRPtHk52vYv1/E0f79knrX3m5/XGeC6MgR+N3vrNs6nTXNbupUqUuy1GUpFAqFQnElMGQE0ZkwnQ5nOLvK7HI5d9s8F7KyrB0kOzowh+v4X85ou5MlFxexov75z+HWW+HGG+G22wZnuReSSZMGewVXBhqNhhj/GB6a/BBGk5G8+jz0Bj05NTl0m+S7V9JUQnVbde9jihuLOVh+kPz6fFJDUsnQZZAcnIzWRevsaS4IPj6QnCzDQltbf5Hk7m4vpJxRXNx/+7XXIDrah+joqVwXPZX7xsvvWND3zHQ2yt8rA3DyPvuIloVDFYecPl9JUwklTSV8mvNp7z6dj44p0VOYGj2VJ5Y/iM5XR3e3/Orv3Qv79olIcvb7sG+f/bbBIBdHLCm0Go2s0yKQpk+XCKxKQVUoFArF5cqw+RcXcPqSpaNIUGtrK34qR0o4dPrkqrsbXn6ZtoTR+I4Ox3XUTfS4SZ3EggWQkCDTduwYwnVDubkSzoqPH+yVKJygddGSEpJCSkgKnT2dvWYMq0+utpuXGpKKn4cfPaYesquzya7OxsvVi7HhY0nXpRPrH3vBUurOhLd3fyvw9naxxrYdra39H7t0qdxnEVSRkdaGtJZ6H1dXGY2N1tfj7w99TDMBiaQ1nhoNe5ZD9F6I2g9+hgHXb2g18EXuF3yR+wX3TrgXkN/hCRNkfPObclxn76elHZkzzGbpfXz8uDRkBhGWU6bAU0/BNdcM/HiFQqFQKIYbw0YQxcbG4ubmRqGDVvAFBQUkJw/cD+WKwGCQS94Ax45BZyc+OYf5Ss7dVG9P47WHj5E0QsPMmdaHOOv3MiT43vekl9J998GTT0J8PCaTnNCd7sWqGEJ4uHowLmIc4yLGcX3K9bx56E3e179PZlUmk6L6hyvae9rZX76fX2z7BS4aF76a8VVuGXMLod6hl3ztXl72kSSzWdIvS0utAqm8HEJDZYwb5/xYPT3S9suWsDD45z/t65G8vSXadrPruzxvY6mdcfMXuCz8CdlV2b0RN0dE+EYQ7Rft8L7H1j3G2ry1TImewpQoGRm6DDxcPfja10TI7dsnKXd9U+0c0doqTZoff9zx/Z2dIgK1Fzfgp1AoFArFRWHYCCJ3d3dmzpzJxo0b+eEPf4jnaYcxvV5PYWEhX/va1wZ5hUMA29qhPvZUOSnLcffQsHz5MCmg3r9f7O9A/LPfeQcyM3ltXSrf/a5opBdflH4siqFHkFcQP5r1I34060ccrjhMbXstWVVZNHQ02M3r6OngUMUhuk3d7C7dzS+2/YLvTvsu8xLmMTZ8LL7u/WsGLwUajXy3AgOtvbmMRrEAt40iVVc7tsWOj5d0VEskKS4OTp6UYSE4WISRbf9kgMeWX89Xv3o9nT2dZFdnc7jiMIcqDnGo4jAHvphAT/g+0B1lStQUp1GgPWV7eiNx7x55FwA3FzfGRYxjatRUZnxtBt96agZxfklkZ2vYu5fecfy449cEkj7niA8+gO9+V9LsZsyQMX26vEaFQqFQKIY6w0YQATzyyCPccccd3HPPPdx77700Njby0ksvkZyczM033zzYyxtczGaJCgF0dWGsb8T2Yu3K8Hu5ceEwEhC/+Y399rRpFLqn8OSTsvnOO6KXPvwQZs269MtTnD0Wd7UFiQsoaSpBb9CTXZVNe087mYZMuyhIa3crbd1trDm1hrWn1pIUlESGLoO00LSLZsZwtmi1ElmJjLQ2Uu3slMiRrUhqapIUs1GjZDijrk4az/aNJrW1yfWM6GgPJkRMZGLkRL7BNygthdgHZI6bRw9FExow397/AkePqYfDFX26FAPdpm4OlB/gQPkB/nRAmhyH+4QzPWY6M8bM4GuLp/OHqCkYO3w4cEDE0Z49sHu3CL/UVOcCZ9cuaGnp3xspNdUqkGbMgNGjVRRJoVAoFEOPYSWI0tPTefvtt/n973/P448/jo+PD3PnzuXxxx+36010RWIw9DbkMbu5c/iq73EwfCkpJ1fSeLSAX3+SRo5GmpgOeRe2U6fgv/+13/fMM3znEY1dXUdrqyovGk5oNBriAuKIC4hjSfISTtWd4pMTn9jNmRAxAReNGKSYMZNXn0defR5uLm6khaaRrktnRNCIi27GcLZ4eEBiogwLTU3965G6uhw/3mSCxYutoqqxUW5/8on1+JYUO8v1DoDuTle8zKEOo70na0/S3uAHnkZwdfLEp6lqreKznM/4LEccFYK9gql+vJqrr3bh6qtljtkM+fkSHXPG7t2O9+fkyLDUIvn720eRpk0T+3SFQqFQKAaTISuIcvo2Fj3NtGnT+Ne//nWJVzMMsDlbamiAItdkKqIns988mbe3yv7//U96mu7aZb3CPSR56SX7nJ1x41jZvYiVK+2nvfACxMRc2qUpLgxaFy2poakcePAA/zv+P17d+yq7S3czMXKiw/ndpm5+svknhPuEMztuNjNjZ5KhyyDaL/qSmTGcLf7+MizRIZNJeiXbCiSDQfZrtWJ3b7G8N5nsIz6dnSJG8vPtIy8gFwMKC6VXkru7df+osFFcn1vK2jVaIlMqcI8/ROu4F6hw337GtU+MnNgrSC1oNHIRZX/7v+go0DE1eio+7j6993d0yOs7G5qaYP16GRbKys6uX5RCoVAoFBeLISuIFOeITXFCcTFU60YDsG2b/bSMDJjo+JxzaFBdDX/9q92u7kcf43vftz/pnTULHnjgUi5McTFwdXHlljG3cMuYW6hpq8Fd605WVRZ6g57KlsreeRXNFRyvOc7xmuNsL97OqNBR3Jh2I2E+YaSHp5OhyyDEO2QQX4lzXFwgPFzGhNO9Wbu7pbbIViTV18tcZ0RGisgqLYXmZqlpevddESzh4RJFio2VcWCfG91dUJwVDVnRHHx2GTGpVewr28fukt3sLt3NvrJ9tHbbW+lNj3ZcJGQym/jWym/R0NGAVqNlYuRE5sTNYXbcbGbHzaaiIoy8PIkUWYZeb9/82RFRUfK6HLFlixg1TJ4szXIVCoVCobhYKEF0OdDaKl0lkROl+nqoHSVNVcaPl6vH2dky9Te/Gfika9D54x/lkrOF2FheKruNU6esu1xcZNoQCwwoviQWd7mZsTOZGTuTqtYqMg2Z6A363pQukJPz9u52tC5a6trr2Fq0la1FW4n2iyZDl8GY8DGDZsZwtri5idFCXJx1X2tr/1Q7Wwe40aNlgKTWWUSC2SwRJ4NBXPcbGnr/HAAyLy0NvL3DuT7leq5PuR4Ao8nI37/I59e/8MIj8QC1YZ8xLmiOw/Xm1OT0GmIYzUb2l+9nf/l+XtrzEiC26nPi5jA7fTaPXT+HxMBEWls17NtnFUh79kjNlC0zZjj/PX76aYlmu7tLT6VZs2TMnCkCUKFQKBSKC4USRJcDBQW9KWZlZdDsG0n36ZSWpUvh3/+GVatg5Up66wKGJG1t0uXShsZ7v8ezv7JvlPTQQ9YUI8XlS7hPOAuSFjA7bjbPb3/e7r4p0VP6zS9rLqOsuYy1efZmDO5a935zhyI+PtKryNKvyGyWskBbgVRRIZGh023ZHNLUJCl7TU2yrdOJI2NkpDWCFBsLfn5aSvUjyTkAHIgBbuDzFrj5vf7H3F3qpEjoNDm1OeTU5vDW4bcAiPSNZE78HGbHzuamBxfy9NOjMJslkG0bRZo92/HxOjrEEhyk/soy/3e/k30jR1oF0qxZYt4wpC/0KBQKhWJIowTR5UBeHiBpOFVVUB+d1HuX5QrsddfJGNL8/e/2l5ADAniq4AHa2qy7goPhF7+49EtTDB4erh7kPZrH34/+ndf2v0Z9Rz0rUldwqu4URrPRbm63sZuPjn3EuIhx5Nbm4unqSVpoGhm6DJKCkvrVxwxlNBoICZGRkSH7jEaJ/tiKpL71O3Fx8P3vW80dXF3lcaWlMiwGCEFB8PHH9o91JlCSg5OZ2/QnCj0+o9htLbg48eU+TUVLBf/J/g//yf4P35nyHf6w9A9oNFbB9/Wvyzxn9t6HDjk3ogCrhbnFrCE4WCJHs2bBVVepPmUKhUKhODeUILocKCgAoCq/mXEHP8SjpQazi5bG0TMYOdJrkBd3lpjNkgdnQ83N3+TPf/Wz2/fcc3KCqLiy8Pfw59tTv83DUx6mrLmMGP8YOno6OFZ9DL1BT2FDIQDZ1dnk1uWSW5eLv4c/s2Jn0WXsQm/Q4+Pmw9jwsWToMojyixpyZgxng1ZrdZ2z0NEhwqekREZpqZgxWMwdnFFbKz2H+u7bskUiSNHR1rS8sX5z2fbSXOBbBAWZSJ5QyYwf/I6dZds4XHkYk9l5sdDsOMcqq769ntf2v8a8hHlMjZ5qF8nz9oa77oKdO8U44kzU1YlhzBdfwIIF/Xs7KRQKhUIxEEoQDXeamqRoAGg7WUZ0UwnT9r/GtP2v0Zg+C5cndgzu+s6WbdsgK8u67eLCj0u+bVeUnZYG999/6ZemGDpoNBpi/MVa0NPVk4mR0qensaORrKqs3iakAE2dTdS11/Vut3a3srdsL3vL9hLiFUKGLoN0XTrBXsO7e6inp7jAWez0TSaJFFsEUkmJ1BX2RaOR9NPi4tNGLNVSs7Rli/X+8HARR6evuQBQX+9CV20Ur1wn9UPNnc3sKd3D9uLt7CjewZ7SPbT3WIufnAmibUXb+MnmnwDg5erFrLhZzIufx7yEeUxJn8L774tAKiuTWqKdO2UcPiwRL2c460tmMsHDD1vrkdLSVJqdQqFQKAQliIY7paWAFGS7VVfY3eU1c8JgrOj86BMdqp17A2+vi7Xb99xzkv6jUPQlwDOAYK9gihqLevdp0DA1eqrD+bXttWwu3Mzmws3E+MeIGUPYGDs76eGKiwtERMiYcrrUqrnZXiBZapGCgmSMG9f/OLZmDX0jLiNGyHEiI8HPw49rRlzDNSOuAeChbxk5nN1CcFomrok7CfWI7n9wYEvhlt7b7T3tbMjfwIZ8eSJvN29mxc5iXoIIpBU3TeaWW0QgtbbCvn1WgbR7t5hMWHAmiE6cgL/8xbodFCSpdZY6pClTJDKlUCgUiisPdXo53DktiKqrwa/FXhC5TZs0GCs6d8rKpEmSDb+o+47d9rRpcOONl3JRiuHGqLBRZH0ri1f3vsrf9X9nYdJCfjDjB+gNerKrsuk0dvbOrW+v50TNCSZETqC0qZTSplLWnFpDcnAyGboMUkNScdO6DfBswws/P3uXOovtd3GxVSTZ1ur1JTwckpNlXmenRI/eflsuUERFWY0aYmJg7WotRUUBsH02MJvdM2DevP7H3FK0xenztXW3sT5/PevzpWGRrUC6OvFq5lw1mfnz5d+X0Sht2CwCabpj53B27rTfrq8Xs5lVq2Tb1VVaEsyebRVJOp3z90ShUCgUlw8as9lZWavCltLSUhYsWMDGjRuJGUrdQN95B4qL2b8fMBho9omgcHcF6d0HeSryXcbdOYYnnxzidTc1NfDKK/DGG1BVRdfI0UTVZlFbZ63x2LzZ8UmVQuGIuvY6GjoaSAoSg5EeUw+5tbnoDXpO1p5k1alV7Cndg5uLG+MixjE9erpdHyN3rTujQkeRocsgMShxWJkxnA8WRzuLOLKk0PXFZJL9AQGOewM1NMivsgV3dzmuj4PA2wur/sWa/Xkc83ibys6C/hMGYNd9u5gRe27OCffcA+85cNAbiBEjRCA9+KCYNigUCoVi+DLQubyKEA1njEYoL6ejQ9JI8NXxcfwP+dO20z1YKmDX6/Dss4O6yjMTGirWcc88Ax99hLuPD4ULNbz+utjsTpyoxJDi3Aj2CrarDXJ1cWV02GhGh42murWa3+76LQDdpm4OlB8gyi/KThB1Gbs4ajjKUcNRfN19e5u/RvhGDEszhjNh62hnsbRvb5cAtK1ZQ3f3wFGTkhL77chI+MMfJHoUFwfx8RJRcnUFl+O3s+n/wMPjKaZPaSfjut20pLzD5oLNVPSJdtvi7+Hv0HYd4ETNCXzdfXvrzGz51rckyuUozc4ZeXkyli4981yFQqFQDF+UIBrO1NZCT09vwXSXuy85ZfYNKa+9dhjlxXt4iLUU4As8/jh8+9uOC8IVivPlo2Mf0dLV0rsd4BHA/Pj5NHQ2OJzf0tXC7tLd7C7dTah3qJgxhKcT5BV0iVY8OHh5Sb+fkdLjGaNR6olso0iWXkcWxo4VwVNUJCMyUlzwLDbZYE2zs1h+d3Zq2LPDm1tvWsD3b1qA2WzmZN1JthRu6R22AmlewjxcXRz/63piwxN8lvMZqSGpLExayMKkhcxLmEegZyDTpknqLUikyzbNbudOyM93/l44q0v69FP49a/teyKpprEKhUIx/FCCaDhTVQVYBUOLbwRBWikOzsqSK7zXXDOI67sAeHsPI0GnGBZ8ZfRXqG2v5U/7/0RFSwXfmfodHp3+KIZWA3qDnkxDJs1dzYCk3nloPXrNFmraathUsIlNBZuIC4gjPTydMeFj8Ha7/L+kWq0Imagoq7BobLQ3a6istEaaJk50fJyeHrHSPnTIfr+7u/zdio/XkBKSQkpICg9OepC2NjO3fbWZgLQj1Ed8zA2pjs1iekw9vUYNlkaxr+1/DReNC5OjJrMwUQTSjNgZeLp6MnasCLiHHpLHV1SIm92OHVY3u54eiWpFO/aFYMsW2LNHxosvyr6+TWPT0iQCp1AoFIqhi6ohOkuGZA3Rxo2Yt21n1y5JZSmJnUneiEV84xtylXL3bhg1ShUGKxSO6DJ28d/j/2VO3Byi/a1nvCazicKGQvQGPU+sf4L8hnzG6cYxPWY6od6h/Y6j1Wh7zRhSQlIuKzOGc6W7G8rL7aNI7e3953V0wNq1Yufd2CgXPR57zCocgoKsKXaFhfCVr1gfO2eOuPT3ZU/pHma8fea6Ik9XT+bGz2XxiMUsHrGY0WGjHaZBtrbC/v1ywcmZocuUKXDgwMDPZ2kaazFrmDzZcf2VQqFQKC4uqobocsVgoLVVTkIAWnx0eHrK1UwXF1V3o1AMhLvWndvH3t5vv4vGhaSgJNq728mtywXgYMVBDlYc5OHJDxPmE2Y332g29kYkPLQejA4bTbounYTAhMvejKEvbm4iYuLjZdtsFhMGS6+joiIRQJ6esGKFzKmvl2GrSSz7jh6FjRvtnyMqStL3tFr7/Z09ncyMnsPe8l0Yzc4bFXX0dLAubx3r8tbxQ35ItF809024j2fn2xdb+vgM/De0rQ2OHBn4/QD7prEgkbBJkyTdLixs4McqFAqF4tKgBNFwxmDoLQz2bamk282L+Phh0mywpwduvx2WL4ebb+ZEsTeZmXIlVvUaUgwFXtr9kt321Oip3DP+HjKrMiluLHb4mE5jJ4crD3O48jB+7n6k68SMQeejuyzNGM6EpblreLhERkAEkUUcFZ9+G4MGKMcq6GNA190tdTvR0VazhthYuCrhKrr+so3pnj0kTjpFT+JqMl3fJbtGP+Aay5rLqG8/90JFb295DbZ1SGdqGgvQ1QXHjzt3/mxoEBe/K/DrolAoFIOGOvUcrnR1QWMjTU2gMZuYdPANJh/8C10f6OAvaXJZte8l1KHE2rVSVf3xx/DIIxTEPsit2S+QkACPPgrf+Ib0TlEoBovxEeOJ9Y+lpEms0x6f+ThToqcwJXoK9e31ZFZlojfoqWmrwWw29xM8zV3N7CrZxa6SXYT7hJMenk66Lp1Az8BBeDVDh4AASE+XAZJSZ4kgFRdLyp2tqFiyRAwPCgokDS8xUURRYaEMEPHg6WlJX3Nl5440NJo0amq+T5d7JZsKNvU2frV8nrYsTl7scK21bbU8+MWDvel18YHxdvdHRcEtt8gAaGnp3zS2r/EESAqdswtXt94qkaeZM611SJMmieeMQqFQKC4OqoboLBlyNUQGA7z+Onv3gqm+kRl7X7beFxbWa7gwZLnpJrtmrH/V3Md95rd7t//3P7jhhkFYl0JhQ7exm4+OfcSHxz7kw1s+ROtif5HBbDZT0lTCvHfnkRCYwISICfh5DKzk4wPiydBlMDpsNF5uXhdz+cOS7m7p1WyJIpWUyPUfkMCyswiyXm/f3zk+Hl591VqLFBgIYCa3Npd/797BzupV7KhcQ7exm7on6vB19+13zH9l/Ys7Pr6jdzs1JJXFIxazaMQi5iXM6zXbcIbRCNnZVqOGnTvlNT33HDz1lOP5QUHQ3Gy/38NDImyzZkkt0syZQ7y3nEKhUAxBVA3R5Uh9PV1dcnXVv7PPJcjY2MFZ09liMMDnn9vtesN8f+/t5GRYtuxSL0qh6I+b1o070u/gjvQ7HN6v0WjYXbKbgoYCChoK2FG8g4VJC5kVO4tuU7fDxxQ1FlHUWMSqk6sYGTKy14zBmZX0lYabGyQkyACxyDYYrCl2JSX9BQOIw50tcXHiZGdxs/P3h7g4DfHxqRx9N5XNn3+DGTNNpM+ooN7gi6+DP5tr89babVtqxV7d9yruWndmx81mafJSlo5cSlpoWr8ooVYLGRkyHn5Y9pWVyWt0RGam49fW2WkVVL+VFlqkpdm72Y0cqdLsFAqF4nxR/4GHK3V1vf84XUzdNHjq8O2owRUj2U2xnPrUWrQ85Pjb3+RS72lyXEaxxzS9d/v73x/a2X4KhQWz2cyLu1/s3e42dRPhG8GPZv2InNoc9AY9p+pOYTKb+j3WaDZyouYEJ2pO4Onqyeiw0WToMogPiL8i642c4eIi/YwiI2H6dDFqaGiwutgVFYlxw6JFMHWqpNfl5Vn7J1loahJb78xMWLNGIlHbtrqwbWs010y3mtFYMJvNrMtb53RdXcauXgv2x9Y/RkJgQq84mp8436kVuzMLb5DX4ufnWBT15cQJGW+fDqwvWQKrVp35cQqFQqHojxJEw5W6OlpO95ZsCEriB1O28972JCKoxPVUD5P/NoQF0d//brf5hukbgJwABgfD178+CGtSKM6DipYKChsKe7c1aHh85uO4ad0YGz6WseFjae1qJbs6G71BT2lTqcPjdPR0cKjiEIcqDuHv4U96+GkzBl/lmd8XjUbSyoKCJPIC4vhma9RQUSGRJUdUVsp8Cx4ecPCgNGq1pNclJEBEBLx51So+3XuYYx7vsqd8Jz2mHscHBQobCvnTgT/xpwN/wkPrgf5belJCUs7pta1YIe56mZn2Zg3Fjj087BgzxvF+s1lKNqdOlb+vCoVCoeiPEkTDlbo6WlutmyWtwZjQUo5cfrw7bZDWdSaOHpX/9qfpQcv73N27/c1vit2tQjEciPKLouh7Rfz1yF95YdcLjI8Yz6iwUXZzfNx9mBo9laSgJL7I/YI4/ziyq7Opba91eMymziZ2luxkZ8lOdD46MnQZjA0fS4BnwKV4ScMSb29JIUs7/XevqwtKS0UgFRXJbUtQuq1NhEFdnWwnJkpkqLMTTp6UAeDmpkGvH8d//jOOwMB7WL6wmwnX76M89B+szVtLfn2+0/UEeAaQHJzs8D5HBhy2aLUwfryMb39b9pWU2Auko0f7C75Zsxwfr6hIokcgfels0+ySk1WanUKhUIASRMOXxsbeCBFARZO9ikgbqoLo/fftNtexiCrkKribG3znO4OxKIXi/PFy8+LhKQ/z4KQHB7Rv/sPeP/DstmcZGTySH874IV/N+Cq5tblkVWXR2t3q8DGGVgPr89ezIX8D8YFWMwZPV9XZcyDc3SEpSQaIWUF5uYiDlBSJplRUSGqdmC30p7sb9u6V2w0N8N+P3PDxmsW9987i8WXQ4X2KDYWrWXVqFZsLNtNp7Ox97JLkJU57UN3y4S10GjtZmryUJSOXkBCYcMbXExsrXQpuP902q7kZ9uyxCqS9e50Lop07rbePH5fx1luyHR5uL5AmTpT3TqFQKK40lCAajpjNGBub7dI+blhh5i83ijVtTg5MmzZ4y3OK0QgffGC3yzY6dPvtUiegUAxHXF1c+zVttdDS1cIf9/8RgJN1J/nmym/y6rWv8si0R1icvJj8+nz0Bj3Hq487NGMwY6awoZDChkJWnVxFSkgKGboMkoOTlRnDWaDViqiw+M2YTGLEWVQk1t1FRfZpdCARo5I+Dt1BQbB1qwytNpmoqEd4LOER/i+tnQ/W5tIU/wFbav7D0pFLHa6jrbuNL3K/oNPYyRe50ql1dNholiQvYenIpcyOm4279syKxM8PrrlGBjhuVGvBVhD1papKnPks7nyenjBlijjZWRztAlRgUqFQXAGo/6TDkc5O2hu6ejdNLq6ERnv2XuUbsmzZIpdpT9OML59wQ++2xYVJobjceOvQW9S11/VuB3oGcs/4ewBw0biQHJxMcnAyXSldnKg5gd6gJ78+36EZQ4+ph2PVxzhWfQwvV69eM4a4gDhlxnCWuLhIjVBEhFw8MpvFmMGSYldYKA1kR4+GU6egowNCQ+0byBqNIphKSiAnx4t//WscMI6RI3/N0Soz1z3VP/23byQJ6P0sX9z9In7ufiwasYjrU65n6cilhPuEn9XrGciEJj4exo0TW/IzNdno6IDt22UA/PWvcM89Z7UEhUKhGNYoQTQcaW6mo8O62enhT0joMDgR6mOm8F9uoh1xYpowYYhGtRSKC8C1yddy1HCU9/Xv02Pq4dtTvu2wX5G71p0MXQYZugxaulrIrhIzhrLmMofHbe9p52DFQQ5WHCTAI4AMXQbpuvSzPpFWCBqNpI+Fh0uExGyWGqO77xbXuu3bodZxyRcgosnCyZMa1q/X4OYmIspi0hAfD9uLtw+4juauZj4+/jEfH/8YDRqmxUzj+pHXc33K9WToMs5L8D7xhIymJvs0uz17sKtDdYSzC2y7dolT3/Tp8ndb9URSKBTDHdWY9SwZUo1Z8/Mpee5v5OWBi6mHqLL9dM+ez6i5YZJzttRxusag0tYGOh22hU/XsI4NSM7HG2/AAw8M1uIUiktDSWMJL+95mSdmP+FUtDz0+UMEegbygxk/6HWZq22rRW/Qozfoqe9wXqdkIcI3QsRRePoZG8UqzozF6ts2xa6+3nrfq6/K/RbuuENqlfoSFGRmf3YNbslbOOn7Hntq1mA0G89qDT+Z+xOenf/sl30pvfT0iDmDrVlDmY3uDguT/k+ONNiTT8Kvf23dTkkRcTRjhvwcO9Z5A12FQqEYLAY6l1eC6CwZUoLo6FFyf/M/ysvBo6OBGXtfsd4XGWmXljZk+Ne/5CzhNOVEEksJJrT4+8uSlbuc4kontzaXUa+NwmQ24enqyYMTH+RXC3/V29PGbDZT1lyG3qAnqyqLtu62AY+nQUNiUCLp4emMDhuNh6vHpXgZVwSNjSKMTp2SCzp6vRg1aLUSkXHUfLWhAV45/ec6LAxGj+1m8or9nDStY0/Lv6gy5lg6EPRj09c2MT9xfr/9Z3KtO1vMZrH33rkTduwQ577f/c7x3HnzpI7KGT4+EmmzCKTp0yX6plAoFIPJQOfy6hrOcKS5mfZ2uenW94QozHFR96DTx13uA+7EhCS+f/3rSgwpFAC/3vHr3rqhjp4ONhVusnOU02g0xPjHEOMfw+IRi8mrz0Nv0JNTk+PUjCG/Pp/8+nxWnlxJakhqrxmD1kV1P/4yBARIH6SMDLjpJgl+HzwoYiIsTMRR38uNFktvkJqlgjw35tXPZAIzGW/+GU2UUu6yC33nZ5zoWQNedaCBAI8AZsfNdriO9/Xv8/Lel3tT6yZFTXLqcDcQGo2k9cXHw513Op/X0wP79w98rNZWKRndssW6LylJhNEf/qD6ISkUiqGHEkTDkaamXkHk3kcQmULCOPd/hZeAxx8Xi6ePP4bqaqoX3Y3rJvnn+s1vDvbiFIrBp727nfX56+32PT3naacnt1oXLSkhKaSEpNDZ02lnxmCmf+C/x9RDdnU22dXZeLl6MTZ8LOm6dGL9Y5UZwwXA1xeuukoGWF3qLGl25eWQm2v/GNu0Oo1GQwCxBJhuY5TbbRzOaaPDJ5faoDVExXbS1OBGcHD/FLbPcz/vber77LZn0fnouG7kdSxLXcbCpIX4uvte0NdpNMLrr8Pu3VKHpNc7b4JrS36+pOC9957j+9vaJCqlUCgUg4FKmTtLhlLKnPk/H7Lt9WzMZvBqr2PvyWC66lsIo5rjjGbyh09w882DukTn9PTIf9LZs6k0aFi7ViJECoVCRNFbh97iNzt/g4+7D8cePuYwkmNJnYvx7/+3qLmzmayqLDKrMilvPnP6bJBnEOm6dDJ0GYR6h16Q16HoT3c3PP88fPKJVUQ8/LDjoL7RCC+8IKIqOBhGjoT588WkISFBmskmJICPfxdhL4TS3NXs8Dndte7MT5jP9SkSPTqbnkfnSksLHDhgFUi7d0v0yxHz5sHmzY7vW7JE3pepU61j8mRl+61QKC4cqoboAjCUBFHHG39jzwfWLun35PyYokprWs22bTBnzmCsTKFQXAg6ezopaiwiJcRBZT6wLm8d131wHfeOv5en5zxNfGC8w3k1bTW9ZgwNHQ1nfN4ovyjSw9MZGz5WmTFcRBoaYP16OeG3WH2XlMj1IpCIkm0kxdsbfvhDsQu3pd5UxKs5P4DAAggsBK96pzVIAGPDx3L9yOtZnrqcaTHTziu17kyYzdIPzyKO9uyBI0fktf34x/CrXzl+TGioOPv1JS1N6pEsImncOPBQpXAKheI8UILoAjCUBFHjb//C4VUVvds3H32G2gZr9mNurlxRVCgUlx9ms5kZb89gb9leANxc3Hh1yat8c7Lz3FOz2UxJUwmZhkyyqrJo72kf8Dk0aEgKSiJDl0FaaJoyY7gE9PRIWl1hoTi4rV5tvW/cOLjhBsePyy9ux9CVT0lnNnlth+nyzRFxdAaBNDFyIgcfPHhhX4QT2trg0CExGnX0vykvD5KTz+5Ybm4wfrxVIE2bBqmpF3S5CoXiMkWZKlxmdDbY1w0lJZiId5Gu49XVQ9dXQaFQfHnWnFrTK4YAuk3dTI6aPOBjNBoNcQFxxAXEcW3ytZyqOyVmDLU59Jh6+s03YyavPo+8+jzcXNxIDRUzhhFBI5QZw0XC1RXi4mSUl0va3JYt0NUlEX83N0m768vm9V6Ulo4hMnIM01NuJjyqmJLWE+QU59BgKhJhFHQ6guTZ0CuQpkZNvWSvzdsbZjv2hADEqc/TE7v+es7o7hZTh/374bXXRBDt2XPh1qpQKK5MlCAahnQ32l/dfeZJE8tvldsq3qdQXP6MCBpBXn0eAMtSlp1RENmiddGSGppKamgqnT2dHK85jt6gp6C+wKEZQ7epm6yqLLKqsvB282Zs+FgydBlE+0UrM4aLxO23y2hpkdS6BQvEibOsTCJIhYWSYldfD6Wl8piKCqiocOH7ExMYE53A4hGLqWmrIbc2l9zKXIpzisGjoTd6ND/iJszm/iYNTZ1NXPv+tSxJXsLy1OXn3RD2XFi8WBrHZmXBvn3WkZ195v9pUwfQdffdJxGpyZNh0iTlbqdQKJyjBNFww2iku7Wrd9OMBp9gazrLkDo/qa6Gb38bbrmFN8uvY8chb+69F+bO7Z8Lr1Aozo4lI5dwYsQJ/n707/xy+y/5+byfO527o3gHI4NH9jZ47YuHqwfjI8YzPmI8TZ1NZFVloTfoqWypdDi/rbuNfWX72Fe2j2CvYNLDxYwhxDvkgrw2hT2+vnDjjdZtSwRp7lyJIPXtExQVBf7+cluj0RDmE0aYTxiz4mZRVddOSWseBc0nKCooQv/ZfMq2izmDZQQFSX3a7tLd7C7dzU+3/JS4gDiWpyxneepyrkq4Cnet+0V5rW5uMGGCjIcekn3NzZJqZyuSiovtH+dMEJWVwV//ar8vMVGEke1QIkmhUICqITprhkwNUUsLOQ/8jorTJUTdbt4EPPejAa+SDRpvvNH7n61N480r5u/yFL8iKQn+8Q/pSaFQKM4fo8noNIWto6eDEa+OoKGjge9N+x6Pz3qcQM/AszpuVWsVmYZM9AY9jZ2NZ5wf7RdNhi6DMeFjLrjNs8I5//0vvPiimBeYzWK8sGyZNYLUY5MN+dlncPSoiIKUFDNjxmj69X8LDIRPKl9ma8O7p2uQ7D97P3c/rk2+luWpy1k6cinBXpdeTVRWSrrcvn3y849/dFx/9Mkn9mLSGRaRZIkiTZyoRJJCcbmiTBUuAENGEFVXk/Wt16ipkc0Jh9/G28OIW0iA+JP+5z9yCXEosHAhbNzYu/kTnuWX/AStVq7e6RxftFYoFBeAV/e+yqNrHu3djvCNIP+7+Xi5eZ31McxmM8WNxegNerKrs+noGbjIw0XjYmfGcLGiCQp7DAb44gvpgWQRBz091hS7/Hx45BFpmGrh7rthxAj745jNZl7Y9QLtlv52ng19apCaeudqNVpmx81meepylqUsY2TI0HLyeeopx452Z/vY5567sOtRKBSDjzJVuJxob7crrPXobMatqRGqT4eMtEOk4Lmqql/Dif8ghU7XXafEkEJxMWnvbudXO+zPBm8bc9s5iSGQtKv4wHjiA+NZMnKJ1YyhJgej2dhvvsls4lTdKU7VncLNxY1RYaPI0GWQFJR0USyeFYJOB9/4hv0+V1eIj5fh5mYvhry8+oshkM/7/nHfIr/xJCfrcsirz8NYGQiV408/sL7X4tsYWMjWoq1sLdrKD9f9kLTQNNbdvY7YgNiL9TLPia9/XXqBHzgABw9KfZKx/1fWIUlJjvfX1sKbb0o0SUWSFIrLCyWIhhvt7XZpEK597XOHShe7Tz+1a19+hHHkIt6o9947WItSKK4MXDQuPDn7SZ7b/hxVrVX4uPnw1JynvtQxXV1cSQtNIy00jY6eDo5VH0Nv0FPUUOTUjMHSA8nHzafXjCHKL0qZMVxiGhrEXODkSdlevhyeeUYMGWxNGoxGyMn0Y9u2iaSkTGTFyB40wfnkN58gtzaX1nagPQgqJsqBvGt7TRqqXJqJ8osalNfniNRUezvu9nZp/HrwoHU4E0mTnXiU7N0LTz5p3Y6Lk5qn8eOtP+Pihlgtr0KhOCuUIBpudHb2Rog0JiOuRqvBgslFy5EcHyZOGqS12fLJJ3abH/MVQIp2ly4dhPUoFFcQHq4efHfad7lvwn28uvdVXF1cCfcJdzi3qrWK3SW7WZ66/KyFiqerJxMjJzIxciKNHY29ZgyGVoPD+a3drewt28vesr2EeIWQocsgXZc+KDUoVyJLl8KSJZCTI9eqJk2SCJLFTAHEzrqsTP50d3SIeNDrXVm0KIXlM1Iwm82UNZeRU5NDTm0O1a1V0BYio3wSsXXjef1PWjuTBt/T5WS/2PoLQr1DWZa6jBj/wUk59/ISi+5p06z7bEWSJZKUnw+jRzs+xsE+bZuKi2V8+ql1X1CQCCNbkZSWJlE6hUIxdFE1RGfJUKkhMu8/wLYffSFWpGYzvy66jawifwJoxJcW7n9jGg88MGjLE5qapBlSl1WsjSWTbMbywAPitaBQKIYG31vzPV7Z+wrToqfx/ILnuTrx6vM+lqHFgN6gJ7Mqk6bOpjPOj/GPETOGsDH4uPuccb7i4lJbC+HhdsF93n8fOjtFLNlGU+ra6smtyyG3NpfChkJuGX0Lo8JG2R0vLAwiYzq5c9MMOn1PgHs7EyMnsjxlOctSlzEhYsKQixZ2dYG7k9K3FSvEnOJc8fCAf/7z7EweFArFxUPVEF1G9LR1WfsyaDTUmkIoQ0cZ8sE+HjR4a+tlzRo7MXSSZLIZA8Cddw7WohQKRV+KG4t5/cDrAOwt28uCvy3gHzf9gzvTz+8XVeer4xrfa1iYtJCixiL0Bj3Hqo85NWMobSqltKmUNafWkBycTIYug9SQVNy06nL6YHD8uEQ4amtlOz0d7rpLbnd1SYpdQcFpo4atQRxcPZ3U1OnMGtlJXHD/+tXqath5oojOzOXAcvA1cCiwkEP6f/J/gb8mJiSUZSnLWJ66nPkJ8/Fw9eh3jEuNMzEEcOutEBoq0aRjx+xd/Aais9O519GhQ/Ivc8IEGDcOIiNVyp1CMRgoQTTM6Gixb1XebrT/6z0kSoj6pMt9wg2Ahuho6Z+hUCiGBj/f8nO6bNJuY/xjuDHty1/G1mg0JAQmkBCYwNKRS8mtzSXTkEluba5TM4bc2lxya3Nx17ozKlTMGBKDEpUZwyVk9myxtd65U9LAbM0F3N1l27Jv/36oqZGxc6cH06bBtdf2P2ZOTY51o0Uno3QaaMyU+lby+qFTvB74XXzCa7k2dT7LUpZxXcp1hHqHXtwXex7cdZdVIHZ2SuPYI0fg8GH5eeSINNPti6srjBnj+JirV0s9l4XgYMjIkJGeLj/HjKGfRbpCobiwKEE0zOhq7rLbvm1WKXHXBdHQAI2NkJIyOOvqpasLVq602yWCCO64QzVkVSiGEt+b/j1q22v5NEeKIH45/5fn7ER3JlxdXBkdNprRYaNp7263mjE0Fjmc32Xs4qjhKEcNR/F19yU9PJ10XTqRvpFDLr3qcsTVVey7r7rK+RyzWWy+bfnRj+QEvrBQokgVFTJvRuwMgryCyKnJobSpFCwGHGYNNEfKKJlJq8bEx/vL+Tjw72iCf8mMMTH8YuFPvlQK58XEw0Oc5iZOtO4zmaQGyVYkHT4sUSVPT8fHOXzYfruuDrZskWFBoxFXwL5CKSlJ/U9VKC4UShANM7pa7QXRhNQ2rv/6IC3GEZs3Sw3RaSrRsQfpwKrS5RSKoUW6Lp1Pbv+EPaV7+Ovhv3J3xt1O567MXcnCpIVfKq3Jy82LSVGTmBQ1iYaOht7mr9Vt1Q7nt3S1sLt0N7tLdxPqHSpmDOHpBHkNhdzgK5eTJ6X3kQVvbzFt8PISNzsQY4biYti3L5QNv5zN2MTZLBnThqFHTBny6vPoMdpkPJhdoCkGmmIwF89ml97I6rpINFPFoCE2VsTaUMbFRfpAJSfDzTdb99tanvflyJEzH9dshlOnZPz3v9b9hw+LaYNCofjyDPE/L4q+dLfYCyI3nyHW+LBPutxnLMeElrQ09YdboRiqTI+ZzvSY6U7v31+2n+v/eT2JgYk8v+B5bh1z65dOZQv0DGRO/Bxmx83G0HrajMGQSXNXs8P5NW01bCrYxKaCTcQFxJEens6Y8DF4u3l/qXUozp2UFKkPWr1a/uR7ecmwxdNT5m3YIO52OTkA3ixcOIHv3jiBsopuChoKyKkRY4aWPp+7h4sPnq0pbN0KW7eKGIqLg8REqHHfT5d3IUtSFuPv4X+pXvZ54yzdzWyGxx+XOqIjR8QGvK3t7I6p1cKoUY7v+8MfYNUqGDtW0u3GjJG5Ftc/hULRHyWIhhnGDusVNe/WKkKOb4Q1BikeSkiQiszBwmSy9x/Fmi53552qUFShGI6YzWYeX/84AAUNBdzx8R2szVvLX1f89YIcX6PREOEbQYRvBAuTFlLYUEimIZNj1cfoNHY6fExxYzHFjcWsPrWakcEjydBlkBKSoswYLiGBgZIGfccdA8/rc42MhQvh4YehtdWNwsIUCgtTyM83kVlY2WvpbWipJDk4Ga2L1aihp0fS0fLz4cPsQo7VH0Ib9DkTRweyYloGd85YSGJQwoV+mRcVjQYeesi6bUm5y8y02J7L7VOnoK8fcGqqpO05Yts2MWpYs8Z+f3y8VSCNGSP24kooKRSCEkTDDFOHNUIUWptD0t9eh7+d3vGjH8FvfjM4CwPYt08Sx0/TjC+bkPzvM/3TVCgUQ5NVJ1extWir3b670u+6KM/lonEhKSiJpKCkXjMGvUHPybqTmMymfvNNZhM5tXIS7aH1YFSYmDEkBCYoM4YhQHOzfS0MiHU1SNTEcmIOLtTXR/Hgg1HMGDsfF+86mlscdwQxmoycqjsFRg+M1Uns3wr7t5byjPtrxMabWDAxkdtnT2PB6Im4avs73w1lbFPubC26W1vFwMEilDIz7ZvO9kWvd7y/qEjGqlX2+xMSrALpmWfAf+gH3RSKC44SRMMMW0GkNdo7zg26DY2vL9xzD+bPPkNTV8fRqKVQ68nkdPkDr1Aohh9Toqfw7Snf5i8H/0KPqYfFIxazMGnhRX9eN60bY8LHMCZ8DG3dbb1mDMWNxQ7ndxo7OVJ5hCOVR/Bz9yNdl06GLgOdj06ZMQwSfn5yIv/ppzJqa6VJqSP0evjoI7nt4RHMihVw/1Ni0FBQIKZBAIUNhXQ5ihx2+VByEt49WcO7/16Jj99nTB0bxnVTRnP7nGlEh/ldnBd5CfDxgalTZZyJtjaJKJ0LhYUy1q2D555zPOfIEelDlZY2+KcaCsXFQAmiYYbRRhD1a4Iw2H+lxo6Fv/4VTU8P7NjBbF9fqlOloZ9CoRiehPuE88elf+TRaY/y1KaneGbOM07nFjYUEuUXhbv2wtY2ert5MzlqMpOjJlPfXk9mlZgx1LTVOJzf3NXMrpJd7CrZRZh3mJgx6NIJ9Ay8oOtSnJmRI+Gxx2R0Os6ABOyzrTs7JR1s3DgZZjPU14sw2nHEk6K2yWSV5VPfXuf0eK3NWjbvrmPz7h386I+7SYkN5ropo7nv6vkkJvavebpc8PCwpttlZ1tHXp59w11HpKSAm5Os05//3Jr+GBsrwqjvUD2UFMMZJYiGGWYbQZTvOpJD6PChFV+XNja/H8/TPxzExVlwdYV58wDww/kVQYVCMXwYGTKSD2/50On9PaYelv1zGV3GLl5c9CLXjbzuokRmgryCmBs/lzlxc6hoqSDTkElmVSYtXQ4awADVbdVsLNjIxoKNxAfEk6HLYHTY6AtuL644M85qXszm/rVGltQ6kJPs4GAZkyZFo22P5kc3mGlyKWLlvix2ZZVTWFvRv9DmNCaTkRNF1fh0t+JbI8eLiBCDhsREqa0ZqCHrcEKrtU1FtNLRIcYWtiLp2DF7oTR6tPPjZmdbb5eUyFi/3n6On59joZSWpuzBFUMfJYiGGcYua1Rof8BCnuCrsmGCBSHw9CCtS6FQXNm8fehtsqqyAFj2z2UsGrGIL+744qIZHWg0GqL8oojyi+KaEddQUF+A3qDneM1xu2azthQ1FlHUWMSqk6sYGWI1Y3B1Uf8KB5u//lWiRJ98AuXlsHix43kGA3z3u2A2a9DpEli+PIGcDyCnqJ7/7NzPuoMnOZpTT2dXT7/HpoZI4Y3ZLOWuFRWwaxd0mzqod8/iuimjGZPqTUzM0Lf4Plc8Pa0RN1va20UoHTsmItERHR0inM5Ec7M07N2/37rPw8O57bglEqhQDAUus1/5yx9Tl7XLe2uPfae3gIBLvRqFQqGAxo5GfrL5J3b7dD66S+b65qJxYUTwCEYEj+B64/Xk1OagN+g5VXfKoRmD0WzkRM0JTtScwNPVk9Fho8nQZRAfEK/qjQYBjcbaDPbFF6V/kTPns88/twaCDAY4eFCiO+kjg0gfuYhf3LOI9s5u/rf3AP/dfZjtR0uoqnDD1cWDxKBEh8fMrcnjo2Nf8JdNq0kITGC0Lpmrx41kythQEhMlFexyjXB4eUlLjIHaYjQ2wvXXS5SooODMqXe2pKRI1MoRN90kXkwpKZJaaRkpKVJ3rNzvFJcSJYiGGaZuqyAyoSHA30xziwaTSWxQFQqF4lJjNBu5Me1G3jr8FiazCW83b55f8PygrMVN68bY8LGMDR9La1cr2dXZ6A16SptKHc7v6OngUMUhDlUcwt/Dn/Tw02YMvrpLvHIFiDiKj3d+f5/ODnapdRa8PNy4c+4MInpm8LvbzDS55bJNX8RIF1cKCqCy0n5+bm0uIKl1+XV55Nfl8cXxtYT76EgJSWFsVDKz02NJTnYhMRHCwq6sWhmdzvq+d3aKaUNODpw4YT+aHbQQGyhl/sQJqKmRsWtX//sjI+1FkuX2iBGXbw2YYvDQmM1Okm4VdpSWlrJgwQI2btxITEzMoK3j0Feeo6nW6i4X9+enSEx1p7VVHGAGJUp05AhUVvJRzTx2HPDk5pth5szL94qaQqFwzNHKo/xg3Q+4Kv4qfnrVTx3OMZlNg2KJXddeR6ZBzBhq22vPOF/noyNDl8HY8LEEeKrw+1Dh44/hww/FOrq5GY4ehYyM/vNMJoiKkijS+PEinH7wA7GUbmuzutfl5Zt48vPf0d49cEdUbzcfRoaMJCUkhYyYJNJGepKUJDVIQUEX57UOJyxpiH1F0tKl8L3v9Z/f0QHe3k7LvgYkIEBMNhyJUrP5yhKrinNjoHN5JYjOkqEiiA4u/znNTdaPLPm9nxATP8i9Fu6/H95+mw4Xb9aaFvIbnqAwciZvvw1Llgzu0hQKxaXFbDZjNBud1uW8sPMFthZt5eVrXyY5+NL78ZvNZsqby9Eb9GRVZdHa7aTA4TQaNMQHWs0YPF09B5yvuDR0dsKOHXD11Y5PgHfvlgtzFvz8oLq6f81KU2cTT678NZ/vzaKkSAv1idA5cCMeFxctCYEJpIakMjFyIqHBrr0GDYmJ8lyKgcnNFSE7kPOgMyZOlFRJRyxeDCdPSsrdiBGQlGT/U302VzYDncurlLnhhMmEsccqhsxocPcc5DCMyQQrVwLgaWpjBZ/xR75DRYU0e1MoFFcWGo0GV43jfy3lzeU8u+1ZWrpaWJ+/nh/O+CFPz3kaH/dL1zJAo9EQ7R9NtH80i5MXk1+fL2YM1cfpNnX3m2/GTGFDIYUNhaw6uYqUkBTSw9MZGTJSmTEMIh4esGCB8/v7ptYtXeq4gN/fw58fjHueX19jpqwzh89OfM7/Dm9lb1YV5roEaEiAbm+7x1hS6yqbK5kcNZmGBjh8WAZISp1FHCUkqPQuR6SkiNlCcbEIGNuRmyvRO6PR8WNHjnR+3OPHxQGvoKC/Cx5AaKhjoZSUBIN4rVsxBFB/zYcTRqNdMeOYY/8h6O69EOAtPYhefvnSx+4PH7ZLyG7Bh23MZfRoGDXq0i5FoVAMbX60/ke99thdxi5eP/A635/+/UsqiGxx0biQHJxMcnAyXSldnKg5QaYhk7z6PIdmDD2mHo5VH+NY9TE8XT0ZEzaGDF0GcQFxyoxhiDF6NMyeDTt3ShqVo1ojCw88ALt2aViwII0bbkhj9Vcfx+RRx+qTq/ks53NWHzlEsyFEokcNCWAUj+6RISMdpn9WV8P+k4VE+Ebg5eZJZKRVIMXFXT4W318Wrdb6vixaZH9fd7c0i7UVSZbbzuqS2ttFDA2EpWZp7177/WFhUFXl+DEVFVKjrYTt5Y0SRMOJPoIoqL4Atw3HrDteeunSr+l0dMjCOhbRhQc333zpl6JQKIYuHT0dFDcW2+375fxfEuYTNkgrssdd606GLoMMXQYtXS1kV4kZQ1mz487SHT0dHKw4yMGKgwR4BPQ2fw33Cb/EK1c44mtfk1FdDV98IREiR9TVwbZtEo1YtUrG/PmQnBzMXRl3cVfGXXTf1M2O4h18nvs5n534mLyiNqhPZM7I63A19e+R3mXs4n39+5jMJmL9Y6X26GQKYd5huLpqiImxCoGYGOcubFcybm5WE4W+OCv0KCo6/+dLSnJ+3623SnpmVJQ16td3xMYqC/HhjhJEw4k+gkjbt9eGzyBcZe0jiFZyHQA33njpl6JQKIYunq6ebL1nKx9kfsDj6x8nzCeMhyY/NNjLcoivuy/TYqYxLWYatW216A16MqsyqWuvczi/sbOR7cXb2V68nQjfiF4zBn+PgWtRFBefsDC4917n969caZ+aNWaM1J/Y4qZ1Y37ifCaHzueFhS9yqj6Xz3M/56FJaXi6WFO0CgqgrAwKGwoxmkQlFTcWUdxYxMb8Dfh7BjAyeCQpVSkk5ifipnXDzU1c9SwCKSJCGRKdCWfB2LQ0aGmB/Hzpm9T3Z2GhRJ4cMZAgsvRgKi+XsXOn4zVFRdmLpKQkuO++s39disFFCaLhhNHYe2VEYzbhYrb+FTdrNFTWeRAZdQnXYzDYd2ADVrGU+Pj+zd8UCoVCo9FwV8ZdLE9dTkVLhdManPz6fLYWbuXr478+KI50toR4hzA/cT7zEuZR1lzWa8bQ5sSVrLKlksqWStbnrScxKJH08HRGh43Gw1VdPh6KmExy4pqfL9s33OB87pNPwkcfabjuulSWLk2FLnDzk8dbTqg7OuDedz+A+j3QkAgtVvv2po5GDpYf4GD5AbQuriQEJpASkkJV00hOnZJ0d09P68l0YqLUvKhszLPHxwfS02X0xWiE0lLHYsmRUyGII2FFxZmf12wWMVxWZhVM0dHOBdG//iUuibYRJk/l1zKoKEE0nOjp6Y0QmYFv+P6byhYffGjFw9zJvSc0l1YQrV5tF7s+wCQqieSR5eoPuEKhcI6fhx9+Hs7tnr6/9vt8lvMZbx1+i9eve50MnZOzlUuIRqMhxj+GGP8YFo9YTF59HpmGTE7UnHBqxpBfn09+fT4rT64kNSSVDF0GycHJaF1UjtRQ4etfl9S6rCwxYnAmiMxm+OwzuQ74zjsy1qwRVzNbPD1hUoYvmaZSsqvXQpf36dqjRPnZHgyA0dRDXt0p8upOsRoI8Q4lJSSFSZGT6OgI4cQJOZ6vrwgji0BS/QbPH61WonHx8eJOeDZUVcn8kpJza0gLAxtL/f730pTWlvBwEUZxcfY/LbcjIlR65cVECaJhhLnHGiFC48J27VWcxHr16TuXOmNu1Sq7TUu63EDFqwqFQjEQa06t4bOczwDYVbKLiX+ZyPZ7tzMjdsYgr8yK1kVLSkgKKSEpdPZ0cqLmBHqDnvz6fMz0L3DoMfWQXZ1NdnU2Xq5ejAkXM4ZY/1hlxjAE0GicRxUsHD5sX7Dv7Q1XXeV47kPpj/GtcY9R3S3OhCtPrmRTwQY6ejqgPUCMGeqTRCR1yoWB2rYadrfVkBKSQoh3SO+xWlogM1MGiG+SrcW3r++XfPGKAUlIkFS7ri6pUSoqku2+o7y8f23TQIKosLD/vqoqGc4sxV1dRZydPOn4onNHh9QxqT8p54cSRMMIY5e9B2WH0c1u+5KWEPX09PO0XMl1BATA3LmXcB0KheKywWw28+MNP7bbNzlqMtNipg3Sis6Mh6sH4yLGMS5iHM2dzWRXixlDeXO5w/ntPe0cKD/AgfIDBHkGka5LJ0OXQah36CVeueJcOHVKetg0N8v2ggXOU5zeeQeeeALmzUtg6dKH+dOKhwm/pZ3NhZtZmbuSlSdXUtT4P0n1aAvpjR55towi1j/W4TErmiswmo2YzdHU12s4dEj2h4dbI0jx8Srt6mLh7u7c5AGkn1JJib1IclY60Nbm3NFuIHp6ZDgTPLffLqdlcXH2UaboaPsRFKREkyOUIBpG9BVEi+Jz8LtmBq2t4ucfein/nx44AA0NvZs1hHCQSdy2VNxhFAqF4lzRaDT877b/8eiaR/k893NcNC68ft3rg15HdLb4efgxPWY602OmU9NWg96gR2/Q09DR4HB+fUc924q2sa1oG1F+UaSHpzM2fOyA6YSKweHWWyX7YccOSY6YPt353FWr5AR57VoZRiN8//teLB25lKUjl/JH8x85XnOclbkrWXVqFTuKd9ATfYBlo2/l4dnaXoOGoiKJTABsL97O8epjeLv5kByczMiQkYwIGkFVlRdVVWIjbSnst6TXxcaq/8eXCg8PMePoa8jhiJ4eeOaZ/hGms0nJi3WslwERZG1tcOIEvSmXjvDyku+JrUj65S+VS57GbHZmYKiwZaDutpeK9pxi9j70jnU7NJYlH31jUNbCz38O//d/vZv/5Hbu5J/8619w222DsySFQnH58OmJTzlWfYwn5zzpdE6PqWfIN0c1m82UNpX2mjG097QPOF+DhqSgJDJ0GaSFpikzhmFGaysEB1uFDMjJaWpq/7nt7dKYVBfXyPr8dUT6RTI7bnbv/UajFOnnnupm2Z8eoaMuDEw2RSSn69pGBo8kOTiZSN9IuxRMV1eJEFgiSJGRysFuqNLTI+YNxcUibBz9rK2FO++Ef/zD8THCwqTH0rni7i7pdo6iRr/+Neze3T/KFB0tlvH+w8xIc6Bz+aH9n0RhR98IEa6D+PGtXWu3uY5FuLnBtdcO0noUCsVlxYq0FaxIc16Q+OmJT3ly45O8ft3rXJXgpJhjCKDRaIgNiCU2IJZrk6/lVN0p9AY9ObU59Jh6+s03YyavPo+8+jzcXNxIDRUzhhFBI5QZwzCgoEBOFgsKZHvECEhJcTx3yxbpjxQXF8CiRbdI/7446/1arQiaU8btdKT/BYyu0BhnNWlojqK0sYTSxhI2F2zC282HEcEjSA5OZkTQCHzwIT9fnNQ2bpR0Oos4SkoS4aZSp4YGrq5WAwVntLWJiHZEZ6fcfz5ERzv/HuzcKX28nOHrK0I7Kqr/zxUrBqcbzPmiBNEwwtRtFURaYxfe7bUSZ/X0lArPS5U8XF/fr83zOhYxbx4EBFyaJSgUiiuXlq4WHln9CCVNJcx7bx5fG/c1XrjmhSHfFFXroiU1NJXU0FQ6ezo5XnMcvUFPQX2BQzOGblM3WVVZZFVl4e3mzdjwsWToMoj2i1ZmDEOUsWPFxjknR1LnPD2dn2yuWyc/i4vhrbfkpLivax2An7sft4y+hbV5a2nS5kPwaY/wbg8xaDhdg9TWGk6mQU+mQQ9oiPKLYnzEeKZETwEkCnD8uAyQ/9cWcaQMGoY+3t4yHOHhIQYcdXX9o0sWO3DL6CucoqOdP2eZ477UvbS0iMnDyZP976upUYJIcZHo6bYmmAbXnWLMjl/BZz+UHTfdBB9/fGkWkpUlv32nL1Vka8ZQbo7mKeUup1AoLgHPbn2Wkiar5df7+vf57tTvDnlBZIuHqwfjI8YzPmI8TZ1NZFVloTfoqWypdDi/rbuNfWX72Fe2j2CvYNLDxYzB1pFMMTTQaKRJaFrawPP6JFqwaJHjeVOip7Cs4z88e1M3le47WZm7ktWnVpNdnQ1hOTIAOn2s9t71SZQ3m4nxd57i39go7nmHD8t2eLhVIMXHq5qS4YZGAyEhMiZMcDzHbJbPvaxMejKVlQ18IftMgsgZ7u4SgRxOKEE0jDD1WK8gavqmWlxKa5k5c+QyxPbtsHYtI0Ki+XAkzJ595ocqFArFl8FsNtPc2Wy37/vTv8+kqEmDtKIvj7+HPzNjZzIzdiZVrVVkGjLRG/Q0djY6nF/XXsfWoq1sLdpKtF80GboMxoSPwdddXeIfLnR0SHpUQYHc1mqd98YpKpJeSeBGUtI8li2bR9bLL1DSWMLavLWsPrWaDfkbaKIJdFkyzEB7EHdO+5zIHnke23Srtu42dpfsJjk4mRj/GLQu2l7b5z17pNYoJsYqkKKjVQ+cywGNRnpZBQbCmDFnnv/JJ1bhZPlpOzo7HT8uKmr4pWMqQTSMMPVYI0QugymILM93zTVwzTV4Ajdf2mdXKBRXKBqNhtevf527M+7mmyu/SUtXCz+f9/PBXtYFI9wnnAVJC7g68WqKG4vRG/Qcqz7m1IyhrLmMsuYy1uattTNjcNe6X+KVK84FT0+JELW3i3PdiRPOr9TbdrjIzwe9Xm7HBsRy/8T7uX/i/XQbu9lTuofVp1az5tQaDlcexsOvjQeWTcDbTSIDlZX01hR9vu8UO4q3s6N4O+6uHiQFJZEclExycDIBngGYTJJyVVwstU7u7tJXx5JeFx4+/E54FefOjAHav5nNcm28okKqNyoqrLf9hqFRphJEwwijTYSoqjsIb008Xi6deNDBnt1+OIm2KxQKxWXHrLhZHHrwEMWNxfi4O05Ub+tuI6cmhwmRTvJHhjAajYb4wHjiA+NZMnJJrxlDbm2uQzMGk9nEqbpTnKo7hZuLG6PCRpEens6I4BHDxrb8SsTLq/faolP6ptY5iiS5ad2YEz+HT/8wh2u0z/P4jDp8RhzG202KTjQaKXaPjIRZs2Cd9+9BuwXqk+iqT+JETQcnqqW4KMwnnORgEUdxAXG4urjS1QW5uTJA6o1sDRpU/fCVh22K3tixg72aL48SRMMIk9EqiLIC53CV+T047bMw1Q8liBQKxRWFm9aNEcEjnN7/f1v+jxd3v8gPpv+An8//ee/J4XDD1cWVtNA00kLT6Ojp4Fj1MfQGPUUNRU7NGCw9kHzcfHrNGKL8opQZwzBk/ny5Er9jh9h5O0ut6+mBN96wNI8Nxs1tASdOiGCxxWQ2sa5gFQRVQVAhsAm6PcWgoT6J6vpEqkuq2F2yCzetOwmBCYwIGsGI4BGEeIWg0WhoaYHMTBkgJ8UWcZSQIEJPoRhOKEE0jDDamCq0G+27rTlzHlEoFIorkcMVh3lp90uYzCZ+t/t3fHz8Yz674zPGhg/vS5merp5MjJzIxMiJNHY09poxGFoNDue3dreyt2wve8v2EuIVQoYug3RdOsFew6zi+Qrm4YdltLbCtm0wZYrjeQcPWsSQEBgoUZy+mMwmfj75dbaUrmVz5SdUtVaBWweEnZAB0OEP9Yl01ydxsr6Ok7USGvL3COD+iff3ax5cWytj/35rNMoikFSDWMVwQAmiYYRthKjLNAiCyGgEjYbNW10ICoJx41QOsUKhGHqYzCbu//x+jGZrq4IeUw/xAfGDuKoLT4BnALPiZjErbhaGFgOZVWLG0NTZ5HB+bXstmws3s7lwMzH+MWLGEDbGacqhYmjh4wNLlji/f9Mm++358x3/j3Z1ceXk5zfx4cs3MW7cn7l6YhUhMz7nqMt77C7ZLb83nk0QeVSGGWgNg/okXNonEezrS3e383WYzVJHUl4uUS2tVkSRJcUuKkoZNCiGHkoQDSdM1gjRhMACPrhnHRmPLaKj4xJ5vW/eDHfcQXvPNbzbsJij4YuYdH0k//d/AzcTUygUikuJi8aFX8z/Bd/84pu99tx/uu5P/a5qX07ofHXofHUsSFxAUWNRrxlDR0+Hw/mlTaWUNpWy5tQakoOTSQ9PJy00DTetupQ/XLn/fhEcmzbJcJZaB2ISazLB4cMaDh/W8eG19/PHm++noaOBDfkbWHNqDatPraa8uRw0gG81+FazYlwaP16moaxMzBkKCqTXjckE/8z8J2bM/dLrjEYoLJSxebO9QUNSEoSFqYurisFHYzab+ycgK/pRWlrKggUL2LhxIzExzn39LyYF/z1M0auf9m67TBzP3JduuHQLePxx+N3vejff5j4edHmbqirJH1YoFIqhRHNnM89seoaa9hr+cdM/Bns5l5weUw+5tblkGjLJrc21i5g5wl3rzqjQUWToMkgMSlRmDMMck0nss/vS0iLpdEabr0NlJeh09vPMZjOr957ix09CV8RWCnz/yV8efJB7ptxmN6+rC46fbGPSb27BWBcHLXIgf88AEUdBI0gKSsLLzXFhka+vVRwlJYG//5d51QqFcwY6l1cRomGE2WSvXTXaS3xJZcMGu821LGb6dCWGFArF0MTPw+//2bvv+BrP94Hjn3OyZA8ZEgkZSMgQm9iCqtEq2lLji9ZorapS365v61fdNap2q1qjaNGBWhEzglqJEJtIyJC95/n9cepw5CSEJMe43q/Xeb08z3M/z3OdyHiuc9/3dTP32bmUqErKbHP25llOxJ/gJd+XnriCA4ZKQxo5NKKRQyNyC3NvF2NIv6qzfUFxAScTTnIy4SQWxhb4O/rj7+SPs4XzE/e1eRroSoZAvZ6MtzecPq3ebtCgdDIE6kqH8afrE7kboD7wGmtiixn+t3Y7Y2OIM95NsdcW8EKzQGxGqifHU69y/MYxQIGLlYsmQbq19hGoE7SIiNvlxO3ttQs0VPeqIuLpJAnRY0RVfPuPumNCBJb7TsHHJ9S/Lfr0gUaNqu7mN2/CiROazRIUhBDMWz2r7pZCCFEZyurpKC4pZsQfIzgYe5C1UWtZ0GsBtSxqVXN01cPUyJRmLs1o5tKMtLw0TiWe4mT8SZJyknS2zyrI4mDsQQ7GHsTezF5djMHRH1tT22qOXFQ2Hx+IilL/Wb9Vua4s4eHa221a6578s+/qPogYBBbx4Hzs9gKxALk2kOrB9TQPrt/wYN9VC4wMjPGw8cDLzov6dvW1vq9u3lS/Dh9WD6WrXft2guTqCoby5CqqgHxbPU7uGN1on3wWx+jTcGi9eoe7e9UmRKGhWpsnCCSFmvSUhEgI8Zj69tC3HIw9CMDG6I3subqHY6OPUdfmySq+cDebGja0q9OOtm5tSchOICIhgsiESDILMnW2v5lzk12Xd7Hr8i7crNzUxRgcfR/bMuZCzd4e+vYtv83Bg9rbrVvrbjej46fMeU5FXq76wwdFzfOoRjWDGplgmgamx8HluKZAQ2GqJ+dSPTl3yZ2cwu50cu+k87oqlbpHKzZWXWHPyAjq1r2dIDk5yfwjUTkkIXqM3FllTnn3wnwmJlV785AQ7U2CcXaGwMCqva0QQlSFzPxMZuydobWvXZ121LGuo6eIqp9CoaCWRS1qWdSiq2dXrqbdLsaQX5yv85xrGde4lnGNvy/8TX27+gQ4BdCgZgMpxvCE+uknCAtT9xSFh0OrVrrbRUUpyMu9nZnUMqnHT6+tZ/vF7Wy/tJ2IhH/HwymAHHsoMQTvP8Eolwk9XqBmPly8CHFxt+tHZeZncj3zOu427pgYqp9xCgvhwgX1C9QVdu+cf2RjUzVfB/Hkk4TocXJHlbmCQu0hIGn5NbCpynvflRDtogvPPiufzAghHk+WJpYcGHmAEX+M4HDcYWxq2LCo16Kndq6MUqHEw9YDD1sPetbvybnkc0QkRHA+5bzOOVglqhLOJp/lbPJZTAxMaOigLsbgbuMuxRieIE2bql/jx5ff7u6epObNFHTz6kY3r258xVfEZ8Wz89JOtl/czm/fdCT3wKtACYqalyj2d6fTAOjUCfLz4epVdQW75aHHWHsqFIVCSW3L2njYeuBp64mrlSuGSvXja04OnDqlfgHY2d1Ojjw8ZIFYcf8kIXqM3FlUYZ3hIBIxpAZ51CAP26NejHq5nJMfRkzM7Y9jgEIM2Ud7fpLhckKIx1gjh0YcGHmA2Qdn42LpgrOls75DeiQYGRjh6+iLr6MvOYU5mmIMMekxOtvnF+dzIv4EJ+JPYGlsib+TPwFOATiZOz21CebTpkEDGD4c/vlHXayheXPt47UsajEkYAhDAoZw/ksV6qlJSlTJ9bC4Y+SliYn6Wg0awMKb8yHSEVWNFGKVUcTG12bfVSsMDYyoa10XT1tPPGw8qGVRS/N9lpKifv3zz+0FYr28bi8QK/OPRFnkW+MxcmdCtLfGM2zkdt/1t1W5DtBdvUPhtCbf0IKuXavwnkIIUQ0MlYZMbTu13DbrT6+nqKSIl/2q6lOnR5eZkRnNXZrT3KU5qbmpmsVfb+bc1Nk+syCTsGthhF0Lw8HMQV2Mwckfmxo21Ru4qFbBweoXQHY2ZS7cWlQEJ05oJ8kBAaXbFZcUs+vIDfh7nXqHohi8tsMLQylK9eRiqicXr3rARRNMjczUvUc2nnjaemoKNNy5QOy+fer5R3Xq3E6QZP6RuJMkRI8R1Z1D5kq0/+uqtCyljvlDbduCtXUV3lMIIR4BcRlxjPprFKl5qfx+9nfm95yPnamdvsPSC1tTWzrU7UD7Ou2Jz4pXF2NIjCSrIEtn+6ScJEIuhxByOYS61nUJcAqgkUOjMtejEU+G8haKz8iAfv3UPTjnz4OtrbqK3N2yC7NpbTgazWIfKgMwyAfzZPXL9QiUKCDThdxUT04feZ3TGXlgug9zm1zGdOyLpZn23OrCQvU8pYsXb8fp4XE7QZJnmqebJESPEdUdRRVczZJp6pGCsZMdeXng6FhVN1XpTIh69aii+wkhxCNCpVLx6p+vkpqXCsCaU2s4FHuIM+POaCZ5P40UCgXOls44WzrTzasbl1MvE5EQwZmbZygo1l3D+Wr6Va6mX2XL+S3Ur3m7GMOtuSDi6WBnB6v+XSM5K0s9X0hXL42ViRUBDOXO1Q/9fJWkWLpwPfO6eodSBdZx6tf2b+B6CwCygaLWgI4iiPkFxZQUG2Bqqu7JKmv+kbu7umCDeHrIb6LHyR1ltwe6hTG8mwGt3wuu2nueOaNewvpf2ZhxiFbM6Va1txVCCH2LTIwk9Ir2kgPjWox7qpOhuykVSrzsvPCy86J3cW/OJp8lIiGCCykXdBZjKFYVE30zmuib0dQwrEEjh0YEOAVQ17quzDd6ylhYgK9v2cebNIEXXoCTJ9VFFt554TkGD+5D9M1oQi6HsPPSTnZf2U16XjokNdQ6d/p0sLSEy5fV5168qO6dWhy6idTw5zEwLsTaphif+oZ066p+FL57/lGtWrcTpDp11EPuxJNLEqLHyJ0LswJlL0Ndme7qHdpLB6xqGtOkSdXfWggh9CnAKYCjo48ybOMwjscfp0PdDrzZ+k19h/XIMjIwws/RDz9HP7ILsolKiiIyIZJrGdd0ts8ryuPYjWMcu3EMKxMr/B3/LcZg4VTNkYtH0ZAh6hdAZqb6kUehUNDQoSENHRoyvuV4ikqK2Hwkkr4fW2jOM7MspFYtIxQK8PdXv1QqOBtzk493RQLPU1xgREqiEWGqU1y1C8fdxh0PGw/crN0wNjBGpYLly9X3tbZWD+3r1k1dcc/TU12soToewUT1eaoSol9//ZWFCxeSmpqKn58fH330EV5eXvoO677dWVQBQGlQDZ+m6Rgu162b/CIQQjwd/Bz9CH8tnE/3fcp/Gv8HA6WBvkN6LJgbm9Oydkta1m5JSm4KkQnqYgzJuck622fkZ3Dg2gEOXDuAk7kTAU4B+Dn6YV1DJnYIdW+PLoZKQzo3bMKaNerqdqeiijA2VpYahqdQQGRWqHou0p2cjxKnPEBc7FUOxOxHqTCgtlVt3G3cOXO2HempxpqmtWtDaqr6scjUVD3/yNNTfe3ly9XHXVzUFfLKWsBWPLqemoQoOjqar7/+mlWrVuHp6cm8efP46KOPWLFihb5Du2+qO4bMBYV9heKfOfCthbqiwqlTVVNZYfhwsLeneHsIBteuYN4nmP79K/82QgjxqDI2MOajTh+V2+bTfZ/Ss35PAmsFVktMjxM7Uzs6unekQ90OXM+8TkRCBKcST5FdmK2zfUJ2Ajsu7WDnpZ3UtbldjKGGYVVWDxKPKysreFlTALLsx9ozN89AkQUY5kLRv4U9PHdB4M9QZATpdShJ9eRaqifXrsRAenut8+8supCbq07ATp+Gs2dhzZrbx4KDYedOdPryS/XwPQcH9euFF9TD8YT+PTUJkY+PD7t27cLc3JyUlBSysrKwtbXVd1gVc2vInEqFcWEOFALZ6ep9xsZlnvZQ+vaFvn0xALh0iY/d3UF6h4QQQmPTuU28t+s9Ptr9EZ90+YS3g96WxUl1UCgU1LaqTW2r2jxT7xkupV5SF2NIOkNhSek6zSpUXEm7wpW0K2w5v4UGNRvg7+hP/Zr1pRiDqLAPO37I2J2JhF7axKajx9hzPI5rRjvUBw0LoeZF9Qsg1Q32fqQ518CwGFNT3b3DWXcVWUxOhkWLblevu3P+0Z9/woEDt9s2bqw7IYqOho4d1UmYtbV62N+yZbrf15496rlSZmbqV2AguLqWbqdSQVKSOpZbL5NKmA6ZlgYFBepXYaH6/Rg8hh3pT9VvFHNzc8LCwhg5ciSWlpaPVe8Q3B4yp1AVax8wNq6eMWyenlV/DyGEeIzczLnJa3++BkBhSSHv7HyH6JvRLHu+jKcXAaiLMdSzq0c9u3oUNCgg+mY0kQmRXEy9qLMYQ1FJEaeTTnM66TQ1DGvg6+BLgFMAdazrSDEGcd8czR152f9FXvZ/EYbD9czr7L6ym9DLoYReCeVi6r8JkUUi/KczpNWF9Lq84j2e555z4NIldZGG3Fx1s+uZ1zlw4SZwezElCwt1Lar4eHXyY2CgThI8PbVqVAHqXiJdUlMhMVH9gvIf8ZYtg59/1t4eMaJ0u6ws9dpLt5ibl07mbvniC/jgg9vb77wD//d/utv6+EBCwu3tGzfUBSkeN09MQrRz507GjRtXav/48eOZMGGCZrt58+ZERESwfPlyxo4dy/bt2zGuqt6VyvbvOkTKEu2EKFdlQlZS2T9YQgghqsavUb+SkH37acBAYcCopqP0GNHjx9jAmACnAAKcAsgqyCIqMYqIhAjiMuN0ts8ryuPojaMcvXEUaxNr/J3UxRgczatq/QnxpHKxdOEV/1d4xf8VAK6lXyP0ijo52mW/i5j03dQ0rcnyqf9DqYBmzdSPYvHx6sRo7t9HSTWLA894KLBAUWDN+cIkjK8aUNemLrUtawMGXL6s7sW5fl37/mU9fqana2+Xt0ZSTo72dlnlwouKtLfLq5pXXKy9uG5xcdlt775OWYvyPuqemIQoODiYqKioUvuVd6XVt5KfUaNG8cMPP3Du3Dn8/PyqJcaHpUL9KVixgTEextfIKjChBnkYFRayO1fPwQkhxFPo9Rav42ThxKi/RpGSm8KHHT+kjVsbfYf12LIwtqCVaytaubYiOSeZyER1MYaU3BSd7dPz09kfs5/9MfupZVFLU4zBysSqmiMXTwI3azeGNR7GsMbDUKlUXE67zOXUy1pDYJVKdfEEFxeYcXkeGIRCuhukeqFK9SQhqxYJl9XPawZKQ9ys3KhrUxd3G3eeedaV/FxDcnLU6yCtWKHutfH0VA+xq1tXPYwtLU07rookRGUtjHt3olJZZcTvTuoKdC9F9sh7YhIihUKBoWHZb2fnzp389ddfzJ07F4CSkhIKCwuxsnp8fmnmOtQBwkChIFVlSzq3v+urop6CSqV7wTQhhBC39WvYj1a1WzE7fDbvtn9X3+E8MWqa1aSTeyc61u1IXGacphhDTmGOzvbxWfHEZ8Wz4+IOPGw98Hf0p6FDQynGIB6IQqHA09YTT1vd0wXyi/LZH7MfDIrA7rL6BVBgCmkekOpJcYoXV9KKuJJ2mT2AUmlAbcva1K1Tly5uQSiVpiQlqef2HDqkTrbc3NSvI0fUvT0ZGWVX2QPo2hXs7dUJVk6OOlHTpaRE3a6wUP2qrIVn7e3V5cmNjNTJ0eP63KhQ3Vm67Al248YNevfuzbx582jRogXfffcdhw4d4pdffrmv8cexsbEEBwcTEhKCq67ZatVgV4iK8+uOY512leEbnyO/8PastfR0daWVyrvZLv7+PZ8Pt7Wl7bNWdOumnuBnYXHvU4UQQmjLL8pn2fFljGo2SgoCPITikmJNMYbom9E6izHcyVBpiHdNbwKcAqhnV0/KpotKo1KpOHPzDKGXQ9lzdQ97r+7VGj6rkWsDKV6Q6ql+FdVAoVAyvd10jA3Kn7JRowa4u9/uQbKzq56Eo6hIM0sDUCdq5fQ5PDbKe5Z/ahIigL179/L555+TlJRE8+bN+fjjj3F0vL8xx49CQrRzJ+zfr/53dDR4e6t/UPLyYNSoSv5m7dYNdu6kGCVHacYE5vH8zFa8Kx9+CiFEhU3dPpWvD35NW7e2rOy3Encbd32H9NjLL8on+mY0EQkRXEq9hIryH2dMDU3xdVQXY3CzcpNiDKJSqVQqziWf0yRHe67uITYjVrtRiQIyXWho0Ie3Gy4kNlY78QDYfWU3eUV5uNu4U8e6DmZGt7tyrK1vV6/z8Ch7eJzQ7bFJiIqKihgyZAgNGjRgxowZWseOHz/OrFmziIqKwsjIiA4dOjBt2jQcqqmSwKOQEO3YoV2usWtXaNeuCm5UUIDK1hbFHQNT63OOX47Up3nzKrifEEI8wXZe2km3Fd0021YmVmwatIn2dduXc5aoiKyCLE4lniIiIYLrmdfv2d62hi3+Tv74O/rjYC4ViUTluzUH6VZytOfKHi6nqYfVTQuaxhfdviAvD65cURdouHhRXbJ7Tvgc0vPSNNdxMHfE3cadutZ1qWNdB0uT2+PnnJ3VydHd5b2FbuU9yz8yHWA5OTlMnTqV48eP06BBA61jZ86cYcSIEfj7+/PFF1+QnJzMnDlzOH36NBs3bnx8qsQ9pLtT1yr7cOuff7SSoes4k2pXjyZNquh+QgjxhFKpVEzbMU1rn00NG/yd/PUU0ZPJwtiC1q6tae3amps5N4lIiCAyIZLUvFSd7VPzUtl7dS97r+7F2cJZU4zhzodNIR7GnXOQhgcOB9RV7PZe3Yufo7qYV40a6rLVPj7qcyKvxvBx/E+3h9gVmpGUnUhSdiJH4g4DYFPDljo2dahjVYc62XW4ft2eAwcUGBreLu/t5aUufS2doPfvkUiI9uzZw2effUZqqu5fXHPnzsXa2prvv/8ek39XkWrUqBEvvvgi69evZ9CgQdUZrt7cSojMspPwuBKKfQ0jSDNW19tu2bLybrRnj9bmbjrRtZvisVxoSwgh9EmhUPD34L8Z8ccI/r7wNwoUrHhhBTY1bPQd2hPL3syeLh5d6OzemdiMWCISIohKiiqzGMONrBvcyLrB9ovb8bT1JMApAB97H0wMK2HVSiHu4GbtxuCAwWUeP562G5xPqF8qIKvW7blHaXWhxJC0vFTS4lOJiD8JgKmRGXWs6+Bj70NRUSCXLqmnWJiZqYfV3RpiZ2NTDW/wMab3hCgjI4MxY8bwzDPP8O6779KhQwet4wUFBYSFhTFgwABNMgQQEBCAu7s7ISEhT01CdGt0oGNSFC/+9jL89u+BDh1KJTEPZfdurc09dKRr18q7vBBCPE2cLJzY/Mpm5h+ZT0puCh3qdrj3SeKhKRQK3KzdcLN2o0e9HlxIuUBEQgRnk89SVFJUqr0KFRdTL3Ix9SJGSiO87dXFGLxsvaQYg6gWfRr0YcNLG9RD7K7u4WT8SVSW8VAnDIoNIL3OvwmSlzpZUinILczh7M1oTA1NCawVqLlWTg5ERalfoC7I4OWlfrm7V0114seZ3hOiGjVqsHnzZry8vHQev3btGvn5+TqPe3h4EB0dXdUhPjL8/SEmBlSJd1XVqcwhg4WFqA4c4M5e1t10Ynpw5d1CCCGeNgqFgvEtx5fbJjYjFlNDU2qa1aymqJ4eBkoDvO298bb3Jr8onzM3zxCREMHl1Ms6izEUlhRyKvEUpxJPYWZkhp+jH/6O/rhauUoxBlFlbE1teaHhC7zQ8AUA0vLS2B+znz1X9nDg2gH+uf4PhXaXgRB1ee9bvUepntSxrqPzmvlF+fx08ifcrNyoE12HOtZ1sDa1pHbt271HtWvz1I8C0ntCZGxsXGYyBJCZmQmAhY56z+bm5prjTwMjI+jbF4qVBbDw9n6VsTGV9uv56FEU2dmazXicKKjbAA+PyrqBEEKIuxUWF/LSry9xLeMaq/utloILVcjE0ITAWoEE1gokIz+DU4mniEyI5EbWDZ3tcwpzOBx3mMNxh7EztcPf0Z8ApwBJXEWVs6lhQ+8GvendoDeg/l48EndEvRjxtf2EXQsjwykKVPDe4Ako0tQFGi5fVlcgBriWcY0bmde5kXmdw3GHALA1taOOdR3qHFYnSC42NfHwUODlBfXqqXuTnjZ6T4jupeTfeoRlfSKjVCp17n+SXc514hgvYkwBxhRw5XBz3qisi+uYP9QlWD4NE0KIqvS/3f/jYOxBADr91ImPO33Mu+3fRal4+v7GVScrEyuC3IIIcgsiMTuRyIRIIhMjSbujytedUnJTNMOZalvWJsApAF9HXyyMZZE+UfXMjMzo6N6Rju4dAfW6XJGJkYTHhtOinicKBbRooS7lff26Ojk6uikCFCWguv27JDU3hdTcFE7Gn/j3uubUOaZOjtys3WjoVgvv+oZ4eannIT0Nw+se+YTI2toaQGdPUHZ2NpblLd/7hMqo35yXWafZDnSl8hIiHfOHgmW4nBBCVJkDMQf4fP/nmu0SVQnhseEoKq/vX9wHR3NHgj2D6eLRhZj0GCISIjiddJrcolyd7eMy44jLjGPbxW1axRjutdimEJXFQGmg6e28k1IJrq7q14eXvgWTMEhzV889SvWEbO1S8zmF2UTfPEP0zTPq654wxMXSha6eXXG3rYOr6+35Ry4u6us/aR75hMjNzQ0jIyOuXLlS6tjly5epV69e9QelZwUF2tuVNoWoqAjV/v2l5g992LmSri+EEKKUFrVb8HbQ23wV9hUALpYuLO+7XOaq6IlCoaCuTV3q2tTl2frPaooxnEs+p7MYQ4mqhAspF7iQcgEjpRENHRri7+iPl52X9PAJvfv1xV8JuxbG/pj97IvZx9EbSyjKMb1dnCHVEwq0V3gtLiniWnoMBgoDSkrU89djYiA0FExNwbVuAd71DalfT8m//RaPvQonRDdu3MDZ2bkqYtHJ2NiYoKAgQkJCmDJlCjX+7beLiIjgypUrDBs2rNpieVSoVOryiQUF6lelJUTHjqHIytJsJuCIwseHavzvFkKIp46xgTFfdvuSLh5dGPHHCFb3W429mb2+wxKAodIQH3sffOx9yCvK40ySuhjDlbQrZRZjiEiIICIhAnMjc/wc/QhwCsDF0kUSXKEXDuYOPO/zPM/7PA/cnhO37+o+9l/bz4GYRWSnmKuToxQvSK8LJQYYKA2pZVGr1PVyc+GnHf+w9/u9uFq54uNWk6AAZ3q0rIdvAwtMHtNq9RVOiIKDg2nWrBnPPfcczzzzDFZWVlURl5YJEyYwaNAghg8fzogRI0hPT2fWrFnUq1ePAQMGVPn9HzVt2sCdSzbdvWDrA7tr/tAeOsr8ISGEqCY96vXg0sRLmBqZltmmRFUivQ56UsOwBk2cm9DEuQkZ+RlEJkQSkRBBQnaCzvbZhdkcijvEobhD1DStib+TuhiDnelTOGNdPDLMjMzo5N6JTu6dACgqKSIiIUJdqCFmPwcur+R6rCE+hs9Ty8mApKTS17iWfo38ojwuplzgYsoFNp+E91YqqGXpSCMvS4ICatGzpTctfVwxMHg8niMVKlXFHqd9fHw0n3IYGhrSoUMH+vTpQ5cuXTCuhK4Kb29vXn75ZWbMmKG1/9ChQ8yePZvTp09jbm5O+/btmTp1Kg4ODmVcqXLFxsYSHBxMSEgIrq6u1XLPaterF2zZotkcx3cErx9Hv356jEkIIQQAcRlxdF/ZnTnPzKGbVzd9hyP+lZCVQGSiOjnKyM+4Z3tXK1d1MQYHX8yNze/ZXojqpFKpuJZxjdTcVBrXakx6Oly8qH5dugQ5OSq+OfgN2QVZ97yWpbkxjRqY0r25J++/+HylrhLzIMp7lq9wQjR69GjCw8Mp+Hciy63kyMLCgu7du9O7d2/atGlTSaE/Op74hEilQuXqiuL6dc0uP06x56YvNaWyqBBC6FVhcSGdf+rMgWsHUKDgw44f8kGHD2TB0EeISqXiavpVTTGGvKK8ctsrFUrq2dXD39EfH3sfjAyMqilSIR5MSQmcPJ9Eh1kjyEpwgHQ3rep1ZfFxaMiYti8zdqx+K9ZVakIE6upuu3fvZvv27ezdu5fcXHUFllvJkYODA7169aJ37974+vpWwlvQv0cqIVq5EpYvV08eMjKCl16CwYMf/rqFhXD0KOzeTcnRY0R9sBb/gMejq1MIIZ5k03ZM0xRduOWnvj8xrPHTN4/2cVBUUsT55POaYgzFquJy2xsbGNPQviEBTgF42HrIsEjxSCtRlXD25ln2XAxn57HzhEfeJO6qMeTonvvYzas7QW5BDBgAfn7VHOwdynuWf6Aqc+bm5vTq1YtevXpRUFDAvn372LFjB6GhoaSnp5OYmMjy5ctZvnw53t7eTJ48mY4dO1bKmxHAhQsQEnJ7OzCwcq5rZAStW0Pr1igB/8q5qhBCiIdkZWKFAoVmIv8zXs8wJGCInqMSZTFUGtLQoSENHRqSW5jL6aTTRCREcDX9qs72BcUFnEw4ycmEk1gYW2iKMThbOEsxBvHIUSqUmu/vsa3V+27m3CTkzD9sO3KBQ5FJnLtYRFGeutezjnUdlEqoVbpGwyPjoctuFxYWkpubS3Z2Nvn5+Zof3FsdT9HR0bz++ut8++23dO3a9WFvJ6AK624LIYR4FL3f4X3auLbhlQ2vYKQ0YmW/ldKL8JgwNTKlmUszmrk0Iy0vjVOJp4hIiCAxO1Fn+6yCLMJjwwmPDcfezJ4ApwD8Hf2xNbWt5siFuH/2Zva83KwHLzdTbxcUFRIScZq/D5+jex1H/BqB/SNcPPOBEqKMjAx27drF1q1bCQsLo7CwELidBNWtW5cePXpw4sQJDh06RElJCUuXLpWEqJLExxZyZ5J9Oc4YD71FI4QQojoEewZzYswJ4rPipSz3Y8qmhg3t6rSjrVtbErITiEiIIDIhksyC0ovPg/pT912Xd7Hr8i7crNzUxRgcfTEzMqvmyIWoGGNDI55t2phnmzbWdyj3pcIJ0WuvvUZ4eDjFxerxsLeSICsrK3r06EHfvn1p2rSppv3HH3/ML7/8woULFyopZLHdfQwr6I4RhRhTQLPMRnyg76CEEEJUOWdLZ5wty14cLvRyKBbGFrSo3aIaoxIVpVAoqGVRi1oWtejq2ZWrabeLMeQX5+s851rGNa5lXOPvC39T364+AU4BNKjZQIoxCFEJKpwQ7d+///bJhoa0a9eO559/nuDgYJ1ltzt27Mgvv/yCoeFDj84T/0qyrsdO6mm26zk9/DU3bVJ3ZTZvDvJfJYQQj5/YjFhe+u0lMvIzmPfsPEY1HSXzTx4DSoUSD1sPPGw96Fm/J+eSzxGREMGFlAs6izGUqEo4m3yWs8lnMTEwoaGDuhiDu427DKMU4gE90KNvw4YNef7553nuueewsyt/gTELCwteeuklvL29HyhAUVplTyFSTXoTg6XXWJPbnnfM2mHdMZAFSwzRdzE9IYQQ96ewuJCXf3uZmzk3ARizaQyH4w6ztM9SSYoeI0YGRvg6+uLr6EtOYY6mGENMeozO9vnF+ZyIP8GJ+BNYGlvi7+SPv6M/tSxqyf+7EBVQ4YTozz//pEGDBuW2SUpKoqSkBCcnJ5o3b07z5s0fOEBRWuvW8M476sSooEC9/TCKftvIs7kxPMsGyIHgnXuxt29fOcEKIYSoclvObyHsWpjWvtqWteWh+DFmZmRGc5fmNHdpTmpuqmbx11tJ790yCzIJuxZG2LUwHMwc1MUYnPyxqWFTvYEL8Riq8DpEPj4+KJVK5s2bR3BwcKnjP/30E59//jl+fn78+uuvlRaovj1S6xBVppgYqFtXs5mPMc91SGfbHj2unCWEEKLC1p5ay6t/vkp2YTbP1nuWTa9skiFUTxiVSkV8Vry6GENiJFkFWfc8p651XQKcAmjk0AhTI9NqiFKIR9NDrUN07do1srOztfapVCpiY2OJjo4u1T4mJgaVSiVFFB4Xd8wJAzhCC4K6SDIkhBCPm5f9XsbP0Y+3d7wtZbmfUAqFQlNYo5tXNy6nXiYyMZLTSacpKC7Qec7V9KtcTb/KlvNbqF/zdjEGQ6VMGBbilnv+NBw6dIgPPrhdw+xW9/vnn39e5jkKhQJLS8tKCE/oFBurHitnbKx+2dg8+ESiuxKifbRH1tAVQojHk6+jL38P/rvcNkUlRfIw/ARQKpR42XnhZedFr/q9OJt8VlOMoURVUqp9saqY6JvRRN+MpoZhDRo5NMLf0R93G3cZWimeevf8+Kh///40atQIlUrFnaPrbm2X9ercuXOVBv5UGzkSvLzAzQ2cnGD37ge+VMGufVrbhwzb0arVQ8YnhBDikRRyKQT/hf6cSjyl71BEJTIyMMLP0Y9X/F9hSpsp9KzfEzcrtzLb5xXlcezGMX46+ROzw2ez4+IOErISqjFiIR4t9/yISKFQMGPGDFauXAnAxo0bUSgUtGrVChcXl1JtTU1NNVXoRBWprDJzqakYn739R7EEBQUt2mIqQ4yFEOKJE5Mew8D1A7mZc5NW37fi+z7fM8h/kL7DEpXM3NiclrVb0rJ2S1JyU4hMUBdjSM5N1tk+Iz+DA9cOcODaAZzMnQhwCsDP0Q/rGtbVHLkQ+nNffea+vr589tlngHoIHcCYMWMICgqqushE2SorITpwQGszEn+adrF5sGsJIYR4ZOUV5TFg3QBNhbKcwhwGbxhME+cm+Nj76Dk6UVXsTO3o6N6RDnU7cD3zOpGJkUQmRJJdmK2zfUJ2Ajsu7WDnpZ3UtbldjKGGocwtFk+2Cg8i3rVrV1XEISrgeHwt7A3dMaYAI1UBZ0+Y0uZBctO75g/tp53MHxJCiCdUY6fGHLl+RLM9o/MMSYaeEgqFgtpWtaltVZvuXt25lHqJiIQIom9G6yzGoELFlbQrXEm7wuZzm2lQswEBTgHUr1lf5p+JJ9I9v6t//vlnADp37oybm5tm+34MGzbswSMTZfo4YAN/XL69vcH5wa5TELKPO/uWwpTtGS6dfkII8cSpYViDpc8tpZVrK8ZtGcczXs/wbvt39R2W0AOlQkk9u3rUs6tHQXEBZ2+qizFcTL1YZjGGMzfPcObmGWoY1sDXwZcApwDqWNeRYgziiXHPhOjTTz9Vf7JQuzZubm6a7fshCVHVqJQRc7m5GBw/orUrs3E7zM0fPC4hhBCPtteavkaTWk3wsvOSstwCYwNj/J388XfyJ7sgm1OJp4hIiCAuM05n+7yiPI7eOMrRG0exNrHG38mfAKcAHM0dqzlyISrXA/V73s9arvKpQdWplIToyBEMigs1m5dxp1H3J2jBWSGEEDo1c2lW7vG9V/fS1q0tBkqDaopIPArMjc1p5dqKVq6tSM5JJjJRXYwhJTdFZ/v0/HT2x+xnf8x+alnU0hRjsDKxqubIhXh490yIbhVT8PX11doW+rNqFWRlqROjggLw8HiAi+zTLrct84eEEEJsv7idZ1c9SzfPbvzS/xdsTW31HZLQg5pmNenk3omOdTsSlxlHREIEUYlRZRZjiM+KJz4rnh0Xd+Bu406AUwANHRpKMQbx2FCo7qe7RxAbG0twcDAhISG4uj7+PSkFXZ/FOGSrZnusYjFfpo3GSj7YEUKIp9LFlIu0WNqC1LxUAOrZ1eOPgX/QyKGRniMTj4LikmKtYgyFJYXltjdUGuJd05sApwDq2dWTHkehd+U9y0upkKeUcVN/iuOuYRAdBUDN59tJMiSEEE+xN7a8oUmGQJ0gxaTHSEIkADBQGlC/Zn3q16xPflE+0TejiUiI4FLqJVSU/my9qKSIqKQoopKiMDU0xddRXYzBzcpNplWIR849e4heeOGFB7uwQsGGDRse6NxH0SPVQzR9OigUYGSknkD0zjvqfz+IlBQIC4OePUEpE2yFEOJpdSPzBv3W9SM8NhyAz4M/55127+g5KvGoyyrI0hRjuJ55/Z7tbWrYEOAUgL+jPw7mDtUQoRBq5T3L3zMh8vHxqXAmr1KpUCgUnDlzpuLRPqIeqYTIyAiKim5v5+c/+OKsQgghxL/yi/IZt2Uc2YXZrO63Wj7JFxVyM+cmEQkRRCZEavU2lsXZwllTjMHSxLIaIhRPs4ceMifTjB4hJSXayRA8eO+QEEIIcQcTQxOW9llKUUmRJEOiwuzN7Oni0YXO7p2JzYhVF2NIiiKnMEdn+xtZN7iRdYPtF7fjaetJgFMAPvY+mBiaVHPk4ml3z4QoOjq6OuIQ96tQexJjiaERxUUKyYmEEEJUCoVCgZFB2X9UFh5ZiI+9D509OldjVOJxolAocLN2w83ajR71enAh5QKRiZFE34ymqKSoVHsVKi6mXuRi6kWMlEZ426uLMXjZekkxBlEtpKjC40ap5EOzrynMKcCYAhRFKsang729vgMTQgjxpNt8bjPjtozDQGnA/J7zGd1stL5DEo84A6UB3vbeeNt7k1+Uz5mbZ4hIiOBy6mWdxRgKSwo5lXiKU4mnMDMyw8/RD39Hf1ytXKXXUlSZ++4hcnV1xcLCokI9Rj4+Pg8emdDNyIg5BlPIvGPXWxWZPhQfz39Gm2DsZEvHjtChA9SpU9lBCiGEeNJEJUYxaP0gVKgoKilizKYxXE27yszgmfoOTTwmTAxNCKwVSGCtQDLyMziVeIrIhEhuZN3Q2T6nMIfDcYc5HHcYO1M7/B39CXAKoKZZzWqOXDzp7pkQ9e3bF6VSybx58wgODqZv3773laErFApOnz5dKUEKbQUF2tsVqaeQ/79P+emveZymIQe/b8MAxvD3zZbUlN8tQgghyrH36l4yC25/HKdUKOnoLit6iwdjZWJFkFsQQW5BJGUnqYsxJEaSlpems31Kbgp7ru5hz9U91Lasjb+TP36OflgYW1Rv4OKJ9EBFFaTIgn516aIuLFdQoH5VZP5Q7q4wTIBGnKERZzjh9pwkQ0IIIe7p9Rav42ThxNCNQ8kpzGH2M7Pp7tVd32GJJ4CDuQPBnsF08ehCTHoMkYmRRCVGkVuUq7N9XGYccZlxpYoxGBtIxV3xYO6rh0ihUODi4gI8+LpEovJs2fKAJ+bkYHnppNYus+A2Dx+QEEKIp0K/hv3wtPVk/en1TGg5Qd/hiCeMQqGgrk1d6trU1RRjiEiI4FzyOZ3FGEpUJVxIucCFlAsYKY1o6NAQf0d/vOy8UCpkbUVx/+6ZEH3++eda25999lmVBSOq2D//YHDHL5SLeNK0h6MeAxJCCPG4uTUHpCy31iIU4mEYKg3xsffBx96HvKI8ziSpizFcSbtSZjGGiIQIIhIiMDcyx8/RjwCnAFwsXeT7UdzTQ1WZy8vL48qVK+Tm5mJpaYm7uzuGhlK47lFVuPcgd46uO0gbOrfTWzhCCCGeQJ/u+5TrmdeZ02NOueW7hbhfNQxr0MS5CU2cm5CRn0FkQiQRCREkZCfobJ9dmM2huEMcijtETdOa+DupizHYmdpVc+TicfFA2cu1a9f44osv2L17N8XFxZr9JiYmPPvss0yZMgV7qQNdNaKioE0b9cQhIyPw84OdO+/r1IytYdw5XeisXRBDaldNmEIIIZ4+G85s4P3Q9wE4m3yWX1/8FVtTWz1HJZ4kViZWtK3TlrZ12pKQlUBkojo5ysjP0Nk+OTeZ3Vd2s/vKblytXAlwCsDXwRdzY/Nqjlw8yiqcEMXExPDyyy+TlpZWqrhCXl4ev//+O+Hh4axZswYnJ6dKC1T8q6AAMu8oul2r1v2dp1JR48RBrV0lrWT+kBBCiMpx/MZxhm4cqtkOuRxCr9W9ODDygAxZElXCycIJJwsngj2CuZp+lYiECE4nnSavKE9n+9iMWGIzYtl6YStetl6aYgzSkykqnBB9/fXXpKamAmBpaUmbNm2ws7MjLS2NgwcPkp6eTnx8PF999RVff/11pQf81Css1N6+3xJzly5hnp2k2czCnDq9/CsxMCGEEE8zQ6Uh9mb2xKTHaLY/Df5UkiFR5RQKBe427rjbuNOzfk/OJ5/XFGMoVhWXal+iKuF8ynnOp5zH2MCYhvYNCXAKwMPWQ4oxPKUqnBCFh4ejUCho2LAhP/30E5aWlppj2dnZjBw5kpMnT7J3795KDVSoJScUaQ17uxpnSN37OK/kwEHu/BE/TEuCOsh8LyGEEJXD38mfI6OO8MLaFwi7FsaCngvo5N5J32GJp4yh0pCGDg1p6NCQ3MJcTiedJiIhgqvpV3W2Lygu4GTCSU4mnMTC2EJTjMHZwlmS+adIhZ+Ib80ZGjx4sFYyBGBubs6gQYM4efKkrlNFJbjp1QpP0jCiEEOKcK+hJPw+zkvbEsadUwmPmbThLd+qilIIIcTTyNHckV3DdrHhzAYG+Q/SdzjiKWdqZEozl2Y0c2lGWl4apxJPEZEQQWJ2os72WQVZhMeGEx4bjr2ZPQFOAfg7+ss8uKdAhROijh078vfff3Pz5k2dx2NjYwHo3Lnzw0UmdCosMSADa812TdP7O68kTHv+UJZfG5TSKyyEEKKSmRialJsMqVQqCooLMDE0qcaoxNPOpoYN7eq0o61bWxKyE4hIiCAyIZLMgkyd7W/m3GTX5V3surwLNys3dTEGR1/MjMyqOXJRHSqcEE2fPp2IiAgWL16Mm5sbXbt2xdjYmKysLP766y+WLl2Km5sb77zzTlXE+9Qrumtdsvuqcp6VhV1shNYu62daV15QQgghxH36eM/HbL+4nd8H/o6juayFJ6qXQqGglkUtalnUoqtnV66m3S7GkF+cr/OcaxnXuJZxjb8v/E19u/oEOAXQoGYDKcbwBLnn43TLli1L7SsoKCAvL48pU6YAUKNGDfLyblf0sLS0ZOrUqfzwww+VGKoAcHeHX39V11YoLARr63ueAkeOoFSVaDbP0oCm3aUsuhBCiOq1MmIlH+/5GIDW37dm0yubaOTQSM9RiaeVUqHEw9YDD1sPetbvybnkc0QmRnI++XyZxRjOJp/lbPJZTAxMaOigLsbgbuMuxRgec/dMiDIyMlAoFFolthUKhda+3NxcrWOnT5+WiWhVxMYGBgyo2DlFe8O0/qMPKdowoEVlRiWEEEKU7+C1g7z656ua7ctpl3nul+eIHh+NoVKK/Aj9MjIwwtfRF19HX3IKczTFGG5VTbxbfnE+J+JPcCL+BJbGlvg7+ePv6E8ti1ryDPwYuudvIBcXl+qIQ1Qhw5f6gbU5qrAwivcdJLBfG8xkCKwQQohq1NChIR3qdmDnJfVi4sYGxizvu1ySIfHIMTMyo7lLc5q7NCctL43IhEhOJpzkZo7u+fOZBZmEXQsj7FoYDmYO6mIMTv7Y1LCp3sDFA1Oo7l5dVegUGxtLcHAwISEhuLq66i+QK1fg0CH1+kOGhlCnDgQGVuwaxcVgYFAV0QkhhBBlKiwuZNyWcSw9tpRV/Vbxiv8r+g5JiPuiUqmIz4pXF2NIjCSrIOue59S1rou/kz++Dr6YGt1nFSxRZcp7lq+yj2VKSkpQShmzyrdnDwwffnt76FD4+eeKXUOSISGEEHpgZGDE4t6LGR44nCC3IH2HI8R9UygUOFs642zpTDevblxJu6IpxlBQXKDznKvpV7mafpW/z/9N/Zq3izFIr+ij54H+R1JSUti7dy+JiYkUFhZqzS8qLCwkPj6effv2ERYWVmmBin/dXWbOSCqcCCGEeHwoFIpykyGVSkVqXip2pnZlthFCn5QKJZ62nnjaetKrfi/OJp8lIiGCCykXKLmjiNUtxapiom9GE30zmhqGNWjk0Ah/R3/cbdxlvtEjosIJ0eXLl3nllVdIS0srs41KpZL/4CpSmFvInSlQidIQ6YcTQgjxpHh/1/usPrWaLa9soaFDQ32HI0S5jAyM8HP0w8/Rj5zCHKISo4hIiOBaxjWd7fOK8jh24xjHbhzDysQKf0d/ApwCcLJwqubIxZ0qnBB99913pKamln9RQ0Nat5Z1bqpC2A1P4nkJIwoxopD4o40ZVU7769ehZk0wkfXvhBBCPOKWHF3Cp/s/BSBoWRB/DPyDDnU76DkqIe6PmZEZLWq3oEXtFqTkphCZEElEQgTJuck622fkZ3Dg2gEOXDuAo7mjuhiDoz/WNe5nTRVRmSrcuXDkyBEUCgXt27dn8+bN2Nra0qJFC3bs2MGcOXOwsLCgpKSEESNGVEW8T7043+4MZC392cBz/MUu7zfKbpyZyebOX9PD8gDBbfP4738hRnf1SCGEEEKvwmPDeWPz7b9paXlpDN4wmPwi3YtlCvEoszO1o6N7R8a3HM/oZqNp7doacyPzMtsnZiey89JOZofPZtnxZRyKPURmfmY1Rvx0q3AP0a2hcs8//zxeXl60bt2asLAw3NzccHNz4+LFi8ybN4/vv/+etm3bVna8T73CQu3t8qYQFYcdYtS5qYwCCsKM+DXsRXKHr6rS+IQQQogH0dylOSMCR/D98e8BMDU05bcXf8PEUIY4iMeXQqHAxdIFF0sXunt151LqJSISIoi+GV1mMYaY9Bhi0mPYemErdazr4OvoSyOHRlgYW1Rz9E+PCidEpqamFBYWUvjvk3lgYCBbt27lwoUL1KtXDzc3NwDOnz9fuZEKQF1p29ZWnRgVFpY/FC7hj4PcWkXKmEJUJqY0aFAtYQohhBAVYqg0ZEmfJXjYevBh6If80v8XWrm20ndYQlQapUJJPbt61LOrR0FxAWdvqosxXEy9qLMYgwqVVqU6dxt3fB19aWjfEHPjsnubRMVVOCFydXXl9OnTLFmyBD8/P5o0aYJKpWLu3LkMGzaM5cuXA5CXl1fZsQpg8GD1634U7DmotZ3WsA1S60IIIcSjSqFQ8G77d3mx0YvUr1lf3+EIUWWMDYzxd/LH38mf7IJsTiWeIjIxktiMWJ3tVai4nHaZy2mX2XJ+C/Xs6hFYK1DKeFeSCn8F+/fvT1RUFFeuXGH//v0MGzYMe3t7du7cyc6d6tWnFQoFDRtKZRi9KinB/rx2QmQW3EZPwQghhBD3r7xkqLikmLS8NGqa1azGiISoOubG5rRybUUr11ak5aVxOuk0pxJPcT3zus72JaoSziWf41zyOUwNTfF38iewViDOFs5S5fkBVbiowiuvvMKIESMwMDDAx8cHAwMDJk2ahEql0ryMjIx48803qyBccb9U0WexKEzTbKdiQ8MXfPQXkBBCCFEJpmyfQoulLTh786y+QxGi0tnUsCHILYjRzUYzqdUkunp2xdnCucz2uUW5HI47zJKjS1j4z0IOxBwgIz+jGiN+MihUd66qWgFJSUlYWFhgamoKwMGDBwkJCcHCwoI+ffrg5eVVqYHqW2xsLMHBwYSEhODq6qq/QBYsgPXr1ZOJjIzgtdegb99SzW5+tQz7aa9qtrcpe9Ap528pvy2EEOKxNSd8DpO3TQbUVbz+GPgH7eq003NUQlS95JxkopKiOJV4isTsxHLbKlDgbuOOv5M/De0bYmpkWk1RPtrKe5Z/4EGHDg4OWttt2rShTRsZklXlzp6FXbtub3frprNZ2pYw7O/YjnVtI8mQEEKIx9aeK3t4a9tbmu2U3BT+8/t/iB4XjZFBOSVXhXgC1DSrSYe6HWhfpz03sm5wIv4EkQmR5Bbllmp753yjTec24WnriZ+jH941vSU5KsMDJ0QhISFs2bKFS5cukZOTg6WlJT4+Pjz33HO0bNmyMmMUd7q77rah7v9Cs5Pa84cUQZKsCiGEeHwFuQUxInAEy04sA8DC2IL1L62XZEg8Ve4u430u+Rwn4k9wIeWCzkp1JaoSLqRc4ELKBQwUBnjZeeHr4IuPvY+UtL9DhROioqIiJk+erCmgcKeoqCjWr19P//79+eSTTyolQKHtekyRppQ2wJU4I9zvbpSWhkvqac1mCQpq95PSpULoU35+PikpKWRmZlJcXKzvcIR47BgYGPB/Lf6PhrYN+e+e/7L+pfUE1grUd1hC6I2h0pBGDo1o5NCIrIIsTiWeIiIhosxiDMWqYk0xBiOlET72PgTWCsTD1gOlosJlBZ4oFU6IFi1axI4dO7T2mZiYkJ9/eyXp9evX4+Hhwauvvnr36eIhrXJ6i228hCFFGFFIz2J/Xr+rTVbIIe5cuusUfrTsalWdYQoh7pCfn09MTAy2tra4u7tjZGQklYCEqACVSkVhYSEZGRk87/w8z41+jgZOsrCeELdYGFvQ2rU1rV1bczPnJqcSTxGVGEVSTpLO9oUlhUQmRhKZGImViRUBTgE0dmqMg7mDzvZPugonRL///jugXqD1ww8/pHv37pibm5OXl8eOHTv46KOPyM7OZu3atZIQVYEb1j6EcLtaXCfH0m3iN4RR747ts7ZtCLCt+tiEELqlpKRga2uLvb39vRsLIUpRKBQYGxtrfoYK7x4+fofikmKyCrKwrmFdXeEJ8UixN7Onk3snOtbtSFJOElGJ6mIMybnJOttn5GewP2Y/+2P2U9uyNo1rNcbP0Q8zI7Nqjlx/KpwQJSYmolAoGDNmDC+88IJmf40aNejTpw/x8fF88803JCQkVGqgQu3uvwFGuoZOH9SeP5TTWOYPCaFPmZmZuLu76zsMIZ4IVlZWXLlyBWfn0qWIVSoVE/6ewN6re9k6ZCuuVnqsCiuEnikUChzNHXH0cKSTeycSsxM1w+rS89N1nhOXGUdcZhzbLmyjQc0GNK7VmPp29TFQGlRz9NWrwglRvXr1OHPmDI6OOromADs7OwB8fGTNm6rQrRuYm6sTo8JCCAy8q0FJCbViDmntsu0pCZEQ+lRcXIyRzk8vhBAVZWRkVOY8vE/3fcrCfxYC0OaHNmwbso1GDo2qMzwhHkkKhQInCyecLJzo4tGFK2lXOJlwktNJpykoLijVvlhVzJmbZzhz8wxmRmb4OfoR4BRAbcvaT+SQ7wonRJMmTeL1119n9erV9OjRAzOz291pqampLF++HKVSycSJEys1UKH23HPqV5kuXsS8JFOzmYwd/v1lnLUQ+vYk/gERQh/K+ln68+yfvB/6vmY7NiOWQesHcXzM8ad+wrgQd1IoFHjYeuBh60HP+j05nXSak/EnuZx2WWf7nMIcDscd5nDcYexM7QhwCsDf0Z+aZjWrOfKqc8+E6LPPPiu1z93dnVOnThEcHExQUBDW1tYkJiYSFhZGbm4u3bp1IyUlpUoCFvdQvz6KtDQ4dAgOHsSmsAQ7D3kQE0II8WTr5tmNvj59+T36dwBsatiwut9qSYaEKIexgTGBtQIJrBVIWl4aEQkRnIw/WeZ8o5TcFHZf2c3uK7upbVmbAKcA/Bz9MDc2r+bIK5dCpVKpymvg4+NT5qcxKpVK69id2wqFgtOnT+s873FU3uq21eraNSgoUK8/ZGQENWsiK64K8Wg7c+YMDRs21HcYQgCl/3ZX17mVqayfqeKSYsZvGc/yk8vZMXQH7eq000N0QjzeVCoVcZlxnIg/wanEU+QV5ZXbXqlQ4mXrRYBTAN723hgbGFdTpBVT3rP8fQ2ZKy9nuvvYPfIr8bCGDYPdu29vh4RAly56C0cIIW6ZPn06Gzdu5IMPPmDIkCGljm/evJm33nqLn3/+mVatHm5ttFt/2O5mbGyMk5MTQUFBTJw4sVRlv6ysLFavXs22bduIiYmhuLgYV1dXevXqxeDBg7GwsCh1zYSEBNauXcuuXbu4fv06ubm5ODo60rp1a0aOHImXl9dDvZf7MX36dDZv3kxkZORDXSc0NJQ1a9awePHiCp+7ZMkS0tPTmTp16kPFcMvFixf56quvOHbsGEVFRbRq1Yp33nnnoQqQGCgNWNBrAZNaT8LHXuYyC/EgFAoFrlauuFq50qNeD84lnyMyIZJzyecoVpWev1eiKuF8ynnOp5zH2MAYH3sfApwC8LT1fGx6aO+ZEP3888/VEYe4X0VF2tuGFZ4GJoQQVeqrr76iTZs21ZIoDBw4kD59+mi2CwoKOHnyJPPnz+fo0aNs3LgRY2P1p5Vnz57l9ddfJzs7m0GDBuHv74+hoSFHjx5l0aJFbNq0iR9//FEridq/fz9TpkzByMiIgQMH4ufnh4mJCRcuXGD16tX89ddfzJs3j44dO1b5e60My5YtIylJ97ok9/LNN9/w8ssvV0ocCQkJDBkyBHt7ez766CMKCwuZN28eQ4YM4c8//9QUaHoQCoWi3GSosLgQhUKBoVL+fgpxL3cu/ppbmMvppNNEJkZyJe2KzvYFxQVEJEQQkRCBhbEFfo5+NHZqTC2LWo9E73JZ7vnboGXLltURh7hf91V3Wwgh9MPAwAADAwOmTp3K2rVrq7y6Xu3atWnevLnWvqCgIIqKivjuu+/YtWsXPXr0IDs7m/Hjx1NSUsL69eu1hkt07NiR9u3bM2zYMGbOnMns2bMBiIuL480338TZ2ZkVK1ZgY2OjOadNmzYMGDCAwYMH8+677xIaGqpJvMS9LVmyhNzcXJYvX07NmuqJ2S1btuSZZ55h2bJlvP3221VyX5VKxai/RpGcm8zaAWufqnVWhHhYpkamNHNpRjOXZqTnpROZGElEQgSJ2Yk622cVZBEeG054bDiNHBrRr2G/R/aDiAeOqqioiG3btnHkyBHS09Oxs7OjRYsWdO/eHaXy8egeexwduV4bJ0MPDCnCSFVI7JkaNPm3qvb+/RAdDe3agbc3PMKJuBDiX4qPH+wHtalzU46OPqrzWLMlzTh249gDXVf1v4cb9mxgYMB7773Hu+++y3fffcfkyZPLbV9UVMTatWtZt24dV69exdTUlPbt2zN58mSd68zcr4CAAACuXr0KwB9//EFMTAxffvmlznmgLVq0YOLEiRgbG2vmySxdupTMzEyWLVumlQzdYmpqyttvv83atWtJTk7G2dmZQ4cOMWzYMGbMmMHKlSu5evUqr776KpMmTSIyMpLFixdz7Ngx0tPTsbCwoEWLFkyZMgUPDw/NdU+dOsWsWbM4fvw4pqamDB48+L6Go8fHx/Ppp59y7Ngx0tLScHFx4dlnn2XcuHEYGxvTpUsX4uLiAPD29uazzz6jX79+JCYmsmDBAvbu3UtiYiJGRkb4+Pjw+uuv06FDB63hiWvXrmXt2rWaMfgpKSnMmTOHXbt2kZaWRt26dRk2bNg9e5J2795Nq1atNMkQgLOzM82bNyckJKTKEqL3dr3HTyd/AiD452D+GvQX9mayYLIQFWVdw5p2ddrRrk47ErISiEiIIDIxkoz8DJ3tTyedpkmtJtSvWb+aI70/D5QQnTt3jgkTJhATE6O1f/Xq1Xh5eTFv3jytX+6i8vy3wXpCrt3e3lHnjoOvjsTwXAmzCOKMdRve+K4Rg4Y82QtpCSEePf379yc0NJSlS5fSsWNHmjZtWmbbN998k127djFs2DDeeustYmNjWbBgAQcOHOC333574KTo4sWLAJrzQ0NDUSqVOucd3fL6669rbW/dupV69eppkitdgoKCCAoKKrX/008/5Z133sHBwQE3NzcuXrzIK6+8QtOmTfnoo48wNzcnKiqKhQsXcuPGDdavX6+Je/DgwdSpU4eZM2cC6t6UCxculDvcpKSkhNdee42SkhKmT5+OnZ0dhw8fZtGiRRQWFjJt2jTmzp3LBx98QHp6Ol999RUeHh4UFBQwdOhQioqKGD9+PM7OzsTFxbF48WImTJjArl27cHR0ZNWqVQwePJjg4GBGjhyJo6MjGRkZDBw4kMzMTMaNG4erqyuhoaF8+OGHJCUlMX78eJ2x5ufnExsbS/fu3Usd8/Dw4ODBgxQVFWFYyUPClx1fxmf7b1fODY8NZ9jGYWwZvKVS7yPE08bJwoluFt0I9gzmatpVIhIiOJ10mvzifK12JoaPbhGwCv+2SUpKYuTIkSQnJ+v8xOrChQuMGDGCDRs2PNQYYKFbmSPmiopocn4d7chmOD9BOuwvOg/Uq+4QhRCCGTNmcOLECaZNm8Yff/yBuXnpkqxhYWHs2LGDyZMnM3bsWM3+li1b0rdvX+bNm8enn35a7n1KSkooumNuZUpKCuHh4SxcuBAXFxe6desGwI0bN7Czs9NZNEGXjIwMUlNTdQ4bLy4uLvX3T6lUao2O6NmzJ6+88opm+/fff8fX15cFCxZovhZt27YlPj6eVatWkZGRgZWVFQsWLECpVGoNJWvXrh1du3YlNze3zHhTUlI4f/48b775Jr179wbQLIthamoKgL+/P5aWluTl5WmGGUZHR2Nra8u0adO0ElcLCwsmTZrE8ePH6dq1q6a9vb295t+LFy8mJiaGtWvX0rhxYwA6deqEkZERixYt4sUXX8TJyUnn1xbA0tKy1DELCwtKSkrIzs7G2tq6zPf7IHrU60Fjp8acTDipfi9m9sztMbdS7yHE00ypUGqtb3Qu+RwRCRGk5aXh5+iHm5WbvkMsU4UTosWLF3Pz5k0UCgU9evTg5ZdfxsnJSVOFZ+vWrSQkJLBkyRKmT59eFTE/1e5OiG59gJa67xS2qmzN/iTsadyv6ic0CyGELnZ2dsycOZPRo0czc+ZMnYlNWFgYAC+88ILW/vr16xMYGMjBgwfveZ/Zs2dr5vzcolQqadWqFf/73/80yYBSqdRKnO6lpKSkzGODBg3i5MmTWvvGjx/PhAkTNNve3t5ax/v27Uvfvn0pLi4mJiaGmJgYLl26xPHjxwF1MQiAQ4cO0bx5c62hZFZWVnTq1Im///67zJhq1qyJr68v8+bN48yZM7Rr1442bdowYsSIct+nj48Pa9asAdSFDq5du8bVq1fZtWuXVly6HDhwABcXF3x9fbW+ts8++ywrVqzg4MGD9O3bt9R5t7625fV4VcXQexdLF/YM30O/df0Ijw1n8yubH9nhO0I87owMjPB19MXX0VffodyXCidEu3fvRqFQ0K1bN+bMmaPZ7+npSZs2bZg4cSLbt28nJCREEqIqsGED5OSoE6OiIrhVnfTauoPY3tEuyjqITlYyiUiIR93DztnRpay5RdWtY8eODBw4kDVr1tBFx/IAaWlpKBQKHBwcSh1zcHAgOjr6nvcYMmSI5qFboVBgamqKs7MzZmbak+VdXV05d+4cmZmZOnsmAJKTkzExMcHCwgIbGxssLCxKDQ0H9YLlOTk5mvfw2muv6Yz/ToWFhcyaNYtff/2VzMxMHBwc8PHx0cR5q8cpLS2tVKlwAEdHx3K/DgqFgmXLlrF06VJ27NjBtm3bAHViNnXqVNq3b1/mub/++isLFy4kLi4OCwsL6tevr+nZKW/uUkpKCnFxcfj66n7giY+P17nfysoKgMzMzFLHsrKyUCgUOnsUK4N1DWu2vLKFyMRImrs0v/cJQoinQoUTooSEBACdf9xu7d++fbumnahctWrp3l+0N0xrO6NRm2qIRgghyjd9+nTCw8P54IMPtIbFAdjY2KBSqUhKSio1tCohIQFbW1vuxcnJCX9//3u269SpEyEhIYSEhOjstQD4v//7P3bu3ElISAhOTk50796dDRs2cP78eerXv92TcGc58fstYT1z5kx+++03PvnkEzp37qwZDvbFF1/wzz//aNrZ2dmRmFi6YlNKSso972FjY8PUqVOZOnUq8fHx7Nu3j8WLFzNu3Dj27Nmj8+u5ZcsW3n//fUaOHMmwYcM0c652797N1q1by72flZUV9evX57PPPtN5vKwkztTUlNq1a3PlypVSxy5fvoyXl1eVFmcyMTQpNxl6VBafFUJUnwr/xrlVaefy5cs6j9/ar6sij6g6jpe0h5ZYdpeESAihf6ampnz11VdkZGTw3XffaR1r00b9e2rjxo1a+y9cuMDJkycfevHWO/Xu3ZvatWsze/Zsbty4Uer4wYMH2bFjB506ddIkZ2PHjsXS0pKpU6eSnJys87qnT5++r/sfOXKERo0a0bdvX00yVFBQwP79+4Hbw8jatWvHkSNHtGLMy8tj37595V7/9OnTtG/fXpPE1KpVixdffJEhQ4aQn5+vSdzuTjSOHDkCwKRJk7QKWNwaMnfn0MG7zw0KCiI2NhZ7e3v8/f01rxs3bvDNN9+Umyx27NiRgwcPan1db9y4wZEjR+jQoUO577UqlahKGLpxKF8e+FJvMQghql+Fe4iaNGnC9u3bWb58Od7e3vTs2VNzbMuWLfz4448oFIpS60KISvLBB1BQoJ48ZGQE06aRdzML17yLmiZFGOA9WL7+QohHQ0BAAGPHji2VELVt25bOnTvz7bffkp6eTuvWrYmLi2PBggVYWVnxxhtvVFoMZmZmzJo1izFjxtCvXz8GDRpE48aNKSwsJDw8nLVr1+Ll5cUnn3yiOadu3brMnz+fKVOm0LNnT/r370/Tpk0xNzcnJiaG7du3s3//fmrXrq2z0tydmjRpwvr161m4cCGBgYEkJCSwcuVKzp8/D6ApmDB+/Hh27tzJf/7zH8aNG4epqSk//vijzuFld2rQoAE2NjZ8/PHHJCcn4+npSWxsLN9//z2NGjWiXj11gR1ra2siIiIIDQ2lUaNGNGnShNWrV/P+++/Tt29f8vLy2Lx5s2a+0p2FHKysrIiMjOTgwYMEBgYyfPhwNm3axNChQxk1ahRubm5ERUWxYMEC6tatW2oe1Z1GjRrFX3/9xZAhQ3jjjTdQqVR8++23WFtb8+qrr5b7XquKSqVi8tbJrIpcxarIVSRlJ/FFty9QKmQpESGedBVOiP7zn/+wc+dOCgoKmDJlCh9++CGOjo4kJiaSnZ2NSqXCwMCAoUOHVkW84ttvIeOOGu9vvsml1eE0uqPJaeNAAupXzfhrIYR4EK+//jp79+4lIiJCa/+3337LkiVL+PPPPzWLn7Zv356JEyc+1DpEugQGBrJx40ZWrFjB1q1bWb58OUqlkjp16jB58mQGDhxYau5Rq1at2Lx5M+vWrWPnzp1s2LCBrKwsbG1t8fPz44svvqBnz573XJT11pzaVatWsXDhQhwdHQkKCmLChAmMHj2aI0eO4O7ujouLC2vWrOHLL79kxowZGBgY0Lt3bwIDA1m5cmWZ1zc0NOT7779n7ty5LFmyhOTkZGxsbOjcuTOTJ0/W9O4MHTqUqKgoJkyYwMSJExk9erSmKNK2bduwsbHB19eXdevWMWrUKA4fPsxLL70EwLhx45g/fz5jx45l2bJlNGvWjLVr1zJnzhzmzZtHWloajo6ODBgwgPHjx5e7KK+LiwsrV67kyy+/5IMPPsDY2JjmzZszbdo0nXOoqsMXB77g28Pfara/Pvg1RgZGfBpcfqVDIcTjT6G6n9Xe7rJy5Uo+//xzTVUZhUKhmXipUCiYMmWKzkmmj7NbC9PdWoxOb8zM4M7Sq1lZHOz1f7TZ84Vm1/YG4+l+dp4eghNC6HLmzBkaNmyo7zCEeGJUxc/U7iu7ee6X58gsUPfG1bKoxYGRB/C09azU+wgh9KO8Z/kHWvVsyJAhNG7cmJ9++olDhw6RlpZGzZo1CQwM5JVXXtG5boOoHCWFRVoTvwpKDLGM1J4/pGwr84eEEEKIiujk3ok9w/fQY1UP8ovy2TZkmyRDQjwlKpwQ7dixA19fX/z9/fn666+rIiZRjv8Zf05BUSFGFGJIEa+ngmfKEa02ri9KQiSEEEJUVBPnJhwYeYDE7EQCnAL0HY4QoppUOCF6//33ycjIYPTo0UyePLkqYhLlmGf0Ful3bD8fcoIm3B5CF6+oRYPu7tUelxBCCPEkqGdXj3p29fQdhhCiGlW4dEp+fj6ApmKNqF6FhdrbNzeFa21fdGyD0kDWTxBCCCEqW3FJMaP/Gs2xG8f0HYoQohJVOCF65plnUKlU7N69W2t9AlE9goPVrw4dICgILCMPaB3PayLD5YQQQojKplKpeH3z6yw9tpROyzsRejlU3yEJISpJhYfMtWjRgkOHDrFlyxYOHTpEkyZNsLW1xcTEpNSibf/9738rLVCh9uefd+14y4GiX+tiGHsVgNoDJCESQgghKtsHoR+w9NhSADILMumxqge/v/w7z9Z/Vs+RCSEe1gPNIVIo1EOybt68yc6dO8tsKwlRNZg1C8NZs+DGDTh4EJ9nm+k7IiGEEOKJolKpKCgu0NrnZO6En6OfniISQlSmB1p+WaVSadYduvXvu1+imjk7Q79+YGqq70iEEEKIJ4pCoeDLbl/yZdcvAahpWpPtQ7fjZu2m58iEEJWhwj1EP//8c1XEIe5HTAw0agSGhmBkBHXrwj//6DsqIYQQ4qkwte1UnCyc8LH3wcfeR9/hCCEqyX0nRCkpKYSFhXHjxg0sLS1p2bIlnp6yYFm1KiyE7Ozb21ZW+otFCCGEeAoNazxM3yEIISrZfSVEP/30E3PmzCEvL09rf//+/ZkxY0apYgqiitxdc9uwwh18QgghhKgiRSVFzAmfw4SWEzAxNNF3OEKI+3TPTGbbtm189tln5ObmlpontH79er755pvqiLNSzJs3Dz8/P5o0aUKTJk1o27atvkOqkLSbRVrb0RcNeestWL8eEhL0FJQQQojHysPM85U5wmUrUZXw6p+vMnXHVPr80oesgix9hySEuE/3TIiWL18OqCcU+vv789prr9GjRw+USiUqlYpVq1aV6jl6VJ09e5ZPPvmE48ePc/z4cQ4cOHDvkx4h6bUbYUkGtqTgSAKDilfwyuzmxA2YyA8jH6/3IoR48kyfPh1vb29Wrlyp8/jmzZvx9vbm0KFDD32v2NhYvL29S738/f3p2rUrH374ITdv3ix1XlZWFkuWLKF///60aNGCpk2b8txzz7F48WKysnQ/wCYkJPDtt9/St29fWrZsib+/P8HBwbz33ntcvHjxod/L/Zg+fTr+/v4PfZ3Q0FDGjh37QOcuWbKEr7/++qFj0OXNN9/k1VdfrZJrVweVSsXkrZP5+aR6nvWOSzvotqIbKbkpeo5MCHE/7jnm6vLlyygUCoKCgvj+++81JbdXrFjBzJkzyc/P5/LlyzRs2LDKg31YZ8+eZfz48foO44EVFivJwlKzbUkmzTlKc44SbuEBPF49XkKIJ9NXX31FmzZt8PLyqvJ7DRw4kD59+mi2CwoKOHnyJPPnz+fo0aNs3LgRY2NjQP034PXXXyc7O5tBgwbh7++PoaEhR48eZdGiRWzatIkff/wRe3t7zfX279/PlClTMDIyYuDAgfj5+WFiYsKFCxdYvXo1f/31F/PmzaNjx45V/l4rw7Jly0hKSnqgc7/55htefvnlSo2nqKiImTNn8vfff9OuXbtKvXZ1ysjPYNvFbVr74rPiySt6PD4wFuJpd8+EKPvfSfy9e/fWJEMAvXr1YubMmYC64MKjLicnh9jYWObMmcPJkyepU6cO7733HgEBAfoO7b4VaY+Yw4jbc4qc+8mCrEI8ju74tVohTZvC0aO6jzVrBseOPdh1H3ZElIGBAQYGBkydOpW1a9diZGT0cBe8h9q1a9O8eXOtfUFBQRQVFfHdd9+xa9cuevToQXZ2NuPHj6ekpIT169fj6uqqad+xY0fat2/PsGHDmDlzJrNnzwYgLi6ON998E2dnZ1asWIGNjY3mnDZt2jBgwAAGDx7Mu+++S2hoqCbxEvfn2LFjfPbZZ5w/f54aNWroO5yHYl3Dmn0j9tFjVQ+O3ThGLYta7By6ExdLF32HJoS4D/ccMlf071O4hYWF1n5bW1vNvwsKtBcr04edO3fqHD4xb948AJKTk2nevDmvvvoqe/bsYcCAAYwZM4b09HQ9R37/XFzU84W+fjeFlQzmC94BIB9j6jzfRM/RCSGEOiF67733iIqK4rvvvrtn+6KiIlatWsXzzz9PYGAgbdq0Ydq0ady4ceOh4rj1YdfVq1cB+OOPP4iJiWHy5MlaydAtLVq0YOLEifj5+WnmySxdupTMzExmzpyplQzdYmpqyttvv03z5s1JTk4G4NChQ3h7e7N27Vr69OlDQEAAc+fOBSAyMpLx48cTFBSEr68vrVq1Yvz48Vy+fFnruqdOnWLkyJE0adKEoKAg5s+ff19zd+Lj45k4cSLt2rXDz8+P7t27M3v2bM3f6C5dunD48GEuX76Mt7c3GzZsACAxMZGPPvqILl26aObZDho0iL179wK3hycCrF27Fm9vb2JjYwH1B6Iffvih5p69evVi7dq194wVYMyYMVhYWLB+/Xpq1qx5X+c8yhzMHQj9TyjPez/PtiHb8LKr+h5SIUTluGcPkUqlQqFQlKokd2dvUUlJSeVHVkHBwcFERUWV2n8rbjc3N1asWKHZ/+KLL/Lzzz9z4sSJx2aog5WVeu3VrVt20YPVmv2X7JrTsIZUsxFCPBr69+9PaGgoS5cupWPHjjRt2rTMtm+++Sa7du1i2LBhvPXWW8TGxrJgwQIOHDjAb7/9hrOz8wPFcGtuz63zQ0NDUSqVBAcHl3nO66+/rrW9detW6tWrV+5IgqCgIIKCgkrt//TTT3nnnXdwcHDAzc2Nixcv8sorr9C0aVM++ugjzM3NiYqKYuHChdy4cYP169dr4h48eDB16tTRjMJYsmQJFy5c0Pq7e7eSkhJee+01SkpKmD59OnZ2dhw+fJhFixZRWFjItGnTmDt3Lh988AHp6el89dVXeHh4UFBQwNChQykqKmL8+PE4OzsTFxfH4sWLmTBhArt27cLR0ZFVq1YxePBggoODGTlyJI6OjmRkZDBw4EAyMzMZN24crq6uhIaG8uGHH5KUlHTPIeorVqzAx+fJWsvHysSK3wf+ru8whBAVdN91m8PDw8nMzKzQsb59+z5wYBWlUCgwLKcM9ZkzZzh8+DD/+c9/NPsKCgoeyyEOxfvDtLaz/GS4nBDi0TJjxgxOnDjBtGnT+OOPPzA3Ny/VJiwsjB07djB58mStif4tW7akb9++zJs3j08//bTc+5SUlGhGMoC6xyI8PJyFCxfi4uJCt27dALhx4wZ2dnalRjuUJSMjg9TUVFq2bFnqWHFxcakeG6VSqfXBYc+ePXnllVc027///ju+vr4sWLBA87Vo27Yt8fHxrFq1ioyMDKysrFiwYAFKpZLly5drek3atWtH165dyc3NLTPelJQUzp8/z5tvvknv3r0BdbJmbW2NqakpAP7+/lhaWpKXl6cZZhgdHY2trS3Tpk3TSlwtLCyYNGkSx48fp2vXrpr29vb2mn8vXryYmJgY1q5dS+PGjQHo1KkTRkZGLFq0iBdffBEnJ6cyY37SkqH7UVBcQERCBM1dmt+7sRCi2tx3QqSratCtT6vKOladCdG91KhRgzlz5tCgQQNatmzJ6tWrKSwspFmzZvoO7f7FxsLevfhc3KS12+oZSYiEeFxVRRXjsuYWVSc7OztmzpzJ6NGjmTlzps7EJixM/eHOCy+8oLW/fv36BAYGcvDgwXveZ/bs2Zo5P7colUpatWrF//73P00yoFQqtRKneylv5MOgQYM4efKk1r7x48czYcIEzfatIWa39O3bl759+1JcXExMTAwxMTFcunSJ48ePA7eHnh86dIjmzZtrDSGzsrKiU6dO/P3332XGVLNmTXx9fZk3bx5nzpyhXbt2tGnThhEjRpT7Pn18fFizZg2grqZ37do1rl69yq5du7Ti0uXAgQO4uLjg6+ur9bV99tlnWbFiBQcPHnykngP0rbikmKEbh/J79O+s6b+GFxq+cO+ThBDV4r4Soidh3QEPDw8+//xzPv74Y+Lj4/Hx8WHhwoWPVw/RkSMweDB3j0p2HyQJkRDi0dOxY0cGDhzImjVr6NKlS6njaWlpKBQKHBwcSh1zcHAgOjr6nvcYMmSI5qFboVBgamqKs7MzZmZmWu1cXV05d+4cmZmZWFpa6riSeq6piYkJFhYW2NjYYGFhQUxMTKl2n332GTk5OZr38Nprr+mM/06FhYXMmjWLX3/9lczMTBwcHPDx8dHEeevvbFpamlaVu1scHR3L/TooFAqWLVvG0qVL2bFjB9u2qSueeXt7M3XqVNq3b1/mub/++isLFy4kLi4OCwsL6tevr+nZKe/vf0pKCnFxcfj6+uo8Hh8fX27MTxOVSsUbm99gXdQ6AAb8OoAfnvuB4YHD9RuYEAK4j4SoOstUFxUVMWTIEBo0aMCMGTO0jh0/fpxZs2YRFRWFkZERHTp0YNq0aTr/kJblmWee4ZlnnqnssKuPjk83b5jUxdlDqtgIIR5N06dPJzw8nA8++KDU+jc2NjaoVCqSkpJKDa1KSEjQKt5TFicnp/tan6dTp06EhIQQEhJSZq/F//3f/7Fz505CQkJwcnKie/fubNiwgfPnz1O/fn1NuzvLid9vCeuZM2fy22+/8cknn9C5c2esra0B+OKLL/jnn3807ezs7EhMTCx1/v1Uc7WxsWHq1KlMnTqV+Ph49u3bx+LFixk3bhx79uzR+fXcsmUL77//PiNHjmTYsGGaOVe7d+9m69at5d7PysqK+vXr89lnn+k8fq8k7mly4NoBlhxbotkuUZXw2f7PGOg3kBqGj3eFPSGeBI9MQpSTk8PUqVM5fvw4DRo00Dp25swZRowYgb+/P1988QXJycnMmTOH06dPa60x8aQrzCnkzgK2JShI9GzDg005FkKIqmdqaspXX33FoEGDSlWda9OmDUuXLmXjxo1aydKFCxc4efJkqaF0D6N3794sWrSI2bNn06pVq1LFGg4ePMiOHTvo3LmzJjkbO3YsO3bsYOrUqfzwww86K6GdPn36vu5/5MgRGjVqpJWMFRQUsH//fuD2EL127dqxadMmbty4oYkxLy+Pffv2lXv906dPM2bMGN577z169OhBrVq1ePHFF8nOzuazzz4jKSkJW1vbUgWSjhw5AsCkSZO0Sl/fGjJ359DBu88NCgri559/xt7eXuvruX37dlavXs3bb79d7hyip0m7Ou1Y9twyXvvrNUpUJbhaubJ9yHZJhoR4RNz3HKKqtGfPHj777DNSU1N1Hp87dy7W1tZ8//33mJioq6k1atSIF198kfXr1zNo0KDqDFdvDt2owzUGYkgRRhSSjB2t2wfqOywhhChXQEAAY8eOLZUQtW3bls6dO/Ptt9+Snp5O69atiYuLY8GCBVhZWfHGG29UWgxmZmbMmjWLMWPG0K9fPwYNGkTjxo0pLCwkPDyctWvX4uXlxSeffKI5p27dusyfP58pU6bQs2dP+vfvT9OmTTE3NycmJobt27ezf/9+ateurbPS3J2aNGnC+vXrWbhwIYGBgSQkJLBy5UrOnz8PoCmYMH78eHbu3Ml//vMfxo0bh6mpKT/++GOZRY1uadCgATY2Nnz88cckJyfj6elJbGws33//PY0aNaJevXoAWFtbExERQWhoKI0aNaJJkyasXr2a999/n759+5KXl8fmzZs185XuLORgZWVFZGQkBw8eJDAwkOHDh7Np0yaGDh3KqFGjcHNzIyoqigULFlC3bt1S86iediOajMC6hjUT/57IjqE7qGtTV98hCSH+pfeEKCMjgzFjxvDMM8/w7rvv0qFDB63jBQUFhIWFMWDAAE0yBOo/sO7u7oSEhDw1CdHl2m0Zxu2VvPuykQEvuekxIiGEuD+vv/46e/fuJSIiQmv/t99+y5IlS/jzzz81i5+2b9+eiRMnPnDJ7bIEBgayceNGVqxYwdatW1m+fDlKpZI6deowefJkBg4cWGruUatWrdi8eTPr1q1j586dbNiwgaysLGxtbfHz8+OLL76gZ8+e9xypMH36dABWrVrFwoULcXR0JCgoiAkTJjB69GiOHDmCu7s7Li4urFmzhi+//JIZM2ZgYGBA7969CQwM1FnA6BZDQ0O+//575s6dy5IlS0hOTsbGxobOnTszefJkTe/O0KFDiYqKYsKECUycOJHRo0eTkJDA2rVr2bZtGzY2Nvj6+rJu3TpGjRrF4cOHeemllwAYN24c8+fPZ+zYsSxbtoxmzZqxdu1a5syZw7x580hLS8PR0ZEBAwYwfvz4Kl+U93HUr2E/nq33LKZGpvoORQhxB4VKzxUTCgoKuHbtmmZMtre3Ny+//LJmDtHFixfp2bMnH374IYMHD9Y6d+zYsURHR7N79+4qjzM2Npbg4GBCQkJ0LupXHf437iYzFtyebNuP9awveA7kj44Qj7QzZ87QsGFDfYchxBPjSf2ZyivKQ6VSScIkRBUo71leWcY51cbY2Fhrgurdbg0T0LV2hLm5+T2HETxJbBOjqclNrEnDjGxKaphLMiSEEEI8AYpKihj420CeXfUsGfkZ+g5HiKeK3hOie7k1obOsFbrvnuT5JHvzEwdufrKY1J5DSLeuw/dDQvUdkhBCCCEeUomqhFf/fJU/zv7Bnqt76PpzV5JzkvUdlhBPDb3PIbqXW6VJdfUEZWdnl7mexBPJ2xveew8FYKhSUTMvT98RCSGEEOIhfR32NT+f/FmzfeT6EcZsGsNvL/2mx6iEeHo88t0rbm5uGBkZceXKlVLHLl++rKmc89RRKMBUxhgLIYQQj7vXmr5Gq9qtNNvuNu7M6TFHfwEJ8ZR55BMiY2NjgoKCCAkJIe+OHpGIiAiuXLlCx44d9RhdNVu+HDp3hm7doGdPWL1a3xEJIYQQ4iHZmdqxY+gOOrt3xtnCmZ1Dd+JqpZ8CTkI8jR75IXMAEyZMYNCgQQwfPpwRI0aQnp7OrFmzqFevHgMGDNB3eNXn4kW4s6JemzZ6C0UIIYQQlcfSxJLNr2zmeuZ1vOzKLjYlhKh8j3wPEYC/vz8//PADAFOnTmX27Nl06NCB5cuXa61N9MQrLNTeNnws8lkhhBBC3AdTI1NJhoTQg0fuifrs2bM697dq1Yo1a9ZUczSPlmtXirhzGdar142Qda6FEEKIJ19BcQFDNw7ltSav0c2rm77DEeKJ8lj0EAm13xzeoDO76M42erKZP5Qv6DskIYQQQlSx4pJihm4cyrqodfT+pTe/R/+u75CEeKI8cj1EomxJlp7sxlOz3dZRj8EIIYQQosqpVCrGbBrDuqh1gLqnaMC6Aax/aT3P+zyv5+iEeDJID9Fj5O4pREZG+olDCCHE40ulUunlXPFgFAoF/o7+Wvs8bD1o5dqqjDOEEBUlPUSPkS5d1ElQYSEUFUHz5vqOSAghbps+fTobN27kgw8+YMiQIaWOb968mbfeeouff/6ZVq0e7mEuNjaW4ODgUvuNjY1xcnIiKCiIiRMnYm9vr3U8KyuL1atXs23bNmJiYiguLsbV1ZVevXoxePBgLCwsSl0zISGBtWvXsmvXLq5fv05ubi6Ojo60bt2akSNH4uVV9ZPgp0+fzubNm4mMjHyo64SGhrJmzRoWL15c4XOXLFlCeno6U6dOfagYbjl06BALFiwgOjoalUpF/fr1GTduHEFBQZVy/SfJpNaTsDSxZNRfo6htWZudQ3dSy6KWvsMS4okhCdFj5Nln1S8hhHiUffXVV7Rp06ZaEoWBAwfSp08fzXZBQQEnT55k/vz5HD16lI0bN2JsbAyoi/a8/vrrZGdnM2jQIPz9/TE0NOTo0aMsWrSITZs28eOPP2olUfv372fKlCkYGRkxcOBA/Pz8MDEx4cKFC6xevZq//vqLefPmPTZr4i1btoykpKQHOvebb77h5ZdfrpQ4Dh48yMiRI2nTpg2ffPIJCoWCX375hREjRjBr1ix69epVKfd5koxsMhKbGjb4OvhS10ZKKglRmSQhEkIIUWkMDAwwMDBg6tSprF27FqMqHttbu3Ztmt/VXR4UFERRURHfffcdu3btokePHmRnZzN+/HhKSkpYv349rq63F73s2LEj7du3Z9iwYcycOZPZs2cDEBcXx5tvvomzszMrVqzAxsZGc06bNm0YMGAAgwcP5t133yU0NFSTeIl7+/bbb3F3d2fx4sWa75FOnTrRp08fFi5cKAlRGfo17KfvEIR4IskcIiGE0CeF4sFezZqVfc1mzR78ug/JwMCA9957j6ioKL777rt7ti8qKmLVqlU8//zzBAYG0qZNG6ZNm8aNGzceKo6AgAAArl69CsAff/xBTEwMkydP1kqGbmnRogUTJ07Ez89PM09m6dKlZGZmMnPmTK1k6BZTU1PefvttmjdvTnJyMqAeBubt7c3atWvp06cPAQEBzJ07F4DIyEjGjx9PUFAQvr6+tGrVivHjx3P58mWt6546dYqRI0fSpEkTgoKCmD9//n3N3YmPj2fixIm0a9cOPz8/unfvzuzZsykoKACgS5cuHD58mMuXL+Pt7c2GDRsASExM5KOPPqJLly74+fnRpEkTBg0axN69ewH18ERvb28A1q5di7e3N7GxsQCkpKTw4Ycfau7Zq1cv1q5de89YAwMDGT58uFbCbGhoiKenJ/Hx8fc8X5SWV5THN2HfUFRSpO9QhHjsSA+REEKIStW/f39CQ0NZunQpHTt2pGnTpmW2ffPNN9m1axfDhg3jrbfeIjY2lgULFnDgwAF+++03nJ2dHyiGixcvAmjODw0NRalU6px3dMvrr7+utb1161bq1aunSa50CQoK0jnn5dNPP+Wdd97BwcEBNzc3Ll68yCuvvELTpk356KOPMDc3JyoqioULF3Ljxg3Wr1+viXvw4MHUqVOHmTNnAuq5OxcuXEBRTsJaUlLCa6+9RklJCdOnT8fOzo7Dhw+zaNEiCgsLmTZtGnPnzuWDDz4gPT2dr776Cg8PDwoKChg6dChFRUWMHz8eZ2dn4uLiWLx4MRMmTGDXrl04OjqyatUqBg8eTHBwMCNHjsTR0ZGMjAwGDhxIZmYm48aNw9XVldDQUD788EOSkpIYP358mfG+8847pfalpqZy5MgRGjZsWOZ5QreikiIGrR/E79G/c+DaAX7p/wsmhk/RwvVCPCRJiIQQQlS6GTNmcOLECaZNm8Yff/yBubl5qTZhYWHs2LGDyZMnM3bsWM3+li1b0rdvX+bNm8enn35a7n1KSkooKrr9iXhKSgrh4eEsXLgQFxcXunVTL2B548YN7OzsdBZN0CUjI4PU1FRatmxZ6lhxcXGpHhulUolSeXvQRc+ePXnllVc027///ju+vr4sWLBA87Vo27Yt8fHxrFq1ioyMDKysrFiwYAFKpZLly5dTs2ZNANq1a0fXrl3Jzc0tM96UlBTOnz/Pm2++Se/evQF1smZtbY2pqSkA/v7+WFpakpeXpxlmGB0dja2tLdOmTdNKXC0sLJg0aRLHjx+na9eumvb29vaafy9evJiYmBjWrl1L48aNAfWwNyMjIxYtWsSLL76Ik5PT/Xy5KSgo4J133iEzM5MJEybc1zlCrURVwog/RmjWJtoYvZE+v/Rh48sbMTcu/XMnhChNEiIhhBCVzs7OjpkzZzJ69GhmzpypM7EJCwsD4IUXtBeZrl+/PoGBgRw8ePCe95k9e7Zmzs8tSqWSVq1a8b///U+TDCiVSq3E6V5KSkrKPDZo0CBOnjyptW/8+PFaD/K3hpjd0rdvX/r27UtxcTExMTHExMRw6dIljh8/DqAZ1nbo0CGaN2+uSYYArKys6NSpE3///XeZMdWsWRNfX1/mzZvHmTNnaNeuHW3atGHEiBHlvk8fHx/WrFkDqKvpXbt2jatXr7Jr1y6tuHQ5cOAALi4u+Pr6an1tn332WVasWMHBgwfp27dvufcHyMzMZNKkSYSFhfHhhx/qTEJF2ZKyk9gfs19rX2xGLLlFuZIQCXGfJCESQgh9qop1XY4erfxrPoCOHTsycOBA1qxZQ5cuXUodT0tLQ6FQ4ODgUOqYg4MD0dHR97zHkCFDNA/dCoUCU1NTnJ2dMTMz02rn6urKuXPnyMzMxNLSUue1kpOTMTExwcLCAhsbGywsLIiJiSnV7rPPPiMnJ0fzHl577TWd8d+psLCQWbNm8euvv5KZmYmDgwM+Pj6aOG/1OKWlpZUqFQ7g6Fj+StwKhYJly5axdOlSduzYwbZt2wB1YjZ16lTat29f5rm//vorCxcuJC4uDgsLC+rXr6/p2Slv7lJKSgpxcXH4+vrqPH4/c4EuXrzIG2+8QVxcHJ9++in9+knRgIpysnBi/4j9dFvRjTM3z+Bh48GOoTuwNyv9fSSE0E0SIiGEEFVm+vTphIeH88EHH2gNiwOwsbFBpVKRlJRUamhVQkICtra297y+k5MT/v7+92zXqVMnQkJCCAkJKbPX4v/+7//YuXMnISEhODk50b17dzZs2MD58+epX7++pt2d5cTvt4T1zJkz+e233/jkk0/o3Lkz1tbWAHzxxRf8888/mnZ2dnYkJiaWOj8lJeWe97CxsWHq1KlMnTqV+Ph49u3bx+LFixk3bhx79uzR+fXcsmUL77//PiNHjmTYsGGaOVe7d+9m69at5d7PysqK+vXr89lnn+k8fq8kLjw8nPHjx2NgYMD3339P69at7/kehW61rWqzZ/geRv45krk95lLbqra+QxLisSJV5oQQQlQZU1NTvvrqKzIyMkpVnWvTpg0AGzdu1Np/4cIFTp48+dCLt96pd+/e1K5dm9mzZ+usYHfw4EF27NhBp06dNMnZ2LFjsbS0ZOrUqZoqcnc7ffr0fd3/yJEjNGrUiL59+2qSoYKCAvbvVw91ujVEr127dhw5ckQrxry8PPbt21fu9U+fPk379u01SUytWrV48cUXGTJkCPn5+ZrE7c55TrfiApg0aZJWAYtbQ+buHDp497lBQUHExsZib2+Pv7+/5nXjxg2+9RhPCgAAazJJREFU+eabcpPFY8eOMWbMGOzs7Fi3bp0kQ5XAwdyBvwb9haetp75DEeKxIz1EQgghqlRAQABjx44tlRC1bduWzp078+2335Kenk7r1q2Ji4tjwYIFWFlZ8cYbb1RaDGZmZsyaNYsxY8bQr18/Bg0aROPGjSksLCQ8PJy1a9fi5eXFJ598ojmnbt26zJ8/nylTptCzZ0/69+9P06ZNMTc3JyYmhu3bt7N//35q166ts9LcnZo0acL69etZuHAhgYGBJCQksHLlSs6fPw+gKZgwfvx4du7cyX/+8x/GjRuHqakpP/74I5mZmeVev0GDBtjY2PDxxx+TnJyMp6cnsbGxfP/99zRq1Ih69eoBYG1tTUREBKGhoTRq1IgmTZqwevVq3n//ffr27UteXh6bN2/WzFe6s5CDlZUVkZGRHDx4UFM2e9OmTQwdOpRRo0bh5uZGVFQUCxYsoG7duqXmUd1SWFjI1KlTNZXtkpKStJInpVJZbmVC8WByCnOIzYilQc0G+g5FiEeOJERCCCGq3Ouvv87evXuJiIjQ2v/tt9+yZMkS/vzzT83ip+3bt2fixIkPXHK7LIGBgWzcuJEVK1awdetWli9fjlKppE6dOkyePJmBAweWmnvUqlUrNm/ezLp169i5cycbNmwgKysLW1tb/Pz8+OKLL+jZs+c9F2WdPn06AKtWrWLhwoU4OjoSFBTEhAkTGD16NEeOHMHd3R0XFxfWrFnDl19+yYwZMzAwMKB3794EBgaycuXKMq9vaGjI999/z9y5c1myZAnJycnY2NjQuXNnJk+erOndGTp0KFFRUUyYMIGJEycyevRoEhISWLt2Ldu2bcPGxgZfX1/WrVvHqFGjOHz4MC+99BIA48aNY/78+YwdO5Zly5bRrFkz1q5dy5w5c5g3bx5paWk4OjoyYMAAxo8fX+aivJGRkZp1jKZOnVrquLGxMZGRkeV+PUXFFBQXMGDdAA7FHWLr4K20qN1C3yEJ8UhRqO5ntTdBbGwswcHBhISE6FzUTwghynLmzBlZW0WISiQ/U/evuKSYQesH8evpXwGwNLbkr0F/0dG9o54jE6J6lfcsL3OIhBBCCCGeUOui1mmSIYDMgkze2flOuRUEhXjaSEIkhBBCCPGEGug3kHfbvavZrm9Xnz8G/oFCodBjVEI8WmQOkRBCCCHEE0qhUDAzeCZWJlYs+GcBO4ftxMnC6d4nCvEUkR4iIYQQQogn3Dvt3iHy9UjqWNfRdyhCPHIkIRJCCCGEeApYmViVeSy/KF/mFYmnliREQgghhBBPsbyiPHqt7sWU7VMkKRJPJZlDJIQQQgjxlCosLmTgbwMJuRxCyOUQMvIzWNx7MQZKA32HJkS1kR4iIYQQQoin1KStk/jj7B+a7R+O/8DHez7WY0RCVD9JiIQQQgghnlJjmo3B0dxRs93IoRGTWk3SY0RCVD9JiIQQQgghnlKNazVm34h9uFm54WHjwY6hO6hpVlPfYQlRrWQOkRBCCFFJVCrVAy94+TDnCvEwGtRswP6R+ykqKcLF0kXf4QhR7aSHSAghRKW5evUq7777Lp07d8bPz4+WLVsydOhQNmzYQElJSYWvt2HDBry9vTlx4kTlB1uJCgoK+Prrr/npp58qfG5CQgITJ04kIiKi0uL5/fff6dOnDwEBAXTq1Ik5c+ZQUFBQadcXT5461nXwtPXUdxhC6IUkREIIISrFxYsX6d+/P2fOnOGNN95gyZIlfPLJJ7i5ufHf//6XDz/8UN8hVpnExESWLl36QEnHgQMH2LZtW6WVO163bh3vvPMOgYGBfPvtt7zwwgssWbKEjz+WifLiweQU5jB041AupV7SdyhCVAkZMieEEKJS/PDDDxQXF7Ny5UrMzc01+7t3746ZmRkrVqxg+PDh1KtXT49RPtny8/OZNWsWzzzzDP/3f/8HQKdOnTA3N+frr79m5MiReHl56TlK8TgpKC6g/7r+bL2wlZBLIewYugNfR199hyVEpZKESAgh9OBy6mU2n9/MzZyb+g5Fi72ZPb3q98LD1qPC5yYnJ6NUKnX2dAwdOhRHR0dMTEw0+3Jzc1m2bBmbN28mNjYWGxsbunbtysSJE7GxsSl1jYSEBDp37szw4cOZNm2a1rE33niDqKgoQkNDUSqVnDlzhrlz5/LPP/9QWFiIv78/kyZNokWLFppzunTpQrt27cjMzGT37t3UqlWLP//8EyMjo1L33rFjB4sWLeLSJfUn5A0bNmTMmDF07NiRQ4cOMWzYMAC++eYbvvnmG86ePQtAWFgYP/zwA6dOnSIrKwtbW1uCgoKYOnUqDg4OzJs3j++++w6Al19+mZYtW7JixQoAwsPDmT9/PqdOnQKgRYsWTJkyBW9v7zL/DyIiIkhNTaVnz55a+/v06cNXX31FSEiIJETivhWVFDF4w2C2XtgKwI2sG3Rc3pFd/9lFgFOAnqMTovLIkDkhhNCDTec2PXLJEMDNnJtsOrfpgc7t2rUrWVlZ9O/fnyVLlnDq1CmKiooAqFu3LqNHj8bNzQ1Qz7kZNmwYS5YsoVevXnz33XcMHTqU33//nUGDBpGVlVXq+k5OTrRv355NmzZpzUdKSUlh79699O3bF6VSSUREBAMHDiQ+Pp6PPvqIL7/8EgMDA4YPH05YWJjWNTds2EBxcTHz5s1j0qRJOpOh48ePM2nSJBo0aMC3337LN998g4GBAWPHjuXs2bM0atSIuXPnAjBw4EBWrVoFwMGDB3n11VcxNzfn888/Z9GiRfTv35+//vpL03vTv39/xo4dC8CMGTN4//33AQgJCWHkyJEoFAq++OILZsyYQVJSEgMHDuTcuXNl/h9cuHABoFTS4+TkhJmZmea4EPerhmENrW1XK1fcrNz0FI0QVUN6iIQQQlSKF198kfT0dObPn6/pKTE1NaVJkyZ069aNF198UZNw/PHHH0RERPD111/Tp08fADp06ICPjw+vvfYaP/30E+PGjSt1j/79+zNhwgTCw8MJCgoCYPPmzRQVFdG/f38AvvrqKywtLVm5ciUWFhYABAcH89JLLzFz5kw2b96suZ6RkRFffPEFpqamZb6vI0eOUFxczJtvvomTkxMAgYGBLFy4kPz8fCwtLfHz8wOgdu3aNG/eHICoqCjatm3L3LlzNdXj2rdvT3R0tCYxc3FxoW7dugB4e3vj7e2NSqVi5syZ1KtXj2XLlmFoqP5T3aVLF3r06MHXX3/NkiVLdMaamZkJgKWlZaljFhYWmuNC3A9DpSE/9f0JS2NLFv6zkAY1G7BtyDZsTW31HZoQlUp6iIQQQg96N+iNg5mDvsMoxcHMgd4Nej/w+a+99hphYWEsWrSIV199FW9vbw4fPszHH3+sSZhAPZTM2NiYXr16aZ3fvn17nJ2dOXjwoM7rd+7cmZo1a/LHH39o9m3cuJEWLVpQp04d8vLyOHr0KG3btqVGjRoUFRVpeqm6d+/OhQsXuH79uubcOnXqlJsMAQQFBWFoaMiAAQOYOXMmoaGhmJiY8N577xEQUPawoddee43vv/+eoqIiLl26xJ49e/j++++5dOlSucUXrly5QlxcHMHBwQCa92BiYkKnTp04ePAghYWFOs+91XNWVvlupVL+7IuKUSqUzO85ny+6fsHOoTtxsnDSd0hCVDrpIRJCCD3wsPVgXMvSPSBPAlNTUzp37kznzp0BSE1NZf78+axYsYKlS5fy9ttvk5aWhoODg84HdAcHBzIyMnRe28jIiOeff541a9bw0UcfERsbS1RUFF988QUA6enpFBcX8/vvv/P777/rvEZ8fDwuLuq1Vuzt7e/5fvz8/FixYgXLly9nw4YN/PzzzxgZGdG1a1c++OADatbUvYhlVlYWM2fOZMuWLeTl5eHs7EyjRo0wMzMrt6JcSkoKAAsWLGDBggVltrnVW3UnKysrQN1TdPfxrKwsnT1HQtyLQqFgWttp924oxGNKEiIhhBAPLSEhgRdeeIFhw4Zp5sTcYmtry3vvvcfWrVs1819sbGz4559/KCkpKZUUJSQk4OFRdlGHAQMGsGzZMkJCQoiOjsbCwoJnnnkGUA8LUygUPP/88wwZMkTn+eVduyxNmzaladOmFBcXExUVxfbt21m2bBkqlUozf+hukydP5uTJk8yaNYtWrVpphu9NnDhRU5xBF2trawBGjRqleV93s7XVPWTp1tyhK1euaFXzS0hIICcnRyr8iSqRmZ/Jn2f/ZHDAYH2HIsQDkb5zIYQQD83BwQFTU1PWrl2r6eG4U2JiIunp6fj4+ADQpk0bCgoKtObzAOzfv5+EhARatWpV5r28vLwIDAxk27ZtbNmyhV69emmGvZmbmxMYGMi5c+do2LAh/v7+mtfOnTtZsGBBhYeNzZkzhy5dupCfn4+BgQEBAQG8/fbb1KtXj2vXrgFgYGBQ6rwjR47Qtm1bgoODNclQRkYGR48e1eohujseT09PnJycuHDhglb8/v7+rFmzRtNDpUuTJk2wtrZm0ybtwhh//fUXCoWCDh06VOi9C3EveUV5PL/meYZsHML/Qv9XaetpCVGdpIdICCHEQ1MqlXzyySeMGTOG559/nsGDB9OoUSMMDQ2Jjo7m559/xsXFhREjRgDQt29f1qxZw/vvv09MTAz+/v6cP3+eBQsWULdu3TJ7d24ZMGAA//vf/yguLtYUU7hl6tSpDB8+nJEjRzJw4EAsLS0JCQnhl19+YfDgwZiZmVXovbVr146lS5cyevRohgwZgqmpKfv27ePs2bP897//BdRFDBQKBYcOHcLf35/WrVvTpEkTQkNDWblyJV5eXsTExPDjjz+SnJyMSqWisLAQIyMjTY/Qjh07MDIywtfXl+nTp/PWW28xceJE+vTpg7GxMRs2bGDr1q1MmzatzDlCxsbGjBs3jk8//ZQaNWrQo0cPIiIiWLRoEf369aNBgwYVeu9ClKewuJCXfn2J0CuhAMzYO4P0/HRmPTMLpUI+cxePD4VKUvn7EhsbS3BwMCEhIbi6uuo7HCHEY+TMmTM0bNhQ32FUi4sXL7Js2TKOHDlCYmIiJSUluLq60rVrV0aNGqU1hyUrK4t58+axbds2bt68ib29PV27dmX8+PGadYg2bNjAf//7X9auXUtgYKDWue3bt8fFxaVULxNAZGQk8+bN4+jRoxQVFVGnTh369+/PsGHDND0yXbp0wcPDgx9++OGe72vfvn0sXryY8+fPk5OTg4eHB4MGDWLQoEGaNl9//TWrV69GpVKxefNmDAwM+PTTTzl06BB5eXnUqlWLLl26UL9+faZPn86PP/5IUFAQubm5TJw4kfDwcNzd3fnrr78AOHDgAIsWLdKsQ+Tp6cnQoUPp27fvPeNdvXo1P//8M7GxsTg6OtK3b1/eeOMNTcW6x93T9DP1KItIiCDohyCyC7M1+5q7NGf3f3ZjbmxezplCVL/ynuUlIbpPkhAJIR6UPLwJUbnkZ+rRER4bzrOrniUtL41GDo3YO3wvNc10FxoRQp/Ke5aX/kwhhBBCCPFAWru2Zs/wPQS5BbFj6A5JhsRj6cnoOxdCCCGEEHoR4BTA/hH7y5zbJsSjTnqIhBBCCCHEQykvGUrPS+dmzs1qjEaIipGESAghhBBCVImcwhx6/9KbDj92IC4jTt/hCKGTJERCCCGEEKLS5Rfl029tP/bH7OfMzTO0/7E9l1LLXpRYCH2RhEgIIYQQQlS6bw5+w7aL2zTbl9MuM3XHVD1GJIRukhAJIYQQQohK91abt+jr01ez3dipMT88d+91v4SobpIQCSGEEEKISlfDsAa/vvgrQwOG4l3Tm+1Dt2NTw0bfYQlRipTdFkIIIYQQVcJQacjyvstJyU3B3sxe3+EIoZP0EAkhhBBCiCqjVCjLTYaKS4qrMRohSpOESAghhKgkKpVKL+cK8bjKzM+k3Y/tWHp0qb5DEU8xSYiEEEJUmqtXr/Luu+/SuXNn/Pz8aNmyJUOHDmXDhg2UlJRU+HobNmzA29ubEydOVH6wlaigoICvv/6an376qcLnJiQkMHHiRCIiIio9rvj4eFq1asXmzZsr/dpCPKy8ojyeX/M84bHhjN40mm/CvtF3SOIpJQmREEKISnHx4kX69+/PmTNneOONN1iyZAmffPIJbm5u/Pe//+XDDz/Ud4hVJjExkaVLl1JQUFDhcw8cOMC2bdsqvYfo2rVrDB8+nLS0tEq9rhCVZfCGwYReCdVsv73jbX46UfEPFYR4WFJUQQghRKX44YcfKC4uZuXKlZibm2v2d+/eHTMzM1asWMHw4cOpV6+eHqN88hUUFLBu3TrmzJmDgYGBvsMRokzDGw9n87nN5BfnA9Cydkv6N+qv56jE00gSIiGE0IPLl2HzZrh5U9+RaLO3h169wMOj4ucmJyejVCp19nQMHToUR0dHTExMNPtyc3NZtmwZmzdvJjY2FhsbG7p27crEiROxsbEpdY2EhAQ6d+7M8OHDmTZtmtaxN954g6ioKEJDQ1EqlZw5c4a5c+fyzz//UFhYiL+/P5MmTaJFixaac7p06UK7du3IzMxk9+7d1KpViz///BMjI6NS996xYweLFi3i0qVLADRs2JAxY8bQsWNHDh06xLBhwwD45ptv+Oabbzh79iwAYWFh/PDDD5w6dYqsrCxsbW0JCgpi6tSpODg4MG/ePL777jsAXn75ZVq2bMmKFSsACA8PZ/78+Zw6dQqAFi1aMGXKFLy9vcv9f9i7dy9ff/01w4YNo1mzZowePbrc9kLoSx/vPvw9+G+eW/McblZubHllCxbGFvoOSzyFZMicEELowaZNj14yBOqYNm16sHO7du1KVlYW/fv3Z8mSJZw6dYqioiIA6taty+jRo3FzcwPUvRjDhg1jyZIl9OrVi++++46hQ4fy+++/M2jQIP6/vfsOi+JqHz7+XboIUaPEXrCAKKAoiAGUpsGgKAaj2IkYYuKjxtiIJSaWGKIxVkRjr5hHsfNYgv2HYoklGruiECOoWEBU2rx/8DJxQ7Emq3B/rmuva3fmnJl7zoq7954yaWlp+Y5fsWJFWrRowebNm7XmI6WkpLB37178/f3R09Pj5MmTBAYGcuPGDb7++mu+//579PX1CQoKIjY2VuuYUVFRZGdnM3PmTAYNGlRgMnTs2DEGDRqElZUVM2bM4IcffkBfX59+/fpx7tw5GjRowPTp0wEIDAxkxYoVABw4cIDg4GBKly7Nd999R0REBAEBAWzatInx48cDEBAQQL9+/QAYN24co0ePBiAmJoY+ffqg0WgICwtj3Lhx3Lx5k8DAQM6fP1/k+2BnZ8fOnTv54osvMDExefobJ4QOeVp6srPXTrb33E550/K6DkeUUNJDJIQQ4pX48MMPuXfvHrNnz1Z7SkqVKoWDgwOtW7fmww8/VBOODRs2cPLkSaZMmYKfnx8ALVu2pH79+vTt25clS5bQv3//fOcICAhgwIABHDx4EBcXFwC2bNlCVlYWAQG5Q20mT56Mubk5y5cvx8ws99dmb29vOnfuzMSJE7UWGDA0NCQsLIxSpUoVel2HDx8mOzubzz//nIoVKwLQuHFj5syZw+PHjzE3N8fW1haAqlWr4ujoCMDp06dxdXVl+vTpaDQaAFq0aMHZs2fVxKxKlSrUrFkTAGtra6ytrVEUhYkTJ1K3bl0WLlyIgUHuR7WXlxdt2rRhypQpzJs3r9B482IU4k3hVNXp6YWE+AdJD5EQQuhAu3ZgYaHrKPKzsMiN7UX17duX2NhYIiIiCA4OxtramkOHDvHNN9+oCRPkDiUzMjKibdu2WvVbtGhB5cqVOXDgQIHH9/T0pHz58mzYsEHdtm7dOpycnKhRowaPHj3i6NGjuLq6YmJiQlZWltpL9d5773Hx4kWuX7+u1q1Ro0aRyRCAi4sLBgYGdOrUiYkTJ7Jr1y6MjY0ZNWoU9vb2RbbF/PnzycrK4vLly+zZs4f58+dz+fLlIhdfiI+P548//sDb2xtAvQZjY2M8PDw4cOAAmZmZRcYsRHFx5+Eduq7typ+pf+o6FFGMSQ+REELogKUlFNABUiyUKlUKT09PPD09Abhz5w6zZ89m2bJl/PTTTwwdOpS7d+9iYWGBnl7+3+UsLCy4f/9+gcc2NDSkQ4cOREZG8vXXX5OYmMjp06cJCwsD4N69e2RnZ7N+/XrWr19f4DFu3LhBlSpVAKhQofCbReaxtbVl2bJlLF68mKioKJYuXYqhoSGtWrVizJgxlC9f8DCftLQ0Jk6cSHR0NI8ePaJy5co0aNAAU1PTIleUS0lJASA8PJzw8PBCy0hPkCjuHmQ8oN2qdsQmxBKXGMeOnjuo83YdXYcliiFJiIQQQry0pKQkOnbsSK9evdQ5MXnKlSvHqFGj2Lp1qzr/pWzZshw5coScnJx8SVFSUhKWRazq0KlTJxYuXEhMTAxnz57FzMwMHx8fAMzMzNBoNHTo0IEePXoUWL+oYxemSZMmNGnShOzsbE6fPs327dtZuHAhiqKo84f+bvDgwZw4cYKpU6fi7OysDt8bOHCgujhDQcqUKQPAxx9/rF7X35UrV+65r0GIN0lGdgad/tuJ2ITc4aVX7l7BdaErez/ai1V5Kx1HJ4obGTInhBDipVlYWFCqVClWr16t9nA8KTk5mXv37lG/fn0A3n33XTIyMvLdMHT//v0kJSXh7Oxc6Lnq1KlD48aN2bZtG9HR0bRt21Yd9la6dGkaN27M+fPnsbGxwc7OTn388ssvhIeHF9grVZRp06bh5eXF48eP0dfXx97enqFDh1K3bl0SEhIAClze+vDhw7i6uuLt7a0mQ/fv3+fo0aNaPUR/j6d27dpUrFiRixcvasVvZ2dHZGSk2kMlRHGWnZONoZ72v3Or8lZUf6u6jiISxZkkREIIIV6anp4eEyZM4Pbt23To0IGIiAj27t1LbGwsCxcupEuXLlSpUoWPPvoIAH9/fxo2bMjo0aOZPXs2e/fuZcGCBQwaNIiaNWsW2ruTp1OnTsTExPDHH3+oiynkGTZsGBcvXqRPnz5ER0ezb98+vv76ayIiIqhcuTKmpqbPdW1ubm4kJSUREhLCjh072L9/P5MmTeLcuXO0b98eAHNzczQaDXFxcRw4cABFUXBwcGDXrl0sX76cAwcOsHr1ajp37szt27fJzMxU5wHl9Qjt2LGD06dPo6enR2hoKLt372bgwIHs2LGDPXv2MGjQINasWUP9+vXVRRqEKK5KGZZibee19LTvCUCjio3Y1HUTpQyLnvMnxIuQIXNCCCFeiXfffZd169axcOFCoqKiiIiIICcnh2rVqtG+fXs+/vhjzM3NATAyMmLp0qXMnDmT//73v8yZM4cKFSrQsWNH/vOf//DWW28Vea7333+fb7/9lipVqtCoUSOtfU2bNmXlypXMnDmTMWPGkJWVRY0aNfjyyy/V+wU9D0dHRyIiIpg7dy6jR48mPT0dS0tLvv76a7p27QrkDtXr27cvK1eu5LPPPmPLli189913fPvtt8yaNYtHjx5RqVIlvLy8qFevHqGhoRw+fBgXFxeaN29Oy5YtWbp0KXv37mXTpk34+vpSpkwZIiIi1Hsu1a5dm7CwMPz9/Z/7GoR4ExnqG7LYfzH1K9Qn2CGYMiZldB2SKKY0SlEzO4UqMTERb29vYmJiqFatmq7DEUK8Qc6cOYONjY2uwxCi2JC/KSHE8yrqu7wMmRNCCCGEEG+kW+m3mHVoVpErNwrxNDJkTgghhBBCvHFSH6fy/or3OXL9COdvn2dam2noaeS3fvH85F+NEEIIIYR4ozzKeoT/an+OXD8CwMxDM+m1rheZ2XLTYvH8JCESQgghhBBvlEspl/j1z1+1tt1Iu0GOkqOjiMSbTBIiIYQQQgjxRmn4TkP2Bu2lslllAJyqOLGuyzqMDYx1HJl4E0lCJIQQQggh3jh2Fe3Y32c/7azaEd09GnNjc12HJN5QsqiCEEIIIYR4I9UuV5tNXTfpOgzxhpMeIiGEEEIIUSz9cf8PTiad1HUY4jUnCZEQQgghhCh27jy8Q5sVbWixqAW743frOhzxGpOESAghhBBCFCsPMx/SPrI9p5JPcf/xfXyW+7D297W6Dku8piQhEkIIIV4RRVF0UlcIoS3yVCT7r+1XX2dkZ7DitxXydyYKJAmREEKIV+bq1auMHDkST09PbG1tadasGT179iQqKoqcnOe/P0hUVBTW1tYcP3781Qf7CmVkZDBlyhSWLFny3HWTkpIYOHAgJ0++mnkOGRkZ/PTTT7Rr147GjRvj5eXFyJEjSU5OfiXHF+JNENQ4iEnek9TXbjXcWPHBCjQajQ6jEq8rSYiEEEK8EpcuXSIgIIAzZ87w2WefMW/ePCZMmED16tX58ssv+eqrr3Qd4j8mOTmZn376iYyMjOeu+3//939s27btlf1y/dVXXzF9+nS8vb2ZOXMmffv2Ze/evXTq1Ik7d+68knMI8brTaDSEuoWysP1CGldqzMbAjZQyLKXrsMRrSpbdFkII8UosWLCA7Oxsli9fTunSpdXt7733HqampixbtoygoCDq1q2rwyiLt6tXr7Ju3ToGDRrEZ599pm63srKie/fuREVFERwcrMMIhfh3feTwET0b9cRAT77yisLJvw4hhNCFK1dgyxa4dUvXkWirUAHatgVLy+euevv2bfT09Ars6ejZsyfvvPMOxsZ/3UX+4cOHLFy4kC1btpCYmEjZsmVp1aoVAwcOpGzZsvmOkZSUhKenJ0FBQQwfPlxr32effcbp06fZtWsXenp6nDlzhunTp3PkyBEyMzOxs7Nj0KBBODk5qXW8vLxwc3MjNTWV3bt3U6lSJTZu3IihoWG+c+/YsYOIiAguX74MgI2NDZ988gnu7u7ExcXRq1cvAH744Qd++OEHzp07B0BsbCwLFizg1KlTpKWlUa5cOVxcXBg2bBgWFhbMnDmTWbNmAdClSxeaNWvGsmXLADh48CCzZ8/m1KlTADg5OTFkyBCsra0LfQ8ePnzIhx9+iI+Pj9b2vDo3btwotK4QxVVRyVDCvQTMjc0pa1L23wtIvHZkyJwQQujC5s2vXzIEuTFt3vxCVVu1akVaWhoBAQHMmzePU6dOkZWVBUDNmjUJCQmhevXqQO48l169ejFv3jzatm3LrFmz6NmzJ+vXr6dr166kpaXlO37FihVp0aIFmzdv1pqPlJKSwt69e/H390dPT4+TJ08SGBjIjRs3+Prrr/n+++/R19cnKCiI2NhYrWNGRUWRnZ3NzJkzGTRoUIHJ0LFjxxg0aBBWVlbMmDGDH374AX19ffr168e5c+do0KAB06dPByAwMJAVK1YAcODAAYKDgyldujTfffcdERERBAQEsGnTJsaPHw9AQEAA/fr1A2DcuHGMHj0agJiYGPr06YNGoyEsLIxx48Zx8+ZNAgMDOX/+fKHvQf369ZkwYQJ16tTR2r5t2zYgN5ETQuS6lX6LVsta0XJRS66nXtd1OEKHpIdICCHEK/Hhhx9y7949Zs+erfaUlCpVCgcHB1q3bs2HH36oJhwbNmzg5MmTTJkyBT8/PwBatmxJ/fr16du3L0uWLKF///75zhEQEMCAAQM4ePAgLi4uAGzZsoWsrCwCAgIAmDx5Mubm5ixfvhwzMzMAvL296dy5MxMnTmTLli3q8QwNDQkLC6NUqcLnFhw+fJjs7Gw+//xzKlasCEDjxo2ZM2cOjx8/xtzcHFtbWwCqVq2Ko6MjAKdPn8bV1ZXp06erE7lbtGjB2bNn1cSsSpUq1KxZE8jtxbG2tkZRFCZOnEjdunVZuHAhBga5H9VeXl60adOGKVOmMG/evGd+X86fP8/3339P3bp1adeu3TPXE6I4S8tIo+3Ktpy/nfsDg8sCF7b33I5VeSsdRyZ0QXqIhBBCF9q1AwsLXUeRn4VFbmwvqG/fvsTGxhIREUFwcDDW1tYcOnSIb775Rk2YIHcomZGREW3bttWq36JFCypXrsyBAwcKPL6npyfly5dnw4YN6rZ169bh5OREjRo1ePToEUePHsXV1RUTExOysrLUXqr33nuPixcvcv36X78E16hRo8hkCMDFxQUDAwM6derExIkT2bVrF8bGxowaNQp7e/si22L+/PlkZWVx+fJl9uzZw/z587l8+XKRiy/Ex8fzxx9/4O3tDaBeg7GxMR4eHhw4cIDMzMwiY85z5MgRevbsiYmJCXPmzMHIyOiZ6glR3E3YO4FDfxxSX1+9d5UfD/yow4iELkkPkRBC6IKlJRTQA1IclCpVCk9PTzw9PQG4c+cOs2fPZtmyZfz0008MHTqUu3fvYmFhgZ5e/t/lLCwsuH//foHHNjQ0pEOHDkRGRvL111+TmJjI6dOnCQsLA+DevXtkZ2ezfv161q9fX+Axbty4QZUqVQCoUKHCU6/H1taWZcuWsXjxYqKioli6dCmGhoa0atWKMWPGUL58+QLrpaWlMXHiRKKjo3n06BGVK1emQYMGmJqaFrmiXEpKCgDh4eGEh4cXWiavt6owkZGRTJgwgWrVqvHTTz+pwxWFEPCV+1f8fvN3Np3fBIC3pTfT2kzTbVBCZyQhEkII8dKSkpLo2LEjvXr1UufE5ClXrhyjRo1i69at6vyXsmXLcuTIEXJycvIlRUlJSVgWsahDp06dWLhwITExMZw9exYzMzN1EQEzMzM0Gg0dOnSgR48eBdYv6tiFadKkCU2aNCE7O5vTp0+zfft2Fi5ciKIo6vyhvxs8eDAnTpxg6tSpODs7q8P3Bg4cqC7OUJAyZcoA8PHHH+dbHCFPuXLlCq2vKArffvstS5cupXnz5syYMUM9phAil6mhKVFdovhk0yccTzpOVJcojA2Mn15RFEsyZE4IIcRLs7CwoFSpUqxevVrt4XhScnIy9+7do379+gC8++67ZGRkaM3nAdi/fz9JSUk4OzsXeq46derQuHFjtm3bRnR0NG3btlWHvZUuXZrGjRtz/vx5bGxssLOzUx+//PIL4eHhBfZKFWXatGl4eXnx+PFj9PX1sbe3Z+jQodStW5eEhAQA9PX189U7fPgwrq6ueHt7q8nQ/fv3OXr0qFYP0d/jqV27NhUrVuTixYta8dvZ2REZGan2UBUmLxnq3LkzCxYskGRIiEIY6Bkwv/18dvXexVvGb+k6HKFD0kMkhBDipenp6TFhwgQ++eQTOnToQPfu3WnQoAEGBgacPXuWpUuXUqVKFT766CMA/P39iYyMZPTo0Vy7dg07OzsuXLhAeHg4NWvWLLR3J0+nTp0YO3Ys2dnZ6mIKeYYNG0ZQUBB9+vQhMDAQc3NzYmJiWLVqFd27d8fU1PS5rs3NzY2ffvqJkJAQevToQalSpdi3bx/nzp3jyy+/BMDc3ByNRkNcXBx2dnY0b94cBwcHdu3axfLly6lTpw7Xrl1j0aJF3L59G0VRyMzMxNDQUE1YduzYgaGhIQ0bNiQ0NJQvvviCgQMH4ufnh5GREVFRUWzdupXhw4erizT83YEDB1i6dCn169enQ4cOHD9+XGv/O++8Q40aNZ7r+oUozjQaTZHJUFJaEhalLdDTSB9CcaZRXtWtsYu5xMREvL29iYmJoVq1aroORwjxBjlz5kyJWe740qVLLFy4kMOHD5OcnExOTg7VqlWjVatWfPzxx5ibm6tl09LSmDlzJtu2bePWrVtUqFCBVq1a8Z///Ee9D1FUVBRffvklq1evpnHjxlp1W7RoQZUqVfL1MgH89ttvzJw5k6NHj5KVlUWNGjUICAigV69eao+Ml5cXlpaWLFiw4KnXtW/fPubOncuFCxdIT0/H0tKSrl270rVrV7XMlClTWLlyJYqisGXLFvT19fn222+Ji4vj0aNHVKpUCS8vL+rVq0doaCiLFi3CxcWFhw8fMnDgQA4ePEitWrXYtCl3TsP//d//ERERod6HqHbt2vTs2RN/f/9C4xwzZgw///xzofu7dOnCuHHjnnq9r7uS9DcldOfP1D9xXejKu9XfZWH7hTKk7g1X1Hd5SYiekSREQogXJV/ehHi15G9K/NPuPbqH+2J3TiSdAMCzlifruqyjjIkMQX1TFfVdvsT0/23cuBEHBweth7W1tfpLnBBCCCGEEAAhm0PUZAhgV/wuZh6aqcOIxD+pxMwhat++Pe3bt1dfL1myhP/973+0adNGh1EJIYQQQojXzSTvSRz78xgXUi4A4GflR6hbqI6jEv+UEpMQPSkxMZFZs2axZs2aIlfqEUIIIYQQJU/tcrWJDY7Fb5UfOUoOqwJWYaBXIr82lwgl8p2dNm0aXbp0oWbNmroORQghhBBCvIYqmFYgplcM6ZnplDYqretwxD+o2Mwh+uWXX7C2ts73mDlTe7xnUlISO3fuVJd+FUIIIYQQoiCmhqZUMK1Q6P7kB8nI+mRvvmLTQ+Tt7c3p06fzbf/7De+2bNlCy5YtKV++/L8VmhBCCCGEKGau3r1K8wXN6dygM1N9pqKvl/8GzeLNUGx6iDQaDQYGBvkef0+I9uzZw3vvvaejKIUQQgghxJsu5WEK7694nxtpN5hxaAZd1nThYeZDXYclXlCxSYiehaIonDp1ikaNGuk6FCGEEEII8Ybqta4XZ26dUV+vPbOW+b/O12FE4mWUqITo3r17pKWlYWFhoetQhBBCCCHEG2qi10SqmFdRX3/Y4EP6N+uvw4jEy3itEqKsrCwCAwP56quv8u07duwYPXv2pEmTJjg7OzNs2DBu3rz5XMcvW7Ys586dw8jI6FWFLIQQQgghSphGlRpxIPgADSwa0KJGC5Z2XIqe5rX6Wi2ew2uzqEJ6ejrDhg3j2LFjWFlZae07c+YMH330EXZ2doSFhXH79m2mTZvG77//zrp16yTBEUII8UZTFAWNRvOv1xVCvLgaZWqw/6P9AJgYmOg4GvEyXotUds+ePXzwwQccOXKkwP3Tp0+nTJkyzJ8/n9atWxMYGMi8efO4ePEia9eu/ZejFUIIUZDQ0NB8tz5o0KABTk5OdOvWjejo6ALrHTp0iCFDhuDp6YmdnR0eHh7079+/0M8EyL3Vwn/+8x+1jpOTEz169CAyMpLMzMx/6hJViYmJWFtbM2/evJc6zv379xkzZgzbtm177rqXLl2iV69e3Lp166ViyKMoCgsXLuS9997Dzs6O9957j8WLF8uSwkIUoVypcpQrVa7Q/ckPkv/FaMSL0nlCdP/+fT755BOsra3ZuHFjvv0ZGRnExsbi7e2NsbGxut3e3p5atWoRExPzb4YrhBCiCIaGhqxYsUJ9LFmyhAkTJpCTk8PgwYNZv369VvlJkybRs2dPUlJSGDhwIHPnzuXzzz/n9u3b9OjRg+XLl2uVz8jIYMCAAfTv35+cnBwGDRrEvHnz+Pbbb6lZsybjx4/n448/Jisr61+86hd35swZfv75Z7Kzs5+7bnR0NHFxca8slmnTpjF58mTee+89Zs6cSYsWLZg0aRJz5sx5ZecQoiQ5f/s8NrNtGBkzkhwlR9fhiCLofMiciYkJW7ZsoU6dOgXuT0hI4PHjxwXut7S05OzZs/90iEIIIZ6RRqPB0dEx33ZXV1fc3d2ZP38+/v7+AKxcuZLFixfz6aef8vnnn2uVb9euHf369WPixIk0b96cunXrAjBlyhS2b9/OuHHj6NKli1ad1q1b4+rqyuDBg1mxYgW9e/f+R66xOEpKSmLBggUEBQUxdOhQADw8PMjJySEiIoIePXrw1ltv6ThKId4cNx/cxHeFLykPU5i0fxKX71xmsf9iGVr3mtJ5D5GRkVGhyRBAamoqAGZmZvn2lS5dWt0vhBBvIs03Gq1HYeYdnadVLmRTSKFlm85rqlX26PWjBZY7ev2oVrmm85q+9PUUxszMjNq1axMfHw9AdnY2c+bMoVatWgwYMCBfeQMDA0JDQ+nWrRuPHj0CIDk5meXLl+Pu7p4vGcrj6+uLv78/hoaG6raePXvSr18/xo0bh6OjI25ubqSkpJCZmcmcOXNo3749jRo1ws7ODl9fXxYvXqx1zKysLGbNmoWXlxf29vYEBgZy+fLlZ7ru1atX4+fnR6NGjXB0dCQ4OJjjx48DEBUVRa9evQD44osv8PLyUutFR0fTo0cPHB0dsbW1xcPDg3HjxpGWlgbkDk+cNWsWAG5uboSGhqp1t23bxocffoi9vT3NmjVj8ODB/PHHH0XGeeDAATIzM/H19dXa7ufnx+PHj9m/f/8zXa8QInf4abeobly6c0ndtvr0alb9tkqHUYmi6LyH6GlycnK7GAubMPr3G68KIYR4/WRkZJCQkECVKrnL1J4+fZrk5GT69OmDvn7Bd3evW7cuY8aMUV/v2LGD7OxsOnbsWOS5wsLC8m3bv38/Dg4O/Pjjj9y+fZu3336bESNGsHXrVgYOHIiNjQ2pqamsXLmSSZMmUbNmTTw9PQEYM2YMGzZsoE+fPjg5OXH06FEGDhz41GuOjo7mq6++Ijg4GFdXV1JTUwkPD+ejjz7il19+wd3dndGjRzNhwgQGDBiAu7s7AGvXrmXkyJEEBgYSEhKCoijs2rWLFStWYGxszIgRI+jXrx8ajYaoqCjCw8PVHxaXL1/O+PHj8fHxoV+/fty9e5fZs2fTpUsXoqKieOeddwqM9eLFiwD5fqC0tLQE4MKFC0+9XiFELo1GwwTPCZxMOqnOIQpqHERQ4yDdBiYK9donRGXKlAEosCfowYMHmJub/9shCSGEKMKT83cyMjK4du0a4eHh3LlzR+0Nun79OgA1atR45uPm9S79/Uu7oigFzsExMPjrIy4zM5OwsDA1IcvIyODWrVsMGTJE7aUBcHJy4t133yU2NhZPT0/i4+OJiooiODhYHUrm7u6OkZERM2fOLDLeuLg4TE1N+fzzz9XVUOvVq8eaNWtITU2lVq1a6qqqlpaW2NnZAbnJh7+/P9988416LHd3d44cOUJsbCwAtWrVUq/F3t4eCwsL0tLSmDp1Ki1btmTGjBlqXTc3N3x8fJgzZw5jx44tMNb79++jr6+Pqamp1va80RkyGkOI5+NczZmDwQfxXelLFfMqzG03V1aDfI299glR9erVMTQ0VD8In3TlyhV1XLkQQgjdy8jIoGHDhvm2ly9fnmHDhtG9e3cAtVfoeRYTKGy1s3379vHxxx/n237u3Dn1ubm5uZpAQO5w7QULFgC5ycDVq1dJSEjgt99+U68DUBctaNOmjdaxO3To8NSEqEWLFkRGRtKuXTt8fHxwcXGhSZMmjBgxosh6ecPfHj9+rMZ17tw5UlJS1B8JC3L8+HEePHiAj4+PVlJavnx5mjZtyt69ewutW9jS3XnbZDSGEM/PspwlsX1i0Wg0GOnLLWJeZ699QmRkZISLiwsxMTEMGTIEE5PcyWgnT54kPj5e65c9IYR40yhjn21J45CmIYQ0LXze0JOOhhQ8Z+jvmlZp+sznf1aGhoasWvXXOHkDAwPKlCmjlYwAVKtWDchdOKco165dU3uR8upcu3ZN6351TZo0Yc2aNerrefPmsX37dq3jVKhQId+xjxw5wuTJkzl+/DiGhobUrl0bBwcH4K/k686dO0BuUvGkwoaePalVq1bMnTuXFStWsHTpUubNm4epqSnt2rVj5MiRlCpVqsB6SUlJTJw4kZiYGHJycqhWrRq2traYmJgUuQR2SkoKAKNGjWLUqFH59j85r+rv3nrrLbKysnj48KFWXHk9QzIaQ4gXU9SS3ACX71zGsqyl9B7p2GufEAEMGDCArl27EhQUxEcffcS9e/eYOnUqdevWpVOnTroOTwghxP+n0WjUoV9FqV+/PpUqVWL37t0MHz68wHlEly5dwtfXl27dujF27Fhat25NWFgYGzdupFWrVmo5MzMzrXOWK1f0FxDITcT69u1LkyZN2Lx5M7Vr10ZfX5/09HQiIyPVcnmJ0M2bN6lataq6PS/5eBoPDw88PDzIyMjgxIkTbNq0idWrV2Nubs7w4cPzlVcUheDgYNLT01m0aBH29vbqD4EBAQE8ePCg0HPl9R6NGjVKTeyeVd4wxPj4eGxsbNTtV65cAZDRGEL8A04ln8J1oSsfNviQOW3nYKhf+I8W4p/1RvSB29nZqUMbhg0bxo8//kjLli1ZvHix1r2JhBBCvBk0Gg0hISHEx8cTHh6eb39WVhYTJ05ET0+Pzp07A1C1alUCAwPZtm2bVtLypIcPH6pf4oty6tQpHj58SFBQEPXq1VMTsp07dwJ/9RC5uLigp6fHhg0btOr/vQeqIF9++SWdOnVCURSMjIxwcnJi3LhxmJubk5iYCJAvEbxz5w4XLlzA19eXZs2aqclQYmIi586d0+oh+vswNgcHB0xMTLh69Sp2dnbqw8bGhoiICLZs2VJorC1atEBfX5/Nmzdrbd+0aRMmJiY4Ozs/9XqFEM/ueup1fFf4cv/xfRYcW0DblW259+iersMqsV67HqInx3w/ydnZudAPQCGEEG+ebt26ceLECWbNmsWJEyfw8/PDwsKChIQEVq5cyblz5xg/frxWj8XIkSN58OABY8eOZevWrbz//vtUr16d9PR0fv31V9avX09KSgoBAQFFntvW1hZDQ0NmzJhBRkYGxsbGxMXFsWTJEjQaDenp6UBuEhYUFMSiRYvQ19enRYsWnD59miVLljz1+tzc3IiKimLQoEF07NgRPT09Nm/eTGpqKn5+fgDqvX327t1LpUqVaNq0KTVr1iQqKgpLS0uqVKnC+fPnmT9/PllZWWpc8FeP0JYtW3B1daVevXoMHDiQyZMnk5GRgZeXF9nZ2Sxbtoy4uDimTp1aaKwWFhb06NGDBQsWkJ2dTfPmzdm3bx8rV65k4MCBvP3220+9XiHEs+u9vjcJ9/8aMrzj8g7WnllLH4c+Ooyq5NIoRQ1IFqrExES8vb2JiYlRx7ELIcSzOHPmjNaX+uIqNDSULVu2qAsTPKvo6GjWrl3L+fPnuXv3LhYWFtja2tK3b1/s7e0LrHPw4EGioqL49ddfSU5OxsDAgBo1avDuu+/SqVMnrZXoevbsyc2bN9m6davWMXbv3s2MGTO4fPkyxsbGWFpa0qtXLzZt2sTZs2eJiYlBT08PRVFYvHgxq1at4s8//6ROnTqMGDGCoKAghgwZQkhI4XO7Nm3axJIlS7hy5QpZWVlYWVkRHBysLtKQk5NDaGgo27Ztw8TEhH379pGQkMCkSZM4ceIE2dnZVK1aFV9fX/T19fnhhx/Yvn07NWvW5ObNmwwYMIBTp07h6urK3LlzgdwEadGiRZw/fx4jIyOsra35+OOP8fDwKPJ9yM7OJiIigrVr13Lz5k2qVKlC9+7dX8u5uiXlb0oUX2dvncV3hS9X7ub2aH/m+BmzfGfJXKJ/UFHf5SUhekaSEAkhXpR8eRPi1ZK/KVEcJD9Ipv2q9pQ3Lc+GwA0Y6L12A7eKlaK+y0vLCyGEEEII8S97p/Q77Oq9i2wlW5IhHXsjFlUQQgghhBCiuCllWAozI7NC9x9MPMjjrMf/YkQlkyREQgghhBBCvGYO/3EYryVeeC/15uaDm7oOp1iThEgIIYQQQojXSOL9RDpEduBh1kP+L+H/cJ7vzOnk07oOq9iShEgIIYQQQojXyJDtQ/gz7U/19ZW7Vzh8/bAOIyreJCESQgghhBDiNRLRNgIvSy/19dB3hxLUOEh3ARVzsqSFEEIIIYQQr5FypcqxtftW/hP9H248uMF3rb7TdUjFmiREQgghhBBCvGYM9Q2JaBdBZk4m+nr6ug6nWJMhc0IIIYQQQryGNBoNRvpGhe5ff3Y9l1Iu/YsRFU+SEAkhhBBCCPGG2X9tP13WdMF5vjN74vfoOpw3miREQgghhI4piqKTukKIN9PlO5fpuLojGdkZ3H54m1bLWrHg1wW6DuuNJQmREEKIVyI0NBRra2utR4MGDXBycqJbt25ER0cXWO/QoUMMGTIET09P7Ozs8PDwoH///hw5cqTQc/3yyy/85z//Ues4OTnRo0cPIiMjyczM/KcuUZWYmIi1tTXz5s17qePcv3+fMWPGsG3btueue+nSJXr16sWtW7deKoaCnDlzBltbW44fP/7Kjy2EeHkLfl3ArfS//vazcrIoY1JGhxG92WRRBSGEEK+MoaEhixcvVl8rikJKSgqLFi1i8ODBZGRk4O/vr+6fNGkSixcvxsXFhYEDB1KxYkWSk5OJjIykR48ejB49mh49eqjlMzIyGDJkCNu3b8fb25tBgwZRsWJF0tLS2L17N+PHj2fr1q3Mnz8fA4PX/yPuzJkz/PzzzzRv3vy560ZHRxMXF/fKY/r999/5+OOP/5XEUgjxYiZ4TcDEwISvdn8FwFctv6JTg046jurN9fp/WgghhHhjaDQaHB0d8213dXXF3d2d+fPnqwnRypUrWbx4MZ9++imff/65Vvl27drRr18/Jk6cSPPmzalbty4AU6ZMYfv27YwbN44uXbpo1WndujWurq4MHjyYFStW0Lt373/kGourBw8esHjxYiIiIjA1NdV1OEKIImg0Gsa4j6HhOw1Zd3YdYz3G6jqkN5oMmRNCCB3SaLQfhZk3T7tcSEjhZZs21S579GjB5Y4e1S7XtOnLXUtRzMzMqF27NvHx8QBkZ2czZ84catWqxYABA/KVNzAwIDQ0lG7duvHo0SMAkpOTWb58Oe7u7vmSoTy+vr74+/tjaGiobuvZsyf9+vVj3LhxODo64ubmRkpKCpmZmcyZM4f27dvTqFEj7Ozs8PX11erhAsjKymLWrFl4eXlhb29PYGAgly9ffqbrXr16NX5+fjRq1AhHR0eCg4PVYWhRUVH06tULgC+++AIvr79uwhgdHU2PHj1wdHTE1tYWDw8Pxo0bR1paGpA7PHHWrFkAuLm5ERoaqtbdtm0bH374Ifb29jRr1ozBgwfzxx9/PDXW//73vyxdupTBgwczdOjQZ7o+IYRufWDzAcs6LkNPI1/pX4b0EAkhhPjHZWRkkJCQQJUqVQA4ffo0ycnJ9OnTB339gu+vUbduXcaMGaO+3rFjB9nZ2XTs2LHIc4WFheXbtn//fhwcHPjxxx+5ffs2b7/9NiNGjGDr1q0MHDgQGxsbUlNTWblyJZMmTaJmzZp4enoCMGbMGDZs2ECfPn1wcnLi6NGjDBw48KnXHB0dzVdffUVwcDCurq6kpqYSHh7ORx99xC+//IK7uzujR49mwoQJDBgwAHd3dwDWrl3LyJEjCQwMJCQkBEVR2LVrFytWrMDY2JgRI0bQr18/NBoNUVFRhIeHU6dOHQCWL1/O+PHj8fHxoV+/fty9e5fZs2fTpUsXoqKieOeddwqN18vLiw8//JDSpUsTFRX11OsTQrz+Zh+azZ1HdxjVYhSaon51K+EkIRJCCPFKZWVlqc8zMjK4du0a4eHh3LlzR+0Nun79OgA1atR45uPm9S7lffnPoygK2dnZ+co/OYcoMzOTsLAwNSHLyMjg1q1bDBkyRO2lAXBycuLdd98lNjYWT09P4uPjiYqKIjg4WO01cXd3x8jIiJkzZxYZb1xcHKampnz++ecYGeXeR6RevXqsWbOG1NRUatWqhZWVFQCWlpbY2dkBcOHCBfz9/fnmm2/UY7m7u3PkyBFiY2MBqFWrlnot9vb2WFhYkJaWxtSpU2nZsiUzZsxQ67q5ueHj48OcOXMYO7bwYTXP814IIV5/MZdjGLR1ENlKNieTTrKowyJKG5XWdVivJUmIhBBCvDIZGRk0bNgw3/by5cszbNgwunfvDqD2ChWUyBSmsOWl9+3bx8cff5xv+7lz59Tn5ubmagIBYGRkxIIFuUvU3r9/n6tXr5KQkMBvv/2mXgegLlrQpk0brWN36NDhqQlRixYtiIyMpF27dvj4+ODi4kKTJk0YMWJEkfXyhr89fvxYjevcuXOkpKRQpkzhq0gdP36cBw8e4OPjo5WUli9fnqZNm7J3794izyuEKD4upVyi85rOZCu5/8f+9/f/ciPtBnuC9khPUQEkIRJCCB161lvIhIQUPW/oSYXNGfq7pk2f/fzPytDQkFWrVqmvDQwMKFOmjFYyAlCtWjUAEhISijzetWvX1J6LvDrXrl1Te1YAmjRpwpo1a9TX8+bNY/v27VrHqVChQr5jHzlyhMmTJ3P8+HEMDQ2pXbs2Dg4OwF/J1507d4DcpOJJRQ09y9OqVSvmzp3LihUrWLp0KfPmzcPU1JR27doxcuRISpUqVWC9pKQkJk6cSExMDDk5OVSrVg1bW1tMTEyKvOdQSkoKAKNGjWLUqFH59j85r0oIUbwlPUhCw1+JjwYNw1yGSTJUCEmIhBBCvDIajUYd+lWU+vXrU6lSJXbv3s3w4cMLnEd06dIlfH196datG2PHjqV169aEhYWxceNGWrVqpZYzMzPTOme5cuWeev6EhAT69u1LkyZN2Lx5M7Vr10ZfX5/09HQiIyPVcnmJ0M2bN6lataq6PS/5eBoPDw88PDzIyMjgxIkTbNq0idWrV2Nubs7w4cPzlVcUheDgYNLT01m0aBH29vaYmJgAEBAQwIMHDwo9V17v0ahRo9TETghRMrlUd+Hwx4fpENmB35J/Y4LXBPys/XQd1mtLlqQQQgjxr9NoNISEhBAfH094eHi+/VlZWUycOBE9PT06d+4MQNWqVQkMDGTbtm1aScuTHj58yJUrV556/lOnTvHw4UOCgoKoV6+empDt3LkT+KuHyMXFBT09PTZs2KBV/+89UAX58ssv6dSpE4qiYGRkhJOTE+PGjcPc3JzExESAfIngnTt3uHDhAr6+vjRr1kxNhhITEzl37pxWD5GenvZHuIODAyYmJly9ehU7Ozv1YWNjQ0REBFu2bHlqzEKI4sOynCWxwbFM85nGl25f6jqc15r0EAkhhNCJbt26ceLECWbNmsWJEyfw8/PDwsKChIQEVq5cyblz5xg/fjw2NjZqnZEjR/LgwQPGjh3L1q1bef/996levTrp6en8+uuvrF+/npSUFAICAoo8t62tLYaGhsyYMYOMjAyMjY2Ji4tjyZIlaDQa0tPTgdwkLCgoiEWLFqGvr0+LFi04ffo0S5Yseer1ubm5ERUVxaBBg+jYsSN6enps3ryZ1NRU/Pxyf6l96623ANi7dy+VKlWiadOm1KxZk6ioKCwtLalSpQrnz59n/vz5ZGVlqXHBXz1CW7ZswdXVlXr16jFw4EAmT55MRkYGXl5eZGdns2zZMuLi4pg6derzvUFCiDeemZEZg5oPKnS/oijcfnibCqb5hxWXJJIQCSGE0AmNRsP333+Ph4cHa9euZcqUKdy9excLCwtsbW0ZP3489vb2WnUMDQ35/vvv+eCDD4iKiuKnn34iOTkZAwMDatSoQYcOHejUqVO+lej+rnr16syaNYsZM2YwdOhQjI2NsbS0JCwsjE2bNvHrr7+Sk5ODnp4ew4cP55133mHVqlWsXr2aOnXqMG3aNIKCgoo8R9u2bcnJyWHJkiUMHTqUrKwsrKysmD59Oq1btwZylxbv0KEDW7duZffu3ezbt485c+YwadIkvvvuO7Kzs6latSrdunVDX1+fH374gatXr1KzZk18fHzYvHkzU6ZM4cCBA8ydO5fg4GAqVarEokWL2LBhA0ZGRlhbWxMREYGHh8fLvF1CiGLou/3fMePQDNZ2XotLdRddh6MzGqWoGZpClZiYiLe3NzExMerEXiGEeBZnzpzR6uUQQrwc+ZsS4uVtvbgV3xW+KCgY6hky8/2ZhDQNKbYLLxT1XV7mEAkhhBBCCFGCJNxLoNvabijk9otk5mQycudIbqXf0nFkuiEJkRBCCCGEECVIFfMqfNL0E/W1nkaPyIBILEpb6DAq3ZGESAghhBBCiBJEX0+fSa0msebDNZQ2LM133t/Ruk5rXYelM7KoghBCCCGEECVQQIMAHKs4UqNMDV2HolPSQySEEEIIIUQJVbNszUIXUshRcph2cBoPMx/+y1H9uyQhEkIIIYQQQuQzbs84Bm8bjOtCV+Lvxus6nH+MJERCCCGEEEIILZvObeKbPd8AcOzGMZrMbcL+a/t1HNU/QxIiIYQQQgghhEpRFL7d/63WNiN9I2qXq62jiP5ZkhAJIYQQQgghVBqNhm09ttGxfkfg/y/L3SmSKuZVdBzZP0NWmRNCCCGEEEJoecv4LdZ2XsuU2CloNBo8annoOqR/jCREQgghhI4pilLoKk//ZF0hhCiKRqNhmOuwIsukZ6ZTyqDUG/3/kAyZE0II8UqEhoZibW2t9WjQoAFOTk5069aN6OjoAusdOnSIIUOG4OnpiZ2dHR4eHvTv358jR44Ueq5ffvmF//znP2odJycnevToQWRkJJmZmf/UJaoSExOxtrZm3rx5L3Wc+/fvM2bMGLZt2/bcdS9dukSvXr24devWS8WQJy0tjR9++AEfHx8aNWrEe++9x6RJk0hNTX0lxxdCFD/ZOdn4rfKjW1Q30jLSdB3OC5MeIiGEEK+MoaEhixcvVl8rikJKSgqLFi1i8ODBZGRk4O/vr+6fNGkSixcvxsXFhYEDB1KxYkWSk5OJjIykR48ejB49mh49eqjlMzIyGDJkCNu3b8fb25tBgwZRsWJF0tLS2L17N+PHj2fr1q3Mnz8fA4PX/yPuzJkz/PzzzzRv3vy560ZHRxMXF/dK4sjOzqZ///6cOnWKfv36YWNjw9mzZ5kzZw4HDhxgzZo1GBkZvZJzCSGKj692fcXOKzsBOHHjBGs7r8XGwkbHUT2/1//TQgghxBtDo9Hg6OiYb7urqyvu7u7Mnz9fTYhWrlzJ4sWL+fTTT/n888+1yrdr145+/foxceJEmjdvTt26dQGYMmUK27dvZ9y4cXTp0kWrTuvWrXF1dWXw4MGsWLGC3r17/yPXWBzFxcVx8OBBvv/+ezp06ACAm5sbFhYWDB8+nJ07d9KmTRsdRymEeJ1sPr9ZayW6M7fOMGzHMDZ326zDqF6MDJkTQghd0mi0H4WZN0+7XEhI4WWbNtUue/RoweWOHtUu17Tpy11LEczMzKhduzbx8fFAbo/EnDlzqFWrFgMGDMhX3sDAgNDQULp168ajR48ASE5OZvny5bi7u+dLhvL4+vri7++PoaGhuq1nz57069ePcePG4ejoiJubGykpKWRmZjJnzhzat29Po0aNsLOzw9fXV6uHCyArK4tZs2bh5eWFvb09gYGBXL58+Zmue/Xq1fj5+dGoUSMcHR0JDg7m+PHjAERFRdGrVy8AvvjiC7y8vNR60dHR9OjRA0dHR2xtbfHw8GDcuHGkpeUOSQkNDWXWrFlAbuISGhqq1t22bRsffvgh9vb2NGvWjMGDB/PHH38UGaeenh4ffPABLVq00NpubW0NwI0bN57peoUQJYdVeSts37FVX1cyq8T89vN1GNGLkx4iIYQQ/7iMjAwSEhKoUiV3ydbTp0+TnJxMnz590NfXL7BO3bp1GTNmjPp6x44dZGdn07FjxyLPFRYWlm/b/v37cXBw4Mcff+T27du8/fbbjBgxgq1btzJw4EBsbGxITU1l5cqVTJo0iZo1a+Lp6QnAmDFj2LBhA3369MHJyYmjR48ycODAp15zdHQ0X331FcHBwbi6upKamkp4eDgfffQRv/zyC+7u7owePZoJEyYwYMAA3N3dAVi7di0jR44kMDCQkJAQFEVh165drFixAmNjY0aMGEG/fv3QaDRERUURHh5OnTp1AFi+fDnjx4/Hx8eHfv36cffuXWbPnk2XLl2IiorinXfeKTDW5s2bFzhsL29uk43NmzcERgjxz7Iqb8XB4IN8uuVTVv62kp87/Uwls0q6DuuFSEIkhBDilcrKylKfZ2RkcO3aNcLDw7lz547aG3T9+nUAatSo8czHzetdyvvyn0dRFLKzs/OVf3IOUWZmJmFhYWpClpGRwa1btxgyZIjaSwPg5OTEu+++S2xsLJ6ensTHxxMVFUVwcDBDhw4FwN3dHSMjI2bOnFlkvHFxcZiamvL555+r82/q1avHmjVrSE1NpVatWlhZWQFgaWmJnZ0dABcuXMDf359vvvlGPZa7uztHjhwhNjYWgFq1aqnXYm9vj4WFBWlpaUydOpWWLVsyY8YMta6bmxs+Pj7MmTOHsWPHFhnzkw4cOMD8+fNxcXHB2dn5mesJIUqO0kalWeK/hKEuQ7GvaK/rcF6YJERCCCFemYyMDBo2bJhve/ny5Rk2bBjdu3cHUHuFCkpkCqMoSoHb9+3bx8cff5xv+7lz59Tn5ubmagIBYGRkxIIFC4Dcld6uXr1KQkICv/32m3odgLpowd/nz3To0OGpCVGLFi2IjIykXbt2+Pj44OLiQpMmTRgxYkSR9fKGvz1+/FiN69y5c6SkpFCmTJlC6x0/fpwHDx7g4+OjlZSWL1+epk2bsnfv3iLP+6Tt27czbNgwatasyQ8//PDM9YQQJY9GoykyGbqeep24xDg62hTdu69LkhAJIYQuFfIlP5+QkKLnDT2psDlDf9e06bOf/xkZGhqyatUq9bWBgQFlypTRSkYAqlWrBkBCQkKRx7t27Zrai5RX59q1a2rPCkCTJk1Ys2aN+nrevHls375d6zgVKlTId+wjR44wefJkjh8/jqGhIbVr18bBwQH4K/m6c+cOkJtUPKmwoWdPatWqFXPnzmXFihUsXbqUefPmYWpqSrt27Rg5ciSlSpUqsF5SUhITJ04kJiaGnJwcqlWrhq2tLSYmJoUmhQApKSkAjBo1ilGjRuXb/+S8qsIoisLMmTMJDw+ncePGhIeH8/bbbz+1nhBCFCQrJ4vRO0cz1WeqrkMpkiREQgghXhmNRqMO/SpK/fr1qVSpErt372b48OEFziO6dOkSvr6+dOvWjbFjx9K6dWvCwsLYuHEjrVq1UsuZmZlpnbNcuXJPPX9CQgJ9+/alSZMmbN68mdq1a6Ovr096ejqRkZFqubxE6ObNm1StWlXdnpd8PI2HhwceHh5kZGRw4sQJNm3axOrVqzE3N2f48OH5yiuKQnBwMOnp6SxatAh7e3tMTEwACAgI4MGDB4WeK6/3aNSoUWpi9zwyMjIYOnQo27Zto23btkyaNAljY+PnPo4QQuT5dt+39HfqT1mTsroOpUiyypwQQoh/nUajISQkhPj4eMLDw/Ptz8rKYuLEiejp6dG5c2cAqlatSmBgINu2bdNKWp708OFDrly58tTznzp1iocPHxIUFES9evXUhGznztz7aeT1xLi4uKCnp8eGDRu06v+9B6ogX375JZ06dUJRFIyMjHBycmLcuHGYm5uTmJgIkC8RvHPnDhcuXMDX15dmzZqpyVBiYiLnzp3T6iHS09P+CHdwcMDExISrV69iZ2enPmxsbIiIiGDLli2FxqooCoMHD2bbtm0MGDCAqVOnSjIkhHgpqY9TsSxrSdMq/9wKpq+K9BAJIYTQiW7dunHixAlmzZrFiRMn8PPzw8LCgoSEBFauXMm5c+cYP3681gpnI0eO5MGDB4wdO5atW7fy/vvvU716ddLT0/n1119Zv349KSkpBAQEFHluW1tbDA0NmTFjBhkZGRgbGxMXF8eSJUvQaDSkp6cDuUlYUFAQixYtQl9fnxYtWnD69GmWLFny1Otzc3MjKiqKQYMG0bFjR/T09Ni8eTOpqan4+fkB8NZbbwGwd+9eKlWqRNOmTalZsyZRUVFYWlpSpUoVzp8/z/z588nKylLjgr96hLZs2YKrqyv16tVj4MCBTJ48mYyMDLy8vMjOzmbZsmXExcUxdWrhQ1bWrFnDL7/8gpubG82bN+fIkSNa+6tVq0alSm/m6lFCCN0wNzanZ6Oeug7jmUhCJIQQQic0Gg3ff/89Hh4erF27lilTpnD37l0sLCywtbVl/Pjx2NtrT9Q1NDTk+++/54MPPiAqKoqffvqJ5ORkDAwMqFGjBh06dKBTp075VqL7u+rVqzNr1ixmzJjB0KFDMTY2xtLSkrCwMDZt2sSvv/5KTk4Oenp6DB8+nHfeeYdVq1axevVq6tSpw7Rp0wgKCiryHG3btiUnJ4clS5YwdOhQsrKysLKyYvr06bRu3RrIXVq8Q4cObN26ld27d7Nv3z7mzJnDpEmT+O6778jOzqZq1ap069YNfX19fvjhB65evUrNmjXx8fFh8+bNTJkyhQMHDjB37lyCg4OpVKkSixYtYsOGDRgZGWFtbU1ERAQeHh6Fxvq///0PyF2efP/+/fn2DxkyhJBnncMmhBBvGI1S1AxNoUpMTMTb25uYmBh1Yq8QQjyLM2fOyH1chHiF5G9KCPG8ivouL3OIhBBCCCGEECWWJERCCCGEEEKIEksSIiGEEEIIIUSJJQmREEIIIYQQosSShEgIIYQQQghRYklCJIQQ/wJZ0FOIV0P+loQQr5okREII8Q/T19cnMzNT12EIUSxkZmair6+v6zCEEMWIJERCCPEPMzc35/79+7oOQ4hi4f79+5ibm+s6DCFEMSIJkRBC/MPefvtt7ty5w61bt8jIyJAhP0I8J0VRyMjI4NatW9y5c4e3335b1yEJIYoRA10HIIQQxZ2xsTE1atQgJSWF+Ph4srOzdR2SEG8cfX19zM3NqVGjBsbGxroORwhRjEhCJIQQ/wJjY2MqV65M5cqVdR2KEEIIIZ4gQ+aEEEIIIYQQJZYkREIIIYQQQogSSxIiIYQQQgghRIklCZEQQgghhBCixJKESAghhBBCCFFiySpzzyhvmdwbN27oOBIhhBBCCCHE88j7Dl/QrS8kIXpGN2/eBKB79+46jkQIIYQQQgjxIm7evEnNmjW1tmkUuWX6M3n06BGnTp3CwsICfX19XYcjhBBCCCGEeEbZ2dncvHkTW1tbTExMtPZJQiSEEEIIIYQosWRRBSGEEEIIIUSJJQmREEIIIYQQosSShEgIIYQQQghRYklCJIQQQgghhCixJCESQgghhBBClFiSEAkhhBBCCCFKLEmIhBBCCCGEECWWJERCCCGEEEKIEksSojfIsWPH6NmzJ02aNMHZ2Zlhw4Zx8+ZNXYelU1lZWQQGBvLVV1/l27d+/Xr8/Pywt7fHw8ODadOmkZGRoVUmIyODH3/8EU9PT+zs7PDz82PDhg35jpWUlMSQIUN49913ady4Mb169eLkyZP5yr3J79H27dsJDAzE0dERFxcX+vbty2+//aZVRtr0+a1fv54OHTrQqFEjWrZsycSJE0lNTc1XRtr1xaxatQpra2vi4uK0tkubvhg/Pz+sra3zPX788Ue1jLTt87l48SL9+/fHycmJJk2a0LVrVw4cOKBVRtr02SUmJhb4b/TJRx5p1+cXGxtL165dadq0KW5ubgwZMoQ///xTq0yxbFdFvBF+//13pVGjRkqPHj2U7du3K6tWrVKcnZ0VX19f5fHjx7oOTycePHigfPbZZ4qVlZUyZswYrX2rV69WrKyslNGjRyu7du1Spk2bptjY2CgjR47UKvfFF18otra2yty5c5WdO3cqgwcPVqysrJT169drnee9995T3N3dlaioKCU6Olrp2LGj0rhxY+XixYtquTf5PVq7dq1iZWWlfPHFF0pMTIyyZcsWJSAgQGnYsKFy+PBhRVGkTV/EsmXLFCsrK+Wrr75S9uzZo6xYsUJp1qyZ0qVLFyUnJ0dRFGnXl3Hx4kWlUaNGipWVlXLw4EF1u7Tpi3n06JHSoEED5ZtvvlEOHz6s9fjjjz8URZG2fV6XLl1SmjZtqvj7+yvR0dHKjh07lO7duysNGzZUjh07piiKtOnzevz4cb5/n4cPH1YWLlyoWFlZKd98842iKNKuL+LQoUOKjY2N0qNHDyUmJkZZt26d4unpqXh7eyv3799XFKX4tqskRG+ITz75RGnZsqXy6NEjdduJEycUKysrZeXKlTqMTDd2796t+Pj4KM2aNcuXED169EhxdnZWBgwYoFXnp59+UqytrdU/st9++02xsrJSli1bplUuJCREcXNzU7KzsxVFUZQFCxYoVlZWyrlz59Qyqampiqurq/L555+r297k96hly5ZK7969tbalpqYqzZo1U0JCQqRNX0BmZqbSvHlzpV+/flrbf/75Z8XKyko5dOiQtOtLePz4seLv7694eHhoJUTSpi/u+PHjipWVlbJ3794C90vbPr+QkBDF09NTSUtLU7c9ePBAadWqlTJ16lRp01fkzp07iru7u9KtWzclMzNT2vUFDRkyRHFwcFAePHigbouNjVWsrKyUdevWFet2lSFzb4CMjAxiY2Px9vbG2NhY3W5vb0+tWrWIiYnRYXT/vvv37/PJJ59gbW3Nxo0b8+0/efIkd+7cwdfXV2u7n58fiqKo7bV7926AfOXatWtHcnKyOlxs9+7d1KtXDysrK7WMmZkZnp6e7N69m5ycnDf6PXrw4AFeXl5069ZNa7uZmRmVK1fmxo0b0qYvQE9Pj8WLFzNy5Eit7XnX8vjxY2nXl/Djjz+Snp7OZ599prVd2vTFnT59GoCGDRsWuF/a9vncv3+fffv2ERgYSOnSpdXtpqam7Nixg8GDB0ubviJTpkwhJSWFiRMnYmBgIO36gh4/foyhoSGlSpVSt5UvXx6Au3fvFut2lYToDZCQkMDjx4+pU6dOvn2WlpZcvHhRB1HpjomJCVu2bGH69OlUrFgx3/689vh7e1WsWBFTU1N1/6VLlyhXrhxvv/22VjlLS0sALly4oJarXbt2vvNYWlqSnp7OH3/88Ua/R6VLl2bs2LG89957WtuvXLnChQsXsLGxkTZ9AXp6elhbW1O9enUA0tLS2L17N1OmTKF+/fo4OztLu76g2NhYli1bxuTJkzE1NdXaJ2364k6fPo2ZmRk//PADLi4uNGzYkICAAPbs2QNI2z6vs2fPkp2dTbVq1ZgwYYLapp07d+bIkSOAtOmrcOrUKdasWUNISAi1atUCpF1fVI8ePUhPTycsLIyUlBQSEhL49ttvKVOmDD4+PsW6XQ1eyVHEPypvAraZmVm+faVLl843Qbu4MzIyKvAPI09ee5ibm+fbZ2Zmpu6/f/9+oWUg9wvss5RLTU1VJxMWl/coLS2NIUOGYGhoSEhICL/88gsgbfqiEhISaNWqFQBly5bl66+/xtDQUP6tvoA7d+4wfPhw+vXrh729PQkJCVr7pU1f3O+//05aWhqmpqZMnz6du3fvsmjRIj755BOmTZsmbfucbt26BcCkSZOoX78+kyZNIjMzk4iICIKCgli2bJm06SsQERFBmTJlCAoKUrdJu74YZ2dnPv/8c6ZMmcKiRYuA3DacN28elStXLtbtKgnRGyAnJwcAjUZT4H49Penoe9KztpeiKIWWebL+08rp6ekVq/coKSmJfv36ceHCBaZPn07t2rWlTV+Subk5ixcvJj09nSVLltCzZ0+mTZsm7foCRo0aRbVq1fj0008L3C9t+uK+/vprMjMzcXR0VLe5u7vj5+fHlClT6NSpEyBt+6wyMzMBqFChAhEREejr6wPQrFkz3nvvPWbMmIGzszMgbfqiEhISiImJ4bPPPtP6wiz/D7yY8ePHs3z5crp06UKbNm14+PAhK1euJDg4mNmzZxfrdn393x1BmTJlAArMgh88eFBgdl2SvfXWW0DB7ZWWlqa211tvvVVoGeCZy5mZmRWb9+jXX38lICCAq1evEh4ejre3NyBt+rLKli3Lu+++i7e3N/Pnz6dKlSrMmDFD2vU5RUZGcuDAAb799lsURSErK0v9sMzJySErK0va9CU0atRIKxmC3B75Fi1akJCQoM4rkLZ9Nnlf0D08PNRkCHKvu0mTJpw6dUr+vb6k6OhoFEWhY8eOWtulXZ9fUlISK1aswN/fn3HjxuHi4oK3tzfz5s2jTp06jB49uli3qyREb4Dq1atjaGhIfHx8vn1Xrlyhbt26/35Qr7G84XR/b6+kpCTS09PV9qpTpw4pKSncu3dPq9yVK1cAtMoV1valS5emcuXKxeI92rx5M71798bQ0JCVK1fSsmVLdZ+06fO7d+8eGzduVK89j5GREdbW1vz555/Srs9py5YtpKen8/7779OwYUMaNmzI0KFDAQgKCqJhw4bSpi8oLS2Nn3/+mWPHjuXb9+jRI4yNjdWJz9K2zyZvvsTjx4/z7cvIyMDExET+vb6kHTt24ODgQLVq1bS2S7s+v+vXr6MoCk5OTlrb9fX1cXR05I8//uCdd94Bime7SkL0BjAyMsLFxYWYmBgePXqkbj958iTx8fG4u7vrMLrXj4ODA2XKlGHz5s1a2zdt2oRGo1G/6Oe125YtW/KVs7CwoEGDBmq5c+fOqZMAIffLw86dO3Fzc0NfX/+Nf4+2bdvGsGHDqF+/PmvWrKF+/fpa+6VNn5+iKIwYMYJZs2Zpbb937x6//vorNjY20q7P6ZtvvmHNmjVaj8GDB2vtkzZ9McbGxoSFhTF58mQURVG337t3j127duHs7EzTpk2lbZ9D7dq1qVWrFlu3btWK/datW/z66684OzvLv9eX8OjRI86cOZPvCzzIZ9aLqFWrFgYGBvlucp2dnc2RI0coX748LVu2LL7t+koW7xb/uJMnTyoNGzZUunTpomzdulVZvXq1elOqJ9dlL4kKujHr4sWLFSsrK2XEiBHKrl27lOnTpys2NjbKl19+qVWuf//+SsOGDZXZs2dr3Ths3bp1apnU1FTFw8NDcXV1Vf773/8q//vf/9Qbh124cEEt96a+R3fu3FGaNm2qODg4KDt37sx3s7tTp04piiJt+iImT56sWFlZKWPHjlX27t2rrFu3TmnXrp3SuHFjaddXZPPmzfluzCpt+mIWLVqkWFlZKQMGDFB27dqlrF+/XvH19VUcHBzUa5W2fT579uxRGjRooHTp0kXZvn27smXLFqVt27aKg4ODEh8fryiKtOmLyrsPzaZNmwrcL+36/KZNm6ZYWVkpoaGhyu7du5Vt27Ypffr0UaysrJSoqChFUYpvu0pC9AY5ePCg0qVLF8XOzk5p3ry5MmzYMCU5OVnXYelcQQmRoijKihUrFB8fH6Vhw4aKp6enMn36dCUzM1OrzMOHD5WJEycqrq6uip2dneLn51fgf64JCQnKgAEDlCZNmigODg5K7969lRMnTuQr9ya+Rxs3blSsrKwKffj4+KhlpU2fT3Z2trJ69WqlQ4cOSqNGjZRmzZopAwYM0LoDt6JIu76MghIiRZE2fVEbN25UPvjgA6Vx48ZK06ZNlU8//VTrpomKIm37vA4fPqz07t1bady4sdKkSRMlJCREOX/+vFYZadPnFxMTo1hZWSl79uwptIy06/Nbs2aN4u/vr9ja2irOzs5K7969lbi4OK0yxbFdNYryRN+4EEIIIYQQQpQgModICCGEEEIIUWJJQiSEEEIIIYQosSQhEkIIIYQQQpRYkhAJIYQQQgghSixJiIQQQgghhBAlliREQgghhBBCiBJLEiIhhBAFCg0Nxdra+pkePXv2BMDLywtra2scHR11HH3R/ve//2Ftbc2wYcMASExMVK/ls88+03F0r1ZCQgL169enQ4cOZGdn6zocIYR47UhCJIQQokRJT0/nu+++A6Br1646juafV716dVxdXTl79iyrVq3SdThCCPHaMdB1AEIIIV5PPXv2pFWrVurrgwcPsmzZMgB8fX1p27atuq9s2bIAjBs3jkePHmFg8Pp+vERGRnLjxg2srKxo0qSJrsP5VwQGBrJ//35mzpxJp06dMDEx0XVIQgjx2nh9P7GEEELoVMOGDWnYsKH6+v79++rz2rVrayVLedzc3P6V2F5UVlaWmtS9//77Oo7m3+Pu7o6pqSl3795l8+bNdOrUSdchCSHEa0OGzAkhhHhlCppDFBUVpc7POX78OJGRkbRp0wY7Ozs6dOjAnj17ANi+fTsdOnTAzs6O1q1bs3jx4nzHz8jIYO7cubRp0wZbW1veffddBg8ezKVLl54pvoMHD3L9+nU11sJcunSJkJAQHBwccHZ25ssvv+T27dv5yj18+JCIiAj8/PxwcHDA0dGRXr16sXXr1nxle/bsqbbDk8nlmTNn1O2hoaHq9rw5XA0aNODs2bP4+flha2uLj48PDx8+5MGDB/z444+8//772NvbY2tri6enJ6NGjSIpKUnr3EZGRri4uACwevXqZ2orIYQoKaSHSAghxL/mxx9/5ODBg+rrs2fP0r9/f7p168aSJUvU7deuXWPSpEkYGhrSvXt3ILd3p1+/fvzf//2fWi4lJYXo6Gh2797NwoULcXBwKPL8+/fvB3KH+NWvX7/AMlevXiUwMFBNWtLT04mKiuLq1ausXLlSLXf79m2CgoI4f/68Vv24uDji4uLo2rUrX3/99TO0StEUReHjjz8mOTkZgHfeeQcTExN69uzJ4cOHtcpev36dNWvWcOTIEVavXq0OZQRwdnbml19+4dSpU9y7d48yZcq8dGxCCFEcSA+REEKIf83Bgwf54IMPCAsLo1GjRgBkZmayZMkS7O3tCQsLw9/fXy2/bt069fmyZcvUZMjLy4vvv/+e0NBQKlSoQHp6OqGhoeTk5BR5/tjYWADq1atXaJmLFy9StWpVvv32W0aMGIGhoSEAR48e5dy5c2q5MWPGqMlQ48aNmTBhAsOGDaNcuXIArFq1ijVr1jxr0xQqJyeHe/fuMWLECEaPHk337t05ceKEmgy5uroSFhbGlClTcHd3ByA+Pp7IyEit4+Rdc05OTr5ESgghSjLpIRJCCPGvad68OZMmTQKgRo0a6ipvpqamzJ8/nzJlytCuXTv+97//8fjxY27cuKHW/fnnnwGoVasWs2fPRk8v9ze96tWr079/f+Lj4zl06BDNmzcv9PwJCQnquQtTqlQpFi1apCY2iYmJrFixAoDLly9jbW3NlStXiImJAaBu3bosX75cTZzc3Nz44IMPyM7OZt68ea9kvk737t3p06eP+jovsQOwt7enTZs2mJiY4OPjw4YNG6hdu3a+pK9WrVrq88uXL790TEIIUVxIQiSEEOJf07RpU/V5hQoV1OfW1tbqEC4DAwPKlSvHjRs3ePz4MQAPHjxQv8THx8djY2NT4PFPnDhRaEL06NEj0tPTATA3Ny80RhsbGzUZAqhSpYr6PC+eQ4cOqdsCAgLUZAigfv36NG7cmKNHj3L16lVu3ryJhYVFoed7Fk+2G0CTJk2oXLkyf/75J3PmzGHhwoU0btwYR0dHWrZsSePGjfMdw8zMTH2ekpLyUvEIIURxIkPmhBBC/Gue/FKe18MD5JvPoq+vD+TOnwFITU19puPnzbMpyL1799TnpUqVKrTc32N5cgnxvCF5d+/eVbdVrFgx3zGe3PbkefPkXRfkzo16micTNAATExOWLVuGt7c3BgYGPH78mLi4OGbPnk2XLl3w9/fnypUrWnWevOYHDx489ZxCCFFSSA+REEKIf82TSdCT8hKgwpQuXVp9bmVlxaBBgwos92Rvzt892SuUkZHx3DE+6e2331af/31Ft79ve7JsnieToGdJTgq6b1D16tUJDw/n7t27xMbGcvToUQ4ePMjFixc5c+YMw4YN05rDlNc7BvDWW2899ZxCCFFSSA+REEKI1565uTnVqlUDcnuBmjVrRqtWrWjVqhXZ2dnExsaSnJycryflSaamppiamgLaPTwv4snV7NatW6eV4Jw/f57jx48DUKdOHTUhyjt33jXk+f333596vr8nabGxsUyYMIGgoCCuX7+Or68vY8aMYcuWLdStWxeAU6dOaSV+T/ayPTlcUQghSjrpIRJCCPFG+OCDD5gxYwZ3796ld+/edOvWjUePHjFz5kzu3buHoaFhkfcWgty5SseOHVMXV3hRdevWxdnZmbi4OM6fP0+PHj0ICAjg/v37zJ8/n+zsbAD69eun1qlevbr6/LvvvuOLL77g4sWLzJo167nPn5aWpt5gdsCAAXz00Ue8/fbb/Pbbb+pQuYoVK2JkZKTWuXr1qvq8QYMGz31OIYQoriQhEkII8UYIDg5m3759HDt2jN9//53Ro0dr7Q8NDaVSpUpFHsPR0ZFjx45x5swZsrOznzpUryjff/89vXv3Jj4+nmPHjnHs2DGt/b1796Z9+/bqa39/f1auXEl2djYHDx6kc+fOAHh6erJ37141iXoWrVu3pn379mzcuJHExETGjx+vtV9PT4+RI0dqbTt79iyQOzzR1tb2ua5VCCGKMxkyJ4QQ4o1gYmLC4sWLGTRoEPXq1cPExISyZcvSrFkz5s6dS48ePZ56jJYtWwK5w8eeZahaUSpVqsT69ev54osvqF+/PqVKlcLMzAxnZ2dmz56dLyGxtbVlzpw5NGzYEGNjY6pWrcqgQYOYOXPmc59bo9Hw3XffERYWRtOmTalQoQIGBgZUqFCBVq1asXz5cnx8fLTqnDp1Csi9b9GTc7KEEKKk0yhPLnUjhBBCFHM+Pj7Ex8czbNgw+vbtq+tw/hU5OTm4ublx+/Ztpk6dStu2bXUdkhBCvDakh0gIIUSJkncz2J07d+o4kn/P4cOHuX37NuXLl8fb21vX4QghxGtFEiIhhBAlSteuXalSpQpHjx7l0qVLug7nX/Hzzz8DuQswFLSEtxBClGSSEAkhhChRjI2NGTJkCABLly7VcTT/vKSkJLZt20adOnXUhRyEEEL8ReYQCSGEEEIIIUos6SESQgghhBBClFiSEAkhhBBCCCFKLEmIhBBCCCGEECWWJERCCCGEEEKIEksSIiGEEEIIIUSJ9f8AJWw66jVLKxgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred[:, 0], label = 'No PCGrad state 0', linewidth = 4, color = 'green', \n",
    "             linestyle='--', ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred[:, 1], label = 'No PCGrad state 1', linewidth = 4, color = 'blue', \n",
    "             linestyle='--', ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred[:, 2], label = 'No PCGrad state 2', linewidth = 4, color = 'red', \n",
    "             linestyle='--', ax=ax)\n",
    "\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['state_0'], label = 'Solver state 0', color = 'green',\n",
    "             alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['state_1'], label = 'Solver state 1', color = 'blue', \n",
    "             alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = matlab_solver_solution_df['state_2'], label = 'Solver state 2', color = 'red', \n",
    "             alpha=0.5, dashes=True, linewidth = 4, ax=ax)\n",
    "\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad[:, 0], label = 'PCGrad state 0', color = 'green', linestyle='dotted',\n",
    "             linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad[:, 1], label = 'PCGrad state 1', color = 'blue', linestyle='dotted',\n",
    "             linewidth = 4, ax=ax)\n",
    "sns.lineplot(x = x_star.flatten(), y = y_pred_pcgrad[:, 2], label = 'PCGrad state 2', color = 'red', linestyle='dotted',\n",
    "             linewidth = 4, ax=ax)\n",
    "\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Time (hours)\", fontdict=dict(weight='bold'), fontsize=20)\n",
    "ax.set_ylabel(\"Probability\", fontdict=dict(weight='bold'), fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_small_homo.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.996152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.996154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.996157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.996159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.996162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_0   state_1   state_2\n",
       "4995  0.000576  0.003273  0.996152\n",
       "4996  0.000575  0.003271  0.996154\n",
       "4997  0.000574  0.003269  0.996157\n",
       "4998  0.000574  0.003267  0.996159\n",
       "4999  0.000573  0.003265  0.996162"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>8.955214e-09</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.998790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>8.916524e-09</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.998790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>8.878033e-09</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.998790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>8.839742e-09</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.998790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>8.801599e-09</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.998789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           state_0   state_1   state_2\n",
       "4995  8.955214e-09  0.001210  0.998790\n",
       "4996  8.916524e-09  0.001210  0.998790\n",
       "4997  8.878033e-09  0.001210  0.998790\n",
       "4998  8.839742e-09  0.001210  0.998790\n",
       "4999  8.801599e-09  0.001211  0.998789"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_pcgrad_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999979</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999970</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999958</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.997073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.997077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.997082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.997086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.997091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_0   state_1   state_2\n",
       "0     0.999989  0.000005  0.000006\n",
       "1     0.999985  0.000007  0.000008\n",
       "2     0.999979  0.000010  0.000012\n",
       "3     0.999970  0.000014  0.000016\n",
       "4     0.999958  0.000019  0.000023\n",
       "...        ...       ...       ...\n",
       "3745  0.000005  0.002922  0.997073\n",
       "3746  0.000005  0.002917  0.997077\n",
       "3747  0.000005  0.002913  0.997082\n",
       "3748  0.000005  0.002909  0.997086\n",
       "3749  0.000005  0.002904  0.997091\n",
       "\n",
       "[3750 rows x 3 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_pcgrad_df.loc[:3749]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff = matlab_solver_solution_df - y_pred_df\n",
    "y_diff_pcgrad = matlab_solver_solution_df - y_pred_pcgrad_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_0</th>\n",
       "      <th>state_1</th>\n",
       "      <th>state_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002045</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.004094</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.002313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.006134</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>0.003472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.008183</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.004630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.010213</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>0.005788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.002697</td>\n",
       "      <td>0.003238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.000541</td>\n",
       "      <td>-0.002696</td>\n",
       "      <td>0.003236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.002696</td>\n",
       "      <td>0.003233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>-0.000539</td>\n",
       "      <td>-0.002695</td>\n",
       "      <td>0.003231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-0.000539</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>0.003228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state_0   state_1   state_2\n",
       "0    -0.002045  0.000886  0.001153\n",
       "1    -0.004094  0.001777  0.002313\n",
       "2    -0.006134  0.002665  0.003472\n",
       "3    -0.008183  0.003549  0.004630\n",
       "4    -0.010213  0.004430  0.005788\n",
       "...        ...       ...       ...\n",
       "4995 -0.000541 -0.002697  0.003238\n",
       "4996 -0.000541 -0.002696  0.003236\n",
       "4997 -0.000540 -0.002696  0.003233\n",
       "4998 -0.000539 -0.002695  0.003231\n",
       "4999 -0.000539 -0.002694  0.003228\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate histogram of prediction difference with respect to each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "sns.histplot(np.abs(y_diff['state_0']), bins=20, ax=axes[0], alpha=0.4, color='blue', label='No PCGrad')\n",
    "sns.histplot(np.abs(y_diff_pcgrad['state_0']), bins=20, ax=axes[0], alpha=1, color='red', label='PCGrad')\n",
    "#plt.axvline(x=np.mean(np.abs(y_diff['state_0'])), linewidth=2, linestyle=\"--\", alpha=0.6, \n",
    "#            color='blue', label='Mean of no PCGrad')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('State 0', fontdict=dict(weight='bold'), fontsize=16)\n",
    "axes[0].set_ylabel('Frequency', fontdict=dict(weight='bold'), fontsize=16)\n",
    "\n",
    "\n",
    "sns.histplot(np.abs(y_diff['state_1']), bins=20, ax=axes[1], alpha=0.4, color='blue', label='No PCGrad')\n",
    "sns.histplot(np.abs(y_diff_pcgrad['state_1']), bins=20, ax=axes[1], color='red', label='PCGrad')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('State 1', fontdict=dict(weight='bold'), fontsize=16)\n",
    "axes[1].set_ylabel('Frequency', fontdict=dict(weight='bold'), fontsize=16)\n",
    "\n",
    "sns.histplot(np.abs(y_diff['state_2']), bins=20, ax=axes[2], alpha=0.4, color='blue', label='No PCGrad')\n",
    "sns.histplot(np.abs(y_diff_pcgrad['state_2']), bins=20, ax=axes[2], color='red', label='PCGrad')\n",
    "axes[2].legend()\n",
    "axes[2].set_xlabel('State 2', fontdict=dict(weight='bold'), fontsize=16)\n",
    "axes[2].set_ylabel('Frequency', fontdict=dict(weight='bold'), fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.savefig(\"histogram_small_homo.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff['mse'] = y_diff.apply(lambda x: x['state_0']**2 + x['state_1']**2 + x['state_2']**2, axis = 1)\n",
    "y_diff['rmse'] = y_diff['mse'].apply(np.sqrt)\n",
    "\n",
    "y_diff_pcgrad['mse'] = y_diff_pcgrad.apply(lambda x: x['state_0']**2 + x['state_1']**2 + x['state_2']**2, axis = 1)\n",
    "y_diff_pcgrad['rmse'] = y_diff_pcgrad['mse'].apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of PINN without PCGrad is 0.11113926525797774\n",
      "RMSE of PINN with PCGrad is 0.008277526607726451\n"
     ]
    }
   ],
   "source": [
    "print ('RMSE of PINN without PCGrad is', np.mean(y_diff['rmse']))\n",
    "print ('RMSE of PINN with PCGrad is', np.mean(y_diff_pcgrad['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of PINN without PCGrad before 60000 is 0.14614631730910296\n",
      "RMSE of PINN with PCGrad before 60000 is 0.010807554478231961\n",
      "RMSE of PINN without PCGrad after 60000 is 0.0062300421247489576\n",
      "RMSE of PINN with PCGrad after 60000 is 0.0006955326137016304\n"
     ]
    }
   ],
   "source": [
    "print ('RMSE of PINN without PCGrad before 60000 is', np.mean(y_diff['rmse'][:3749]))\n",
    "print ('RMSE of PINN with PCGrad before 60000 is', np.mean(y_diff_pcgrad['rmse'][:3749]))\n",
    "\n",
    "print ('RMSE of PINN without PCGrad after 60000 is', np.mean(y_diff['rmse'][3749:]))\n",
    "print ('RMSE of PINN with PCGrad after 60000 is', np.mean(y_diff_pcgrad['rmse'][3749:]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "027e948c137d6acde9006b592cf7cd1a70a2e52a69a7472c7b58352c83d453a1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
